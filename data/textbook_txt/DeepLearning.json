{"Filename": "DeepLearning", "Pages": [{"Page_number": 1, "text": "deep learning\n\nian goodfellow\nyoshua bengio\naaron courville\n\n "}, {"Page_number": 2, "text": "contents\n\nwebsite\n\nacknowledgments\n\nnotation\n\n1 introduction\n\n1.1 who should read this book? .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\n1.2\nhistorical trends in deep learning .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n\ni applied math and machine learning basics\n\n2 linear algebra\n\n2.1\nscalars, vectors, matrices and tensors .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n2.2 multiplying matrices and vectors .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\nidentity and inverse matrices\n2.3\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\nlinear dependence and span .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\n2.4\n2.5\nnorms .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\nspecial kinds of matrices and vectors\n2.6\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\neigendecomposition .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n2.7\n2.8\nsingular value decomposition .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\nthe moore-penrose pseudoinverse .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\n2.9\n2.10 the trace operator\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n2.11 the determinant .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\n2.12 example: principal components analysis\n\n3 probability and information theory\n\n3.1 why probability? .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\n\ni\n\nvii\n\nviii\n\nxi\n\n1\n8\n11\n\n29\n\n31\n31\n34\n36\n37\n39\n40\n42\n44\n45\n46\n47\n48\n\n53\n54\n\n "}, {"Page_number": 3, "text": "contents\n\nrandom variables\n.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\n3.2\n3.3\nprobability distributions .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n3.4 marginal probability .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n3.5\nconditional probability .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\nthe chain rule of conditional probabilities .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\n3.6\nindependence and conditional independence .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\n3.7\n3.8\nexpectation, variance and covariance\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\n3.9\ncommon probability distributions\n3.10 useful properties of common functions\n.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\n3.11 bayes\u2019 rule .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\n3.12 technical details of continuous variables\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n3.13\ninformation theory .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n3.14 structured probabilistic models .\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n\n4 numerical computation\n\n4.1 overflow and underflow .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n4.2\npoor conditioning .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n4.3 gradient-based optimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\nconstrained optimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\n4.4\n4.5\nexample: linear least squares .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\n\n56\n56\n58\n59\n59\n60\n60\n62\n67\n70\n71\n72\n75\n\n80\n80\n82\n82\n93\n96\n\n5 machine learning basics\n\n98\n99\nlearning algorithms .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\n5.1\ncapacity, overfitting and underfitting .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 110\n5.2\nhyperparameters and validation sets . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 120\n5.3\nestimators, bias and variance .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 122\n5.4\n5.5 maximum likelihood estimation .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 131\nbayesian statistics\n5.6\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 135\n5.7\nsupervised learning algorithms .\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 139\nunsupervised learning algorithms\n5.8\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 145\n5.9\nstochastic gradient descent .\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 150\n5.10 building a machine learning algorithm .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 152\n5.11 challenges motivating deep learning .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 154\n\nii deep networks: modern practices\n\n165\n\n6 deep feedforward networks\n\n167\nexample: learning xor . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 170\n6.1\n6.2 gradient-based learning . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 176\n\nii\n\n "}, {"Page_number": 4, "text": "contents\n\n6.3\n6.4\n6.5\n6.6\n\nhidden units\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 190\narchitecture design .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 196\nback-propagation and other differentiation algorithms\n.\u00a0.\u00a0.\u00a0.\u00a0. 203\nhistorical notes .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 224\n\n7 regularization for deep learning\n\n228\nparameter norm penalties .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 230\n7.1\nnorm penalties as constrained optimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 237\n7.2\n7.3\nregularization and under-constrained problems\n.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 239\n7.4 dataset augmentation .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 240\nnoise robustness .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 242\n7.5\n7.6\nsemi-supervised learning .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 244\n7.7 multi-task learning .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 245\nearly stopping .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 246\n7.8\n7.9\nparameter tying and parameter sharing\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 251\n7.10 sparse representations .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 253\n7.11 bagging and other ensemble methods . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 255\n7.12 dropout .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 257\n7.13 adversarial training .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 267\n7.14 tangent distance, tangent prop, and manifold tangent classifier 268\n\n8 optimization for training deep models\n\n274\nhow learning differs from pure optimization .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 275\n8.1\nchallenges in neural network optimization .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 282\n8.2\nbasic algorithms .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 294\n8.3\nparameter initialization strategies\n. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 301\n8.4\nalgorithms with adaptive learning rates .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 306\n8.5\n8.6\napproximate second-order methods .\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 310\n8.7 optimization strategies and meta-algorithms .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 318\n\n9 convolutional networks\n\n331\n9.1\nthe convolution operation .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 332\n9.2 motivation .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 336\npooling .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 340\n9.3\nconvolution and pooling as an infinitely strong prior .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 346\n9.4\n9.5\nvariants of the basic convolution function .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 348\n9.6\nstructured outputs . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 359\n9.7 data types .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 361\n9.8\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 363\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 364\n9.9\n\nefficient convolution algorithms\nrandom or unsupervised features\n\niii\n\n "}, {"Page_number": 5, "text": "contents\n\n9.10 the neuroscientific basis for convolutional networks .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 365\n9.11 convolutional networks and the history of deep learning .\u00a0.\u00a0.\u00a0. 372\n\n10\u00a0sequence modeling: recurrent and recursive nets\n\n374\n10.1 unfolding computational graphs .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 376\n10.2 recurrent neural networks\n.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 379\n10.3 bidirectional rnns\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 396\n10.4 encoder-decoder sequence-to-sequence architectures .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 397\n10.5 deep recurrent networks\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 399\n10.6 recursive neural networks .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 401\n10.7 the challenge of long-term dependencies .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 403\n10.8 echo state networks .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 406\n10.9 leaky units and other strategies for multiple time scales .\u00a0.\u00a0. . 409\n10.10\u00a0the long short-term memory and other gated rnns .\u00a0. .\u00a0.\u00a0.\u00a0. 411\n10.11\u00a0optimization for long-term dependencies .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 415\n10.12\u00a0explicit memory .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 419\n\n11\u00a0practical methodology\n\n424\n11.1 performance metrics .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 425\n11.2 default baseline models .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 428\n11.3 determining whether to gather more data .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 429\n11.4 selecting hyperparameters .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 430\n11.5 debugging strategies .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 439\n11.6 example: multi-digit number recognition .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 443\n\n12\u00a0applications\n\n446\n12.1 large scale deep learning . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 446\n12.2 computer vision .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 455\n12.3 speech recognition\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 461\n12.4 natural language processing .\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 464\n12.5 other applications .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 480\n\niii deep learning research\n\n489\n\n13\u00a0linear factor models\n\n492\n13.1 probabilistic pca and factor analysis .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 493\nindependent component analysis (ica) .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 494\n13.2\n13.3 slow feature analysis\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 496\n13.4 sparse coding .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 499\n\niv\n\n "}, {"Page_number": 6, "text": "contents\n\n13.5 manifold interpretation of pca .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 502\n\n14\u00a0autoencoders\n\n505\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 506\n14.1 undercomplete autoencoders\n14.2 regularized autoencoders .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 507\n14.3 representational power, layer size and depth .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 511\n14.4 stochastic encoders and decoders .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 512\n14.5 denoising autoencoders .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 513\n14.6 learning manifolds with autoencoders .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 518\n14.7 contractive autoencoders . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 524\n14.8 predictive sparse decomposition .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 526\n14.9 applications of autoencoders .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 527\n\n15\u00a0representation learning\n\n529\n15.1 greedy layer-wise unsupervised pretraining .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 531\n15.2 transfer learning and domain adaptation .\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 539\n15.3 semi-supervised disentangling of causal factors\n.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 544\n15.4 distributed representation .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 549\n15.5 exponential gains from depth .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 556\n15.6 providing clues to discover underlying causes .\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 557\n\n16\u00a0structured probabilistic models for deep learning\n\n561\n16.1 the challenge of unstructured modeling . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 562\n16.2 using graphs to describe model structure .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 566\n16.3 sampling from graphical models .\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 583\n16.4 advantages of structured modeling\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 584\n16.5 learning about dependencies .\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 585\n16.6\ninference and approximate inference .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 586\n16.7 the deep learning approach to structured probabilistic models 587\n\n17\u00a0monte carlo methods\n\n593\n17.1 sampling and monte carlo methods\n.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 593\n17.2\nimportance sampling .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 595\n17.3 markov chain monte carlo methods .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 598\n17.4 gibbs sampling\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 602\n17.5 the challenge of mixing between separated modes .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 602\n\n18\u00a0confronting the partition function\n\n608\n. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 609\n18.1 the log-likelihood gradient\n18.2 stochastic maximum likelihood and contrastive divergence .\u00a0.\u00a0. 610\n\nv\n\n "}, {"Page_number": 7, "text": "contents\n\n18.3 pseudolikelihood .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 618\n18.4 score matching and ratio matching .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 620\n18.5 denoising score matching .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 622\n18.6 noise-contrastive estimation .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 623\n18.7 estimating the partition function .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 626\n\n19\u00a0approximate inference\n\n634\n19.1\ninference as optimization . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 636\n19.2 expectation maximization .\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 637\n19.3 map inference and sparse coding . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 638\n19.4 variational inference and learning .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 641\n19.5 learned approximate inference .\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 653\n\n20\u00a0deep generative models\n\n656\n20.1 boltzmann machines .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0. 656\n20.2 restricted boltzmann machines .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0. 658\n20.3 deep belief networks . .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 662\n20.4 deep boltzmann machines .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 665\n20.5 boltzmann machines for real-valued data .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 678\n20.6 convolutional boltzmann machines\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. . 685\n20.7 boltzmann machines for structured or sequential outputs .\u00a0.\u00a0.\u00a0. 687\n20.8 other boltzmann machines\n.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 688\n20.9 back-propagation through random operations .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0. 689\n20.10\u00a0directed generative nets .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 694\n20.11\u00a0drawing samples from autoencoders .\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0. 712\n20.12\u00a0generative stochastic networks .\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 716\n20.13\u00a0other generation schemes .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 717\n20.14\u00a0evaluating generative models\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 719\n20.15\u00a0conclusion .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. .\u00a0.\u00a0.\u00a0.\u00a0.\u00a0. 721\n\nbibliography\n\nindex\n\n723\n\n780\n\nvi\n\n "}, {"Page_number": 8, "text": "website\n\nwww.deeplearningbook.org\n\nthis book is accompanied by the above website. the website provides a\nvariety of supplementary material, including exercises, lecture slides, corrections of\nmistakes, and other resources that should be useful to both readers and instructors.\n\nvii\n\n "}, {"Page_number": 9, "text": "acknowledgments\n\nthis book would not have been possible without the contributions of many people.\n\nwe would like to thank those who commented on our proposal for the book\nand helped plan its contents and organization: guillaume alain, kyunghyun cho,\n\u00e7a\u011flar g\u00fcl\u00e7ehre, david krueger, hugo larochelle, razvan pascanu and thomas\nroh\u00e9e.\n\nwe would like to thank the people who offered feedback on the content of the\nbook itself. some offered feedback on many chapters: mart\u00edn abadi, guillaume\nalain, ion androutsopoulos, fred bertsch, olexa bilaniuk, ufuk can bi\u00e7ici, matko\nbo\u0161njak, john boersma, greg brockman, pierre luc carrier, sarath chandar,\npawel chilinski, mark daoust, oleg dashevskii, laurent dinh, stephan dreseitl,\njim fan, miao fan, meire fortunato, fr\u00e9d\u00e9ric francis, nando de freitas, \u00e7a\u011flar\ng\u00fcl\u00e7ehre, jurgen van gael, javier alonso garc\u00eda, jonathan hunt, gopi jeyaram,\nchingiz kabytayev, lukasz kaiser, varun kanade, akiel khan, john king, diederik\np. kingma, yann lecun, rudolf mathey, mat\u00edas mattamala, abhinav maurya,\nkevin murphy, oleg m\u00fcrk, roman novak, augustus q. odena, simon pavlik,\nkarl pichotta, kari pulli, tapani raiko, anurag ranjan, johannes roith, halis\nsak, c\u00e9sar salgado, grigory sapunov, mike schuster, julian serban, nir shabat,\nken shirriff, scott stanley, david sussillo, ilya sutskever, carles gelada s\u00e1ez,\ngraham taylor, valentin tolmer, an tran, shubhendu trivedi, alexey umnov,\nvincent vanhoucke, marco visentini-scarzanella, david warde-farley, dustin\nwebb, kelvin xu, wei xue, li yao, zygmunt zaj\u0105c and ozan \u00e7a\u011flayan.\n\nwe would also like to thank those who provided us with useful feedback on\n\nindividual chapters:\n\n\u2022 chapter ,\n\n1 introduction\n\n: yusuf akgul, sebastien bratieres, samira ebrahimi,\ncharlie gorichanaz, brendan loudermilk, eric morris, cosmin p\u00e2rvulescu\nand alfredo solano.\n\n\u2022 chapter\n\n,\n2 linear algebra\n\n: amjad almahairi, nikola bani\u0107, kevin bennett,\n\nviii\n\n "}, {"Page_number": 10, "text": "contents\n\nphilippe castonguay, oscar chang, eric fosler-lussier, sergey oreshkov,\nistv\u00e1n petr\u00e1s, dennis prangle, thomas roh\u00e9e, colby toland, massimiliano\ntomassoli, alessandro vitale and bob welland.\n\n\u2022 chapter\n\n,\n3 probability and information theory\n\n: john philip anderson, kai\narulkumaran, vincent dumoulin, rui fa, stephan gouws, artem oboturov,\nantti rasmus, andre simpelo, alexey surkov and volker tresp.\n\n\u2022 chapter\nyuhuang.\n\n4 numerical computation\n\n,\n\n: tran lam an, ian fischer, and hu\n\n,\u00a0\n\n\u2022 chapter\n\n5 machine learning\u00a0basics\n\n: dzmitry bahdanau,\u00a0nikhil\u00a0garg,\nmakoto otsuka, bob pepin, philip popien, emmanuel rayner, kee-bong\nsong, zheng sun and andy wu.\n\n\u2022 chapter\n\n,6 deep feedforward networks: uriel berdugo, fabrizio bottarel,\nelizabeth burl, ishan durugkar, jeff hlywa, jong wook kim, david krueger\nand aditya kumar praharaj.\n\n\u2022 chapter\n\njoshua salisbury.\n\n,\n7 regularization for deep learning\n\n: inkyu lee, sunil mohan and\n\n\u2022 chapter\n,8 optimization for training deep models: marcel ackermann,\nrowel atienza, andrew brock, tegan maharaj, james martens and klaus\nstrobl.\n\n\u2022 chapter\n\n,9 convolutional networks: mart\u00edn arjovsky, eugene brevdo, eric\njensen, asifullah khan, mehdi mirza, alex paino, eddie pierce, marjorie\nsayer, ryan stout and wentao wu.\n\n\u2022 chapter\n\n,10 sequence modeling: recurrent and recursive nets: g\u00f6k\u00e7en\neraslan, steven hickson, razvan pascanu, lorenzo von ritter, rui rodrigues,\nmihaela rosca, dmitriy serdyuk, dongyu shi and kaiyu yang.\n\n\u2022 chapter\n\u2022 chapter\n\u2022 chapter\n\u2022 chapter\n\n11 practical methodology\n\n,\n\n: daniel beckstein.\n\n12 applications\n\n,\n\n: george dahl and ribana roscher.\n\n15 representation learning\n\n,\n\n: kunal ghosh.\n\n16 structured probabilistic models for deep learning\n\n,\n\n: minh l\u00ea\n\nand anton varfolom.\n\n\u2022 chapter\n\n,18 confronting the partition function: sam bowman.\n\nix\n\n "}, {"Page_number": 11, "text": "contents\n\n\u2022 chapter\n\n,20 deep generative models: nicolas chapados, daniel galvez,\n\nwenming ma, fady medhat, shakir mohamed and gr\u00e9goire montavon.\n\n\u2022 bibliography: leslie n. smith.\n\nwe also want to thank those who allowed us to reproduce images, figures or\ndata from their publications. we indicate their contributions in the figure captions\nthroughout the text.\n\nwe\u00a0would like to\u00a0thank ian\u2019s wife\u00a0daniela flori\u00a0goodfellow for patiently\nsupporting ian during the writing of the book as well as for help with proofreading.\n\nwe would like to thank the google brain team for providing an intellectual\nenvironment where ian could devote a tremendous amount of time to writing this\nbook and receive feedback and guidance from colleagues. we would especially like\nto thank ian\u2019s former manager, greg corrado, and his current manager, samy\nbengio, for their support of this project. finally, we would like to thank geoffrey\nhinton for encouragement when writing was difficult.\n\nx\n\n "}, {"Page_number": 12, "text": "notation\n\nthis section provides a concise reference describing the notation used throughout\nthis book.\nif you are unfamiliar with any of the corresponding mathematical\nconcepts, this notation reference may seem intimidating. however, do not despair,\nwe describe most of these ideas in chapters 2-4.\n\na\n\na\n\na\n\na\n\nin\n\ni\n\ne( )i\n\nnumbers and arrays\n\na scalar (integer or real)\n\na vector\n\na matrix\n\na tensor\n\nidentity matrix with\n\nn\n\nrows and\n\nn\n\ncolumns\n\nidentity matrix with dimensionality implied by\ncontext\n\nstandard basis vector [0, . . . , 0, 1, 0, . . . , 0] with a\n1 at position i\n\ndiag( )a a square, diagonal matrix with diagonal entries\n\ngiven by a\n\na scalar random variable\n\na vector-valued random variable\n\na matrix-valued random variable\n\na\n\na\n\na\n\nxi\n\n "}, {"Page_number": 13, "text": "contents\n\na\n\nr\n\n{\n}0 1,\n}\n{\n, . . . , n\n0 1,\n\n[\n]a, b\n\n(\n]a, b\na b\\\n\ng\n\np ag(xi)\n\nsets and graphs\n\na set\n\nthe set of real numbers\n\nthe set containing 0 and 1\n\nthe set of all integers between\n\n0\n\nand\n\nn\n\nthe real interval including\n\na\n\nand\n\nb\n\nthe real interval excluding\n\na\n\nbut including\n\nb\n\nset subtraction, i.e.,\u00a0the set containing the ele-\nments of\n\nthat are not in\n\na\n\nb\n\na graph\nthe parents of xi in g\n\nindexing\n\nai\na\u2212i\nai,j\n\nelement i of vector a, with indexing starting at 1\n\nall elements of vector\n\na\n\nexcept for element\n\ni\n\nelement\n\ni, j\n\nof matrix\n\na\n\nai,: row of matrix\n\ni\n\na\n\na:,i column of matrix\n\ni\n\na\n\nai,j,k element\n\n(\ni, j, k\n\n)\n\nof a 3-d tensor\n\na\n\na: :, ,i\n\n2-d slice of a 3-d tensor\n\nai\n\nelement\n\ni\n\nof the random vector\n\na\n\nlinear algebra operations\n\ntranspose of matrix a\n\nmoore-penrose pseudoinverse of a\n\nelement-wise (hadamard) product of\n\nanda\n\nb\n\n)a determinant of a\n\na>\na+\na b(cid:130)\ndet(\n\nxii\n\n "}, {"Page_number": 14, "text": "contents\n\ndy\ndx\n\u2202y\n\u2202x\n\u2207xy\n\u2207x y\n\u2207xy\n\u2202f\n\u2202x\n\n\u22072\nxf\n\nx or h )( )x\n( )\n\nf\n(\n\nz f\nzs\n\nf\n\n( )x x\n\nd\n\n( )x x\n\nd\n\ncalculus\n\nderivative of with respect to .\nx\n\ny\n\npartial derivative of with respect to\n\ny\n\nx\n\ngradient of with respect to\n\ny\n\nx\n\nmatrix derivatives of with respect to\n\ny\n\nx\n\ntensor containing derivatives of y with respect to\nx\n\njacobian matrix j \u2208 rm n\u00d7 of f : rn \u2192 rm\nat input point x\nthe hessian matrix of\n\nf\n\ndefinite integral over the entire domain of x\n\ndefinite integral with respect to\n\nx\n\nover the set\n\ns\n\nprobability and information theory\n\nthe random variables a and b are independent\n\nthey are are conditionally independent given c\n\na probability distribution over a discrete variable\n\na probability distribution over a continuous vari-\nable, or over a variable whose type has not been\nspecified\n\nrandom variable a has distribution\n\np\n\n\u22a5\na b\n\u22a5 |\na b c\np ( )a\n\np( )a\n\na\n\n\u223c p\n\nex\u223cp[ ( )]\n\nf x or ef x\n( )\n\nexpectation of f x with respect to p x\n( )\n\n( )\n\nvar( ( ))\n\nf x\n\nvariance of\n\nf x( )\n\nunder\n\nx\np ( )\n\ncov( ( )\n\nf x , g x\n\n( ))\n\ncovariance of\n\nf x( )\n\nand\n\ng x( )\n\nunder\n\nx\np ( )\n\nh( )x\n\nshannon entropy of the random variable x\n\np qk\n)\nx \u00b5, \u03c3\n)\n\ndkl(\nn ( ;\n\nkullback-leibler divergence of p and q\n\ngaussian distribution over x with mean \u00b5 and\ncovariance \u03c3\n\nxiii\n\n "}, {"Page_number": 15, "text": "contents\n\nf\n\nb\u2192\n: a\n\u25e6\ng\n\nf\n\nf ( ; )x \u03b8\n\nfunctions\n\nthe function with domain\n\nf\n\na\n\nand range\n\nb\n\ncomposition of the functions\n\nf\n\nand\n\ng\n\na function of x parametrized by \u03b8. sometimes\nwe just write f(x) and ignore the argument \u03b8 to\nlighten notation.\n\nlog x\n\n\u03c3 x( )\n\n\u03b6 x\n( )\n||\n||x p\n||\n||x\nx+\n\nnatural logarithm of\n\nx\n\nlogistic sigmoid,\n\n1\n\n1 + exp(\n\n)\u2212x\n\nsoftplus,\n\nlog(1 + exp(\n\nx\n\n))\n\nlp norm of x\n\nl2 norm of x\n\npositive part of\n\nx\n\n, i.e.,\n\nmax(0 ), x\n\n1condition\n\nis 1 if the condition is true, 0 otherwise\n\nsometimes we use a function f whose argument is a scalar, but apply it to a vector,\nmatrix, or tensor: f(x), f (x), or f (x). this means to apply f to the array\nelement-wise. for example, if c = \u03c3(x), then ci,j,k = \u03c3(xi,j,k) for all valid values\nof\n\nand .\nk\n\n,\ni j\n\npdata\n\n\u02c6pdata\n\nx\n\nx( )i\n\ndatasets and distributions\n\nthe data generating distribution\n\nthe empirical distribution defined by the training\nset\n\na set of training examples\n\nthe -th example (input) from a dataset\n\ni\n\ny( )i or y ( )i the target associated with x( )i for supervised learn-\n\nx\n\ning\nthe m n\u00d7 matrix with input example x( )i\nxi,:\n\nin row\n\nxiv\n\n "}, {"Page_number": 16, "text": "chapter 1\n\nintroduction\n\ninventors have long dreamed of creating machines that think. this desire dates\nback to at least the time of ancient greece. the mythical figures pygmalion,\ndaedalus, and hephaestus may all be interpreted as legendary inventors, and\ngalatea, talos, and pandora may all be regarded as artificial life (\novid and martin\n,\n2004 sparkes 1996 tandy 1997\n\n).\n\n;\n\n,\n\n;\n\n,\n\nwhen programmable computers were first conceived, people wondered whether\nthey might become intelligent, over a hundred years before one was built (lovelace,\n1842). today, artificial intelligence (ai) is a thriving field with many practical\napplications and active research topics. we look to intelligent software to automate\nroutine labor,\u00a0understand speech or images,\u00a0make diagnoses in medicine and\nsupport basic scientific research.\n\nin the early days of artificial intelligence, the field rapidly tackled and solved\nproblems that are intellectually difficult for human beings but relatively straight-\nforward for computers\u2014problems that can be described by a list of formal, math-\nematical rules.\u00a0the true challenge to artificial intelligence proved to be solving\nthe tasks that are easy for people to perform but hard for people to describe\nformally\u2014problems that we solve intuitively, that feel automatic, like recognizing\nspoken words or faces in images.\n\nthis book is about a solution to these more intuitive problems. this solution is\nto allow computers to learn from experience and understand the world in terms of a\nhierarchy of concepts, with each concept defined in terms of its relation to simpler\nconcepts. by gathering knowledge from experience, this approach avoids the need\nfor human operators to formally specify all of the knowledge that the computer\nneeds. the hierarchy of concepts allows the computer to learn complicated concepts\nby building them out of simpler ones. if we draw a graph showing how these\n\n1\n\n "}, {"Page_number": 17, "text": "chapter 1.\n\nintroduction\n\nconcepts are built on top of each other, the graph is deep, with many layers. for\nthis reason, we call this approach to ai deep learning.\n\nmany of the early successes of ai took place in relatively sterile and formal\nenvironments and did not require computers to have much knowledge about\nthe world.\u00a0for example, ibm\u2019s deep blue chess-playing system defeated world\nchampion garry kasparov in 1997 (\n). chess is of course a very simple\nworld, containing only sixty-four locations and thirty-two pieces that can move\nin only rigidly circumscribed ways. devising a successful chess strategy is\u00a0a\ntremendous accomplishment,\u00a0but the challenge is not due to the difficulty of\ndescribing the set of chess pieces and allowable moves to the computer. chess\ncan be completely described by a very brief list of completely formal rules, easily\nprovided ahead of time by the programmer.\n\nhsu 2002\n\n,\n\nironically, abstract and formal tasks that are among the most difficult mental\nundertakings for a human being are among the easiest for a computer. computers\nhave long been able to defeat even the best human chess player, but are only\nrecently matching some of the abilities of average human beings to recognize objects\nor speech. a person\u2019s everyday life requires an immense amount of knowledge\nabout the world. much of this knowledge is subjective and intuitive, and therefore\ndifficult to articulate in a formal way. computers need to capture this same\nknowledge in order to behave in an intelligent way. one of the key challenges in\nartificial intelligence is how to get this informal knowledge into a computer.\n\nseveral artificial intelligence projects have sought to hard-code knowledge about\nthe world in formal languages. a computer can reason about statements in these\nformal languages automatically using logical inference rules. this is known as the\nknowledge base approach to artificial intelligence. none of these projects has led to\na major success. one of the most famous such projects is cyc (\nlenat and guha\n,\n1989). cyc is an inference engine and a database of statements in a language\ncalled cycl. these statements are entered by a staff of human supervisors. it is an\nunwieldy process. people struggle to devise formal rules with enough complexity\nto accurately describe the world. for example, cyc failed to understand a story\nabout a person named fred shaving in the morning (\n). its inference\nengine detected an inconsistency in the story:\u00a0it knew that people do not have\nelectrical parts, but because fred was holding an electric razor, it believed the\nentity \u201cfredwhileshaving\u201d contained electrical parts. it therefore asked whether\nfred was still a person while he was shaving.\n\nlinde 1992\n\n,\n\nthe difficulties faced by systems relying on hard-coded knowledge suggest that\nai systems need the ability to acquire their own knowledge, by extracting patterns\nfrom raw data. this capability is known as machine learning. the introduction\n\n2\n\n "}, {"Page_number": 18, "text": "chapter 1.\n\nintroduction\n\nof machine learning allowed computers to tackle problems involving knowledge\nof the real world and make decisions that appear subjective. a simple machine\nlearning algorithm called logistic regression can determine whether to recommend\ncesarean delivery (mor-yosef\n). a simple machine learning algorithm\ncalled\n\ncan separate legitimate e-mail from spam e-mail.\n\nnaive bayes\n\net al.,\n\n1990\n\nthe performance of these simple machine learning algorithms depends heavily\non the representation of the data they are given. for example, when logistic\nregression is used to recommend cesarean delivery, the ai system does not examine\nthe patient directly. instead, the doctor tells the system several pieces of relevant\ninformation, such as the presence or absence of a uterine scar. each piece of\ninformation included in the representation of the patient is known as a feature.\nlogistic regression learns how each of these features of the patient correlates with\nvarious outcomes. however, it cannot influence the way that the features are\ndefined in any way.\u00a0if logistic regression was given an mri scan of the patient,\nrather than the doctor\u2019s formalized report, it would not be able to make useful\npredictions. individual pixels in an mri scan have negligible correlation with any\ncomplications that might occur during delivery.\n\nthis dependence on representations is a general phenomenon that appears\nthroughout computer science and even daily life. in computer science, opera-\ntions such as searching a collection of data can proceed exponentially faster if\nthe collection is structured and indexed intelligently.\u00a0people can easily perform\narithmetic on arabic numerals, but find arithmetic on roman numerals much\nmore time-consuming. it is not surprising that the choice of representation has an\nenormous effect on the performance of machine learning algorithms. for a simple\nvisual example, see fig.\n\n.1.1\n\nmany artificial intelligence tasks can be solved by designing the right set of\nfeatures to extract for that task, then providing these features to a simple machine\nlearning algorithm. for example, a useful feature for speaker identification from\nsound is an estimate of the size of speaker\u2019s vocal tract. it therefore gives a strong\nclue as to whether the speaker is a man, woman, or child.\n\nhowever, for many tasks, it is difficult to know what features should be extracted.\nfor example, suppose that we would like to write a program to detect cars in\nphotographs. we know that cars have wheels, so we might like to use the presence\nof a wheel as a feature.\u00a0unfortunately, it is difficult to describe exactly what a\nwheel looks like in terms of pixel values. a wheel has a simple geometric shape but\nits image may be complicated by shadows falling on the wheel, the sun glaring off\nthe metal parts of the wheel, the fender of the car or an object in the foreground\nobscuring part of the wheel, and so on.\n\n3\n\n "}, {"Page_number": 19, "text": "chapter 1.\n\nintroduction\n\ncartesian\u00a0coordinates\n\npolar\u00a0coordinates\n\ny\n\n\u00b5\n\nx\n\nr\n\nfigure 1.1: example of\u00a0different representations: suppose we want to separate\u00a0two\ncategories of data by drawing a line between them in a scatterplot. in the plot on the left,\nwe represent some data using cartesian coordinates, and the task is impossible. in the plot\non the right, we represent the data with polar coordinates and the task becomes simple to\nsolve with a vertical line. (figure produced in collaboration with david warde-farley)\n\none solution to this problem is to use machine learning to discover not only\nthe mapping from representation to output but also the representation itself.\nthis approach is known as representation learning. learned representations often\nresult in much better performance\u00a0than can be obtained\u00a0with hand-designed\nrepresentations. they also allow ai systems to rapidly adapt to new tasks, with\nminimal human intervention. a representation learning algorithm can discover a\ngood set of features for a simple task in minutes, or a complex task in hours to\nmonths. manually designing features for a complex task requires a great deal of\nhuman time and effort; it can take decades for an entire community of researchers.\n\nthe quintessential example of a representation learning algorithm is the au-\ntoencoder. an autoencoder is the combination of an encoder function that converts\nthe input data into a different representation, and a decoder function that converts\nthe new representation back into the original format. autoencoders are trained to\npreserve as much information as possible when an input is run through the encoder\nand then the decoder, but are also trained to make the new representation have\nvarious nice properties. different kinds of autoencoders aim to achieve different\nkinds of properties.\n\nwhen designing features or algorithms for learning features, our goal is usually\nto separate the\nthat explain the observed data. in this context,\nwe use the word \u201cfactors\u201d simply to refer to separate sources of influence; the factors\nare usually not combined by multiplication. such factors are often not quantities\n\nfactors of variation\n\n4\n\n "}, {"Page_number": 20, "text": "chapter 1.\n\nintroduction\n\nthat are directly observed. instead, they may exist either as unobserved objects\nor unobserved forces in the physical world that affect observable quantities. they\nmay also exist as constructs in the human mind that provide useful simplifying\nexplanations or inferred causes of the observed data. they can be thought of as\nconcepts or abstractions that help us make sense of the rich variability in the data.\nwhen analyzing a speech recording, the factors of variation include the speaker\u2019s\nage, their sex, their accent and the words that they are speaking. when analyzing\nan image of a car, the factors of variation include the position of the car, its color,\nand the angle and brightness of the sun.\n\na major source of difficulty in many real-world artificial intelligence applications\nis that many of the factors of variation influence every single piece of data we are\nable to observe. the individual pixels in an image of a red car might be very close\nto black at night. the shape of the car\u2019s silhouette depends on the viewing angle.\nmost applications require us to\nthe factors of variation and discard the\nones that we do not care about.\n\ndisentangle\n\nof course, it can be very difficult to extract such high-level, abstract features\nfrom raw data. many of these factors of variation, such as a speaker\u2019s accent,\ncan be identified only using sophisticated, nearly human-level understanding of\nthe data. when it is nearly as difficult to obtain a representation as to solve the\noriginal problem, representation learning does not, at first glance, seem to help us.\n\ndeep learning solves this central problem in representation learning by introduc-\ning representations that are expressed in terms of other, simpler representations.\ndeep learning allows the computer to build complex concepts out of simpler con-\ncepts. fig.\nshows how a deep learning system can represent the concept of an\nimage of a person by combining simpler concepts, such as corners and contours,\nwhich are in turn defined in terms of edges.\n\n1.2\n\nthe quintessential example of a deep learning model is the feedforward deep\nnetwork or multilayer perceptron (mlp). a multilayer perceptron is just a mathe-\nmatical function mapping some set of input values to output values. the function\nis formed by composing many simpler functions. we can think of each application\nof a different mathematical function as providing a new representation of the input.\n\nthe idea of learning the right representation for the data provides one perspec-\ntive on deep learning. another perspective on deep learning is that depth allows the\ncomputer to learn a multi-step computer program. each layer of the representation\ncan be thought of as the state of the computer\u2019s memory after executing another\nset of instructions in parallel. networks with greater depth can execute more\ninstructions in sequence. sequential instructions offer great power because later\ninstructions can refer back to the results of earlier instructions. according to this\n\n5\n\n "}, {"Page_number": 21, "text": "chapter 1.\n\nintroduction\n\ncar\n\nperson animal\n\noutput\n\n(object\u00a0identity)\n\n3rd\u00a0hidden\u00a0layer\n(object\u00a0parts)\n\n2nd\u00a0hidden\u00a0layer\n\n(corners\u00a0and\n\ncontours)\n\n1st\u00a0hidden\u00a0layer\n\n(edges)\n\nvisible\u00a0layer\n(input\u00a0pixels)\n\nfigure 1.2: illustration of a deep learning model. it is difficult for a computer to understand\nthe meaning of raw sensory input data, such as this image represented as a collection\nof pixel values. the function mapping from a set of pixels to an object identity is very\ncomplicated. learning or evaluating this mapping seems insurmountable if tackled directly.\ndeep learning resolves this difficulty by breaking the desired complicated mapping into a\nseries of nested simple mappings, each described by a different layer of the model. the\n, so named because it contains the variables that we\ninput is presented at the\nvisible layer\nare able to observe. then a series of\nextracts increasingly abstract features\nhidden layers\nfrom the image.\u00a0these layers are called \u201chidden\u201d because their values are not given in\nthe data; instead the model must determine which concepts are useful for explaining\nthe relationships in the observed data. the images here are visualizations of the kind\nof feature represented by each hidden unit. given the pixels, the first layer can easily\nidentify edges, by comparing the brightness of neighboring pixels. given the first hidden\nlayer\u2019s description of the edges, the second hidden layer can easily search for corners and\nextended contours, which are recognizable as collections of edges. given the second hidden\nlayer\u2019s description of the image in terms of corners and contours, the third hidden layer\ncan detect entire parts of specific objects, by finding specific collections of contours and\ncorners. finally, this description of the image in terms of the object parts it contains can\nbe used to recognize the objects present in the image. images reproduced with permission\nfrom zeiler and fergus 2014\n\n).\n\n(\n\n6\n\n "}, {"Page_number": 22, "text": "chapter 1.\n\nintroduction\n\nelement\n\nset\n\n+\n\u21e5\n(cid:114)\n\nelement\n\nset\n\n(cid:114)\n\n+\n\n\u21e5\n\n\u21e5\n\nlogistic\n\nlogistic\n\nregression\n\nregression\n\nw1w1\n\nx1x1\n\nw2w2\n\nx2x2\n\nww\n\nxx\n\nfigure 1.3: illustration of computational graphs mapping an input to an output where\neach node performs an operation. depth is the length of the longest path from input to\noutput but depends on the definition of what constitutes a possible computational step.\nthe computation depicted in these graphs is the output of a logistic regression model,\n\u03c3(wt x), where \u03c3 is the logistic sigmoid function. if we use addition, multiplication and\nlogistic sigmoids as the elements of our computer language, then this model has depth\nthree. if we view logistic regression as an element itself, then this model has depth one.\n\nview of deep learning, not all of the information in a layer\u2019s activations necessarily\nencodes factors of variation that explain the input. the representation also stores\nstate information that helps to execute a program that can make sense of the input.\nthis state information could be analogous to a counter or pointer in a traditional\ncomputer program. it has nothing to do with the content of the input specifically,\nbut it helps the model to organize its processing.\n\nthere are two main ways of measuring the depth of a model. the first view is\nbased on the number of sequential instructions that must be executed to evaluate\nthe architecture. we can think of this as the length of the longest path through\na flow chart that describes how to compute each of the model\u2019s outputs given\nits inputs. just as two equivalent computer programs will have different lengths\ndepending on which language the program is written in, the same function may be\ndrawn as a flowchart with different depths depending on which functions we allow\nto be used as individual steps in the flowchart. fig.\nillustrates how this choice\nof language can give two different measurements for the same architecture.\n\n1.3\n\nanother approach, used by deep probabilistic models, regards the depth of a\nmodel as being not the depth of the computational graph but the depth of the\ngraph describing how concepts are related to each other. in this case, the depth\nof the flowchart of the computations needed to compute the representation of\n\n7\n\n "}, {"Page_number": 23, "text": "chapter 1.\n\nintroduction\n\neach concept may be much deeper than the graph of the concepts themselves.\nthis is because the system\u2019s understanding of the simpler concepts can be refined\ngiven information about the more complex concepts. for example, an ai system\nobserving an image of a face with one eye in shadow may initially only see one eye.\nafter detecting that a face is present, it can then infer that a second eye is probably\npresent as well.\u00a0in this case, the graph of concepts only includes two layers\u2014a\nlayer for eyes and a layer for faces\u2014but the graph of computations includes 2n\nlayers if we refine our estimate of each concept given the other\n\ntimes.\n\nn\n\nbecause it is not always clear which of these two views\u2014the depth of the\ncomputational graph, or the depth of the probabilistic modeling graph\u2014is most\nrelevant, and because different people choose different sets of smallest elements\nfrom which to construct their graphs, there is no single correct value for the\ndepth of an architecture, just as there is no single correct value for the length of\na computer program.\u00a0nor is there a consensus about how much depth a model\nrequires to qualify as \u201cdeep.\u201d however, deep learning can safely be regarded as the\nstudy of models that either involve a greater amount of composition of learned\nfunctions or learned concepts than traditional machine learning does.\n\nto summarize, deep learning, the subject of this book, is an approach to ai.\nspecifically, it is a type of machine learning, a technique that allows computer\nsystems to improve with experience and data.\u00a0according to the authors of this\nbook, machine learning is the only viable approach to building ai systems that\ncan operate in complicated, real-world environments. deep learning is a particular\nkind of machine learning that achieves great power and flexibility by learning to\nrepresent the world as a nested hierarchy of concepts, with each concept defined in\nrelation to simpler concepts, and more abstract representations computed in terms\nof less abstract ones. fig.\nillustrates the relationship between these different\nai disciplines. fig.\n\ngives a high-level schematic of how each works.\n\n1.4\n\n1.5\n\n1.1 who should read this book?\n\nthis book can be useful for a variety of readers, but we wrote it with two main\ntarget audiences in mind. one of these target audiences is university students\n(undergraduate or graduate) learning about machine learning, including those who\nare beginning a career in deep learning and artificial intelligence research.\u00a0the\nother target audience is software engineers who do not have a machine learning\nor statistics background, but want to rapidly acquire one and begin using deep\nlearning in their product or platform. deep learning has already proven useful in\nmany software disciplines including computer vision, speech and audio processing,\n\n8\n\n "}, {"Page_number": 24, "text": "chapter 1.\n\nintroduction\n\ndeep\u00a0learning\n\nexample:\n\nmlps\n\nexample:\nshallow\n\nautoencoders\n\nexample:\nlogistic\nregression\n\nexample:\nknowledge\n\nbases\n\nrepresentation\u00a0learning\n\nmachine\u00a0learning\n\nai\n\nfigure 1.4: a venn diagram showing how deep learning is a kind of representation learning,\nwhich is in turn a kind of machine learning, which is used for many but not all approaches\nto ai. each section of the venn diagram includes an example of an ai technology.\n\n9\n\n "}, {"Page_number": 25, "text": "chapter 1.\n\nintroduction\n\noutput\n\noutput\n\noutput\n\nmapping\u00a0from\u00a0\n\nfeatures\n\noutput\n\nmapping\u00a0from\u00a0\n\nmapping\u00a0from\u00a0\n\nlayers\u00a0of\u00a0more\u00a0\n\nadditional\u00a0\n\nfeatures\n\nfeatures\n\nabstract\u00a0\nfeatures\n\nhand-\n\ndesigned\u00a0\nprogram\n\nhand-\n\ndesigned\u00a0\nfeatures\n\nfeatures\n\nsimple\u00a0\nfeatures\n\ninput\n\ninput\n\ninput\n\ninput\n\nrule-based\n\nsystems\n\nclassic\nmachine\nlearning\n\ndeep\n\nlearning\n\nrepresentation\n\nlearning\n\nfigure 1.5:\u00a0flowcharts showing how the different parts of an ai system relate to each\nother within different ai disciplines. shaded boxes indicate components that are able to\nlearn from data.\n\n10\n\n "}, {"Page_number": 26, "text": "chapter 1.\n\nintroduction\n\nnatural language processing, robotics, bioinformatics and chemistry, video games,\nsearch engines, online advertising and finance.\n\nthis book has been organized into three parts in order to best accommodate a\nintroduces basic mathematical tools and machine learning\ndescribes the most established deep learning algorithms that are\ndescribes more speculative ideas that are\n\nvariety of readers. part\nconcepts. part\nessentially solved technologies. part\nwidely believed to be important for future research in deep learning.\n\niii\n\nii\n\ni\n\nreaders should feel free to skip parts that are not relevant given their interests\nor background. readers familiar with linear algebra, probability, and fundamental\nmachine learning concepts can skip part , for example, while readers who just want\nto implement a working system need not read beyond part\n. to help choose which\nchapters to read, fig.\nprovides a flowchart showing the high-level organization\nof the book.\n\n1.6\n\nii\n\ni\n\nwe do assume that all readers come from a computer science background. we\nassume familiarity with programming, a basic understanding of computational\nperformance issues, complexity theory, introductory level calculus and some of the\nterminology of graph theory.\n\n1.2 historical trends in deep learning\n\nit is easiest to understand deep learning with some historical context. rather than\nproviding a detailed history of deep learning, we identify a few key trends:\n\n\u2022 deep learning has had a long and rich history, but has gone by many names\nreflecting different philosophical viewpoints, and has waxed and waned in\npopularity.\n\n\u2022 deep learning has become more useful as the amount of available training\n\ndata has increased.\n\n\u2022 deep learning models have grown in size over time as computer hardware\n\nand software infrastructure for deep learning has improved.\n\n\u2022 deep learning has solved increasingly complicated applications with increasing\n\naccuracy over time.\n\n11\n\n "}, {"Page_number": 27, "text": "chapter 1.\n\nintroduction\n\n1.\u00a0introduction\n\npart\u00a0i:\u00a0applied\u00a0math\u00a0and\u00a0machine\u00a0learning\u00a0basics\n\n2.\u00a0linear\u00a0algebra\n\n3.\u00a0probability\u00a0and\u00a0\ninformation\u00a0theory\n\n4.\u00a0numerical\u00a0\ncomputation\n\n5.\u00a0machine\u00a0learning\u00a0\n\nbasics\n\npart\u00a0ii:\u00a0deep\u00a0networks:\u00a0modern\u00a0practices\n\n6.\u00a0deep\u00a0feedforward\u00a0\n\nnetworks\n\n7.\u00a0regularization\n\n8.\u00a0optimization\n\n9.\u00a0\u00a0cnns\n\n10.\u00a0\u00a0rnns\n\n11.\u00a0practical\u00a0\nmethodology\n\n12.\u00a0applications\n\npart\u00a0iii:\u00a0deep\u00a0learning\u00a0research\n\n13.\u00a0linear\u00a0factor\u00a0\n\nmodels\n\n14.\u00a0autoencoders\n\n15.\u00a0representation\u00a0\n\nlearning\n\n16.\u00a0structured\u00a0\n\nprobabilistic\u00a0models\n\n19.\u00a0inference\n\n17.\u00a0monte\u00a0carlo\u00a0\n\nmethods\n\n18.\u00a0partition\u00a0\n\nfunction\n\n20.\u00a0deep\u00a0generative\u00a0\n\nmodels\n\nfigure 1.6: the high-level organization of the book. an arrow from one chapter to another\nindicates that the former chapter is prerequisite material for understanding the latter.\n\n12\n\n "}, {"Page_number": 28, "text": "chapter 1.\n\nintroduction\n\n1.2.1 the many names and changing fortunes of neural net-\n\nworks\n\nwe expect that many readers of this book have heard of deep learning as an\nexciting new technology, and are surprised to see a mention of \u201chistory\u201d in a book\nabout an emerging field. in fact, deep learning dates back to the 1940s. deep\nlearning only appears to be new, because it was relatively unpopular for several\nyears preceding its current popularity, and because it has gone through many\ndifferent names, and has only recently become called \u201cdeep learning.\u201d the field\nhas been rebranded many times, reflecting the influence of different researchers\nand different perspectives.\n\na comprehensive history of deep learning is beyond the scope of this textbook.\nhowever, some basic context is useful for understanding deep learning. broadly\nspeaking, there have been three waves of development of deep learning: deep learn-\ning known as cybernetics\nconnectionism\nin the 1980s\u20131990s, and the current resurgence under the name deep learning\nbeginning in 2006. this is quantitatively illustrated in fig.\n\nin the 1940s\u20131960s, deep learning known as\n\n.1.7\n\nsome of the earliest learning algorithms we recognize today were intended\nto be computational models of biological learning, i.e. models of how learning\nhappens or could happen in the brain.\u00a0as a result, one of the names that deep\nlearning has gone by is artificial neural networks (anns). the corresponding\nperspective on deep learning models is that they are engineered systems inspired\nby the biological brain (whether the human brain or the brain of another animal).\nwhile the kinds of neural networks used for machine learning have sometimes\nbeen used to understand brain function (\n), they are\ngenerally not designed to be realistic models of biological function.\u00a0the neural\nperspective on deep learning is motivated by two main ideas. one idea is that\nthe brain provides a proof by example that intelligent behavior is possible, and a\nconceptually straightforward path to building intelligence is to reverse engineer the\ncomputational principles behind the brain and duplicate its functionality. another\nperspective is that it would be deeply interesting to understand the brain and the\nprinciples that underlie human intelligence, so machine learning models that shed\nlight on these basic scientific questions are useful apart from their ability to solve\nengineering applications.\n\nhinton and shallice 1991\n\n,\n\nthe modern term \u201cdeep learning\u201d goes beyond the neuroscientific perspective\non the current breed of machine learning models. it appeals to a more general\nprinciple of learning multiple levels of composition, which can be applied in machine\nlearning frameworks that are not necessarily neurally inspired.\n\n13\n\n "}, {"Page_number": 29, "text": "chapter 1.\n\nintroduction\n\ncybernetics\n(connectionism + neural networks)\n\ne\ns\na\nr\nh\np\nr\no\n\nd\nr\no\n\nw\n\nf\no\n\ny\nc\nn\ne\nu\nq\ne\nr\nf\n\n0.000250\n\n0.000200\n\n0.000150\n\n0.000100\n\n0.000050\n\n0.000000\n\n1940\n\n1950\n\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\nyear\n\nmcculloch and pitts 1943 hebb 1949\n\nfigure 1.7: the figure shows two of the three historical waves of artificial neural nets\nresearch, as measured by the frequency of the phrases \u201ccybernetics\u201d and \u201cconnectionism\u201d or\n\u201cneural networks\u201d according to google books (the third wave is too recent to appear). the\nfirst wave started with cybernetics in the 1940s\u20131960s, with the development of theories\n,\nof biological learning (\n) and implementations of\nthe first models such as the perceptron (rosenblatt 1958\n) allowing the training of a single\nneuron. the second wave started with the connectionist approach of the 1980\u20131995 period,\nwith back-propagation (\n) to train a neural network with one or two\nhidden layers. the current and third wave, deep learning, started around 2006 (hinton\n), and is just now appearing in book\n,\net al.\nform as of 2016. the other two waves similarly appeared in book form much later than\nthe corresponding scientific activity occurred.\n\nrumelhart et al. 1986a\n\n2007 ranzato\n\n2006 bengio\n\n2007a\n\n,\net al.\n\n,\net al.\n\n;\n,\n\n,\n\n,\n\n;\n\n;\n\n14\n\n "}, {"Page_number": 30, "text": "chapter 1.\n\nintroduction\n\nthe earliest predecessors of modern deep learning were simple linear models\nmotivated from a neuroscientific perspective. these models were designed to\ntake a set of n input values x1, . . . , xn and associate them with an output y.\nthese models would learn a set of weights w1, . . . , wn and compute their output\n) = x1w1 + \u00b7\u00b7\u00b7 + xnwn . this first wave of neural networks research was\nf(x w,\nknown as cybernetics, as illustrated in fig.\n\n.1.7\n\n,\n\nmcculloch and pitts 1943\n\nthe mcculloch-pitts neuron (\n\n) was an early model\nof brain function. this linear model could recognize two different categories of\ninputs by testing whether f (x w,\n) is positive or negative. of course, for the model\nto correspond to the desired definition of the categories, the weights needed to be\nset correctly.\u00a0these weights could be set by the human operator.\u00a0in the 1950s,\nthe perceptron (rosenblatt 1958 1962\n) became the first model that could learn\nthe weights defining the categories given examples of inputs from each category.\nthe adaptive linear element (adaline), which dates from about the same time,\nsimply returned the value of f (x) itself to predict a real number (widrow and\nhoff 1960\n\n), and could also learn to predict these numbers from data.\n\n,\n\n,\n\n,\n\nthese simple learning algorithms greatly affected the modern landscape of\nmachine learning. the training algorithm used to adapt the weights of the ada-\nline was a special case of an algorithm called stochastic gradient descent. slightly\nmodified versions of the stochastic gradient descent algorithm remain the dominant\ntraining algorithms for deep learning models today.\n\nmodels based on the f(x w,\n\n) used by the perceptron and adaline are called\nlinear models. these models remain some of the most widely used machine learning\nmodels, though in many cases they are trained in different ways than the original\nmodels were trained.\n\nlinear models have many limitations. most famously, they cannot learn the\nxor function, where f ([0, 1] , w) = 1 and f([1, 0], w) = 1 but f([1, 1], w) = 0\nand f ([0, 0], w) = 0. critics who observed these flaws in linear models caused\na backlash against biologically inspired learning in general (minsky and papert,\n1969). this was the first major dip in the popularity of neural networks.\n\ntoday, neuroscience is regarded as an important source of inspiration for deep\n\nlearning researchers, but it is no longer the predominant guide for the field.\n\nthe main reason for the diminished role\u00a0of neuroscience in deep learning\nresearch today is that we simply do not have enough information about the brain\nto use it as a guide. to obtain a deep understanding of the actual algorithms used\nby the brain, we would need to be able to monitor the activity of (at the very\nleast) thousands of interconnected neurons simultaneously. because we are not\nable to do this, we are far from understanding even some of the most simple and\n\n15\n\n "}, {"Page_number": 31, "text": "chapter 1.\n\nintroduction\n\nwell-studied parts of the brain (\n\nolshausen and field 2005\n\n,\n\n).\n\nneuroscience has given us a reason to hope that a single deep learning algorithm\ncan solve many different tasks. neuroscientists have found that ferrets can learn to\n\u201csee\u201d with the auditory processing region of their brain if their brains are rewired\nto send visual signals to that area (von melchner\n). this suggests that\nmuch of the mammalian brain might use a single algorithm to solve most of the\ndifferent tasks that the brain solves. before this hypothesis, machine learning\nresearch was more fragmented, with different communities of researchers studying\nnatural language processing, vision, motion planning and speech recognition. today,\nthese application communities are still separate, but it is common for deep learning\nresearch groups to study many or even all of these application areas simultaneously.\n\net al.,\n\n2000\n\n,\n\n,\n\nlecun et al. 1998b\n\n), as we will see in sec.\n\nwe are able to draw some rough guidelines from neuroscience. the basic idea of\nhaving many computational units that become intelligent only via their interactions\nwith each other is inspired by the brain. the neocognitron (fukushima 1980\n)\nintroduced a powerful model architecture for processing images that was inspired\nby the structure of the mammalian visual system and later became the basis for\nthe modern convolutional network (\n.\n9.10\nmost neural networks today are based on a model neuron called the rectified linear\nunit. the original cognitron (fukushima 1975\n) introduced a more complicated\nversion that was highly inspired by our knowledge of brain function. the simplified\nmodern version was developed incorporating ideas from many viewpoints, with nair\nand hinton 2010\n) citing neuroscience as an influence, and\n) and\njarrett\n) citing more engineering-oriented influences. while neuroscience\n2009\nis an important source of inspiration, it need not be taken as a rigid guide. we\nknow that actual neurons compute very different functions than modern rectified\nlinear units, but greater neural realism has not yet led to an improvement in\nmachine learning performance. also, while neuroscience has successfully inspired\nseveral neural network architectures, we do not yet know enough about biological\nlearning for neuroscience to offer much guidance for the learning algorithms we\nuse to train these architectures.\n\n(\net al. (\n\nglorot\n\net al. (\n\n2011a\n\n,\n\nmedia accounts often emphasize the similarity of deep learning to the brain.\nwhile it is true that deep learning researchers are more likely to cite the brain as an\ninfluence than researchers working in other machine learning fields such as kernel\nmachines or bayesian statistics, one should not view deep learning as an attempt\nto simulate the brain. modern deep learning draws inspiration from many fields,\nespecially applied math fundamentals like linear algebra, probability, information\ntheory, and numerical optimization. while some deep learning researchers cite\nneuroscience as an important source of inspiration, others are not concerned with\n\n16\n\n "}, {"Page_number": 32, "text": "chapter 1.\n\nintroduction\n\nneuroscience at all.\n\nit is\u00a0worth\u00a0noting that\u00a0the effort to understand how the\u00a0brain works on\nan\u00a0algorithmic\u00a0level\u00a0is\u00a0alive\u00a0and well. this\u00a0endeavor\u00a0is primarily\u00a0known as\n\u201ccomputational neuroscience\u201d and is a separate field of study from deep learning.\nit is common for researchers to move back and forth between both fields. the\nfield of deep learning is primarily concerned with how to build computer systems\nthat are able to successfully solve tasks requiring intelligence, while the field of\ncomputational neuroscience is primarily concerned with building more accurate\nmodels of how the brain actually works.\n\n;\n\net al.\n,\n\n1986c mcclelland\n\nin the 1980s, the second wave of neural network research emerged in great part\nor\n(rumelhart\nvia a movement called connectionism parallel distributed processing\net al.\n). connectionism arose in the context of\n1995\n,\ncognitive science. cognitive science is an interdisciplinary approach to understand-\ning the mind, combining multiple different levels of analysis. during the early\n1980s, most cognitive scientists studied models of symbolic reasoning. despite their\npopularity, symbolic models were difficult to explain in terms of how the brain\ncould actually implement them using neurons. the connectionists began to study\nmodels of cognition that could actually be grounded in neural implementations\n(touretzky and minton 1985\n), reviving many ideas dating back to the work of\npsychologist donald hebb in the 1940s (\n\nhebb 1949\n\n).\n\n,\n\n,\n\nthe central idea in connectionism is that a large number of simple computational\nunits can achieve intelligent behavior when networked together. this insight\napplies equally to neurons in biological nervous systems and to hidden units in\ncomputational models.\n\nseveral key concepts arose during the connectionism movement of the 1980s\n\nthat remain central to today\u2019s deep learning.\n\n,\n\nhinton et al. 1986\n\none of these concepts is that of distributed representation (\n\n).\nthis is the idea that each input to a system should be represented by many features,\nand each feature should be involved in the representation of many possible inputs.\nfor example, suppose we have a vision system that can recognize cars, trucks, and\nbirds and these objects can each be red, green, or blue. one way of representing\nthese inputs would be to have a separate neuron or hidden unit that activates for\neach of the nine possible combinations: red truck, red car, red bird, green truck, and\nso on. this requires nine different neurons, and each neuron must independently\nlearn the concept of color and object identity. one way to improve on this situation\nis to use a distributed representation, with three neurons describing the color and\nthree neurons describing the object identity. this requires only six neurons total\ninstead of nine, and the neuron describing redness is able to learn about redness\n\n17\n\n "}, {"Page_number": 33, "text": "chapter 1.\n\nintroduction\n\nfrom images of cars, trucks and birds, not only from images of one specific category\nof objects. the concept of distributed representation is central to this book, and\nwill be described in greater detail in chapter\n\n.15\n\nanother major accomplishment of the connectionist movement was the suc-\ncessful use of back-propagation to train deep neural networks with internal repre-\nsentations and the popularization of the back-propagation algorithm (rumelhart\net al.,\n). this algorithm has waxed and waned in popularity\nbut as of this writing is currently the dominant approach to training deep models.\n\n1986a lecun 1987\n\n;\n\n,\n\n(\n\nhochreiter 1991\n\nduring the 1990s, researchers made important advances in modeling sequences\nwith neural networks.\n) identified some\nof the fundamental mathematical difficulties in modeling long sequences, described\nin sec.\n) introduced the long short-term\nmemory or lstm network to resolve some of these difficulties. today, the lstm\nis widely used for many sequence modeling tasks, including many natural language\nprocessing tasks at google.\n\n10.7 hochreiter and schmidhuber 1997\n\nbengio et al. 1994\n\n) and\n\n(\n\n(\n\n.\n\nthe second wave of neural networks research lasted until the mid-1990s. ven-\ntures based on neural networks and other ai technologies began to make unrealisti-\ncally ambitious claims while seeking investments. when ai research did not fulfill\nthese unreasonable expectations, investors were disappointed. simultaneously,\n,\nother fields of machine learning made advances. kernel machines (\nboser et al.\n1992 cortes and vapnik 1995 sch\u00f6lkopf\njor-\n) both achieved good results on many important tasks. these two factors\ndan 1998\nled to a decline in the popularity of neural networks that lasted until 2007.\n\n) and graphical models (\n\net al.,\n\n1999\n\n;\n\n,\n\n;\n\n,\n\n,\n\n;\n\n,\n\nlecun et al. 1998b bengio et al. 2001\n\nduring this time, neural networks continued to obtain impressive performance\non some tasks (\n). the canadian institute\nfor advanced research (cifar) helped to keep neural networks research alive\nvia its neural computation and adaptive perception (ncap) research initiative.\nthis program united machine learning research groups led by geoffrey hinton\nat university of toronto, yoshua bengio at university of montreal, and yann\nlecun at new york university. the cifar ncap research initiative had a\nmulti-disciplinary nature that also included neuroscientists and experts in human\nand computer vision.\n\nat this point in time, deep networks were generally believed to be very difficult\nto train.\u00a0we now know that algorithms that have existed since the 1980s work\nquite well, but this was not apparent circa 2006. the issue is perhaps simply that\nthese algorithms were too computationally costly to allow much experimentation\nwith the hardware available at the time.\n\nthe third wave of neural networks research began with a breakthrough in\n\n18\n\n "}, {"Page_number": 34, "text": "chapter 1.\n\nintroduction\n\n;\n\n2006\n\n2007a\n\net al.,\n\net al.,\n\n2006. geoffrey hinton showed that a kind of neural network called a deep belief\nnetwork could\u00a0be\u00a0efficiently trained using a\u00a0strategy called\u00a0greedy layer-wise\npretraining (hinton\n), which will be described in more detail in sec.\n15.1. the other cifar-affiliated research groups quickly showed that the same\nstrategy could be used to train many other kinds of deep networks (\nbengio et al.\n,\n2007 ranzato\n) and systematically helped to improve generalization\non test examples. this wave of neural networks research popularized the use of the\nterm deep learning to emphasize that researchers were now able to train deeper\nneural networks than had been possible before, and to focus attention on the\ntheoretical importance of depth (\nbengio and lecun 2007 delalleau and bengio\n,\n,\n2011 pascanu\n). at this time, deep neural\nnetworks outperformed competing ai systems based on other machine learning\ntechnologies as well as hand-designed functionality. this third wave of popularity\nof neural networks continues to the time of this writing, though the focus of deep\nlearning research has changed dramatically within the time of this wave. the\nthird wave began with a focus on new unsupervised learning techniques and the\nability of deep models to generalize well from small datasets, but today there is\nmore interest in much older supervised learning algorithms and the ability of deep\nmodels to leverage large labeled datasets.\n\n2014a montufar\n\net al.,\n\net al.,\n\n2014\n\n;\n\n;\n\n;\n\n1.2.2\n\nincreasing dataset sizes\n\none may wonder why deep learning has only recently become recognized as a\ncrucial technology though the first experiments with artificial neural networks were\nconducted in the 1950s. deep learning has been successfully used in commercial\napplications since the 1990s, but was often regarded as being more of an art than\na technology and something that only an expert could use, until recently. it is true\nthat some skill is required to get good performance from a deep learning algorithm.\nfortunately, the amount of skill required reduces as the amount of training data\nincreases. the learning algorithms reaching human performance on complex tasks\ntoday are nearly identical to the learning algorithms that struggled to solve toy\nproblems in the 1980s, though the models we train with these algorithms have\nundergone changes that simplify the training of very deep architectures. the most\nimportant new development is that today we can provide these algorithms with\nthe resources they need to succeed. fig.\nshows how the size of benchmark\ndatasets has increased remarkably over time. this trend is driven by the increasing\ndigitization of society. as more and more of our activities take place on computers,\nmore and more of what we do is recorded. as our computers are increasingly\nnetworked together, it becomes easier to centralize these records and curate them\n\n1.8\n\n19\n\n "}, {"Page_number": 35, "text": "chapter 1.\n\nintroduction\n\ninto a dataset appropriate for machine learning applications. the age of \u201cbig\ndata\u201d has made machine learning much easier because the key burden of statistical\nestimation\u2014generalizing well to new data after observing only a small amount\nof data\u2014has been considerably lightened. as of 2016, a rough rule of thumb\nis that a supervised deep learning algorithm will generally achieve acceptable\nperformance with around 5,000 labeled examples per category, and will match or\nexceed human performance when trained with a dataset containing at least 10\nmillion labeled examples. working successfully with datasets smaller than this is\nan important research area, focusing in particular on how we can take advantage\nof large quantities of unlabeled examples, with unsupervised or semi-supervised\nlearning.\n\n1.2.3\n\nincreasing model sizes\n\nanother key reason that neural networks are wildly successful today after enjoying\ncomparatively little success since the 1980s is that we have the computational\nresources to run much larger models today. one of the main insights of connection-\nism is that animals become intelligent when many of their neurons work together.\nan individual neuron or small collection of neurons is not particularly useful.\n\nbiological neurons are not especially densely connected. as seen in fig.\n\n1.10\n,\nour machine learning models have had a number of connections per neuron that\nwas within an order of magnitude of even mammalian brains for decades.\n\n1.11\n\nin terms of the total number of neurons, neural networks have been astonishingly\nsmall until quite recently, as shown in fig.\n. since the introduction of hidden\nunits, artificial neural networks have doubled in size roughly every 2.4 years. this\ngrowth is driven by faster computers with larger memory and by the availability\nof larger datasets. larger networks are able to achieve higher accuracy on more\ncomplex tasks. this trend looks set to continue for decades. unless new technologies\nallow faster scaling, artificial neural networks will not have the same number of\nneurons as the human brain until at least the 2050s. biological neurons may\nrepresent more complicated functions than current artificial neurons, so biological\nneural networks may be even larger than this plot portrays.\n\nin retrospect, it is not particularly surprising that neural networks with fewer\nneurons than a leech were unable to solve sophisticated artificial intelligence prob-\nlems. even today\u2019s networks, which we consider quite large from a computational\nsystems point of view, are smaller than the nervous system of even relatively\nprimitive vertebrate animals like frogs.\n\nthe increase in model size over time, due to the availability of faster cpus,\n\n20\n\n "}, {"Page_number": 36, "text": "chapter 1.\n\nintroduction\n\n)\ns\ne\nl\n\np\nm\na\nx\ne\n\u00a0\nr\ne\nb\nm\nu\nn\n(\n\u00a0\ne\nz\ni\ns\n\u00a0\nt\ne\ns\na\nt\na\nd\n\n109\n108\n107\n106\n105\n104\n103\n102\n101\n100\n\nincreasing\u00a0dataset\u00a0size\u00a0over\u00a0time\n\ncanadian\u00a0hansard\n\nwmt\n\nsports-1m\n\nimagenet10k\n\npublic\u00a0svhn\n\ncriminals\n\nimagenet\n\nilsvrc\u00a02014\n\nmnist\n\ncifar-10\n\niris\n\n1900\n\nt\u00a0vs\u00a0g\u00a0vs\u00a0f\n\nrotated\u00a0t\u00a0vs\u00a0c\n\n1950\n\nyear\n\n1985 2000 2015\n\n;\n\n,\n\n;\n\n,\n\n;\n\n,\n\n;\n\n,\n\n1986b\n\net al.,\n\nfigure 1.8: dataset sizes have increased greatly over time. in the early 1900s, statisticians\nstudied datasets using hundreds or thousands of manually compiled measurements (\ngarson\n,\n). in the 1950s through 1980s, the pioneers\n1900 gosset 1908 anderson 1935 fisher 1936\nof biologically inspired machine learning often worked with small, synthetic datasets, such\nas low-resolution bitmaps of letters, that were designed to incur low computational cost and\ndemonstrate that neural networks were able to learn specific kinds of functions (widrow\nand hoff 1960 rumelhart\n). in the 1980s and 1990s, machine learning\nbecame more statistical in nature and began to leverage larger datasets containing tens\nof thousands of examples such as the mnist dataset (shown in fig.\n) of scans of\nin the first decade of the 2000s,\u00a0more\nhandwritten numbers (\nsophisticated datasets of this same size, such as the cifar-10 dataset (krizhevsky and\nhinton 2009\n) continued to be produced. toward the end of that decade and throughout\nthe first half of the 2010s, significantly larger datasets, containing hundreds of thousands\nto tens of millions of examples, completely changed what was possible with deep learning.\nthese datasets included the public street view house numbers dataset (\nnetzer et al.\n,\n,\n2011), various versions of the imagenet dataset (\ndeng et al. 2009 2010a russakovsky\n;\n2014\n). at the top of the\n,\net al.\ngraph, we see that datasets of translated sentences, such as ibm\u2019s dataset constructed\nfrom the canadian hansard (\n) and the wmt 2014 english to french\ndataset (schwenk 2014\n\n) are typically far ahead of other dataset sizes.\n\n), and the sports-1m dataset (\n\nlecun et al. 1998b\n\nbrown et al. 1990\n\n,\n,\net al.\n\nkarpathy\n\n2014a\n\n1.9\n\n).\n\n,\n\n,\n\n,\n\n,\n\n21\n\n "}, {"Page_number": 37, "text": "chapter 1.\n\nintroduction\n\nfigure 1.9: example inputs from the mnist dataset. the \u201cnist\u201d stands for national\ninstitute of standards and technology, the agency that originally collected this data.\nthe \u201cm\u201d stands for \u201cmodified,\u201d since the data has been preprocessed for easier use with\nmachine learning algorithms. the mnist dataset consists of scans of handwritten digits\nand associated labels describing which digit 0-9 is contained in each image. this simple\nclassification problem is one of the simplest and most widely used tests in deep learning\nresearch. it remains popular despite being quite easy for modern techniques to solve.\ngeoffrey hinton has described it as \u201cthe drosophila of machine learning,\u201d meaning that\nit allows machine learning researchers to study their algorithms in controlled laboratory\nconditions, much as biologists often study fruit flies.\n\n22\n\n "}, {"Page_number": 38, "text": "chapter 1.\n\nintroduction\n\nthe advent of general purpose gpus (described in sec.\n), faster network\nconnectivity and better software infrastructure for distributed computing, is one of\nthe most important trends in the history of deep learning. this trend is generally\nexpected to continue well into the future.\n\n12.1.2\n\n1.2.4\n\nincreasing accuracy, complexity and real-world impact\n\nsince the 1980s, deep learning has consistently improved in its ability to provide\naccurate recognition or prediction. moreover, deep learning has consistently been\napplied with success to broader and broader sets of applications.\n\n,\n\n,\n\nrumelhart et al. 1986a\n\nthe earliest deep models were used to recognize individual objects in tightly\ncropped, extremely small images (\n). since then there has\nbeen a gradual increase in the size of images neural networks could process. modern\nobject recognition networks process rich high-resolution photographs and do not\nhave a requirement that the photo be cropped near the object to be recognized\n(\nkrizhevsky et al. 2012\n). similarly, the earliest networks could only recognize\ntwo kinds of objects (or in some cases, the absence or presence of a single kind of\nobject), while these modern networks typically recognize at least 1,000 different\ncategories of objects.\u00a0the largest contest in object recognition is the imagenet\nlarge-scale visual recognition challenge (ilsvrc) held each year. a dramatic\nmoment in the meteoric rise of deep learning came when a convolutional network\nwon this challenge for the first time and by a wide margin, bringing down the\nstate-of-the-art top-5 error rate from 26.1% to 15.3% (\nkrizhevsky et al. 2012\n),\nmeaning that the convolutional network produces a ranked list of possible categories\nfor each image and the correct category appeared in the first five entries of this\nlist for all but 15.3% of the test examples. since then, these competitions are\nconsistently won by deep convolutional nets, and as of this writing, advances in\ndeep learning have brought the latest top-5 error rate in this contest down to 3.6%,\nas shown in fig.\n\n.\n1.12\n\n,\n\ndeep learning has also had a dramatic impact on speech recognition. after\nimproving throughout the 1990s, the error rates for speech recognition stagnated\ndahl et al. 2010 deng\nstarting in about 2000. the introduction of deep learning (\net al.\n) to speech recognition resulted\n,\nin a sudden drop of error rates, with some error rates cut in half. we will explore\nthis history in more detail in sec.\n\n2011 hinton\n\n2010b seide\n\net al.\n,\n\net al.\n,\n\n2012a\n\n.\n12.3\n\n,\n\n;\n\n;\n\n;\n\ndeep networks have also had spectacular successes for pedestrian detection and\net al.,\nimage segmentation (\n2013) and yielded superhuman performance in traffic sign classification (ciresan\n\nsermanet et al. 2013 farabet\n\n2013 couprie\n\net al.,\n\n,\n\n;\n\n;\n\n23\n\n "}, {"Page_number": 39, "text": "chapter 1.\n\nintroduction\n\nn\no\nr\nu\ne\nn\n\u00a0\nr\ne\np\n\u00a0\ns\nn\no\ni\nt\nc\ne\nn\nn\no\nc\n\n104\n\n103\n\n102\n\n101\n\nnumber\u00a0of\u00a0connections\u00a0per\u00a0neuron\u00a0over\u00a0time\n\n2\n\n9\n\n7\n\n10\n\n6\n\n4\n\n5\n\n8\n\n3\n\n1985\n\nyear\n\n2000\n\n2015\n\n1\n\n1950\n\nhuman\n\ncat\n\nmouse\n\nfruit\u00a0fly\n\nfigure 1.10: initially, the number of connections between neurons in artificial neural\nnetworks was limited by hardware capabilities. today, the number of connections between\nneurons is mostly a design consideration. some artificial neural networks have nearly as\nmany connections per neuron as a cat, and it is quite common for other neural networks\nto have as many connections per neuron as smaller mammals like mice. even the human\nbrain does not have an exorbitant amount of connections per neuron. biological neural\nnetwork sizes from\n\nwikipedia 2015\n\n).\n\n(\n\n1. adaptive linear element (\n\nwidrow and hoff 1960\n)\n\n,\n\n2. neocognitron (fukushima 1980\n)\n\n,\n\n3. gpu-accelerated convolutional network (\n\nchellapilla et al. 2006\n)\n\n,\n\n4. deep boltzmann machine (salakhutdinov and hinton 2009a\n)\n\n,\n\n5. unsupervised convolutional network (\n\njarrett et al. 2009\n)\n\n,\n\n6. gpu-accelerated multilayer perceptron (\n\nciresan et al. 2010\n)\n\n,\n\n7. distributed autoencoder (\n\nle et al. 2012\n)\n\n,\n\n8. multi-gpu convolutional network (\n\nkrizhevsky et al. 2012\n)\n\n,\n\n9. cots hpc unsupervised convolutional network (\n\ncoates et al. 2013\n)\n\n,\n\n10. googlenet (\n\nszegedy et al. 2014a\n)\n\n,\n\n24\n\n "}, {"Page_number": 40, "text": "chapter 1.\n\nintroduction\n\net al.,\n\n2012\n\n).\n\nat the same time that the scale and accuracy of deep networks has increased,\nso has the complexity of the tasks that they can solve.\ngoodfellow et al. 2014d\n)\nshowed that neural networks could learn to output an entire sequence of characters\ntranscribed from an image, rather than just identifying a single object. previously,\nit was widely believed that this kind of learning required labeling of the individual\nelements of the sequence (\n). recurrent neural networks,\nsuch as the lstm sequence model mentioned above, are now used to model\nrelationships between sequences\nrather than just fixed inputs.\nthis sequence-to-sequence learning seems to be on the cusp of revolutionizing\nanother application: machine translation (sutskever\net al.,\n2015).\n\ng\u00fcl\u00e7ehre and bengio 2013\n\n2014 bahdanau\n\nand other\n\nsequences\n\net al.,\n\n(\n\n;\n\n,\n\nthis trend of increasing complexity has been pushed to its logical conclusion\nwith the introduction of neural turing machines (graves\n) that learn\nto read from memory cells and write arbitrary content to memory cells. such\nneural networks can learn simple programs from examples of desired behavior. for\nexample, they can learn to sort lists of numbers given examples of scrambled and\nsorted sequences. this self-programming technology is in its infancy, but in the\nfuture could in principle be applied to nearly any task.\n\net al.,\n\n2014a\n\nanother crowning achievement of deep learning is its extension to the domain\nof reinforcement learning. in the context of reinforcement learning, an autonomous\nagent must learn to perform a task by trial and error, without any guidance from\nthe human operator. deepmind demonstrated that a reinforcement learning system\nbased on deep learning is capable of learning to play atari video games, reaching\nhuman-level performance on many tasks (\n). deep learning has\nalso significantly improved the performance of reinforcement learning for robotics\n(\nfinn et al. 2015\n\nmnih et al. 2015\n\n).\n\n,\n\n,\n\nmany of these applications of deep learning are highly profitable. deep learning\nis now used\u00a0by many top technology companies\u00a0including google,\u00a0microsoft,\nfacebook, ibm, baidu, apple, adobe, netflix, nvidia and nec.\n\nadvances in deep learning have also depended heavily on advances in software\nbergstra et al. 2010 bastien\n),\n), and\n) have all supported important research projects or\n\ninfrastructure. software libraries such as theano (\net al.\n2012\n,\ndistbelief (\ntensorflow (\ncommercial products.\n\ncollobert et al. 2011b\nchen et al. 2015\n\n), pylearn2 (\ndean et al. 2012\n,\n\n), torch (\n), mxnet (\n\nabadi et al. 2015\n\net al.\n,\n,\n\ngoodfellow\n\n), caffe (\n\njia 2013\n\n2013c\n\n,\n\n;\n\n,\n\n,\n\n,\n\ndeep learning has also made contributions back to other sciences. modern\nconvolutional networks for object recognition provide a model of visual processing\n\n25\n\n "}, {"Page_number": 41, "text": "chapter 1.\n\nintroduction\n\n,\n\ndicarlo 2013\n\nthat neuroscientists can study (\n). deep learning also provides useful\ntools for processing massive amounts of data and making useful predictions in\nscientific fields. it has been successfully used to predict how molecules will interact\nin order to help pharmaceutical companies design new drugs (\n),\nto search for subatomic particles (\n), and to automatically parse\nmicroscope images used to construct a 3-d map of the human brain (knowles-\nbarley\n). we expect deep learning to appear in more and more scientific\nfields in the future.\n\nbaldi et al. 2014\n\ndahl et al. 2014\n\net al.,\n\n2014\n\n,\n\n,\n\nin summary, deep learning is an approach to machine learning that has drawn\nheavily on our knowledge of the human brain, statistics and applied math as it\ndeveloped over the past several decades. in recent years, it has seen tremendous\ngrowth in its popularity and usefulness, due in large part to more powerful com-\nputers, larger datasets and techniques to train deeper networks. the years ahead\nare full of challenges and opportunities to improve deep learning even further and\nbring it to new frontiers.\n\n26\n\n "}, {"Page_number": 42, "text": "chapter 1.\n\nintroduction\n\n)\ne\nl\na\nc\ns\n\nc\ni\nm\nh\nt\ni\nr\na\ng\no\nl\n(\n\ns\nn\no\nr\nu\ne\nn\n\nf\no\n\nr\ne\nb\nm\nu\nn\n\n1011\n1010\n109\n108\n107\n106\n105\n104\n103\n102\n101\n100\n10\u22121\n10\u22122\n\nincreasing neural network size over time\n\n16\n\n19\n\n20\n\n18\n\n17\n\n14\n\n11\n\n8\n\n3\n\n1\n\n2\n\n1950\n\n13\n\n15\n\n12\n\n10\n\n6\n\n4\n\n1985\n\n2000\nyear\n\n5\n\n9\n\n7\n\n2015\n\nhuman\n\noctopus\n\nfrog\n\nbee\n\nant\n\nleech\n\nroundworm\n\nsponge\n\n2056\n\nfigure 1.11: since the introduction of hidden units, artificial neural networks have doubled\nin size roughly every 2.4 years. biological neural network sizes from\n\nwikipedia 2015\n\n).\n\n(\n\n1. perceptron (\n\nrosenblatt 1958 1962\n)\n\n,\n\n,\n\n2. adaptive linear element (\n\nwidrow and hoff 1960\n)\n\n,\n\n3. neocognitron (fukushima 1980\n)\n\n,\n\n4. early back-propagation network (\n\nrumelhart et al. 1986b\n)\n\n,\n\n5. recurrent neural network for speech recognition (robinson and fallside 1991\n)\n\n,\n\n6. multilayer perceptron for speech recognition (\n\nbengio et al. 1991\n)\n\n,\n\n7. mean field sigmoid belief network (\n\nsaul et al. 1996\n)\n\n,\n\n8. lenet-5 (\n\nlecun et al. 1998b\n)\n\n,\n\n9. echo state network (\n\njaeger and haas 2004\n)\n\n,\n\n10. deep belief network (\n\nhinton et al. 2006\n)\n\n,\n\n11. gpu-accelerated convolutional network (\n\nchellapilla et al. 2006\n)\n\n,\n\n12. deep boltzmann machine (salakhutdinov and hinton 2009a\n)\n\n,\n\n13. gpu-accelerated deep belief network (\n\nraina et al. 2009\n)\n\n,\n\n14. unsupervised convolutional network (\n\njarrett et al. 2009\n)\n\n,\n\n15. gpu-accelerated multilayer perceptron (\n\nciresan et al. 2010\n)\n\n,\n\n16. omp-1 network (\n\ncoates and ng 2011\n)\n\n,\n\n17. distributed autoencoder (\n\nle et al. 2012\n)\n\n,\n\n18. multi-gpu convolutional network (\n\nkrizhevsky et al. 2012\n)\n\n,\n\n19. cots hpc unsupervised convolutional network (\n\ncoates et al. 2013\n)\n\n,\n\n20. googlenet (\n\nszegedy et al. 2014a\n)\n\n,\n\n27\n\n "}, {"Page_number": 43, "text": "chapter 1.\n\nintroduction\n\ne\nt\na\nr\n\nr\no\nr\nr\ne\n\nn\no\ni\nt\na\nc\nfi\ni\ns\ns\na\nl\nc\n \nc\nr\nv\ns\nl\ni\n\n0 30.\n\n0 25.\n\n0 20.\n\n0 15.\n\n0 10.\n\n0 05.\n\n0 00.\n\n2010\n\ndecreasing error rate over time\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\nyear\n\nfigure 1.12: since deep networks reached the scale necessary to compete in the imagenet\nlarge scale visual recognition challenge, they have consistently won the competition\nevery year, and yielded lower and lower error rates each time.\u00a0data from russakovsky\net al.\n\n(\n2014b\n\n(\n2015\n\n) and\n\net al.\n\nhe\n\n).\n\n28\n\n "}, {"Page_number": 44, "text": "part i\n\napplied math and machine\n\nlearning basics\n\n29\n\n "}, {"Page_number": 45, "text": "this part of the book introduces the basic mathematical concepts needed to\nunderstand deep learning. we begin with general ideas from applied math that\nallow us to define functions of many variables, find the highest and lowest points\non these functions and quantify degrees of belief.\n\nnext, we describe the fundamental goals of machine learning. we describe how\nto accomplish these goals by specifying a model that represents certain beliefs,\ndesigning a cost function that measures how well those beliefs correspond with\nreality and using a training algorithm to minimize that cost function.\n\nthis elementary framework is the basis for a broad variety of machine learning\nalgorithms, including approaches to machine learning that are not deep.\u00a0in the\nsubsequent parts of the book, we develop deep learning algorithms within this\nframework.\n\n30\n\n "}, {"Page_number": 46, "text": "chapter 2\n\nlinear algebra\n\nlinear algebra is a branch of mathematics that is widely used throughout science\nand engineering. however, because linear algebra is a form of continuous rather\nthan discrete mathematics, many computer scientists have little experience with it.\na good understanding of linear algebra is essential for understanding and working\nwith many machine learning algorithms, especially deep learning algorithms. we\ntherefore precede our introduction to deep learning with a focused presentation of\nthe key linear algebra prerequisites.\n\nif you are already familiar with linear algebra, feel free to skip this chapter. if\nyou have previous experience with these concepts but need a detailed reference\nsheet to review key formulas, we recommend the matrix cookbook (petersen and\npedersen 2006\n). if you have no exposure at all to linear algebra, this chapter\nwill teach you enough to read this book, but we highly recommend that you also\nconsult another resource focused exclusively on teaching linear algebra, such as\nshilov 1977\n). this chapter will completely omit many important linear algebra\ntopics that are not essential for understanding deep learning.\n\n(\n\n,\n\n2.1 scalars, vectors, matrices and tensors\n\nthe study of linear algebra involves several types of mathematical objects:\n\n\u2022 scalars: a scalar is just a single number, in contrast to most of the other\nobjects studied in linear algebra, which are usually arrays of multiple numbers.\nwe write scalars in italics. we usually give scalars lower-case variable names.\nwhen we introduce them, we specify what kind of number they are. for\n\n31\n\n "}, {"Page_number": 47, "text": "chapter 2. linear algebra\n\nexample, we might say \u201clet s \u2208 r be the slope of the line,\u201d while defining a\nreal-valued scalar, or \u201clet n \u2208 n be the number of units,\u201d while defining a\nnatural number scalar.\n\n\u2022 vectors: a vector is an array of numbers. the numbers are arranged in\norder. we can identify each individual number by its index in that ordering.\ntypically we give vectors lower case names written in bold typeface, such\nas x. the elements of the vector are identified by writing its name in italic\ntypeface, with a subscript. the first element of x is x1, the second element\nis x2 and so on. we also need to say what kind of numbers are stored in\nthe vector. if each element is in r, and the vector has n elements, then the\nvector lies in the set formed by taking the cartesian product of r n times,\ndenoted as rn. when we need to explicitly identify the elements of a vector,\nwe write them as a column enclosed in square brackets:\n\nx =(cid:27)(cid:28)(cid:28)(cid:28)(cid:29)\n\nx1\nx2\n...\nxn\n\n.\n\n(cid:30)(cid:31)(cid:31)(cid:31)(cid:32)\n\n(2.1)\n\nwe can think of vectors as identifying points in space, with each element\ngiving the coordinate along a different axis.\n\nsometimes we need to index a set of elements of a vector. in this case, we\ndefine a set containing the indices and write the set as a subscript. for\nexample, to access x1, x3 and x6, we define the set s = {1, 3, 6} and write\nxs. we use the \u2212 sign to index the complement of a set. for example x\u22121 is\nthe vector containing all elements of x except for x1, and x\u2212s is the vector\ncontaining all of the elements of\n\nexcept for\n\nx1, x3 and x6 .\n\nx\n\n\u2022 matrices: a matrix is a 2-d array of numbers, so each element is identified by\ntwo indices instead of just one. we usually give matrices upper-case variable\nnames with bold typeface, such as a. if a real-valued matrix a has a height\nof m and a width of n, then we say that a \u2208 rm n\u00d7 . we usually identify\nthe elements of a matrix using its name in italic but not bold font, and the\nindices are listed with separating commas. for example, a1 1,\nis the upper\nleft entry of a and am,n is the bottom right entry. we can identify all of\nthe numbers with vertical coordinate i by writing a \u201c \u201d for the horizontal\ncoordinate. for example, ai,: denotes the horizontal cross section of a with\nvertical coordinate i. this is known as the i-th row of a. likewise, a:,i is\n\n:\n\n32\n\n "}, {"Page_number": 48, "text": "chapter 2. linear algebra\n\na =24\n\na1 1, a1 2,\na2 1, a2 2,\na3 1, a3 2,\n\n35 ) a> =\uf8ff a1 1, a2 1,\n\na1 2, a2 2,\n\na3 1,\n\na3 2, (cid:20)\n\nfigure 2.1: the transpose of the matrix can be thought of as a mirror image across the\nmain diagonal.\n\nthe i-th column of a. when we need to explicitly identify the elements of a\nmatrix, we write them as an array enclosed in square brackets:\n\n(cid:25) a1 1, a1 2,\na2 1, a2 2, (cid:26).\n\n(2.2)\n\nsometimes we may need to index matrix-valued expressions that are not just\na single letter. in this case, we use subscripts after the expression, but do\nnot convert anything to lower case. for example, f (a )i,j gives element (i, j)\nof the matrix computed by applying the function\n\n.\nf a\n\nto\n\n\u2022 tensors: in some cases we will need an array with more than two axes. in\nthe general case, an array of numbers arranged on a regular grid with a\nvariable number of axes is known as a\nwe denote a tensor named \u201ca\u201d\nwith this typeface: a. we identify the element of a at coordinates (i, j, k)\nby writing ai,j,k.\n\ntensor.\n\none important operation on matrices is the transpose. the transpose of a\nmatrix is the mirror image of the matrix across a diagonal line, called the main\ndiagonal, running down and to the right, starting from its upper left corner. see\nfor a graphical depiction of this operation. we denote the transpose of a\nfig.\n2.1\nasa a> , and it is defined such that\nmatrix\n\n(a>)i,j = aj,i.\n\n(2.3)\n\nvectors can be thought of as matrices that contain only one column. the\ntranspose of a vector is therefore a matrix with only one row. sometimes we\n\n33\n\n "}, {"Page_number": 49, "text": "chapter 2. linear algebra\n\ndefine a vector by writing out its elements in the text inline as a row matrix,\nthen using the transpose operator to turn it into a standard column vector, e.g.,\nx = [x1, x2, x3]>.\n\na scalar can be thought of as a matrix with only a single entry. from this, we\n\ncan see that a scalar is its own transpose: a\n\na=  > .\n\nwe can add matrices to each other, as long as they have the same shape, just\n\nby adding their corresponding elements:\n\nc a b\n\n=  +\n\nwhere\n\nci,j = ai,j + bi,j .\n\ni,j + c.\n\nwe can also add a scalar to a matrix or multiply a matrix by a scalar, just\nby performing that operation on each element of a matrix: d = a \u00b7 b + c where\ndi,j = a b\u00b7\n\nin the context of deep learning, we also use some less conventional notation.\nwe allow the addition of matrix and a vector, yielding another matrix: c = a + b,\nwhere ci,j = ai,j + bj. in other words, the vector b is added to each row of the\nmatrix. this shorthand eliminates the need to define a matrix with b copied into\neach row before doing the addition. this implicit copying of b to many locations\nis called broadcasting.\n\n2.2 multiplying matrices and vectors\n\none of the most important operations involving matrices is multiplication of two\nmatrices. the matrix product of matrices a and b is a third matrix c . in order\nfor this product to be defined, a must have the same number of columns as b has\nrows. if a is of shape m n\u00d7 and b is of shape n\np\u00d7 , then c is of shape m p\u00d7 .\nwe can write the matrix product just by placing two or more matrices together,\ne.g.\n\nthe product operation is defined by\n\nc ab= \n\n.\n\nci,j =xk\n\nai,kbk,j.\n\n(2.4)\n\n(2.5)\n\nnote that the standard product of two matrices is\n\njust a matrix containing\nthe product of the individual elements. such an operation exists and is called the\nelement-wise product\n\n, and is denoted as\n\nhadamard product\n\nnot\n\nor\n\n.\n\nthe dot product between two vectors x and y of the same dimensionality is the\nmatrix product x>y. we can think of the matrix product c = ab as computing\nci,j as the dot product between row of\n\nand column\n\nj b\n\ni a\n\nof\n\n.\n\na b(cid:129)\n\n34\n\n "}, {"Page_number": 50, "text": "chapter 2. linear algebra\n\nmatrix product operations have many useful properties that make mathematical\nanalysis\u00a0of matrices\u00a0more convenient. for\u00a0example,\u00a0matrix\u00a0multiplication\u00a0is\ndistributive:\n\na b c\n\n( + ) = \n\nab ac\n\n+\n\nit is also associative:\n\na bc\n\n(\n\n) = (\n\nab c\n\n)\n\n.\n\n.\n\n(2.6)\n\n(2.7)\n\nmatrix multiplication is\nab = ba does not\nalways hold), unlike scalar multiplication. however, the dot product between two\nvectors is commutative:\n\ncommutative (the condition\n\nnot\n\nx>y\n\ny=  >x.\n\nthe transpose of a matrix product has a simple form:\n\n)ab > = b >a>.\n(\n\n(2.8)\n\n(2.9)\n\nthis allows us to demonstrate eq.\nsuch a product is a scalar and therefore equal to its own transpose:\n\n, by exploiting the fact that the value of\n\n2.8\n\nx>y =(cid:21)x>y(cid:22)> = y> x.\n\n(2.10)\n\nsince the focus of this textbook is not linear algebra, we do not attempt to\ndevelop a comprehensive list of useful properties of the matrix product here, but\nthe reader should be aware that many more exist.\n\nwe now know enough linear algebra notation to write down a system of linear\n\nequations:\n\nax b= \n\n(2.11)\nwhere a \u2208 rm n\u00d7 is a known matrix, b \u2208 rm is a known vector, and x \u2208 rn is a\nvector of unknown variables we would like to solve for. each element xi of x is one\nof these unknown variables. each row of a and each element of b provide another\nconstraint. we can rewrite eq.\n\n2.11\n\nas:\n\na1 :, x = b1\n\na2 :, x = b2\n\n. . .\n\nam,:x = bm\n\nor, even more explicitly, as:\n\na1 1, x1 + a 1 2, x2 +\n\n+\u00b7\u00b7\u00b7 a1,nx n = b1\n\n35\n\n(2.12)\n\n(2.13)\n\n(2.14)\n\n(2.15)\n\n(2.16)\n\n "}, {"Page_number": 51, "text": "chapter 2. linear algebra\n\n(cid:27)(cid:29)\n\n1 0 0\n0 1 0\n0 0 1\n\n(cid:30)(cid:32)\n\nfigure 2.2:\n\nexample identity matrix\n\n: this is\n\ni 3.\n\na2 1, x1 + a 2 2, x2 +\n\na m,1x1 + am,2x2 +\n\n+\u00b7\u00b7\u00b7 a2,nx n = b2\n. . .\n+\u00b7\u00b7\u00b7 am,nxn = bm .\n\n(2.17)\n\n(2.18)\n\n(2.19)\n\nmatrix-vector product notations provides a more compact representation for\n\nequations of this form.\n\n2.3\n\nidentity and inverse matrices\n\nlinear algebra offers a powerful tool called\nfor many values of\nanalytically solve eq.\n\n2.11\n\nmatrix inversion\n.\na\n\nthat allows us to\n\nto describe matrix inversion, we first need to define the concept of an identity\nmatrix. an identity matrix is a matrix that does not change any vector when we\nmultiply that vector by that matrix. we denote the identity matrix that preserves\nn-dimensional vectors as in. formally, i n \u2208 rn n\u00d7 , and\n\n\u2200 \u2208x rn, in x x= \n.\n\n(2.20)\n\nthe structure of the identity matrix is simple: all of the entries along the main\ndiagonal are 1, while all of the other entries are zero. see fig.\nfor an example.\nmatrix inverse a is denoted as a\u22121, and it is defined as the matrix\n\nthe\n\n2.2\n\nof\n\nsuch that\n\na\u22121 a i=  n .\n\nwe can now solve eq.\n\n2.11\n\nby the following steps:\n\nax b= \n\na\u22121ax a=  \u22121b\n\nin x a=  \u22121b\n\n36\n\n(2.21)\n\n(2.22)\n\n(2.23)\n\n(2.24)\n\n "}, {"Page_number": 52, "text": "chapter 2. linear algebra\n\nx a=  \u22121b.\n\n(2.25)\n\nof course, this depends on it being possible to find a\u22121. we discuss the\n\nconditions for the existence of a\u22121 in the following section.\n\nwhen a\u22121 exists, several different algorithms exist for finding it in closed form.\nin theory, the same inverse matrix can then be used to solve the equation many\ntimes for different values of b. however, a\u22121 is primarily useful as a theoretical\ntool, and should not actually be used in practice for most software applications.\nbecause a\u22121 can be represented with only limited precision on a digital computer,\nalgorithms that make use of the value of b can usually obtain more accurate\nestimates of\n\n.x\n\n2.4 linear dependence and span\n\n2.11\n\nin order for a\u22121 to exist, eq.\nmust have exactly one solution for every value\nof b. however, it is also possible for the system of equations to have no solutions\nor infinitely many solutions for some values of b. it is not possible to have more\nthan one but less than infinitely many solutions for a particular b; if both x and y\nare solutions then\n\nz\n\n= \u03b1 + (1\n\nx\n\n)\u2212 \u03b1\ny\n\n(2.26)\n\nis also a solution for any real\n\n.\u03b1\n\nto analyze how many solutions the equation has, we can think of the columns\nof a as specifying different directions we can travel from the\n(the point\nspecified by the vector of all zeros), and determine how many ways there are of\nreaching b. in this view, each element of x specifies how far we should travel in\neach of these directions, with xi specifying how far to move in the direction of\ncolumn :i\n\norigin\n\nxia :,i.\n\n(2.27)\n\nax = xi\n\nin general, this kind of operation is called a linear combination. formally, a linear\ncombination of some set of vectors {v (1), . . . , v( )n } is given by multiplying each\nvector v( )i by a corresponding scalar coefficient and adding the results:\n\nciv( )i .\n\nxi\n\n(2.28)\n\nthe span of a set of vectors is the set of all points obtainable by linear combination\nof the original vectors.\n\n37\n\n "}, {"Page_number": 53, "text": "chapter 2. linear algebra\n\ndetermining whether ax = b has a solution thus amounts to testing whether\nb is in the span of the columns of a. this particular span is known as the column\nspace\n\nor the\n\nrange\n\n.a\n\nof\n\nin order for the system ax = b to have a solution for all values of b \u2208 rm,\nwe therefore require that the column space of a be all of rm. if any point in r m\nis excluded from the column space, that point is a potential value of b that has\nno solution. the requirement that the column space of a be all of rm implies\nimmediately that a must have at least m columns, i.e., n m\u2265 .\u00a0otherwise, the\ndimensionality of the column space would be less than m. for example, consider a\n3 \u00d7 2 matrix. the target b is 3-d, but x is only 2-d, so modifying the value of x\nat best allows us to trace out a 2-d plane within r3. the equation has a solution\nif and only if\n\nlies on that plane.\n\nb\n\nhaving n m\u2265\n\nis only a necessary condition for every point to have a solution.\nit is not a sufficient condition, because it is possible for some of the columns to be\nredundant. consider a 2 \u00d7 2 matrix where both of the columns are equal to each\nother. this has the same column space as a 2 \u00d7 1 matrix containing only one copy\nof the replicated column. in other words, the column space is still just a line, and\nfails to encompass all of r 2, even though there are two columns.\n\nformally, this kind of redundancy is known as linear dependence. a set of\nvectors is linearly independent if no vector in the set is a linear combination of the\nother vectors. if we add a vector to a set that is a linear combination of the other\nvectors in the set, the new vector does not add any points to the set\u2019s span. this\nmeans that for the column space of the matrix to encompass all of rm, the matrix\nmust contain at least one set of m linearly independent columns. this condition\nis both necessary and sufficient for eq.\nto have a solution for every value of\nb. note that the requirement is for a set to have exactly m linear independent\ncolumns, not at least m. no set of m-dimensional vectors can have more than m\nmutually linearly independent columns, but a matrix with more than\ncolumns\nmay have more than one such set.\n\n2.11\n\nm\n\nin order for the matrix to have an inverse, we additionally need to ensure that\neq.\nb. to do so, we need to ensure\n2.11\nthat the matrix has at most m columns. otherwise there is more than one way of\nparametrizing each solution.\n\none solution for each value of\n\nat most\n\nhas\n\ntogether, this means that the matrix must be square, that is, we require that\nm = n and that all of the columns must be linearly independent. a square matrix\nwith linearly dependent columns is known as\n\nsingular\n.\n\nif a is not square or is square but singular, it can still be possible to solve the\n\n38\n\n "}, {"Page_number": 54, "text": "chapter 2. linear algebra\n\nequation. however, we can not use the method of matrix inversion to find the\nsolution.\n\nso far we have discussed matrix inverses as being multiplied on the left. it is\n\nalso possible to define an inverse that is multiplied on the right:\n\naa\u22121 = i.\n\n(2.29)\n\nfor square matrices, the left inverse and right inverse are equal.\n\n2.5 norms\n\nsometimes we need to measure the size of a vector. in machine learning, we usually\nlp norm\nmeasure the size of vectors using a function called a\nis given by\n\n. formally, the\n\nnorm\n\n||x p =\u00a0 xi\n\n||\n\np\n\n|xi|p! 1\n\n(2.30)\n\n, p\n\nfor p\n\n\u2208 r \u2265 1\n.\nnorms, including the lp norm, are functions mapping vectors to non-negative\nvalues. on an intuitive level, the norm of a vector x measures the distance from\nthe origin to the point x. more rigorously, a norm is any function f that satisfies\nthe following properties:\n\nf\n\nf\n\nx\n\n( ) +x\n\nx y\n\nx = 0\n\nf ( ) = 0 \n\n\u03b1 r, f \u03b1( x) =  \u03b1 f( )x\n\n( )y (the triangle inequality)\n\n\u21d2\n\u2264\nf ( + ) \n\n\u2022\n\u2022\n\u2022 \u2200 \u2208\nthe l2 norm, with p = 2, is known as the euclidean norm.\u00a0it is simply the\neuclidean distance from the origin to the point identified by x. the l 2 norm is\nused so frequently in machine learning that it is often denoted simply as ||\n||x , with\nomitted. it is also common to measure the size of a vector using\nthe subscript\nthe squared l2 norm, which can be calculated simply as x>x.\n\n2\n\n|\n\n|\n\nthe squared l2 norm is more convenient to work with mathematically and\ncomputationally than the l 2 norm itself. for example, the derivatives of the\nsquared l2 norm with respect to each element of x each depend only on the\ncorresponding element of x, while all of the derivatives of the l2 norm depend\non the entire vector. in many contexts, the squared l2 norm may be undesirable\n\n39\n\n "}, {"Page_number": 55, "text": "chapter 2. linear algebra\n\nbecause it increases very slowly near the origin.\nin several machine learning\napplications, it is important to discriminate between elements that are exactly\nzero and elements that are small but nonzero. in these cases, we turn to a function\nthat grows at the same rate in all locations, but retains mathematical simplicity:\nthe l1 norm. the l1 norm may be simplified to\n\n||\n\n||x 1 =xi\n\n|xi|.\n\n(2.31)\n\nthe l1 norm is commonly used in machine learning when the difference between\nzero and nonzero elements is very important. every time an element of x moves\naway from 0 by , the\n\nl1 norm increases by .(cid:115)\n\n(cid:115)\n\nwe sometimes measure the size of the vector by counting its number of nonzero\nelements. some authors refer to this function as the \u201cl0 norm,\u201d but this is incorrect\nterminology. the number of non-zero entries in a vector is not a norm, because\nscaling the vector by \u03b1 does not change the number of nonzero entries.\u00a0the l 1\nnorm is often used as a substitute for the number of nonzero entries.\n\none other norm that commonly arises in machine learning is the l\u221e norm,\n.\u00a0this norm simplifies to the absolute value of the\n\nmax norm\n\nalso known as the\nelement with the largest magnitude in the vector,\n\n||\n||x \u221e = max\n\ni\n\n|xi|.\n\n(2.32)\n\nsometimes we may also wish to measure the size of a matrix. in the context\nof deep learning, the most common way to do this is with the otherwise obscure\nfrobenius norm\n\na 2\n\ni,j,\n\n(2.33)\n\n||a f =sxi,j\n\n||\n\nwhich is analogous to the l2 norm of a vector.\n\nthe dot product of two vectors can be rewritten in terms of norms. specifically,\n\nwhere\n\n\u03b8\n\nis the angle between\n\nx\n\nx>y\n\nx= ||\nand .\ny\n\n|| 2||\n\n||y 2 cos \u03b8\n\n(2.34)\n\n2.6 special kinds of matrices and vectors\n\nsome special kinds of matrices and vectors are particularly useful.\n\n40\n\n "}, {"Page_number": 56, "text": "chapter 2. linear algebra\n\ndiagonal matrices consist mostly of zeros and have non-zero entries only along\nthe main diagonal.\u00a0formally, a matrix d is diagonal if and only if di,j = 0 for\nall i 6= j .\u00a0we have already seen one example of a diagonal matrix:\u00a0the identity\nmatrix, where all of the diagonal entries are 1. we write diag(v) to denote a square\ndiagonal matrix whose diagonal entries are given by the entries of the vector v.\ndiagonal matrices are of interest in part because multiplying by a diagonal matrix\nis very computationally efficient. to compute diag(v)x, we only need to scale each\nelement xi by vi. in other words, diag(v)x = v x(cid:129) . inverting a square diagonal\nmatrix is also efficient. the inverse exists only if every diagonal entry is nonzero,\nand in that case, diag(v)\u22121 = diag([1/v1, . . . , 1/vn]> ). in many cases, we may\nderive some very general machine learning algorithm in terms of arbitrary matrices,\nbut obtain a less expensive (and less descriptive) algorithm by restricting some\nmatrices to be diagonal.\n\nnot all diagonal matrices need be square. it is possible to construct a rectangular\ndiagonal matrix. non-square diagonal matrices do not have inverses but it is still\npossible to multiply by them cheaply. for a non-square diagonal matrix d, the\nproduct dx will involve scaling each element of x, and either concatenating some\nzeros to the result if d is taller than it is wide, or discarding some of the last\nelements of the vector if\n\nis wider than it is tall.\n\nd\n\na\n\nsymmetric\n\nmatrix is any matrix that is equal to its own transpose:\n\na a=  >.\n\n(2.35)\n\nsymmetric matrices often arise when the entries are generated by some function of\ntwo arguments that does not depend on the order of the arguments. for example,\nif a is a matrix of distance measurements, with ai,j giving the distance from point\ni to point\n\nai,j = aj,i because distance functions are symmetric.\nis a vector with\n\na unit vector\n\n, then\n\nj\n\nunit norm\n:\n||\n||x 2 = 1.\n\n(2.36)\n\na vector x and a vector y are orthogonal to each other if x>y = 0.\u00a0if both\nvectors have nonzero norm, this means that they are at a 90 degree angle to each\nother. in rn, at most n vectors may be mutually orthogonal with nonzero norm.\nif the vectors are not only orthogonal but also have unit norm, we call them\northonormal.\n\nan orthogonal matrix is a square matrix whose rows are mutually orthonormal\n\nand whose columns are mutually orthonormal:\n\na> a aa= \n\n> = i.\n\n41\n\n(2.37)\n\n "}, {"Page_number": 57, "text": "chapter 2. linear algebra\n\nthis implies that\n\na\u22121 = a>,\n\n(2.38)\n\nso orthogonal matrices are of interest because their inverse is very cheap to compute.\npay careful attention to the definition of orthogonal matrices. counterintuitively,\ntheir rows are not merely orthogonal but fully orthonormal. there is no special\nterm for a matrix whose rows or columns are orthogonal but not orthonormal.\n\n2.7 eigendecomposition\n\nmany mathematical objects can be understood better by breaking them into\nconstituent parts, or finding some properties of them that are universal, not caused\nby the way we choose to represent them.\n\nfor example, integers can be decomposed into prime factors. the way we\nrepresent the number\nwill change depending on whether we write it in base ten\nor in binary, but it will always be true that 12 = 2\u00d72\u00d7 3. from this representation\nwe can conclude useful properties, such as that\nis not divisible by , or that any\ninteger multiple of\n\nwill be divisible by .\n3\n\n12\n\n12\n\n12\n\n5\n\nmuch as we can discover something about the true nature of an integer by\ndecomposing it into prime factors, we can also decompose matrices in ways that\nshow us information about their functional properties that is not obvious from the\nrepresentation of the matrix as an array of elements.\n\none of the most widely used kinds of matrix decomposition is called eigen-\ndecomposition, in which we decompose a matrix into a set of eigenvectors and\neigenvalues.\n\nan eigenvector of a square matrix a is a non-zero vector v such that multipli-\n\ncation by\n\na\n\nalters only the scale of\n\nv\n\n:\n\nav\n\nv= \u03bb .\n\n(2.39)\n\ncorresponding to this eigenvector. (one\nthe scalar \u03bb is known as the\ncan also find a left eigenvector such that v>a = \u03bbv >, but we are usually concerned\nwith right eigenvectors).\n\neigenvalue\n\nif v is an eigenvector of a, then so is any rescaled vector sv for s\n\n\u2208 r 6 = 0.\nmoreover, sv still has the same eigenvalue. for this reason, we usually only look\nfor unit eigenvectors.\n\n, s\n\nsuppose that a matrix a has n linearly independent eigenvectors, {v(1), . . . ,\nv( )n }, with corresponding eigenvalues {\u03bb1, . . . , \u03bbn}. we may concatenate all of the\n\n42\n\n "}, {"Page_number": 58, "text": "chapter 2. linear algebra\n\ne\u0000ect\u00a0of\u00a0eigenvectors\u00a0and\u00a0eigenvalues\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n1\nx\n\nbefore\u00a0multiplication\n\nv (1)\n\nv(2)\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n01\nx\n\nafter\u00a0multiplication\n\n\u00b81 v(1)\n\nv (1)\n\n\u00b8 2v(2)\nv(2)\n\n\u22123\n\n\u22123 \u22122 \u22121\n\n0\nx0\n\n1\n\n2\n\n3\n\n\u22123\n\n\u22123 \u22122 \u22121\n\n1\n\n2\n\n3\n\n0\nx00\n\nfigure 2.3: an example of the effect of eigenvectors and eigenvalues. here, we have\na matrix a with two orthonormal eigenvectors, v(1) with eigenvalue \u03bb1 and v(2) with\neigenvalue \u03bb2. (left) we plot the set of all unit vectors u \u2208 r 2 as a unit circle. (right)\nwe plot the set of all points au. by observing the way that a distorts the unit circle, we\ncan see that it scales space in direction v ( )i by \u03bbi.\n\neigenvectors to form a matrix v with one eigenvector per column: v = [v(1), . . . ,\nv( )n ]. likewise, we can concatenate the eigenvalues to form a vector \u03bb = [\u03bb1 , . . . ,\n\u03bbn ]>. the eigendecomposition of\n\nis then given by\n\na\n\na v\n\n=  diag( ) \u22121.\n\n\u03bb v\n\n(2.40)\n\nwe have seen that constructing matrices with specific eigenvalues and eigenvec-\ntors allows us to stretch space in desired directions.\u00a0however, we often want to\ndecompose matrices into their eigenvalues and eigenvectors. doing so can help us\nto analyze certain properties of the matrix, much as decomposing an integer into\nits prime factors can help us understand the behavior of that integer.\n\nnot every matrix can be decomposed into eigenvalues and eigenvectors. in some\n\n43\n\n "}, {"Page_number": 59, "text": "chapter 2. linear algebra\n\ncases, the decomposition exists, but may involve complex rather than real numbers.\nfortunately, in this book, we usually need to decompose only a specific class of\nmatrices that have a simple decomposition. specifically, every real symmetric\nmatrix can be decomposed into an expression using only real-valued eigenvectors\nand eigenvalues:\n\na q q\n\n=  \u03bb >,\n\n(2.41)\n\nwhere q is an orthogonal matrix composed of eigenvectors of a, and \u03bb is a\ndiagonal matrix. the eigenvalue \u03bbi,i is associated with the eigenvector in column i\nof q, denoted as q:,i. because q is an orthogonal matrix, we can think of a as\nscaling space by \u03bbi in direction v( )i . see fig.\n\nfor an example.\n\n2.3\n\nwhile any real symmetric matrix a is guaranteed to have an eigendecomposi-\ntion, the eigendecomposition may not be unique. if any two or more eigenvectors\nshare the same eigenvalue, then any set of orthogonal vectors lying in their span\nare also eigenvectors with that eigenvalue, and we could equivalently choose a q\nusing those eigenvectors instead. by convention, we usually sort the entries of \u03bb\nin descending order. under this convention, the eigendecomposition is unique only\nif all of the eigenvalues are unique.\n\nthe eigendecomposition of\u00a0a matrix\u00a0tells us\u00a0many\u00a0useful facts about\u00a0the\nmatrix. the matrix is singular if and only if any of the eigenvalues are 0. the\neigendecomposition of\u00a0a real symmetric matrix can also be\u00a0used to optimize\nquadratic expressions of the form f (x) = x> ax subject to ||\n||x 2 = 1. whenever x\nis equal to an eigenvector of a, f takes on the value of the corresponding eigenvalue.\nthe maximum value of f within the constraint region is the maximum eigenvalue\nand its minimum value within the constraint region is the minimum eigenvalue.\n\na matrix whose eigenvalues are all positive is called positive definite. a matrix\nwhose eigenvalues are all positive or zero-valued is called positive semidefinite.\nlikewise, if all eigenvalues are negative, the matrix is negative definite, and if\nall eigenvalues are negative or zero-valued, it is negative semidefinite. positive\nsemidefinite matrices are interesting because they guarantee that \u2200x x, >ax \u2265 0.\npositive definite matrices additionally guarantee that x>ax\n\nx\n\n= 0 \u21d2 = 0.\n\n2.8 singular value decomposition\n\n2.7\n\nin sec.\n, we saw how to decompose a matrix into eigenvectors and eigenvalues.\nthe singular value decomposition (svd) provides another way to factorize a matrix,\ninto singular vectors\n. the svd allows us to discover some of\nthe same kind of information as the eigendecomposition. however, the svd is\n\nsingular values\n\nand\n\n44\n\n "}, {"Page_number": 60, "text": "chapter 2. linear algebra\n\nmore generally applicable. every real matrix has a singular value decomposition,\nbut the same is not true of the eigenvalue decomposition. for example, if a matrix\nis not square, the eigendecomposition is not defined, and we must use a singular\nvalue decomposition instead.\n\nrecall that the eigendecomposition involves analyzing a matrix a to discover\na matrix v of eigenvectors and a vector of eigenvalues \u03bb such that we can rewrite\na as\n\na v\n\n=  diag( ) \u22121.\n\n\u03bb v\n\n(2.42)\n\nthe singular value decomposition is similar, except this time we will write a\n\nas a product of three matrices:\n\na u dv\n\n= \n\n>.\n\n(2.43)\n\nm n\u00d7\n\nsuppose that a is an m n\u00d7 matrix. then u is defined to be an m m\u00d7 matrix,\nto be an\n\nmatrix, and\n\nto be an\n\nmatrix.\n\nv\n\nn\n\nd\n\neach of these matrices is defined to have a special structure. the matrices u\nand v are both defined to be orthogonal matrices. the matrix d is defined to be\na diagonal matrix. note that\n\nis not necessarily square.\n\nd\n\nn\u00d7\n\nthe elements along the diagonal of d are known as the\n\nof the\nmatrix a. the columns of u are known as the left-singular vectors. the columns\nof\n\nright-singular vectors.\n\nare known as as the\n\nsingular values\n\nv\n\nwe can actually interpret the singular value decomposition of a in terms of\nthe eigendecomposition of functions of a . the left-singular vectors of a are the\neigenvectors of aa>. the right-singular vectors of a are the eigenvectors of a>a.\nthe non-zero singular values of a are the square roots of the eigenvalues of a>a.\nthe same is true for aa>.\n\nperhaps the most useful feature of the svd is that we can use it to partially\ngeneralize matrix inversion to non-square matrices, as we will see in the next\nsection.\n\n2.9 the moore-penrose pseudoinverse\n\nmatrix inversion is not defined for matrices that are not square. suppose we want\nto make a left-inverse\n\n, so that we can solve a linear equation\n\nof a matrix\n\na\n\nb\n\nax y= \n\n45\n\n(2.44)\n\n "}, {"Page_number": 61, "text": "chapter 2. linear algebra\n\nby left-multiplying each side to obtain\n\nx by= \n\n.\n\n(2.45)\n\ndepending on the structure of the problem, it may not be possible to design a\nunique mapping from to\n\na b\n\n.\n\nif a is taller than it is wide,\u00a0then it is possible for this equation to have\nno solution. if a is wider than it is tall, then there could be multiple possible\nsolutions.\n\nthe moore-penrose pseudoinverse allows us to make some headway in these\n\ncases. the pseudoinverse of\n\na\n\nis defined as a matrix\n\na+ = lim\n\u03b1&0\n\n(a>a\n\ni+ \u03b1 )\u22121a>.\n\n(2.46)\n\npractical algorithms for computing the pseudoinverse are not based on this defini-\ntion, but rather the formula\n\na + = v d +u>,\n\n(2.47)\n\nwhere u, d and v are the singular value decomposition of a , and the pseudoinverse\nd+ of a diagonal matrix d is obtained by taking the reciprocal of its non-zero\nelements then taking the transpose of the resulting matrix.\n\nwhen a has more columns than rows, then solving a linear equation using the\npseudoinverse provides one of the many possible solutions. specifically, it provides\nthe solution x = a+y with minimal euclidean norm ||\n||x 2 among all possible\nsolutions.\n\nwhen a has more rows than columns, it is possible for there to be no solution.\nin this case, using the pseudoinverse gives us the x for which ax is as close as\npossible to\n\nin terms of euclidean norm\n\ny\n\n\u2212 ||\n||\nax y 2.\n\n2.10 the trace operator\n\nthe trace operator gives the sum of all of the diagonal entries of a matrix:\n\ntr(\n\n) =a xi\n\nai,i.\n\n(2.48)\n\nthe trace operator is useful for a variety of reasons. some operations that are\ndifficult to specify without resorting to summation notation can be specified using\n\n46\n\n "}, {"Page_number": 62, "text": "chapter 2. linear algebra\n\nmatrix products and the trace operator. for example, the trace operator provides\nan alternative way of writing the frobenius norm of a matrix:\n\n||a f =qtr(aa> ).\n\n||\n\n(2.49)\n\nwriting an expression in terms of the trace operator opens up opportunities to\nmanipulate the expression using many useful identities.\u00a0for example, the trace\noperator is invariant to the transpose operator:\n\ntr(\n\na\n\n) = tr(\n\na> ).\n\n(2.50)\n\nthe trace of a square matrix composed of many factors is also invariant to\nmoving the last factor into the first position, if the shapes of the corresponding\nmatrices allow the resulting product to be defined:\n\ntr(\n\nabc\n\n) = tr(\n\ncab\n\n) = tr(\n\n)\nbca\n\n(2.51)\n\nor more generally,\n\ntr(\n\nf ( )i ) = tr(f ( )n\n\nf( )i ).\n\n(2.52)\n\nnyi=1\n\nn\u22121yi=1\n\nthis invariance to cyclic permutation holds even if the resulting product has a\ndifferent shape. for example, for a \u2208 rm n\u00d7 and b \u2208 rn m\u00d7 , we have\n\ntr(\n\nab\n\n) = tr(\n\n)\nba\n\n(2.53)\n\neven though ab \u2208 rm m\u00d7 and ba \u2208 rn n\u00d7 .\n\nanother useful fact to keep in mind is that a scalar is its own trace: a = tr(a).\n\n2.11 the determinant\n\nthe determinant of a\u00a0square matrix,\u00a0denoted det (a ),\u00a0is a\u00a0function mapping\nmatrices to\u00a0real scalars. the determinant is equal\u00a0to the product of\u00a0all the\neigenvalues of the matrix. the absolute value of the determinant can be thought\nof as a measure of how much multiplication by the matrix expands or contracts\nspace. if the determinant is 0, then space is contracted completely along at least\none dimension, causing it to lose all of its volume. if the determinant is 1, then\nthe transformation is volume-preserving.\n\n47\n\n "}, {"Page_number": 63, "text": "chapter 2. linear algebra\n\n2.12 example: principal components analysis\n\none simple machine learning algorithm, principal components analysis\nbe derived using only knowledge of basic linear algebra.\n\nor\n\npca\n\ncan\n\nsuppose we have a collection of m points {x(1), . . . , x(\n\n)m } in rn. suppose we\nwould like to apply lossy compression to these points. lossy compression means\nstoring the points in a way that requires less memory but may lose some precision.\nwe would like to lose as little precision as possible.\n\none way we can encode these points is to represent a lower-dimensional version\nof them. for each point x( )i \u2208 rn we will find a corresponding code vector c( )i \u2208 r l.\nif l is smaller than n, it will take less memory to store the code points than the\noriginal data. we will want to find some encoding function that produces the code\nfor an input, f (x) = c, and a decoding function that produces the reconstructed\ninput given its code,\n\nx\n\n\u2248 g f( ( ))\n.\nx\n\ng( ) = \n\nc dc\n\npca is defined by our choice of the decoding function. specifically, to make the\ndecoder very simple, we choose to use matrix multiplication to map the code back\ninto rn . let\n\nd \u2208 rn l\u00d7 is the matrix defining the decoding.\n\ncomputing the optimal code for this decoder could be a difficult problem. to\nkeep the encoding problem easy, pca constrains the columns of d to be orthogonal\nto each other. (note that d is still not technically \u201can orthogonal matrix\u201d unless\nl\n\n, where\n\nn=  )\n\nwith the problem as described so far, many solutions are possible, because we\ncan increase the scale of d:,i if we decrease ci proportionally for all points. to give\nthe problem a unique solution, we constrain all of the columns of\nto have unit\nnorm.\n\nd\n\nin order to turn this basic idea into an algorithm we can implement, the first\nthing we need to do is figure out how to generate the optimal code point c\u2217 for\neach input point x. one way to do this is to minimize the distance between the\ninput point x and its reconstruction, g(c\u2217). we can measure this distance using a\nnorm. in the principal components algorithm, we use the l2 norm:\n\nc\u2217 = arg min\n\nc\n\n||\n|| \u2212\nx g( )c 2 .\n\n(2.54)\n\nwe can switch to the squared l 2 norm instead of the l2 norm itself, because\nboth are minimized by the same value of c . this is because the l2 norm is non-\nnegative and the squaring operation is monotonically increasing for non-negative\n\n48\n\n "}, {"Page_number": 64, "text": "chapter 2. linear algebra\n\narguments.\n\nc\u2217 = arg min\n\nc\n\nx g( )c 2\n|| \u2212\n||\n2 .\n\nthe function being minimized simplifies to\nx \u2212 g c >(\nx \u2212 g c\n( ))\n(\n2.30\n)\n\n(by the definition of the l2 norm, eq.\n\n( ))\n\n= x>x x\u2212 >g\n\n( )c \u2212 ( )c >x\n\ng\n\nc+ (g )>g( )c\n\n(by the distributive property)\n\n= x>x\n\nx\u2212 2 >g\n\n( ) +c\n\n( )c >g( )c\n\ng\n\n(2.55)\n\n(2.56)\n\n(2.57)\n\n(2.58)\n\n(because the scalar g( )x > x is equal to the transpose of itself).\n\nwe can now change the function being minimized again, to omit the first term,\n\nsince this term does not depend on :c\n\nc\u2217 = arg min\n\nc\n\n\u22122x> g\n\n( ) +c\n\n( )c >g\n\ng\n\n.\n( )c\n\nto make further progress, we must substitute in the definition of\n\n:\ng( )c\n\nc\u2217 = arg min\n\nc\n\n= arg min\n\nc\n\n\u22122x>dc\n\u22122x> dc\n\nc+ >d>dc\n\nc+ > ilc\n\n(by the orthogonality and unit norm constraints on\n\n)d\n\n= arg min\n\nc\n\n\u22122x>dc\n\nc+ >c\n\n(2.59)\n\n(2.60)\n\n(2.61)\n\n(2.62)\n\nwe can solve this optimization problem using vector calculus (see sec.\n\n4.3\n\nif\n\nyou do not know how to do this):\n\n\u2207c( 2\u2212 x>dc\n\u2212 2d>x\n\nc+ >c) = 0\n\nc+ 2 = 0\n\nc d=  >x.\n\n(2.63)\n\n(2.64)\n\n(2.65)\n\nthis makes the algorithm efficient:\u00a0we can optimally encode x just using a\n\nmatrix-vector operation. to encode a vector, we apply the encoder function\n\nf( ) = \n\nx d >x.\n\n49\n\n(2.66)\n\n "}, {"Page_number": 65, "text": "chapter 2. linear algebra\n\nusing a further matrix multiplication, we can also define the pca reconstruction\noperation:\n\n( ) = \nx\n\ng f\n\n( ( )) = \n\nx\n\ndd> x.\n\nr\n\n(2.67)\n\nnext, we need to choose the encoding matrix d . to do so, we revisit the\nidea of minimizing the l2 distance between inputs and reconstructions. however,\nsince we will use the same matrix d to decode all of the points, we can no longer\nconsider the points in isolation. instead, we must minimize the frobenius norm of\nthe matrix of errors computed over all dimensions and all points:\n\nd\u2217 = arg min\n\nd sxi,j (cid:21)x( )i\n\nj \u2212 r(x( )i )j(cid:22)2\n\nsubject to d>d i=  l\n\n(2.68)\n\nto derive the algorithm for finding d\u2217, we will start by considering the case\ninto\n\nwhere l = 1. in this case, d is just a single vector, d. substituting eq.\neq.\n\ninto , the problem reduces to\n\nand simplifying\n\n2.67\n\n2.68\n\nd\n\nd\n\nd\u2217 = arg min\n\nd xi\n\n||x( )i \u2212 dd>x( )i ||2\n\n2 subject to ||\n\n||d 2 = 1.\n\n(2.69)\n\nthe above formulation is the most direct way of performing the substitution,\nbut is not the most stylistically pleasing way to write the equation. it places the\nscalar value d>x ( )i on the right of the vector d. it is more conventional to write\nscalar coefficients on the left of vector they operate on. we therefore usually write\nsuch a formula as\n\nd\u2217 = arg min\n\n||x( )i \u2212 d>x( )i d||2\n\n2 subject to ||\n\n||d 2 = 1,\n\nor, exploiting the fact that a scalar is its own transpose, as\n\nd\u2217 = arg min\n\n||x( )i \u2212 x ( )i >dd||2\n\n2 subject to ||\n\n||d 2 = 1.\n\n(2.70)\n\n(2.71)\n\nd xi\nd xi\n\nthe reader should aim to become familiar with such cosmetic rearrangements.\n\nat this point, it can be helpful to rewrite the problem in terms of a single\ndesign matrix of examples, rather than as a sum over separate example vectors.\nthis will allow us to use more compact notation. let x \u2208 rm n\u00d7 be the matrix\ndefined by stacking all of the vectors describing the points, such that xi,: = x ( )i >.\nwe can now rewrite the problem as\n\nd\u2217 = arg min\n\nd\n\n|| \u2212x xdd>||2\n\nf subject to d> d = 1.\n\n(2.72)\n\n50\n\n "}, {"Page_number": 66, "text": "chapter 2. linear algebra\n\ndisregarding the constraint for the moment, we can simplify the frobenius norm\nportion as follows:\n\narg min\n\nd\n\n|| \u2212x xdd>||2\n\nf\n\ntr(cid:23)(cid:21)x xdd\n\n\u2212\n\n>(cid:22)(cid:24)\n>(cid:22)>(cid:21) x xdd\n\n\u2212\n\n(2.73)\n\n(2.74)\n\n= arg min\n\nd\n\n(by eq.\n\n)\n2.49\n\n= arg min\n\nd\n\ntr(x>x x\u2212 >xdd>\u2212 dd>x >x dd+ > x>xdd>)\n\n(2.75)\n\n= arg min\n\nd\n\ntr(x> x) tr(\u2212 x>xdd>) tr(\u2212\n\ndd>x>x) + tr(dd>x >xdd> )\n\n= arg min\n\nd \u2212 tr(x> xdd>) tr(\u2212\n\ndd>x >x) + tr(dd>x >xdd> )\n\n(because terms not involving\n\nd\n\ndo not affect the\n\n)\narg min\n\n= arg min\n\nd \u22122 tr(x>xdd>) + tr(dd>x>xdd>)\n\n(because we can cycle the order of the matrices inside a trace, eq.\n\n2.52\n)\n\n= arg min\n\nd \u22122 tr(x>xdd>) + tr(x> xdd> dd>)\n\n(using the same property again)\n\nat this point, we re-introduce the constraint:\n\narg min\n\nd \u22122 tr(x >xdd>) + tr(x >xdd>dd>) subject to d>d = 1\nd \u22122 tr(x>xdd>) + tr(x >xdd>) subject to d>d = 1\n\n= arg min\n\n(due to the constraint)\n\n= arg min\n\nd \u2212 tr(x>xdd>) subject to d>d = 1\ntr(x >xdd> ) subject to d>d = 1\n\n= arg max\n\nd\n\n= arg max\n\nd\n\ntr(d>x >xd\n\n) subject to >d = 1\n\nd\n\n51\n\n(2.76)\n(2.77)\n\n(2.78)\n\n(2.79)\n\n(2.80)\n\n(2.81)\n\n(2.82)\n\n(2.83)\n\n(2.84)\n\n "}, {"Page_number": 67, "text": "chapter 2. linear algebra\n\nthis optimization problem may be solved using eigendecomposition. specifically,\nthe optimal d is given by the eigenvector of x>x corresponding to the largest\neigenvalue.\n\nin the general case, where l > 1, the matrix d is given by the l eigenvectors\ncorresponding to the largest eigenvalues. this may be shown using proof by\ninduction. we recommend writing this proof as an exercise.\n\nlinear algebra is one of the fundamental mathematical disciplines that is\nnecessary to understand deep learning. another key area of mathematics that is\nubiquitous in machine learning is probability theory, presented next.\n\n52\n\n "}, {"Page_number": 68, "text": "chapter 3\n\nprobability and information\ntheory\n\nin this chapter, we describe probability theory and information theory.\n\nprobability theory is a mathematical framework for representing uncertain\nstatements. it provides a means of quantifying uncertainty and axioms for deriving\nnew uncertain statements. in artificial intelligence applications, we use probability\ntheory in two major ways. first, the laws of probability tell us how ai systems\nshould reason, so we design our algorithms to compute or approximate various\nexpressions derived using probability theory. second, we can use probability and\nstatistics to theoretically analyze the behavior of proposed ai systems.\n\nprobability theory is a fundamental tool of many disciplines of science and\nengineering. we provide this chapter to ensure that readers whose background is\nprimarily in software engineering with limited exposure to probability theory can\nunderstand the material in this book.\n\nwhile probability theory allows us to make uncertain statements and reason\nin the presence of uncertainty, information allows us to quantify the amount of\nuncertainty in a probability distribution.\n\nif you are already familiar with probability theory and information theory,\nyou may wish to skip all of this chapter except for sec.\n, which describes the\ngraphs we use to describe structured probabilistic models for machine learning. if\nyou have absolutely no prior experience with these subjects, this chapter should\nbe sufficient to successfully carry out deep learning research projects, but we do\nsuggest that you consult an additional resource, such as jaynes 2003\n\n3.14\n\n).\n\n(\n\n53\n\n "}, {"Page_number": 69, "text": "chapter 3. probability and information theory\n\n3.1 why probability?\n\nmany branches of computer science deal mostly with entities that are entirely\ndeterministic and certain. a programmer can usually safely assume that a cpu will\nexecute each machine instruction flawlessly. errors in hardware do occur, but are\nrare enough that most software applications do not need to be designed to account\nfor them. given that many computer scientists and software engineers work in a\nrelatively clean and certain environment, it can be surprising that machine learning\nmakes heavy use of probability theory.\n\nthis is because machine learning must always deal with uncertain quantities,\nand sometimes may also need to deal with stochastic (non-deterministic) quantities.\nuncertainty and stochasticity can arise from many sources. researchers have made\ncompelling arguments for quantifying uncertainty using probability since at least\nthe 1980s. many of the arguments presented here are summarized from or inspired\nby pearl 1988\n\n).\n\n(\n\nnearly all activities require some ability to reason in the presence of uncertainty.\nin fact, beyond mathematical statements that are true by definition, it is difficult\nto think of any proposition that is absolutely true or any event that is absolutely\nguaranteed to occur.\n\nthere are three possible sources of uncertainty:\n\n1. inherent stochasticity in the system being modeled. for example, most\ninterpretations of quantum mechanics describe the dynamics of subatomic\nparticles as being probabilistic. we can also create theoretical scenarios that\nwe postulate to have random dynamics, such as a hypothetical card game\nwhere we assume that the cards are truly shuffled into a random order.\n\n2. incomplete observability. even deterministic systems can appear stochastic\nwhen we cannot observe all of the variables that drive the behavior of the\nsystem. for example, in the monty hall problem, a game show contestant is\nasked to choose between three doors and wins a prize held behind the chosen\ndoor. two doors lead to a goat while a third leads to a car.\u00a0the outcome\ngiven the contestant\u2019s choice is deterministic, but from the contestant\u2019s point\nof view, the outcome is uncertain.\n\n3. incomplete modeling. when we use a model that must discard some of\nthe\u00a0information we have\u00a0observed,\u00a0the\u00a0discarded\u00a0information results\u00a0in\nuncertainty in the model\u2019s predictions. for example, suppose we build a\nrobot that can exactly observe the location of every object around it. if the\n\n54\n\n "}, {"Page_number": 70, "text": "chapter 3. probability and information theory\n\nrobot discretizes space when predicting the future location of these objects,\nthen the discretization makes the robot immediately become uncertain about\nthe precise position of objects:\u00a0each object could be anywhere within the\ndiscrete cell that it was observed to occupy.\n\nin many cases, it is more practical to use a simple but uncertain rule rather\nthan a complex but certain one, even if the true rule is deterministic and our\nmodeling system has the fidelity to accommodate a complex rule. for example, the\nsimple rule \u201cmost birds fly\u201d is cheap to develop and is broadly useful, while a rule\nof the form, \u201cbirds fly, except for very young birds that have not yet learned to\nfly, sick or injured birds that have lost the ability to fly, flightless species of birds\nincluding the cassowary, ostrich and kiwi. . . \u201d\u00a0is expensive to develop, maintain and\ncommunicate, and after all of this effort is still very brittle and prone to failure.\n\ngiven that we need a means of representing and reasoning about uncertainty,\nit is not immediately obvious that probability theory can provide all of the tools\nwe want for artificial intelligence applications. probability theory was originally\ndeveloped to analyze the frequencies of events. it is easy to see how probability\ntheory can be used to study events like drawing a certain hand of cards in a\ngame of poker. these kinds of events are often repeatable. when we say that\nan outcome has a probability p of occurring, it means that if we repeated the\nexperiment (e.g., draw a hand of cards) infinitely many times, then proportion p\nof the repetitions would result in that outcome. this kind of reasoning does not\nseem immediately applicable to propositions that are not repeatable. if a doctor\nanalyzes a patient and says that the patient has a 40% chance of having the flu,\nthis means something very different\u2014we can not make infinitely many replicas of\nthe patient, nor is there any reason to believe that different replicas of the patient\nwould present with the same symptoms yet have varying underlying conditions. in\nthe case of the doctor diagnosing the patient, we use probability to represent a\ndegree of belief, with 1 indicating absolute certainty that the patient has the flu\nand 0 indicating absolute certainty that the patient does not have the flu.\u00a0the\nformer kind of probability, related directly to the rates at which events occur, is\nknown as frequentist probability, while the latter, related to qualitative levels of\ncertainty, is known as bayesian probability.\n\nif we list several properties that we expect common sense reasoning about\nuncertainty to have, then the only way to satisfy those properties is to treat\nbayesian probabilities as behaving exactly the same as frequentist probabilities.\nfor example, if we want to compute the probability that a player will win a poker\ngame given that she has a certain set of cards, we use exactly the same formulas\nas when we compute the probability that a patient has a disease given that she\n\n55\n\n "}, {"Page_number": 71, "text": "chapter 3. probability and information theory\n\nhas certain symptoms. for more details about why a small set of common sense\nassumptions implies that the same axioms must control both kinds of probability,\nsee\n\nramsey 1926\n\n).\n\n(\n\nprobability can be seen as the extension of logic to deal with uncertainty. logic\nprovides a set of formal rules for determining what propositions are implied to\nbe true or false given the assumption that some other set of propositions is true\nor false. probability theory provides a set of formal rules for determining the\nlikelihood of a proposition being true given the likelihood of other propositions.\n\n3.2 random variables\n\na random variable is a variable that can take on different values randomly.\u00a0we\ntypically denote the random variable itself with a lower case letter in plain typeface,\nand the values it can take on with lower case script letters. for example, x1 and x2\nare both possible values that the random variable x can take on. for vector-valued\nvariables, we would write the random variable as x and one of its values as x. on\nits own, a random variable is just a description of the states that are possible; it\nmust be coupled with a probability distribution that specifies how likely each of\nthese states are.\n\nrandom variables may be discrete or continuous. a discrete random variable\nis one that has a finite or countably infinite number of states. note that these\nstates are not necessarily the integers; they can also just be named states that\nare not considered to have any numerical value. a continuous random variable is\nassociated with a real value.\n\n3.3 probability distributions\n\na probability distribution is\u00a0a description\u00a0of how likely a random variable\u00a0or\nset of random variables is to take on each of its possible states. the way we\ndescribe probability distributions depends on whether the variables are discrete or\ncontinuous.\n\n3.3.1 discrete variables and probability mass functions\n\na probability distribution over discrete variables may be described using a proba-\nbility mass function (pmf). we typically denote probability mass functions with a\ncapital p . often we associate each random variable with a different probability\n\n56\n\n "}, {"Page_number": 72, "text": "chapter 3. probability and information theory\n\nmass function and the reader must infer which probability mass function to use\nbased on the identity of the random variable, rather than the name of the function;\np\n\n( )x is usually not the same as\n\n( )y .\n\np\n\nthe probability mass function maps from a state of a random variable to\nthe probability of that random variable taking on that state. the probability\nthat x = x is denoted as p (x), with a probability of 1 indicating that x = x is\ncertain and a probability of 0 indicating that x = x is impossible. sometimes\nto disambiguate which pmf to use, we write the name of the random variable\nexplicitly: p (x = x). sometimes we define a variable first, then use \u223c notation to\nspecify which distribution it follows later: x\n\n\u223c p ( )\nx .\n\nprobability mass functions can act on many variables at the same time. such\na probability distribution over many variables is known as a joint probability\ndistribution. p (x = x, y = y) denotes the probability that x = x and y = y\nsimultaneously. we may also write\n\nfor brevity.\n\np x, y\n\n(\n\n)\n\nto be a probability mass function on a random variable x, a function p must\n\nsatisfy the following properties:\n\np\n\n\u2022 the domain of must be the set of all possible states of x.\n\u2022 \u2200 \u2208x\n0 \n\nx, 0 \u2264 p (x) \u2264 1. an impossible event has probability and no state can\nbe less probable than that. likewise, an event that is guaranteed to happen\nhas probability , and no state can have a greater chance of occurring.\n\n1\n\n\u2022 px\u2208x p (x) = 1. we refer to this property as being normalized. without this\n\nproperty, we could obtain probabilities greater than one by computing the\nprobability of one of many events occurring.\n\nfor example, consider a single discrete random variable x with k different states.\nx\u2014that is, make each of its states equally\n\nuniform distribution\n\nwe can place a\nlikely\u2014by setting its probability mass function to\n\non\n\np\n\n( = x\n\nx\n\ni) =\n\n1\nk\n\n(3.1)\n\nfor all i. we can see that this fits the requirements for a probability mass function.\nthe value 1\n\nis a positive integer. we also see that\n\nk is positive because\n\nk\n\np\n\nxi\n\n( = x\n\nx\n\ni) =xi\n\n1\nk\n\n=\n\nk\nk\n\n= 1,\n\n(3.2)\n\nso the distribution is properly normalized.\n\n57\n\n "}, {"Page_number": 73, "text": "chapter 3. probability and information theory\n\n3.3.2 continuous variables and probability density functions\n\nwhen working with continuous random variables, we describe probability dis-\ntributions using a probability density function (pdf) rather than a probability\nmass function. to be a probability density function, a function p must satisfy the\nfollowing properties:\n\np\n\n\u2022 the domain of must be the set of all possible states of x.\n\u2022 \u2200 \u2208\n\nnote that we do not require\n\n\u2264\n( ) \nx\n\n0\n.\n\n1.\n\nx\n\np\n\n\u2265\nx, p x( ) \n( ) = 1.\n\n\u2022 r p x dx\n\na probability density function p(x) does not give the probability of a specific\nstate directly, instead the probability of landing inside an infinitesimal region with\nvolume\n\nis given by\n\n.\np x \u03b4x\n\n( )\n\n\u03b4x\n\nr[\n\nwe can integrate the density function to find the actual probability mass of a\nset of points. specifically, the probability that x lies in some set s is given by the\nintegral of p(x) over that set. in the univariate example, the probability that x\nlies in the interval\n\nis given by\n\n[\n]a, b\n\n.\n]a,b p x dx\n\n( )\n\nfor an example of a probability density function corresponding to a specific\nprobability density over a continuous random variable, consider a uniform distribu-\ntion on an interval of the real numbers. we can do this with a function u(x; a, b),\nwhere a and b are the endpoints of the interval, with b > a. the \u201c;\u201d notation means\n\u201cparametrized by\u201d; we consider x to be the argument of the function, while a and\nb are parameters that define the function. to ensure that there is no probability\nmass outside the interval, we say u(x; a, b) = 0 for all x 6\u2208 [a, b]\n. within a, b],\n. we can see that this is nonnegative everywhere. additionally, it\nu x a, b\n( ;\nintegrates to 1. we often denote that x follows the uniform distribution on [a, b ]\nby writing x\n\n) = 1\nb a\u2212\n\u223c u a, b\n.\n)\n\n(\n\n[\n\n3.4 marginal probability\n\nsometimes we know the probability distribution over a set of variables and we want\nto know the probability distribution over just a subset of them. the probability\ndistribution over the subset is known as the marginal probability distribution.\n\nfor example, suppose we have discrete random variables x and y, and we know\n\np ,(x y . we can find\n\n)\n\nx with the\n\nsum rule\n\n:\n\np\n\n( = x\n\nx,\n\ny =  )\ny .\n\n(3.3)\n\np ( )\n\u2200 \u2208x\n\nx\n\nx\n\n, p ( =  ) =x xy\n\n58\n\n "}, {"Page_number": 74, "text": "chapter 3. probability and information theory\n\nthe name \u201cmarginal probability\u201d comes from the process of computing marginal\nprobabilities on paper. when the values of p (x y,\n) are written in a grid with\ndifferent values of x in rows and different values of y in columns, it is natural to\nsum across a row of the grid, then write p(x) in the margin of the paper just to\nthe right of the row.\n\nfor continuous variables, we need to use integration instead of summation:\n\np x( ) =z p x, y dy.\n\n(\n\n)\n\n(3.4)\n\n3.5 conditional probability\n\nin many cases, we are interested in the probability of some event, given that some\nother event has happened. this is called a conditional probability. we denote\nthe conditional probability that y = y given x = x as p (y = y | x = x). this\nconditional probability can be computed with the formula\n\np\n\n( = y\ny\n\n| x =  ) =\n\nx\n\np\n\n( = y\n\ny,\n\nx =  )\nx\nx\n)\n\np\n\n( = x\n\n.\n\n(3.5)\n\nthe conditional probability is only defined when p(x = x) > 0. we cannot compute\nthe conditional probability conditioned on an event that never happens.\n\nit is important not to confuse conditional probability with computing what\nwould happen if some action were undertaken. the conditional probability that\na person is from germany given that they speak german is quite high, but if\na randomly selected person is taught to speak german, their country of origin\ndoes not change. computing the consequences of an action is called making an\nintervention query. intervention queries are the domain of causal modeling, which\nwe do not explore in this book.\n\n3.6 the chain rule of conditional probabilities\n\nany joint probability distribution over many random variables may be decomposed\ninto conditional distributions over only one variable:\n\np (x(1), . . . , x( )n ) = \n\n(p x(1))\u03c0n\n\ni=2p (x( )i\n\nthis observation is known as the\n\nchain rule\n\nor\n\n1)\n\ni\u2212 ).\n\n| x (1), . . . , x(\nproduct rule of probability. it\n. for\n\n(3.6)\n\n3.5\n\nfollows immediately from the definition of conditional probability in eq.\n\n59\n\n "}, {"Page_number": 75, "text": "chapter 3. probability and information theory\n\nexample, applying the definition twice, we get\n\np ,\n\n(a b c) =\n\n,\n\n(b c) =\np ,\n(a b c) =\n,\n\np ,\n\np\n\np\np\n\n(b c)\n\nc)\n, p ,\n\n(a b|\nb c|\n( )c\n(\np\n)\n(a b|\nb c|\nc)\n(\n, p\n)\np\n\n( )c\n.\n\n3.7\n\nindependence and conditional independence\n\ntwo random variables x and y are independent if their probability distribution can\nbe expressed as a product of two factors, one involving only x and one involving\nonly y:\n\n\u2200 \u2208\nx\n\nx, y\n\n\u2208\n\ny\n\n, p( = x, =  ) =  (\n\np =  ) (x p =  )y .\n\nx\n\ny\n\nx\n\ny\n\ny\n\n(3.7)\n\ntwo random variables x and y are conditionally independent given a random\nvariable z if the conditional probability distribution over x and y factorizes in this\nway for every value of z:\n\n\u2200 \u2208\nx\n\nx, y\n\ny, z\n\n\u2208\n\n\u2208\n\n|\nz\n, p( = x, = y z\n\nx\n\ny\n\n=  ) =  (\n\nz\n\nx\n\n|\np = x z\n\n=  ) (\n\nz p = y z =  )z .\n\ny\n\n|\n\n(3.8)\n\nwe\u00a0can denote independence\u00a0and conditional independence\u00a0with compact\nnotation: x y\u22a5 means that x and y are independent, while x y z\u22a5 | means that x\nand y are conditionally independent given z.\n\n3.8 expectation, variance and covariance\n\nor\n\nthe expectation\nof some function f(x ) with respect to a probability\ndistribution p (x) is the average or mean value that f takes on when x is drawn\nfrom . for discrete variables this can be computed with a summation:\n\nexpected value\n\np\n\nex\u223cp [ ( )] =\n\nf x xx\n\np x f x ,\n( ) ( )\n\n(3.9)\n\nwhile for continuous variables, it is computed with an integral:\n\nex\u223cp[ ( )] =\n\nf x\n\nz p x f x dx.\n\n( ) ( )\n\n60\n\n(3.10)\n\n "}, {"Page_number": 76, "text": "chapter 3. probability and information theory\n\nwhen the identity of the distribution is clear from the context, we may simply\nwrite the name of the random variable that the expectation is over, as in ex[f(x)].\nif it is clear which random variable the expectation is over, we may omit the\nsubscript entirely, as in e[f (x)]. by default, we can assume that e[\u00b7] averages over\nthe values of all the random variables inside the brackets. likewise, when there is\nno ambiguity, we may omit the square brackets.\n\nexpectations are linear, for example,\n\nex[\n\n\u03b1f x\n\n( ) + ( )] = \n\n\u03b2g x\n\n\u03b1ex[ ( )] +\n\nf x\n\n\u03b2ex[ ( )]\ng x ,\n\n(3.11)\n\nwhen\n\n\u03b1\n\nand\n\n\u03b2\n\nare not dependent on .\nx\n\nthe variance gives a measure of how much the values of a function of a random\nvariable x vary as we sample different values of x from its probability distribution:\n\nvar( ( )) = \n\nf x\n\neh( ( )\nf x \u2212 e f x 2i .\n\n[ ( )])\n\n(3.12)\n\nwhen the variance is low, the values of f(x) cluster near their expected value. the\nsquare root of the variance is known as the standard deviation.\n\nthe covariance gives some sense of how much two values are linearly related to\n\neach other, as well as the scale of these variables:\n\ncov( ( )\n\nf x , g y\n\n( )) = \n\ne f x \u2212 e f x\n[( ( )\n\n[ ( )]) ( ( )\n\ng y \u2212 e g y\n\n[ ( )])]\n\n.\n\n(3.13)\n\nhigh absolute values of the covariance mean that the values change very much\nand are both far from their respective means at the same time. if the sign of the\ncovariance is positive, then both variables tend to take on relatively high values\nsimultaneously. if the sign of the covariance is negative, then one variable tends to\ntake on a relatively high value at the times that the other takes on a relatively low\nvalue and vice versa. other measures such as correlation normalize the contribution\nof each variable in order to measure only how much the variables are related, rather\nthan also being affected by the scale of the separate variables.\n\nthe notions of covariance and dependence are related, but are in fact distinct\nconcepts. they are related because two variables that are independent have zero\ncovariance, and two variables that have non-zero covariance are dependent. how-\never, independence is a distinct property from covariance. for two variables to have\nzero covariance, there must be no linear dependence between them. independence\nis a stronger requirement than zero covariance, because independence also excludes\nnonlinear relationships. it is possible for two variables to be dependent but have\nzero covariance. for example, suppose we first sample a real number x from a\nuniform distribution over the interval [\u22121,1]. we next sample a random variable\n\n61\n\n "}, {"Page_number": 77, "text": "chapter 3. probability and information theory\n\ns. with probability 1\n2, we choose the value of s to be . otherwise, we choose\nthe value of s to be \u2212 1. we can then generate a random variable y by assigning\ny = sx. clearly, x and y are not independent, because x completely determines\nthe magnitude of\n\n. however,\n\n) = 0\n\ncov(\n\nx, y\n\n1\n\ny\n\n.\n\nthe covariance matrix of a random vector x \u2208 rn is an n n\u00d7 matrix, such that\n(3.14)\n\ncov( )x i,j = cov(xi, x j).\n\nthe diagonal elements of the covariance give the variance:\n\ncov(xi, xi) = var(xi ).\n\n(3.15)\n\n3.9 common probability distributions\n\nseveral simple probability distributions are useful in many contexts in machine\nlearning.\n\n3.9.1 bernoulli distribution\n\nbernoulli\n\nthe\ndistribution is a distribution over a single binary random variable.\nit is controlled by a single parameter \u03c6 \u2208 [0, 1], which gives the probability of the\nrandom variable being equal to 1. it has the following properties:\n\np\n\nx\n( = 1) = \n\n\u03c6\n\np\n\nx\n( = 0) = 1\n) =  x (1\nx\n\n\u03c6\n\np\n\nx\n( = \n\nex[ ] = \n\nx\n\n\u03c6\n\n\u2212\n)\u2212 \u03c6 1\u2212x\n\u03c6\n\nvar x( ) =  (1\n\nx\n\n\u03c6 \u2212 \u03c6\n)\n\n(3.16)\n\n(3.17)\n\n(3.18)\n\n(3.19)\n\n(3.20)\n\n3.9.2 multinoulli distribution\n\nmultinoulli\n\nthe\ncategorical distribution is a distribution over a single discrete\nvariable with k different states, where k is finite.1 the multinoulli distribution is\n\nor\n\n(\n\n). the multinoulli distribution is a special case of the\n\n1 \u201cmultinoulli\u201d is a term that was recently coined by gustavo lacerdo and popularized by\nmurphy 2012\ndistribution. a\nmultinomial distribution is the distribution over vectors in {0, . . . , n} k representing how many\ntimes each of the k categories is visited when n samples are drawn from a multinoulli distribution.\nmany texts use the term \u201cmultinomial\u201d to refer to multinoulli distributions without clarifying\nthat they refer only to the\n\nmultinomial\n\nn = 1\n\ncase.\n\n62\n\n "}, {"Page_number": 78, "text": "chapter 3. probability and information theory\n\nparametrized by a vector p \u2208 [0, 1]k\u22121 , where pi gives the probability of the i-th\nstate. the final, k-th state\u2019s probability is given by 1\u2212 1>p. note that we must\nconstrain 1>p \u2264 1. multinoulli distributions are often used to refer to distributions\nover categories of objects, so we do not usually assume that state 1 has numerical\nvalue 1, etc. for this reason, we do not usually need to compute the expectation\nor variance of multinoulli-distributed random variables.\n\nthe bernoulli and multinoulli distributions are sufficient to describe any distri-\nbution over their domain. this is because they model discrete variables for which\nit is feasible to simply enumerate all of the states. when dealing with continuous\nvariables, there are uncountably many states, so any distribution described by a\nsmall number of parameters must impose strict limits on the distribution.\n\n3.9.3 gaussian distribution\n\nthe most commonly used distribution over real numbers is the\nalso known as the\n\ngaussian distribution\n:\n\nnormal distribution\n,\n\nn ( ;x \u00b5, \u03c32 ) =r 1\n\n2\u03c0\u03c32 exp(cid:28)\u2212\n\n1\nx\n2\u03c3 2(\n\n\u00b5\u2212 2(cid:29) .\n\n)\n\n(3.21)\n\n3.1\n\nfor a plot of the density function.\n\nsee fig.\nthe two parameters \u00b5 \u2208 r and \u03c3 \u2208 (0,\u221e ) control the normal distribution.\nthe parameter \u00b5 gives the coordinate of the central peak. this is also the mean of\nthe distribution: e[x] = \u00b5. the standard deviation of the distribution is given by\n\u03c3, and the variance by \u03c3 2.\n\nwhen we evaluate the pdf, we need to square and invert \u03c3. when we need to\nfrequently evaluate the pdf with different parameter values, a more efficient way\nof parametrizing the distribution is to use a parameter \u03b2 \u2208 (0,\u221e) to control the\nprecision or inverse variance of the distribution:\n\nn ( ;x \u00b5, \u03b2 \u22121) =r \u03b2\n\n2\u03c0\n\nexp(cid:28)\u2212\n\n1\n2\n\n\u03b2 x\n\n( \u2212 )2(cid:29) .\n\n\u00b5\n\n(3.22)\n\nnormal distributions are a sensible choice for many applications. in the absence\nof prior knowledge about what form a distribution over the real numbers should\ntake, the normal distribution is a good default choice for two major reasons.\n\nfirst, many distributions we wish to model are truly close to being normal\ndistributions. the central limit theorem shows that the sum of many independent\nrandom variables is approximately normally distributed. this means that in\n\n63\n\n "}, {"Page_number": 79, "text": "chapter 3. probability and information theory\n\nthe\u00a0normal\u00a0distribution\n\nmaximum\u00a0at\u00a0x \u00b9=\nin\u0000ection\u00a0points\u00a0at\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0x \u00b9 \u00be\n= \u00a7\n\n)\nx\n(\np\n\n0.40\n0.35\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n\n\u22122.0\n\n\u22121.5\n\n\u22121.0\n\n\u22120.5\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n0.0\nx\n\nthe normal distribution\n\nn (x; \u00b5, \u03c3 2) exhibits a classic\nfigure 3.1:\n\u201cbell curve\u201d shape, with the x coordinate of its central peak given by \u00b5, and the width\nof its peak controlled by \u03c3. in this example, we depict the standard normal distribution,\nwith\n\n: the normal distribution\n\n.\n\u03c3 = 1\n\n\u00b5 = 0\n\nand\n\npractice,\u00a0many complicated systems can be modeled successfully as normally\ndistributed noise, even if the system can be decomposed into parts with more\nstructured behavior.\n\nsecond, out of all possible probability distributions with the same variance,\nthe normal distribution encodes the maximum amount of uncertainty over the\nreal numbers. we can thus think of the normal distribution as being the one that\ninserts the least amount of prior knowledge into a model.\u00a0fully developing and\njustifying this idea requires more mathematical tools, and is postponed to sec.\n19.4.2.\n\nthe normal distribution generalizes to rn, in which case it is known as the\nmultivariate normal distribution. it may be parametrized with a positive definite\nsymmetric matrix\n\n:\u03c3\n\nx \u00b5, \u03c3 s\n\n) =\n\nn ( ;\n\n1\n\n(2 )\u03c0 ndet(\n\n)\u03c3\n\nexp(cid:28)\u2212\n\n1\n2\n\nx \u00b5\u2212 >\u03c3\u22121(\n(\n\n)\n\nx \u00b5\u2212 (cid:29) .\n\n)\n\n(3.23)\n\nthe parameter \u00b5 still gives the mean of the distribution, though now it is\nvector-valued. the parameter \u03c3 gives the covariance matrix of the distribution.\nas in the univariate case, when we wish to evaluate the pdf several times for\n\n64\n\n "}, {"Page_number": 80, "text": "chapter 3. probability and information theory\n\nmany different values of the parameters, the covariance is not a computationally\nefficient way to parametrize the distribution, since we need to invert \u03c3 to evaluate\nthe pdf. we can instead use a precision matrix \u03b2:\n\nn ( ;x \u00b5 \u03b2, \u22121) =sdet( )\u03b2\n\n(2 )\u03c0 n exp(cid:28)\u2212\n\n1\n2\n\n( \u2212 )(cid:29) .\nx \u00b5\u2212 >\u03b2 x \u00b5\n(\n\n)\n\n(3.24)\n\nwe often fix the covariance matrix to be a diagonal matrix. an even simpler\nversion is the isotropic gaussian distribution, whose covariance matrix is a scalar\ntimes the identity matrix.\n\n3.9.4 exponential and laplace distributions\n\nin the context of deep learning, we often want to have a probability distribution\nwith a sharp point at x = 0. to accomplish this, we can use the exponential\ndistribution:\n\n(3.25)\nthe exponential distribution uses the indicator function 1x\u22650 to assign probability\nzero to all negative values of\n\np x \u03bb\n( ; ) =  1x\u22650exp (\n\n)\u2212\u03bbx .\n\n.x\n\n\u03bb\n\na closely related probability distribution that allows us to place a sharp peak\n\nof probability mass at an arbitrary point\n\n\u00b5\n\nis the\n\nlaplace distribution\n\nlaplace( ;\n\nx \u00b5, \u03b3\n\n) =\n\n1\n2\u03b3\n\nexp(cid:28)\u2212| \u2212 |\n\u03b3 (cid:29).\n\n\u00b5\n\nx\n\n(3.26)\n\n3.9.5 the dirac distribution and empirical distribution\n\nin some cases, we wish to specify that all of the mass in a probability distribution\nclusters around a single point. this can be accomplished by defining a pdf using\nthe dirac delta function,\n\n:\n\u03b4 x( )\n\n( ) =  ( \u2212 )\np x\n\u00b5 .\n\n\u03b4 x\n\n(3.27)\n\nthe dirac delta function is defined such that it is zero-valued everywhere except\n0, yet integrates to 1. the dirac delta function is not an ordinary function that\nassociates each value x with a real-valued output, instead it is a different kind of\nmathematical object called a generalized function that is defined in terms of its\nproperties when integrated. we can think of the dirac delta function as being the\nlimit point of a series of functions that put less and less mass on all points other\nthan .\u00b5\n\n65\n\n "}, {"Page_number": 81, "text": "chapter 3. probability and information theory\n\nby defining p( x) to be \u03b4 shifted by \u2212\u00b5 we obtain an infinitely narrow and\n\ninfinitely high peak of probability mass where\n\nx\n\n.\n\u00b5= \n\na common use of the dirac delta distribution is as a component of an empirical\n\ndistribution,\n\n\u02c6p( ) =x\n\n1\nm\n\nmxi=1\n\n\u03b4(x x\u2212 ( )i )\n\n(3.28)\n\nm on each of the m points x(1), . . . , x(\n\nwhich puts probability mass 1\n)m forming\na given data set or collection of samples. the dirac delta distribution is only\nnecessary to define the empirical distribution over continuous variables. for discrete\nvariables, the situation is simpler: an empirical distribution can be conceptualized\nas a multinoulli distribution, with a probability associated to each possible input\nvalue that is simply equal to the empirical frequency of that value in the training\nset.\n\nwe can view the empirical distribution formed from a dataset of training\nexamples as specifying the distribution that we sample from when we train a model\non this dataset.\u00a0another important perspective on the empirical distribution is\nthat it is the probability density that maximizes the likelihood of the training data\n(see sec.\n\n5.5\n\n).\n\n3.9.6 mixtures of distributions\n\nit is also common to define probability distributions by combining other simpler\nprobability distributions. one common\u00a0way of\u00a0combining\u00a0distributions is\u00a0to\nconstruct a mixture distribution. a mixture distribution is made up of several\ncomponent distributions. on each trial, the choice of which component distribution\ngenerates the sample is determined by sampling a component identity from a\nmultinoulli distribution:\n\np ( ) =x xi\n\np\n\nc\n( = \n\ni p\n) (\n\nx c|\n\ni\n= \n)\n\n(3.29)\n\nwhere\n\np ( )\n\nc is the multinoulli distribution over component identities.\n\nwe have already seen one example of a mixture distribution: the empirical\ndistribution over real-valued variables is a mixture distribution with one dirac\ncomponent for each training example.\n\nthe mixture model is one simple strategy for combining probability distributions\n, we explore the art of building complex\n\nto create a richer distribution. in chapter\nprobability distributions from simple ones in more detail.\n\n16\n\n66\n\n "}, {"Page_number": 82, "text": "chapter 3. probability and information theory\n\nlatent variable\n\nthe mixture model allows us to briefly glimpse a concept that will be of\nparamount importance later\u2014the\n. a latent variable is a random\nvariable that we cannot observe directly. the component identity variable c of the\nmixture model provides an example. latent variables may be related to x through\nthe joint distribution, in this case, p (x c, ) = p (x c|\n)p (c). the distribution p (c)\nover the latent variable and the distribution p (x c| ) relating the latent variables\nto the visible variables determines the shape of the distribution p (x) even though\nit is possible to describe p (x) without reference to the latent variable. latent\nvariables are discussed further in sec.\n\n16.5\n.\n\na very powerful and common type of mixture model is the gaussian mixture\nmodel, in which the components p(x | c = i) are gaussians. each component has\na separately parametrized mean \u00b5( )i and covariance \u03c3( )i . some mixtures can have\nmore constraints. for example, the covariances could be shared across components\nvia the constraint \u03c3( )i = \u03c3\u2200i. as with a single gaussian distribution, the mixture\nof gaussians might constrain the covariance matrix for each component to be\ndiagonal or isotropic.\n\nin addition to the means and covariances, the parameters of a gaussian mixture\nspecify the prior probability \u03b1i = p (c = i) given to each component i. the word\n\u201cprior\u201d indicates that it expresses the model\u2019s beliefs about c before it has observed\nx. by comparison, p(c | x) is a posterior probability, because it is computed after\nobservation\u00a0of x. a gaussian\u00a0mixture model\u00a0is a\u00a0universal approximator of\ndensities, in the sense that any smooth density can be approximated with any\nspecific, non-zero amount of error by a gaussian mixture model with enough\ncomponents.\n\nfig.\n\n3.2\n\nshows samples from a gaussian mixture model.\n\n3.10 useful properties of common functions\n\ncertain functions arise often while working with probability distributions, especially\nthe probability distributions used in deep learning models.\n\none of these functions is the logistic sigmoid:\n\n\u03c3 x( ) =\n\n1\n\n1 + exp(\n\n)\u2212x\n\n.\n\n(3.30)\n\nthe logistic sigmoid is commonly used to produce the \u03c6 parameter of a bernoulli\ndistribution because its range is (0,1), which lies within the valid range of values\nfor a graph of the sigmoid function. the sigmoid\nfor the \u03c6 parameter. see fig.\n\n3.3\n\n67\n\n "}, {"Page_number": 83, "text": "chapter 3. probability and information theory\n\n2\nx\n\nx1\n\nfigure 3.2:\u00a0samples from a gaussian mixture model. in this example, there are three\ncomponents. from left to right, the first component has an isotropic covariance matrix,\nmeaning it has the same amount of variance in each direction. the second has a diagonal\ncovariance matrix, meaning it can control the variance separately along each axis-aligned\ndirection. this example has more variance along the x 2 axis than along the x1 axis. the\nthird component has a full-rank covariance matrix, allowing it to control the variance\nseparately along an arbitrary basis of directions.\n\nfunction saturates when its argument is very positive or very negative, meaning\nthat the function becomes very flat and insensitive to small changes in its input.\n\nanother commonly encountered function is the\n\nsoftplus\n\nfunction (\n\ndugas et al.\n,\n\n2001):\n\n\u03b6 x\nx .\n( ) = log (1 + exp( ))\n\n(3.31)\n\nthe softplus function can be useful for producing the \u03b2 or \u03c3 parameter of a normal\ndistribution because its range is (0,\u221e). it also arises commonly when manipulating\nexpressions involving sigmoids. the name of the softplus function comes from the\nfact that it is a smoothed or \u201csoftened\u201d version of\n\nx+ = max(0 ), x .\n\n(3.32)\n\nsee fig.\n\n3.4\n\nfor a graph of the softplus function.\n\nthe following properties are all useful enough that you may wish to memorize\n\nthem:\n\n\u03c3 x( ) =\n\nexp( )x\nx\n\nexp( ) + exp(0)\n\nd\ndx\n\n( ) =  ( )(1 \u2212 ( ))\n\u03c3 x\n\u03c3 x\n\n\u03c3 x\n\n68\n\n(3.33)\n\n(3.34)\n\n "}, {"Page_number": 84, "text": "chapter 3. probability and information theory\n\nthe\u00a0logistic\u00a0sigmoid\u00a0function\n\n)\nx\n(\n\u00be\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\u221210\n\n10\n\n8\n\n6\n\n4\n\n2\n\n)\nx\n(\n\u00b3\n\n\u22125\n\n0\nx\n\n5\n\n10\n\nfigure 3.3: the logistic sigmoid function.\n\nthe\u00a0softplus\u00a0function\n\n0\n\u221210\n\n\u22125\n\n0\nx\n\n5\n\n10\n\nfigure 3.4: the softplus function.\n\n69\n\n "}, {"Page_number": 85, "text": "chapter 3. probability and information theory\n\n1\n\nlog ( ) = \n\n\u2212 \u03c3 x\n\u03c3 x\n\n( ) =  (\n\n\u03c3 \u2212x\n)\n\u2212\u03b6 \u2212x\n)\n\u03b6 x\n\u03c3 x\n( ) =  ( )\n\n(\n\nd\ndx\n\n, \u03c3\u22121 ( ) = log\n\nx\n\n\u22121( ) = log (exp( )\n\nx\n\n\u03b6 x( ) =z x\n\n\u2212\u221e\nx\n\n( ) \u2212 (\u2212 ) = \n\u03b6 x\nx\n\n\u03b6\n\n\u03c3 y dy\n\n( )\n\n(cid:28) x\n1 \u2212 x(cid:29)\n\nx \u2212\n\n1)\n\n(0 1),\n\n\u2200 \u2208x\n\u2200x > , \u03b60\n\n(3.35)\n\n(3.36)\n\n(3.37)\n\n(3.38)\n\n(3.39)\n\n(3.40)\n\n(3.41)\n\nthe function \u03c3\u22121(x) is called the logit in statistics, but this term is more rarely\nused in machine learning.\n\neq.\n\n3.41\n\nprovides extra justification for the name \u201csoftplus.\u201d the softplus\nfunction is intended as a smoothed version of the positive part function, x+ =\nmax{0, x}. the positive part function is the counterpart of the negative part\nfunction, x\u2212 = max{0, x\u2212 }. to obtain a smooth function that is analogous to the\nnegative part, one can use \u03b6 (\u2212x). just as x can be recovered from its positive part\nand negative part via the identity x+ \u2212 x\u2212 = x, it is also possible to recover x\nusing the same relationship between\n\n, as shown in eq.\n\n.\n3.41\n\n\u03b6 x( )\n\nand\n\n\u03b6\n\nx(\u2212 )\n\n3.11 bayes\u2019 rule\n\nwe often find ourselves in a situation where we know p( y x| ) and need to know\np (x y|\n). fortunately, if we also know p(x), we can compute the desired quantity\n:\nbayes\u2019 rule\nusing\n\np (\n\nx y|\n\n) =\n\np\n\np( )x\n\ny x|\n(\n)\n\np ( )y\n\n.\n\n(3.42)\n\np ( ) =y px p\n\n(y |\n\nnote that while p (y) appears in the formula, it is usually feasible to compute\n\nx p x\n\n) ( ), so we do not need to begin with knowledge of\n\np\n\n( )y .\n\nbayes\u2019 rule is\u00a0straightforward to\u00a0derive from\u00a0the definition of conditional\nprobability, but it is useful to know the name of this formula since many texts\nrefer to it by name. it is named after the reverend thomas bayes, who first\ndiscovered a special case of the formula. the general version presented here was\nindependently discovered by pierre-simon laplace.\n\n70\n\n "}, {"Page_number": 86, "text": "chapter 3. probability and information theory\n\n3.12 technical details of continuous variables\n\na proper formal understanding of continuous random variables and probability\ndensity functions requires developing probability theory in terms of a branch of\nmathematics known as measure theory. measure theory is beyond the scope of\nthis textbook, but we can briefly sketch some of the issues that measure theory is\nemployed to resolve.\n\n3.3.2\n\nin sec.\n\n, we saw that the probability of a continuous vector-valued\n\nx lying\nin some set s is given by the integral of p( x) over the set s. some choices of set s\ncan produce paradoxes. for example, it is possible to construct two sets s 1 and\ns2 such that p(x \u2208 s1) + p(x \u2208 s2) > 1 but s1 \u2229 s2 = \u2205. these sets are generally\nconstructed making very heavy use of the infinite precision of real numbers, for\nexample by making fractal-shaped sets or sets that are defined by transforming\nthe set of rational numbers.2 one of the key contributions of measure theory is to\nprovide a characterization of the set of sets that we can compute the probability\nof without encountering paradoxes. in this book, we only integrate over sets with\nrelatively simple descriptions, so this aspect of measure theory never becomes a\nrelevant concern.\n\nfor our purposes, measure theory is more useful for describing theorems that\napply to most points in rn but do not apply to some corner cases. measure theory\nprovides a rigorous way of describing that a set of points is negligibly small. such\na set is said to have \u201cmeasure zero.\u201d\u00a0we do not formally define this concept in this\ntextbook. however, it is useful to understand the intuition that a set of measure\nzero occupies no volume in the space we are measuring. for example, within r2, a\nline has measure zero, while a filled polygon has positive measure. likewise, an\nindividual point has measure zero. any union of countably many sets that each\nhave measure zero also has measure zero (so the set of all the rational numbers\nhas measure zero, for instance).\n\nanother useful term from measure theory is \u201calmost everywhere.\u201d\u00a0a property\nthat holds almost everywhere holds throughout all of space except for on a set of\nmeasure zero. because the exceptions occupy a negligible amount of space, they\ncan be safely ignored for many applications. some important results in probability\ntheory hold for all discrete values but only hold \u201calmost everywhere\u201d for continuous\nvalues.\n\nanother technical detail of continuous variables relates to handling continuous\nrandom variables that are deterministic functions of one another. suppose we have\ntwo random variables, x and y, such that y = g(x), where g is an invertible, con-\n\n2the banach-tarski theorem provides a fun example of such sets.\n\n71\n\n "}, {"Page_number": 87, "text": "chapter 3. probability and information theory\n\ntinuous, differentiable transformation. one might expect that py (y) = px(g\u22121(y )).\nthis is actually not the case.\n\nas a simple example, suppose we have scalar random variables x and y. suppose\n2 and x \u223c u (0, 1).\nif we use the rule py (y) = p x(2 y) then py will be 0\non this interval. this means\n\ny = x\neverywhere except the interval [0, 1\n2 ]\n\n, and it will be\n\n1\n\nz py ( ) =\n\ny dy\n\n1\n2\n\n,\n\n(3.43)\n\nwhich violates the definition of a probability distribution.\n\nthis common mistake is wrong because it fails to account for the distortion\nof space introduced by the function g.\u00a0recall that the probability of x lying in\nan infinitesimally small region with volume \u03b4x is given by p(x)\u03b4x.\u00a0since g can\nexpand or contract space, the infinitesimal volume surrounding x in x space may\nhave different volume in\n\nspace.\n\ny\n\nto see how to correct the problem, we return to the scalar case. we need to\n\npreserve the property\n\nsolving from this, we obtain\n\n|py( ( ))\n\ng x dy|\n\n= \n\n|px( )x dx .|\n\nor equivalently\n\nin higher dimensions, the derivative generalizes to the determinant of the jacobian\nmatrix\u2014the matrix with ji,j = \u2202xi\n\u2202yj\n\n. thus, for real-valued vectors\n\nand ,\ny\n\nx\n\npy( ) = \n\ny\n\npx( ) = \n\nx\n\n\u2202x\n\n\u2202g x( )\n\npy ( ( ))\n\npx(g\u22121( ))y\n\n(cid:25)(cid:25)(cid:25)(cid:25)\n\u2202y(cid:25)(cid:25)(cid:25)(cid:25)\ng x (cid:25)(cid:25)(cid:25)(cid:25)\n\u2202x (cid:25)(cid:25)(cid:25)(cid:25) .\ng x (cid:25)(cid:25)(cid:25)(cid:25)det(cid:28)\u2202g( )x\n\u2202x (cid:29)(cid:25)(cid:25)(cid:25)(cid:25) .\n\npx ( ) = \n\nx\n\npy( ( ))\n\n3.13\n\ninformation theory\n\ninformation theory\u00a0is a\u00a0branch of\u00a0applied mathematics\u00a0that revolves around\nquantifying how much information is present in a signal. it was originally invented\nto study sending messages from discrete alphabets over a noisy channel, such as\ncommunication via radio transmission. in this context, information theory tells how\nto design optimal codes and calculate the expected length of messages sampled from\n\n72\n\n(3.44)\n\n(3.45)\n\n(3.46)\n\n(3.47)\n\n "}, {"Page_number": 88, "text": "chapter 3. probability and information theory\n\nspecific probability distributions using various encoding schemes. in the context of\nmachine learning, we can also apply information theory to continuous variables\nwhere some of these message length interpretations do not apply. this field is\nfundamental to many areas of electrical engineering and computer science. in this\ntextbook, we mostly use a few key ideas from information theory to characterize\nprobability distributions or quantify similarity between probability distributions.\nfor more detail on information theory, see cover and thomas 2006\nmackay\n(\n2003\n\n) or\n\n).\n\n(\n\nthe basic intuition behind information theory is that learning that an unlikely\nevent has\u00a0occurred is more informative than learning that a\u00a0likely\u00a0event has\noccurred. a message saying \u201cthe sun rose this morning\u201d is so uninformative as\nto be unnecessary to send, but a message saying \u201cthere was a solar eclipse this\nmorning\u201d is very informative.\n\nwe would like to quantify information in a way that formalizes this intuition.\n\nspecifically,\n\n\u2022 likely events should have low information content, and in the extreme case,\nevents that are guaranteed to happen should have no information content\nwhatsoever.\n\n\u2022 less likely events should have higher information content.\n\u2022 independent events should have additive information. for example, finding\nout that a tossed coin has come up as heads twice should convey twice as\nmuch information as finding out that a tossed coin has come up as heads\nonce.\n\nin order to satisfy all three of these properties, we define the self-information\n\nof an event x\n\n= x\n\nto be\n\ni x\n( ) = \n\nlog\u2212\n\np x .\n( )\n\n(3.48)\n\nin this book, we always use log to mean the natural logarithm, with base e. our\ndefinition of i( x) is therefore written in units of\n. one nat is the amount of\ninformation gained by observing an event of probability 1\ne . other texts use base-2\nlogarithms and units called\nshannons\n; information measured in bits is just\na rescaling of information measured in nats.\n\nnats\n\nbits\n\nor\n\nwhen x is continuous, we use the same definition of information by analogy,\nbut some of the properties from the discrete case are lost. for example, an event\nwith unit density still has zero information, despite not being an event that is\nguaranteed to occur.\n\n73\n\n "}, {"Page_number": 89, "text": "chapter 3. probability and information theory\n\ns\nt\na\nn\n\u00a0\nn\n\ni\n\u00a0\ny\np\no\nr\nt\nn\ne\n\u00a0\nn\no\nn\nn\na\nh\ns\n\n0.7\n\n0.6\n\n0.5\n0.4\n\n0.3\n\n0.2\n\n0.1\n0.0\n\n0.0\n\nshannon\u00a0entropy\u00a0of\u00a0a\u00a0binary\u00a0random\u00a0variable\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\np\n\n1\n\nfigure 3.5: this plot shows how distributions that are closer to deterministic have low\nshannon entropy while distributions that are close to uniform have high shannon entropy.\non the horizontal axis, we plot p, the probability of a binary random variable being equal\nto . the entropy is given by\nlog . when p is near 0, the distribution\nis nearly deterministic, because the random variable is nearly always 0. when p is near 1,\nthe distribution is nearly deterministic, because the random variable is nearly always 1.\nwhen p = 0.5, the entropy is maximal, because the distribution is uniform over the two\noutcomes.\n\n(p\u2212 1) log(1\u2212 p ) \u2212 p\n\np\n\nself-information deals only with a single outcome. we can quantify the amount\n\nof uncertainty in an entire probability distribution using the shannon entropy:\n\nh( ) = \n\nx\n\nex\u223cp [ ( )] = \n\ni x\n\n\u2212ex\u223cp [log ( )]\np x .\n\n(3.49)\n\nalso denoted h(p ). in other words, the shannon entropy of a distribution is the\nexpected amount of information in an event drawn from that distribution. it gives\na lower bound on the number of bits (if the logarithm is base 2, otherwise the units\nare different) needed on average to encode symbols drawn from a distribution p.\ndistributions that are nearly deterministic (where the outcome is nearly certain)\nhave low entropy; distributions that are closer to uniform have high entropy. see\nfig.\nx is continuous, the shannon entropy is known\nas the differential entropy.\n\nfor a demonstration. when\n\n3.5\n\nif we have two separate probability distributions p (x) and q(x) over the same\nrandom variable x, we can measure how different these two distributions are using\nthe kullback-leibler (kl) divergence:\n\ndkl(\n\n) = \n\np qk\n\ne x\u223cp(cid:30)log\n\np x( )\n\nq x( )(cid:31) = ex\u223cp [log ( )\n\np x \u2212\n\n74\n\nlog ( )]\nq x .\n\n(3.50)\n\n "}, {"Page_number": 90, "text": "chapter 3. probability and information theory\n\nin the case of discrete variables, it is the extra amount of information (measured\nin bits if we use the base\nlogarithm, but in machine learning we usually use nats\nand the natural logarithm) needed to send a message containing symbols drawn\nfrom probability distribution p, when we use a code that was designed to minimize\nthe length of messages drawn from probability distribution\n\n.q\n\n2\n\nthe kl divergence has many useful properties, most notably that it is non-\nnegative. the kl divergence is 0 if and only if p and q are the same distribution in\nthe case of discrete variables, or equal \u201calmost everywhere\u201d in the case of continuous\nvariables. because the kl divergence is non-negative and measures the difference\nbetween two distributions, it is often conceptualized as measuring some sort of\ndistance between these distributions. however, it is not a true distance measure\nbecause it is not symmetric: dkl(p qk ) 6= dkl(q pk ) for some p and q.\u00a0this\nasymmetry means that there are important consequences to the choice of whether\nto use dkl(\n\nfor more detail.\n\n. see fig.\n\nor dkl(\n\n3.6\n\np qk\n)\n\n)q pk\n\na quantity that is closely related to the kl divergence is the cross-entropy\nh (p, q) = h (p) + dkl(p qk ), which is similar to the kl divergence but lacking\nthe term on the left:\n(3.51)\n\nh p, q(\n\n) = \u2212ex\u223cp log ( )q x .\n\nminimizing the cross-entropy with respect to q is equivalent to minimizing the\nkl divergence, because\n\ndoes not participate in the omitted term.\n\nq\n\nwhen computing many of these quantities, it is common to encounter expres-\nsions of the form 0 log 0. by convention, in the context of information theory, we\ntreat these expressions as limx\u21920 x\n\nlog = 0.\n\nx\n\n3.14 structured probabilistic models\n\nmachine learning algorithms often involve probability distributions over a very\nlarge number of random variables. often, these probability distributions involve\ndirect interactions between relatively few variables. using a single function to\ndescribe the entire joint probability distribution can be very inefficient (both\ncomputationally and statistically).\n\ninstead of using a single function to represent a probability distribution, we\ncan split a probability distribution into many factors that we multiply together.\nfor example, suppose we have three random variables: a, b and c. suppose that\na influences the value of b and b influences the value of c, but that a and c are\nindependent given b. we can represent the probability distribution over all three\n\n75\n\n "}, {"Page_number": 91, "text": "chapter 3. probability and information theory\n\nq\u2217 = argminq dkl(\n\n)p qk\np x( )\nq\u2217( )x\n\ny\nt\ni\ns\nn\ne\nd\n \ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\nq\u2217 = argmin q dkl (\n\nq pk\n)\np( )x\nq\u2217( )x\n\ny\nt\ni\ns\nn\ne\nd\n \ny\nt\ni\nl\ni\n\nb\na\nb\no\nr\np\n\nx\n\nx\n\nfigure 3.6: the kl divergence is asymmetric. suppose we have a distribution p(x) and\nwish to approximate it with another distribution q (x). we have the choice of minimizing\neither dkl(p qk ) or dkl (q pk ). we illustrate the effect of this choice using a mixture of\ntwo gaussians for p, and a single gaussian for q.\u00a0the choice of which direction of the\nkl divergence to use is problem-dependent. some applications require an approximation\nthat usually places high probability anywhere that the true distribution places high\nprobability, while other applications require an approximation that rarely places high\nprobability anywhere that the true distribution places low probability. the choice of the\ndirection of the kl divergence reflects which of these considerations takes priority for each\napplication. (left) the effect of minimizing dkl( p qk ). in this case, we select a q that has\nhigh probability where p has high probability. when p has multiple modes, q chooses to\nblur the modes together, in order to put high probability mass on all of them.\nthe\neffect of minimizing dkl (q pk ). in this case, we select a q that has low probability where\np has low probability. when p has multiple modes that are sufficiently widely separated,\nas in this figure, the kl divergence is minimized by choosing a single mode, in order to\navoid putting probability mass in the low-probability areas between modes of p. here, we\nillustrate the outcome when q is chosen to emphasize the left mode. we could also have\nachieved an equal value of the kl divergence by choosing the right mode. if the modes\nare not separated by a sufficiently strong low probability region, then this direction of the\nkl divergence can still choose to blur the modes.\n\n(right)\n\n76\n\n "}, {"Page_number": 92, "text": "chapter 3. probability and information theory\n\nvariables as a product of probability distributions over two variables:\n\np ,\n\n(a b c) =  ( )a (\np\n\np\n\n,\n\nb a|\nc b|\n(\np\n)\n)\n.\n\n(3.52)\n\nthese factorizations can greatly reduce the number of parameters needed\nto describe the distribution. each factor uses a number of parameters that is\nexponential in the number of variables in the factor. this means that we can greatly\nreduce the cost of representing a distribution if we are able to find a factorization\ninto distributions over fewer variables.\n\nwe can describe these kinds of factorizations using graphs.\u00a0here we use the\nword \u201cgraph\u201d in the sense of graph theory: a set of vertices that may be connected\nto each other with edges. when we represent the factorization of a probability\ndistribution with a graph, we call it a structured probabilistic model\ngraphical\nmodel.\n\nor\n\nthere are two main kinds of structured probabilistic models: directed and\nundirected. both kinds of graphical models use a graph g in which each node\nin the graph corresponds to a random variable,\u00a0and an edge connecting two\nrandom variables means that the probability distribution is able to represent direct\ninteractions between those two random variables.\n\ndirected models use graphs with directed edges, and they represent factoriza-\ntions into conditional probability distributions, as in the example above. specifically,\na directed model contains one factor for every random variable xi in the distribution,\nand that factor consists of the conditional distribution over x i given the parents of\nxi, denoted p ag(xi):\n\n(3.53)\n\np( ) =x yi\n\np (xi | p ag (xi )) .\n\n3.7\n\nsee fig.\ndistributions it represents.\n\nfor an example of a directed graph and the factorization of probability\n\nundirected models use graphs with undirected edges, and they represent fac-\ntorizations into a set of functions; unlike in the directed case, these functions are\nusually not probability distributions of any kind. any set of nodes that are all\nconnected to each other in g is called a clique. each clique c ( )i\nin an undirected\nmodel is associated with a factor \u03c6( )i (c( )i ). these factors are just functions, not\nprobability distributions. the output of each factor must be non-negative, but\nthere is no constraint that the factor must sum or integrate to 1 like a probability\ndistribution.\n\nthe probability of a configuration of random variables is proportional to the\nproduct of all of these factors\u2014assignments that result in larger factor values are\n\n77\n\n "}, {"Page_number": 93, "text": "chapter 3. probability and information theory\n\naa\n\nbb\n\ndd\n\ncc\n\nee\n\nfigure 3.7: a directed graphical model over random variables a, b, c, d and e. this graph\ncorresponds to probability distributions that can be factored as\n\n(a b c d e) =  ( )a (\np ,\np\n\np\n\n,\n\n,\n\n,\n\n(c a|\nb a|\np\n)\n,\n\nb) (\np\n\ne c|\nd b|\n(\np\n)\n)\n.\n\n(3.54)\n\nthis graph allows us to quickly see some properties of the distribution. for example, a\nand c interact directly, but a and e interact only indirectly via c.\n\nmore likely. of course, there is no guarantee that this product will sum to 1. we\ntherefore divide by a normalizing constant z, defined to be the sum or integral\nover all states of the product of the \u03c6 functions, in order to obtain a normalized\nprobability distribution:\n\np( ) =x\n\n1\n\nz yi\n\n\u03c6( )i (cid:26)c( )i(cid:27) .\n\n(3.55)\n\nsee\u00a0fig.\nprobability distributions it represents.\n\n3.8\u00a0\n\nfor an\u00a0example of\u00a0an undirected\u00a0graph\u00a0and the\u00a0factorization of\n\nkeep\u00a0in mind\u00a0that these\u00a0graphical representations of\u00a0factorizations are\u00a0a\nlanguage for describing probability distributions. they are not mutually exclusive\nfamilies of probability distributions. being directed or undirected is not a property\nof a probability distribution;\u00a0it is a property of a\u00a0particular\nof a\nprobability distribution, but any probability distribution may be described in both\nways.\n\ndescription\n\ni\n\nii\n\nthroughout part and part\n\nof this book, we will use structured probabilistic\nmodels merely as a language to describe which direct probabilistic relationships\ndifferent machine learning algorithms choose to represent. no further understanding\nof structured probabilistic models is needed until the discussion of research topics,\nin part\n, where we will explore structured probabilistic models in much greater\ndetail.\n\niii\n\n78\n\n "}, {"Page_number": 94, "text": "chapter 3. probability and information theory\n\naa\n\nbb\n\ndd\n\ncc\n\nee\n\nfigure 3.8: an undirected graphical model over random variables a, b, c, d and e . this\ngraph corresponds to probability distributions that can be factored as\n\n(a b c d e) =\np ,\n\n,\n\n,\n\n,\n\n1\nz\n\n\u03c6(1)(\n\na b c\n)\n,\n\n, \u03c6(2)(\n\n)b d, \u03c6(3)(\n\n)c e,\n.\n\n(3.56)\n\nthis graph allows us to quickly see some properties of the distribution. for example, a\nand c interact directly, but a and e interact only indirectly via c.\n\nthis chapter has reviewed the basic concepts of probability theory that are\nmost relevant to deep learning. one more set of fundamental mathematical tools\nremains: numerical methods.\n\n79\n\n "}, {"Page_number": 95, "text": "chapter 4\n\nnumerical computation\n\nmachine learning algorithms usually require a high amount of numerical compu-\ntation. this typically refers to algorithms that solve mathematical problems by\nmethods that update estimates of the solution via an iterative process, rather than\nanalytically deriving a formula providing a symbolic expression for the correct so-\nlution. common operations include optimization (finding the value of an argument\nthat minimizes or maximizes a function) and solving systems of linear equations.\neven just evaluating a mathematical function on a digital computer can be difficult\nwhen the function involves real numbers, which cannot be represented precisely\nusing a finite amount of memory.\n\n4.1 overflow and underflow\n\nthe fundamental difficulty in performing continuous math on a digital computer\nis that we need to represent infinitely many real numbers with a finite number\nof bit patterns. this means that for almost all real numbers,\u00a0we incur some\napproximation error when we represent the number in the computer. in many\ncases, this is just rounding error. rounding error is problematic, especially when\nit compounds across many operations, and can cause algorithms that work in\ntheory to fail in practice if they are not designed to minimize the accumulation of\nrounding error.\n\none form of rounding error that is particularly devastating is\n\n. under-\nflow occurs when numbers near zero are rounded to zero. many functions behave\nqualitatively differently when their argument is zero rather than a small positive\nnumber. for example, we usually want to avoid division by zero (some software\n\nunderflow\n\n80\n\n "}, {"Page_number": 96, "text": "exp(xi)\nj=1 exp(xj)\n\npn\n\nchapter 4. numerical computation\n\nenvironments will raise exceptions when this occurs, others will return a result\nwith a placeholder not-a-number value) or taking the logarithm of zero (this is\nusually treated as \u2212\u221e, which then becomes not-a-number if it is used for many\nfurther arithmetic operations).\n\nanother highly damaging form of numerical error is\n\n. overflow occurs\nwhen numbers with large magnitude are approximated as \u221e or \u2212\u221e. further\narithmetic will usually change these infinite values into not-a-number values.\n\noverflow\n\none example of a function that must be stabilized against underflow and\noverflow is the softmax function. the softmax function is often used to predict the\nprobabilities associated with a multinoulli distribution. the softmax function is\ndefined to be\n\nsoftmax( )x i =\n\n.\n\n(4.1)\n\nconsider what happens when all of the xi are equal to some constant c. analytically,\nwe can see that all of the outputs should be equal to 1\nn. numerically, this may\nnot occur when c has large magnitude. if c is very negative, then exp(c) will\nunderflow. this means the denominator of the softmax will become 0, so the final\nresult is undefined. when c is very large and positive, exp(c) will overflow, again\nresulting in the expression as a whole being undefined. both of these difficulties\ncan be resolved by instead evaluating softmax(z ) where z = x \u2212 maxi xi. simple\nalgebra shows that the value of the softmax function is not changed analytically by\nadding or subtracting a scalar from the input vector. subtracting maxi xi results\nin the largest argument to exp being 0, which rules out the possibility of overflow.\nlikewise, at least one term in the denominator has a value of 1, which rules out\nthe possibility of underflow in the denominator leading to a division by zero.\n\nthere is still one small problem. underflow in the numerator can still cause\nthe expression as a whole to evaluate to zero. this means that if we implement\nlog softmax(x) by first running the softmax subroutine then passing the result to\nthe log function, we could erroneously obtain \u2212\u221e. instead, we must implement\na separate function that calculates log softmax in a numerically stable way. the\nlog softmax function can be stabilized using the same trick as we used to stabilize\nthe\n\nfunction.\n\nsoftmax\n\nfor the most part, we do not explicitly detail all of the numerical considerations\ninvolved in implementing the various algorithms described in this book. developers\nof low-level libraries should keep numerical issues in mind when implementing\ndeep learning algorithms. most readers of this book can simply rely on low-\nlevel libraries that provide stable implementations. in some cases, it is possible\nto implement a new algorithm and have the new implementation automatically\n\n81\n\n "}, {"Page_number": 97, "text": "chapter 4. numerical computation\n\nstabilized. theano (\n) is an example\nof a software package that automatically detects and stabilizes many common\nnumerically unstable expressions that arise in the context of deep learning.\n\nbergstra et al. 2010 bastien et al. 2012\n\n,\n\n;\n\n,\n\n4.2 poor conditioning\n\nconditioning refers to how rapidly a function changes with respect to small changes\nin its inputs. functions that change rapidly when their inputs are perturbed slightly\ncan be problematic for scientific computation because rounding errors in the inputs\ncan result in large changes in the output.\n\nconsider the function f(x) = a\u22121x. when a \u2208 r n n\u00d7 has an eigenvalue\n\ndecomposition, its condition number is\n\nmax\ni,j\n\n(cid:20)(cid:20)(cid:20)(cid:20)\n\n\u03bbi\n\n\u03bbj(cid:20)(cid:20)(cid:20)(cid:20).\n\n(4.2)\n\nthis is the ratio of the magnitude of the largest and smallest eigenvalue. when\nthis number is large, matrix inversion is particularly sensitive to error in the input.\n\nthis sensitivity is an intrinsic property of the matrix itself, not the result\nof rounding error during matrix inversion. poorly conditioned matrices amplify\npre-existing errors when we multiply by the true matrix inverse. in practice, the\nerror will be compounded further by numerical errors in the inversion process itself.\n\n4.3 gradient-based optimization\n\nmost deep learning algorithms involve optimization of some sort.\u00a0optimization\nrefers to the task of either minimizing or maximizing some function f(x) by altering\nx.\u00a0we usually phrase most optimization problems in terms of minimizing f (x).\nmaximization may be accomplished via a minimization algorithm by minimizing\n\u2212f ( )x .\n\nthe function we want to minimize or maximize is called the objective function\nor\ncriterion\ncost function,\n. in this book, we use these terms interchangeably,\nloss function\nthough some machine learning publications assign special meaning to some of these\nterms.\n\n. when we are minimizing it, we may also call it the\n, or\n\nerror function\n\nwe often denote the value that minimizes or maximizes a function with a\n\nsuperscript\n\n\u2217\n. for example, we might say\n\nx\u2217 = arg min ( )\nf x .\n\n82\n\n "}, {"Page_number": 98, "text": "chapter 4. numerical computation\n\n2.0\n\n1.5\n\n1.0\n\n0.5\n\n0.0\n\n\u22120.5\n\n\u22121.0\n\n\u22121.5\n\ngradient\u00a0descent\n\nglobal\u00a0minimum\u00a0at\u00a0\nsince\u00a0f0( ) =0\ndescent\u00a0halts\u00a0here.\n\nx =0\n,\u00a0gradient\n\nx\n\n.\n\nx <0\n\n,\u00a0we\u00a0have\u00a0\n\nfor\u00a0\nso\u00a0we\u00a0can\u00a0decrease\u00a0f\u00a0by\nmoving\u00a0rightward.\n\nx < ,\nf0( ) 0\n\nx >0\n\n,\u00a0we\u00a0have\u00a0\n\nfor\u00a0\nso\u00a0we\u00a0can\u00a0decrease\u00a0f\u00a0by\nmoving\u00a0leftward.\n\nx > ,\nf0( ) 0\n\nx2\n\nf x( ) =1\n2\nf0( ) =x\nx\n\n\u22122.0\n\n\u22122.0\n\n\u22121.5\n\n\u22121.0\n\n\u22120.5\n\n0.0\nx\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\nfigure 4.1: an illustration of how the derivatives of a function can be used to follow the\nfunction downhill to a minimum. this technique is called gradient descent.\n\nwe assume the reader is already familiar with calculus, but provide a brief\n\nreview of how calculus concepts relate to optimization here.\n\nderivative\n\nof this function is denoted as\n\nsuppose we have a function y = f (x), where both x and y are real numbers.\ndx . the derivative f0(x)\nthe\ngives the slope of f(x) at the point x. in other words, it specifies how to scale\na small change in the input in order to obtain the corresponding change in the\noutput: f x\n\nf 0(x) or as dy\n\nf x\n\n(cid:115)f\n\n(cid:115)\n\n( + ) \u2248 ( ) + 0( )x .\n\nthe derivative is therefore useful for minimizing a function because it tells us\nhow to change x in order to make a small improvement in y. for example, we\n(cid:115)\u2212 sign (f0 (x))) is less than f (x) for small enough (cid:115). we can thus\nknow that f(x\nreduce f (x) by moving x in small steps with opposite sign of the derivative. this\nfor an example of\ntechnique is called gradient descent (cauchy 1847\nthis technique.\n\n). see fig.\n\n4.1\n\n,\n\nwhen f0(x) = 0, the derivative provides no information about which direction\nto move. points where f0(x) = 0 are known as critical points\nstationary points\n.\na local minimum is a point where f (x) is lower than at all neighboring points,\nso it is no longer possible to decrease f (x) by making infinitesimal steps. a local\nmaximum is a point where f(x) is higher than at all neighboring points, so it is\n\nor\n\n83\n\n "}, {"Page_number": 99, "text": "chapter 4. numerical computation\n\ntypes\u00a0of\u00a0critical\u00a0points\n\nminimum\n\nmaximum\n\nsaddle\u00a0point\n\nfigure 4.2: examples of each of the three types of critical points in 1-d. a critical point is\na point with zero slope. such a point can either be a local minimum, which is lower than\nthe neighboring points, a local maximum, which is higher than the neighboring points, or\na saddle point, which has neighbors that are both higher and lower than the point itself.\n\nnot possible to increase f(x) by making infinitesimal steps. some critical points\nare neither maxima nor minima. these are known as saddle points. see fig. 4.2\nfor examples of each type of critical point.\n\na point that obtains the absolute lowest value of f (x) is a global minimum. it\nis possible for there to be only one global minimum or multiple global minima of\nthe function. it is also possible for there to be local minima that are not globally\noptimal. in the context of deep learning, we optimize functions that may have\nmany local minima that are not optimal, and many saddle points surrounded by\nvery flat regions. all of this makes optimization very difficult, especially when the\ninput to the function is multidimensional. we therefore usually settle for finding a\nvalue of f that is very low, but not necessarily minimal in any formal sense. see\nfig.\n\n4.3\nwe often minimize functions that have multiple inputs: f : rn \u2192 r. for the\nconcept of \u201cminimization\u201d\u00a0to make sense, there must still be only one (scalar)\noutput.\n\nfor an example.\n\nfor functions with multiple inputs, we must make use of the concept of partial\nderivatives. the partial derivative \u2202\nf(x) measures how f changes as only the\n\u2202xi\nvariable xi increases at point x. the gradient generalizes the notion of derivative\nto the case where the derivative is with respect to a vector: the gradient of f is the\nvector containing all of the partial derivatives, denoted \u2207xf (x). element i of the\ngradient is the partial derivative of f with respect to xi . in multiple dimensions,\n\n84\n\n "}, {"Page_number": 100, "text": "chapter 4. numerical computation\n\napproximate\u00a0minimization\n\nthis\u00a0local\u00a0minimum\nperforms\u00a0nearly\u00a0as\u00a0well\u00a0as\nthe\u00a0global\u00a0one,\nso\u00a0it\u00a0is\u00a0an\u00a0acceptable\nhalting\u00a0point.\n\nx\n(\nf\n\n) ideally,\u00a0we\u00a0would\u00a0like\u00a0\nto\u00a0arrive\u00a0at\u00a0the\u00a0global\n\u00a0minimum,\u00a0but\u00a0this\u00a0\nmight\u00a0not\u00a0be\u00a0possible.\n\nthis\u00a0local\u00a0minimum\u00a0performs\npoorly,\u00a0and\u00a0should\u00a0be\u00a0avoided.\n\nx\n\nfigure 4.3: optimization algorithms may fail to find a global minimum when there are\nmultiple local minima or plateaus present. in the context of deep learning, we generally\naccept such solutions even though they are not truly minimal, so long as they correspond\nto significantly low values of the cost function.\n\ncritical points are points where every element of the gradient is equal to zero.\n\nthe directional derivative in direction u (a unit vector) is the slope of the\nfunction f in direction u. in other words, the directional derivative is the derivative\nof the function f (x + \u03b1u) with respect to \u03b1 , evaluated at \u03b1 = 0. using the chain\nrule, we can see that \u2202\n\n( +x\n\n\u03b1\n\n\u2202\u03b1 f\n\nu) =  > \u2207x f ( )x .\nu\n\nto minimize f , we would like to find the direction in which f decreases the\n\nfastest. we can do this using the directional derivative:\n\nmin\n\nu u, > u=1\n\nu>\u2207 xf ( )x\n\n= min\n\nu u, >u=1 ||\n\n||u 2||\u2207xf ( )x ||2 cos \u03b8\n\n(4.3)\n\n(4.4)\n\n||u 2 = 1 and\nwhere \u03b8 is the angle between u and the gradient. substituting in ||\nignoring factors that do not depend on u, this simplifies to minu cos \u03b8. this is\nminimized when u points in the opposite direction as the gradient.\nin other\nwords, the gradient points directly uphill, and the negative gradient points directly\ndownhill. we can decrease f by moving in the direction of the negative gradient.\nthis is known as the method of steepest descent\n\ngradient descent\n.\n\nor\n\nsteepest descent proposes a new point\n\nx0 = x \u2212 \u2207(cid:115) x f ( )x\n\n85\n\n(4.5)\n\n "}, {"Page_number": 101, "text": "chapter 4. numerical computation\n\nwhere (cid:115) is the learning rate, a positive scalar determining the size of the step. we\ncan choose (cid:115) in several different ways. a popular approach is to set (cid:115) to a small\nconstant. sometimes, we can solve for the step size that makes the directional\n(x \u2212 \u2207xf ( ))x for several\nderivative vanish. another approach is to evaluate f\nvalues of (cid:115) and choose the one that results in the smallest objective function value.\nthis last strategy is called a line search.\n\n(cid:115)\n\nx\n\nsteepest descent converges when every element of the gradient is zero (or, in\npractice, very close to zero). in some cases, we may be able to avoid running this\niterative algorithm, and just jump directly to the critical point by solving the\nequation \u2207xf ( ) = 0\n\nalthough gradient descent is limited to optimization in continuous spaces, the\ngeneral concept of making small moves (that are approximately the best small move)\ntowards better configurations can be generalized to discrete spaces. ascending an\nobjective function of discrete parameters is called\nhill climbing russel and norvig\n,\n2003).\n\nfor\n\n.x\n\n(\n\n4.3.1 beyond the gradient: jacobian and hessian matrices\n\nf\n\nsometimes we need to find all of the partial derivatives of a function whose input\nand output are both vectors. the matrix containing all such partial derivatives is\nknown as a jacobian matrix. specifically, if we have a function f : rm \u2192 rn, then\nthe jacobian matrix j \u2208 rn m\u00d7 of\n\nis defined such that\n\nji,j = \u2202\n\u2202x j\n\nf ( )x i.\n\n\u2202xi \u2202xj\n\nwe are also sometimes interested in a derivative of a derivative. this is known\nas a second derivative. for example, for a function f : rn \u2192 r, the derivative\nf.\nwith respect to xi of the derivative of f with respect to xj is denoted as\nin a single dimension, we can denote d2\ndx2 f by f 00(x). the second derivative tells\nus how the first derivative will change as we vary the input. this is important\nbecause it tells us whether a gradient step will cause as much of an improvement\nas we would expect based on the gradient alone. we can think of the second\nderivative as measuring curvature. suppose we have a quadratic function (many\nfunctions that arise in practice are not quadratic but can be approximated well\nas quadratic, at least locally). if such a function has a second derivative of zero,\nthen there is no curvature. it is a perfectly flat line, and its value can be predicted\nusing only the gradient. if the gradient is\n(cid:115)\nalong the negative gradient, and the cost function will decrease by (cid:115). if the second\nderivative is negative, the function curves downward, so the cost function will\nactually decrease by more than (cid:115). finally, if the second derivative is positive, the\nfunction curves upward, so the cost function can decrease by less than (cid:115). see fig.\n\n, then we can make a step of size\n\n\u2202 2\n\n1\n\n86\n\n "}, {"Page_number": 102, "text": "chapter 4. numerical computation\n\nnegative curvature\n\nno curvature\n\npositive curvature\n\n)\nx\n(\nf\n\n)\nx\n(\nf\n\n)\nx\n(\nf\n\nx\n\nx\n\nx\n\nfigure 4.4: the second derivative determines the curvature of a function. here we show\nquadratic functions with various curvature. the dashed line indicates the value of the cost\nfunction we would expect based on the gradient information alone as we make a gradient\nstep downhill. in the case of negative curvature, the cost function actually decreases\nfaster than the gradient predicts. in the case of no curvature, the gradient predicts the\ndecrease correctly. in the case of positive curvature, the function decreases slower than\nexpected and eventually begins to increase, so too large of step sizes can actually increase\nthe function inadvertently.\n\n4.4 to see how different forms of curvature affect the relationship between the value\nof the cost function predicted by the gradient and the true value.\n\nwhen our function has multiple input dimensions, there are many second\nderivatives. these derivatives can be collected together into a matrix called the\nhessian matrix. the hessian matrix\n\nis defined such that\n\nh x\n)\n\n( )(f\n\nh x\n\n( )(f\n\n)i,j =\n\n\u2202 2\n\n\u2202xi\u2202xj\n\nf\n\n.\n( )x\n\n(4.6)\n\nequivalently, the hessian is the jacobian of the gradient.\n\nanywhere that the second partial derivatives are continuous, the differential\n\noperators are commutative, i.e. their order can be swapped:\n\n\u22022\n\n\u2202xi\u2202xj\n\nf ( ) =x\n\n\u22022\n\n\u2202xj\u2202xi\n\nf\n\n.\n( )x\n\n(4.7)\n\nthis implies that hi,j = h j,i, so the hessian matrix is symmetric at such points.\nmost of the functions we encounter in the context of deep learning have a symmetric\nhessian almost everywhere.\u00a0because the hessian matrix is real and symmetric,\nwe can decompose it into a set of real eigenvalues and an orthogonal basis of\n\n87\n\n "}, {"Page_number": 103, "text": "chapter 4. numerical computation\n\neigenvectors. the second derivative in a specific direction represented by a unit\nvector d is given by d> hd. when d is an eigenvector of h , the second derivative\nin that direction is given by the corresponding eigenvalue. for other directions of\nd, the directional second derivative is a weighted average of all of the eigenvalues,\nwith weights between 0 and 1, and eigenvectors that have smaller angle with d\nreceiving more weight. the maximum eigenvalue determines the maximum second\nderivative and the minimum eigenvalue determines the minimum second derivative.\n\nthe (directional) second derivative tells us how well we can expect a gradient\ndescent step to perform. we can make a second-order taylor series approximation\nto the function\n\naround the current point\n\nx(0):\n\nf ( )x\n\nf\n\n( ) x \u2248 (x(0)) + (x x\u2212 (0))>g +\n\nf\n\n1\n2\n\n(x x\u2212 (0))>h x x\n\n( \u2212 (0)).\n\n(4.8)\n\nwhere g is the gradient and h is the hessian at x(0).\u00a0if we use a learning rate\nof (cid:115), then the new point x will be given by x(0) \u2212 (cid:115)g. substituting this into our\napproximation, we obtain\n\nf (x(0)\u2212\n\n\u2248(cid:115)g) \n\nf (x(0) ) \u2212 (cid:115)g>g +\n\n(cid:115)2g>hg.\n\n1\n2\n\n(4.9)\n\nthere are three\u00a0terms here: the original value\u00a0of the function,\u00a0the expected\nimprovement due to the slope of the function, and the correction we must apply\nto account for the curvature of the function. when this last term is too large, the\ngradient descent step can actually move uphill. when g>hg is zero or negative,\nthe taylor series approximation predicts that increasing (cid:115) forever will decrease f\nforever. in practice, the taylor series is unlikely to remain accurate for large (cid:115), so\none must resort to more heuristic choices of (cid:115) in this case. when g>hg is positive,\nsolving for the optimal step size that decreases the taylor series approximation of\nthe function the most yields\n\n(cid:115)\u2217 =\n\ng>g\ng>hg\n\n.\n\n(4.10)\n\nin the worst case, when g aligns with the eigenvector of h corresponding to the\nmaximal eigenvalue \u03bbmax, then this optimal step size is given by\n. to the\nextent that the function we minimize can be approximated well by a quadratic\nfunction, the eigenvalues of the hessian thus determine the scale of the learning\nrate.\n\n\u03bb max\n\n1\n\nthe second derivative can be used to determine whether a critical point is a\nlocal maximum, a local minimum, or saddle point. recall that on a critical point,\nf 0(x) = 0. when f00(x) > 0, this means that f 0(x) increases as we move to the\nright, and f 0(x) decreases as we move to the left. this means f0(x\n(cid:115)\u2212 ) < 0 and\n\n88\n\n "}, {"Page_number": 104, "text": "chapter 4. numerical computation\n\nf 0(x + (cid:115)) > 0 for small enough (cid:115) . in other words, as we move right, the slope begins\nto point uphill to the right, and as we move left, the slope begins to point uphill\nto the left. thus, when f0 (x) = 0 and f00(x ) > 0, we can conclude that x is a local\nminimum. similarly, when f 0(x) = 0 and f00 (x) < 0, we can conclude that x is a\nlocal maximum. this is known as the second derivative test. unfortunately, when\nf 00(x) = 0, the test is inconclusive. in this case x may be a saddle point, or a part\nof a flat region.\n\nin multiple dimensions, we need to examine all of the second derivatives of the\nfunction. using the eigendecomposition of the hessian matrix, we can generalize\nthe second derivative test to multiple dimensions. at a critical point, where\n\u2207xf(x) = 0, we can examine the eigenvalues of the hessian to determine whether\nthe critical point is a local maximum, local minimum, or saddle point. when the\nhessian is positive definite (all its eigenvalues are positive), the point is a local\nminimum. this can be seen by observing that the directional second derivative\nin any direction must be positive, and making reference to the univariate second\nderivative test. likewise, when the hessian is negative definite (all its eigenvalues\nare negative), the point is a local maximum. in multiple dimensions, it is actually\npossible to find positive evidence of saddle points in some cases.\u00a0when at least\none eigenvalue is positive and at least one eigenvalue is negative, we know that\nx is a local maximum on one cross section of f but a local minimum on another\ncross section. see fig.\nfor an example. finally, the multidimensional second\nderivative test can be inconclusive, just like the univariate version. the test is\ninconclusive whenever all of the non-zero eigenvalues have the same sign, but at\nleast one eigenvalue is zero. this is because the univariate second derivative test is\ninconclusive in the cross section corresponding to the zero eigenvalue.\n\n4.5\n\nin multiple dimensions, there can be a wide variety of different second derivatives\nat a single point, because there is a different second derivative for each direction.\nthe condition number of the hessian measures how much the second derivatives\nvary. when the hessian has a poor condition number, gradient descent performs\npoorly. this is because in one direction, the derivative increases rapidly, while in\nanother direction, it increases slowly. gradient descent is unaware of this change\nin the derivative so it does not know that it needs to explore preferentially in\nthe direction where the derivative remains negative for longer. it also makes it\ndifficult to choose a good step size. the step size must be small enough to avoid\novershooting the minimum and going uphill in directions with strong positive\ncurvature. this usually means that the step size is too small to make significant\nprogress in other directions with less curvature. see fig.\n\nfor an example.\n\n4.6\n\nthis issue can be resolved by using information from the hessian matrix to\n\n89\n\n "}, {"Page_number": 105, "text": "chapter 4. numerical computation\n\n)\n2\nx\n;\n1\nx\n(\nf\n\n500\n0\n\u2212500\n\n\u221215\n\n0\n\nx1\n\n15\n\n15\n\n0\n\nx\n\n2\n\n\u221215\n\n1 \u2212 x 2\n\nfigure 4.5: a saddle point containing both positive and negative curvature. the function\nin this example is f(x) = x2\n2. along the axis corresponding to x1, the function\ncurves upward. this axis is an eigenvector of the hessian and has a positive eigenvalue.\nalong the axis corresponding to x2, the function curves downward. this direction is an\neigenvector of the hessian with negative eigenvalue. the name \u201csaddle point\u201d derives from\nthe saddle-like shape of this function. this is the quintessential example of a function\nwith a saddle point. in more than one dimension, it is not necessary to have an eigenvalue\nof 0 in order to get a saddle point: it is only necessary to have both positive and negative\neigenvalues. we can think of a saddle point with both signs of eigenvalues as being a local\nmaximum within one cross section and a local minimum within another cross section.\n\n90\n\n "}, {"Page_number": 106, "text": "chapter 4. numerical computation\n\n20\n\n10\n\n0\n\n2\nx\n\n\u221210\n\u221220\n\u221230\n\n\u2212 \u2212 \u2212\n30\n\n20\n\n10\n\n10\n\n20\n\n0\nx1\n\nfigure 4.6: gradient descent fails to exploit the curvature information contained in the\nhessian matrix. here we use gradient descent to minimize a quadratic function f ( x) whose\nhessian matrix has condition number 5. this means that the direction of most curvature\nhas five times more curvature than the direction of least curvature. in this case, the most\ncurvature is in the direction [1 ,1]> and the least curvature is in the direction [1,\u22121]> . the\nred lines indicate the path followed by gradient descent. this very elongated quadratic\nfunction resembles a long canyon. gradient descent wastes time repeatedly descending\ncanyon walls, because they are the steepest feature. because the step size is somewhat\ntoo large, it has a tendency to overshoot the bottom of the function and thus needs to\ndescend the opposite canyon wall on the next iteration. the large positive eigenvalue\nof the hessian corresponding to the eigenvector pointed in this direction indicates that\nthis directional derivative is rapidly increasing, so an optimization algorithm based on\nthe hessian could predict that the steepest direction is not actually a promising search\ndirection in this context.\n\n91\n\n "}, {"Page_number": 107, "text": "chapter 4. numerical computation\n\nguide the search. the simplest method for doing so is known as newton\u2019s method.\nnewton\u2019s method is based on using a second-order taylor series expansion to\napproximate\n\nnear some point\n\nx(0):\n\nf ( )x\n\nf\n\n( ) x \u2248 (x(0) )+(x x\u2212 (0))>\u2207xf (x(0))+\n\nf\n\n1\n2\n\n(x x\u2212 (0))>h x\n( )(f\n\n(0) )(x x\u2212 (0)). (4.11)\n\nif we then solve for the critical point of this function, we obtain:\n\nx\u2217 = x(0) \u2212 h x\n( )(f\n\n(0) )\u22121\u2207x f (x(0)).\n\n(4.12)\n\n4.12\n\nonce to jump to the minimum of the function directly. when\n\nwhen f is a positive definite quadratic function, newton\u2019s method consists of\napplying eq.\nf is\nnot truly quadratic but can be locally approximated as a positive definite quadratic,\nnewton\u2019s method consists of applying eq.\nmultiple times. iteratively updating\nthe approximation and jumping to the minimum of the approximation can reach\nthe critical point much faster than gradient descent would. this is a useful property\nnear a local minimum, but it can be a harmful property near a saddle point. as\ndiscussed in sec.\n, newton\u2019s method is only appropriate when the nearby\ncritical point is a minimum (all the eigenvalues of the hessian are positive), whereas\ngradient descent is not attracted to saddle points unless the gradient points toward\nthem.\n\n8.2.3\n\n4.12\n\noptimization algorithms such as gradient descent that use only the gradient are\ncalled first-order optimization algorithms. optimization algorithms such as new-\nton\u2019s method that also use the hessian matrix are called second-order optimization\nalgorithms (nocedal and wright 2006\n\n).\n\n,\n\nthe\u00a0optimization algorithms\u00a0employed in\u00a0most contexts in\u00a0this book\u00a0are\napplicable to a wide variety of functions, but come with almost no guarantees. this\nis because the family of functions used in deep learning is quite complicated. in\nmany other fields, the dominant approach to optimization is to design optimization\nalgorithms for a limited family of functions.\n\nin the context of deep learning, we sometimes gain some guarantees by restrict-\ning ourselves to functions that are either lipschitz continuous or have lipschitz\ncontinuous derivatives. a lipschitz continuous function is a function f whose rate\nof change is bounded by a lipschitz constant l:\n\n\u2200\nx, y, f ( )x\n\n\u2200 |\n\n\u2212\n\nf ( )y\n\n| \u2264 l|| \u2212 ||\n\nx y 2 .\n\n(4.13)\n\nthis property is useful because it allows us to quantify our assumption that a\nsmall change in the input made by an algorithm such as gradient descent will have\na small change in the output. lipschitz continuity is also a fairly weak constraint,\n\n92\n\n "}, {"Page_number": 108, "text": "chapter 4. numerical computation\n\nand many optimization problems in deep learning can be made lipschitz continuous\nwith relatively minor modifications.\n\nperhaps the most successful field of specialized optimization is convex optimiza-\ntion. convex optimization algorithms are able to provide many more guarantees\nby making stronger restrictions. convex optimization algorithms are applicable\nonly to convex functions\u2014functions for which the hessian is positive semidefinite\neverywhere. such functions are well-behaved because they lack saddle points and\nall of their local minima are necessarily global minima. however, most problems\nin deep learning are difficult to express in terms of convex optimization. convex\noptimization is used only as a subroutine of some deep learning algorithms. ideas\nfrom the analysis of convex optimization algorithms can be useful for proving the\nconvergence of deep learning algorithms. however, in general, the importance of\nconvex optimization is greatly diminished in the context of deep learning. for\nmore information about convex optimization, see boyd and vandenberghe 2004\n)\nor rockafellar 1997\n\n).\n\n(\n\n(\n\n4.4 constrained optimization\n\nsometimes we wish not only to maximize or minimize a function f(x) over all\npossible values of x. instead we may wish to find the maximal or minimal value of\nf (x) for values of x in some set s. this is known as constrained optimization. points\nx that lie within the set s are called feasible points in constrained optimization\nterminology.\n\nwe often wish to find a solution that is small in some sense. a common\n\napproach in such situations is to impose a norm constraint, such as\n\n||\n|| \u2264x\n\n.\n1\n\none simple approach to constrained optimization is simply to modify gradient\ndescent taking the constraint into account. if we use a small constant step size (cid:115),\nwe can make gradient descent steps, then project the result back into s. if we use\na line search, we can search only over step sizes (cid:115) that yield new x points that are\nfeasible, or we can project each point on the line back into the constraint region.\nwhen possible, this method can be made more efficient by projecting the gradient\ninto the tangent space of the feasible region before taking the step or beginning\nthe line search (\n\nrosen 1960\n\n).\n\n,\n\na more sophisticated approach is to design a different, unconstrained opti-\nmization problem whose solution can be converted into a solution to the original,\nconstrained optimization problem. for example, if we want to minimize f(x) for\nx \u2208 r2 with x constrained to have exactly unit l2 norm, we can instead minimize\n\n93\n\n "}, {"Page_number": 109, "text": "chapter 4. numerical computation\n\n\u03b8,\n\nsin\n\n\u03b8]>) with respect to \u03b8, then return [ cos\n\n\u03b8] as the solution\ng(\u03b8 ) = f ([cos\nto the original problem. this approach requires creativity; the transformation\nbetween optimization problems must be designed specifically for each case we\nencounter.\n\nsin\n\n\u03b8,\n\nthe karush\u2013kuhn\u2013tucker (kkt) approach1 provides a very general solution\nto constrained optimization. with the kkt approach, we introduce a new function\ncalled the generalized lagrangian\n\ngeneralized lagrange function\n.\n\nor\n\nto define the lagrangian, we first need to describe s in terms of equations\nand inequalities.\u00a0we want a description of s in terms of m functions g ( )i and n\ni, g( )i (x) = 0 and \u2200j, h( )j (x) \u2264 0}. the equations\nfunctions h( )j\nso that s = { | \u2200x\ninvolving g( )i are called the equality constraints and the inequalities involving h ( )j\nare called inequality constraints.\n\nwe introduce new variables \u03bbi and \u03b1 j for each constraint, these are called the\n\nkkt multipliers. the generalized lagrangian is then defined as\n\nl ,\n\n,\n\n(x \u03bb \u03b1) =  ( ) +x xi\n\nf\n\n\u03bb ig( )i ( ) +x xj\n\n\u03b1jh ( )j ( )x .\n\n(4.14)\n\nwe can now solve a constrained minimization problem using unconstrained\noptimization of the generalized lagrangian. observe that, so long as at least one\nfeasible point exists and\n\nis not permitted to have value\n\n, then\n\nf ( )x\n\n\u221e\n\nmin\n\nx\n\nmax\n\n\u03bb\n\nmax\n\u03b1 \u03b1, \u22650\n\nl ,\n\n(x \u03bb \u03b1)\n.\n\n,\n\n(4.15)\n\nhas the same optimal objective function value and set of optimal points\n\nasx\n\nthis follows because any time the constraints are satisfied,\n\nf\n\n.\n( )x\n\nmin\nx\u2208s\n\nmax\n\n\u03bb\n\nmax\n\u03b1 \u03b1, \u22650\n\nl ,\n\n(x \u03bb \u03b1) =  ( )x\n,\n\nf\n\n,\n\nwhile any time a constraint is violated,\n\nmax\n\n\u03bb\n\nmax\n\u03b1 \u03b1, \u22650\n\n(x \u03bb \u03b1) = \u221e\nl ,\n.\n\n,\n\n(4.16)\n\n(4.17)\n\n(4.18)\n\nthese properties guarantee that no infeasible point will ever be optimal, and that\nthe optimum within the feasible points is unchanged.\n\n1the kkt approach generalizes the method of lagrange multipliers which allows equality\n\nconstraints but not inequality constraints.\n\n94\n\n "}, {"Page_number": 110, "text": "chapter 4. numerical computation\n\nto perform constrained maximization, we can construct the generalized la-\n\n, which leads to this optimization problem:\n\ngrange function of\n\nmin\nx\n\n\u2212f ( )x\nmax\n\n\u03bb\n\nwe may also convert this to a problem with maximization in the outer loop:\n\nmax\n\n\u03b1 \u03b1, \u22650\u2212f ( ) +x xi\nf ( ) +x xi\n\nmin\n\u03b1 \u03b1, \u22650\n\n\u03bbig( )i ( ) +x xj\n\u03bb ig( )i ( )x \u2212xj\n\nmax\n\nx\n\nmin\n\u03bb\n\n\u03b1 jh( )j ( )x .\n\n(4.19)\n\n\u03b1j h( )j ( )x .\n\n(4.20)\n\nthe sign of the term for the equality constraints does not matter; we may define it\nwith addition or subtraction as we wish, because the optimization is free to choose\nany sign for each \u03bbi.\n\nif\n\nactive\n\nthe inequality constraints are particularly interesting. we say that a constraint\nh( )i (x\u2217) = 0. if a constraint is not active, then the solution to\nh( )i (x ) is\nthe problem found using that constraint would remain at least a local solution if\nthat constraint were removed. it is possible that an inactive constraint excludes\nother solutions. for example, a convex problem with an entire region of globally\noptimal points (a wide, flat, region of equal cost) could have a subset of this\nregion eliminated by constraints, or a non-convex problem could have better local\nstationary points excluded by a constraint that is inactive at convergence. however,\nthe point found at convergence remains a stationary point whether or not the\ninactive constraints are included. because an inactive h( )i has negative value, then\nthe solution to minx max\u03bb max\u03b1 \u03b1, \u22650 l(x \u03bb \u03b1,\n) will have \u03b1i = 0. we can thus\nobserve that at the solution, \u03b1h(x ) = 0. in other words, for all i, we know that at\nleast one of the constraints \u03b1i \u2265 0 and h( )i (x) \u2264 0 must be active at the solution.\nto gain some intuition for this idea, we can say that either the solution is on\nthe boundary imposed by the inequality and we must use its kkt multiplier to\ninfluence the solution to x, or the inequality has no influence on the solution and\nwe represent this by zeroing out its kkt multiplier.\n\n,\n\nthe properties that the gradient of the generalized lagrangian is zero, all\nconstraints on both x and the kkt multipliers are satisfied, and \u03b1 h(cid:129) (x) = 0\nare called the karush-kuhn-tucker (kkt) conditions (\nkarush 1939 kuhn and\ntucker 1951\n). together, these properties describe the optimal points of constrained\noptimization problems.\n\n,\n\n;\n\n,\n\nfor more information about the kkt approach, see nocedal and wright 2006\n\n(\n\n).\n\n95\n\n "}, {"Page_number": 111, "text": "chapter 4. numerical computation\n\n4.5 example: linear least squares\n\nsuppose we want to find the value of\n\nx\n\nthat minimizes\n\nf ( ) =x\n\n1\n2 ||\n\nax b 2\n\u2212 ||\n2.\n\n(4.21)\n\nthere are specialized linear algebra algorithms that can solve this problem efficiently.\nhowever, we can also explore how to solve it using gradient-based optimization as\na simple example of how these techniques work.\n\nfirst, we need to obtain the gradient:\n\nx a> (\n\n\u2207 xf ( ) = \n\nax b\u2212\n\n) = \n\na>ax a\u2212 >b.\n\n(4.22)\n\nwe can then follow this gradient downhill, taking small steps. see algorithm 4.1\n\nfor details.\n\nalgorithm 4.1 an algorithm to minimize f(x) = 1\n2 ||\nusing gradient descent.\n\nax b 2\n\u2212 ||\n\n2 with respect to x\n\n(cid:115)\n\nset the step size ( ) and tolerance ( ) to small, positive numbers.\nwhile ||a>ax a\u2212 > b||2 > \u03b4 do\nx\u2190 \u2212 (cid:115)(cid:18)a>ax a\u2212 >b(cid:19)\nend while\n\nx\n\n\u03b4\n\none can also solve this problem using newton\u2019s method. in this case, because\nthe true function is quadratic, the quadratic approximation employed by newton\u2019s\nmethod is exact, and the algorithm converges to the global minimum in a single\nstep.\n\nnow suppose\u00a0we\u00a0wish to\u00a0minimize the same\u00a0function, but subject\u00a0to the\n\nconstraint x>x \u2264 1. to do so, we introduce the lagrangian\n(cid:21)x> x \u2212 1(cid:22) .\n\n(x ) =  ( ) +x\n\nl , \u03bb\n\n\u03bb\n\nf\n\nwe can now solve the problem\n\nmin\n\nx\n\nmax\n\u03bb,\u03bb\u22650\n\nl , \u03bb .\n(x )\n\n(4.23)\n\n(4.24)\n\nthe smallest-norm solution to the unconstrained least squares problem may be\nfound using the moore-penrose pseudoinverse: x = a+ b. if this point is feasible,\nthen it is the solution to the constrained problem. otherwise, we must find a\n\n96\n\n "}, {"Page_number": 112, "text": "chapter 4. numerical computation\n\nsolution where the constraint is active. by differentiating the lagrangian with\nrespect to , we obtain the equation\n\nx\n\na>ax a\u2212 >b\n\nx+ 2\u03bb = 0.\n\nthis tells us that the solution will take the form\n\nx a= ( >a\n\ni+ 2\u03bb )\u22121a>b.\n\n(4.25)\n\n(4.26)\n\nthe magnitude of \u03bb must be chosen such that the result obeys the constraint. we\ncan find this value by performing gradient ascent on . to do so, observe\n\n\u03bb\n\n\u2202\n\u2202\u03bb\n\nl , \u03bb(x ) = x>x \u2212 1.\n\n(4.27)\n\nwhen the norm of x exceeds 1, this derivative is positive, so to follow the derivative\nuphill and increase the lagrangian with respect to \u03bb, we increase \u03bb. because the\ncoefficient on the x>x penalty has increased, solving the linear equation for x will\nnow yield a solution with smaller norm. the process of solving the linear equation\nand adjusting \u03bb continues until x has the correct norm and the derivative on \u03bb is\n0.\n\nthis concludes the mathematical preliminaries that we use to develop machine\nlearning algorithms. we are now ready to build and analyze some full-fledged\nlearning systems.\n\n97\n\n "}, {"Page_number": 113, "text": "chapter 5\n\nmachine learning basics\n\ndeep learning is a specific kind of machine learning.\nin order to understand\ndeep learning well, one must have a solid understanding of the basic principles\nof machine learning. this chapter provides a brief course in the most important\ngeneral principles that will be applied throughout the rest of the book. novice\nreaders or those who want a wider perspective are encouraged to consider machine\nlearning textbooks with a more comprehensive coverage of the fundamentals, such\n). if you are already familiar with machine\nas murphy 2012\nlearning basics, feel free to skip ahead to sec.\n. that section covers some per-\nspectives on traditional machine learning techniques that have strongly influenced\nthe development of deep learning algorithms.\n\nbishop 2006\n\n5.11\n\n) or\n\n(\n\n(\n\nwe begin with a definition of what a learning algorithm is, and present an\nexample: the linear regression algorithm.\u00a0we then proceed to describe how the\nchallenge of fitting the training data differs from the challenge of finding patterns\nthat generalize to new data. most machine learning algorithms have settings\ncalled hyperparameters that must be determined external to the learning algorithm\nitself; we discuss how to set these using additional data. machine learning is\nessentially a form of applied statistics with increased emphasis on the use of\ncomputers to statistically estimate complicated functions and a decreased emphasis\non proving confidence intervals around these functions; we therefore present the\ntwo central approaches to statistics: frequentist estimators and bayesian inference.\nmost machine learning algorithms can be divided into the categories of supervised\nlearning and unsupervised learning; we describe these categories and give some\nexamples of simple learning algorithms from each category. most deep learning\nalgorithms are\u00a0based on an\u00a0optimization algorithm called\u00a0stochastic gradient\ndescent. we describe how to combine various algorithm components such as an\n\n98\n\n "}, {"Page_number": 114, "text": "chapter 5. machine learning basics\n\noptimization algorithm, a cost function, a model, and a dataset to build a machine\nlearning algorithm. finally, in sec.\n, we describe some of the factors that have\nlimited the ability of traditional machine learning to generalize. these challenges\nhave motivated the development of deep learning algorithms that overcome these\nobstacles.\n\n5.11\n\n5.1 learning algorithms\n\n(\n\nmitchell 1997\n\na machine learning algorithm is an algorithm that is able to learn from data. but\nwhat do we mean by learning?\n) provides the definition \u201ca computer\nprogram is said to learn from experience e with respect to some class of tasks t\nand performance measure p , if its performance at tasks in t , as measured by p ,\nimproves with experience e.\u201d one can imagine a very wide variety of experiences\ne, tasks t , and performance measures p , and we do not make any attempt in this\nbook to provide a formal definition of what may be used for each of these entities.\ninstead, the following sections provide intuitive descriptions and examples of the\ndifferent kinds of tasks, performance measures and experiences that can be used\nto construct machine learning algorithms.\n\n5.1.1 the task, t\n\nmachine learning allows us to tackle tasks that are too difficult to solve with\nfixed programs written and designed by human beings. from a scientific and\nphilosophical point of view, machine learning is interesting because developing our\nunderstanding of machine learning entails developing our understanding of the\nprinciples that underlie intelligence.\n\nin this relatively formal definition of the word \u201ctask,\u201d the process of learning\nitself is not the task. learning is our means of attaining the ability to perform the\ntask. for example, if we want a robot to be able to walk, then walking is the task.\nwe could program the robot to learn to walk, or we could attempt to directly write\na program that specifies how to walk manually.\n\nmachine learning tasks are usually described in terms of how the machine\nlearning system should process an\nfeatures\nthat have been quantitatively measured from some object or event that we want\nthe machine learning system to process. we typically represent an example as a\nvector x \u2208 rn where each entry xi of the vector is another feature. for example,\nthe features of an image are usually the values of the pixels in the image.\n\n. an example is a collection of\n\nexample\n\n99\n\n "}, {"Page_number": 115, "text": "chapter 5. machine learning basics\n\nmany kinds of tasks can be solved with machine learning. some of the most\n\ncommon machine learning tasks include the following:\n\n\u2022 classification: in this type of task, the computer program is asked to specify\nwhich of k categories some input belongs to. to solve this task, the learning\nalgorithm is usually asked to produce a function f : rn \u2192 {1, . . . , k}. when\ny = f (x), the model assigns an input described by vector x to a category\nidentified by numeric code y. there are other variants of the classification\ntask, for example, where f outputs a probability distribution over classes.\nan example of a classification task is object recognition, where the input\nis an image (usually described as a set of pixel brightness values), and the\noutput is a numeric code identifying the object in the image. for example,\nthe willow garage pr2 robot is able to act as a waiter that can recognize\ndifferent kinds of drinks and deliver them to people on command (good-\n). modern object recognition is best accomplished with\nfellow\ndeep learning (\n). object\nrecognition is the same basic technology that allows computers to recognize\nfaces (taigman\n), which can be used to automatically tag people\nin photo collections and allow computers to interact more naturally with\ntheir users.\n\nkrizhevsky et al. 2012 ioffe and szegedy 2015\n\net al.,\n\net al.,\n\n2010\n\n2014\n\n,\n\n;\n\n,\n\nset\n\nsingle\n\nof functions. each function corresponds to classifying\n\n\u2022 classification with missing inputs: classification becomes more challenging if\nthe computer program is not guaranteed that every measurement in its input\nvector will always be provided. in order to solve the classification task, the\nlearning algorithm only has to define a\nfunction mapping from a vector\ninput to a categorical output. when some of the inputs may be missing,\nrather than providing a single classification function, the learning algorithm\nmust learn a\nx with\na different subset of its inputs missing. this kind of situation arises frequently\nin medical diagnosis, because many kinds of medical tests are expensive or\ninvasive. one way to efficiently define such a large set of functions is to learn\na probability distribution over all of the relevant variables, then solve the\nclassification task by marginalizing out the missing variables. with n input\nvariables, we can now obtain all 2n different classification functions needed\nfor each possible set of missing inputs, but we only need to learn a single\nfunction describing the joint probability distribution. see goodfellow et al.\n(\n) for an example of a deep probabilistic model applied to such a task\n2013b\nin this way.\u00a0many of the other tasks described in this section can also be\ngeneralized to work with missing inputs; classification with missing inputs is\njust one example of what machine learning can do.\n\n100\n\n "}, {"Page_number": 116, "text": "chapter 5. machine learning basics\n\n\u2022 regression: in this type of task, the computer program is asked to predict a\nnumerical value given some input. to solve this task, the learning algorithm\nis asked to output a function f : rn \u2192 r. this type of task is similar to\nclassification, except that the format of output is different. an example of\na regression task is the prediction of the expected claim amount that an\ninsured person will make (used to set insurance premiums), or the prediction\nof future prices of securities. these kinds of predictions are also used for\nalgorithmic trading.\n\n\u2022 transcription: in this type of task, the machine learning system is asked to\nobserve a relatively unstructured representation of some kind of data and\ntranscribe it into discrete, textual form. for example, in optical character\nrecognition, the computer program is shown a photograph containing an\nimage of text and is asked to return this text in the form of a sequence\nof characters (e.g., in ascii or unicode format). google street view uses\ndeep learning to process address numbers in this way (goodfellow et al.,\n2014d). another example is speech recognition, where the computer program\nis provided an audio waveform and emits a sequence of characters or word\nid codes describing the words that were spoken in the audio recording. deep\nlearning is a crucial component of modern speech recognition systems used\nat major companies including microsoft, ibm and google (hinton et al.,\n2012b).\n\n\u2022 machine translation: in a machine translation task, the input already consists\nof a sequence of symbols in some language, and the computer program must\nconvert this into a sequence of symbols in another language. this is commonly\napplied to natural languages, such as to translate from english to french.\ndeep learning has recently begun to have an important impact on this kind\nof task (sutskever\n\n2014 bahdanau\n\net al.,\n\net al.,\n\n2015\n\n).\n\n;\n\n\u2022 structured output: structured output tasks involve any task where the output\nis a vector (or other data structure containing multiple values) with important\nrelationships between the different elements. this is a broad category, and\nsubsumes the transcription and translation tasks described above, but also\nmany other tasks. one example is parsing\u2014mapping a natural language\nsentence into a tree that describes its grammatical structure and tagging nodes\nof the trees as being verbs, nouns, or adverbs, and so on. see\ncollobert 2011\n)\nfor an example of deep learning applied to a parsing task. another example\nis pixel-wise segmentation of images, where the computer program assigns\nevery pixel in an image to a specific category. for example, deep learning can\n\n(\n\n101\n\n "}, {"Page_number": 117, "text": "chapter 5. machine learning basics\n\n,\n\nbe used to annotate the locations of roads in aerial photographs (mnih and\nhinton 2010\n).\u00a0the output need not have its form mirror the structure of\nthe input as closely as in these annotation-style tasks. for example, in image\ncaptioning, the computer program observes an image and outputs a natural\n,\nkiros et al. 2014a b mao et al.\nlanguage sentence describing the image (\n2015 vinyals\n;\n;\n2014 karpathy and li 2015\n). these tasks are called structured output\nfang\ntasks because the program must output several values that are all tightly\ninter-related. for example, the words produced by an image captioning\nprogram must form a valid sentence.\n\n2015b donahue\n;\net al.,\n\net al.,\n2015 xu\n\net al.,\n\net al.,\n\n2015\n\n, ;\n\n,\n\n;\n\n,\n\n;\n\n\u2022 anomaly detection: in this type of task, the computer program sifts through\na set of events or objects, and flags some of them as being unusual or atypical.\nan example of an anomaly detection task is credit card fraud detection. by\nmodeling your purchasing habits, a credit card company can detect misuse\nof your cards.\u00a0if a thief steals your credit card or credit card information,\nthe thief\u2019s purchases will often come from a different probability distribution\nover purchase types than your own. the credit card company can prevent\nfraud by placing a hold on an account as soon as that card has been used\nfor an uncharacteristic purchase. see\n) for a survey of\nanomaly detection methods.\n\nchandola et al. 2009\n\n(\n\n\u2022 synthesis and sampling: in this type of task, the machine learning algorithm\nis asked to generate new examples that are similar to those in the training\ndata. synthesis and sampling via machine learning can be useful for media\napplications where it can be expensive or boring for an artist to generate large\nvolumes of content by hand.\u00a0for example, video games can automatically\ngenerate textures for large objects or landscapes, rather than requiring an\nartist to manually label each pixel (\n). in some cases, we\nwant the sampling or synthesis procedure to generate some specific kind of\noutput given the input. for example, in a speech synthesis task, we provide a\nwritten sentence and ask the program to emit an audio waveform containing\na spoken version of that sentence. this is a kind of structured output task,\nbut with the added qualification that there is no single correct output for\neach input, and we explicitly desire a large amount of variation in the output,\nin order for the output to seem more natural and realistic.\n\nluo et al. 2013\n\n,\n\n\u2022 imputation of missing values: in this type of task, the machine learning\nalgorithm is given a new example x \u2208 rn, but with some entries xi of x\nmissing. the algorithm must provide a prediction of the values of the missing\nentries.\n\n102\n\n "}, {"Page_number": 118, "text": "chapter 5. machine learning basics\n\n\u2022 denoising: in this type of task, the machine learning algorithm is given in\ninput a corrupted example \u02dcx \u2208 rn obtained by an unknown corruption process\nfrom a clean example x \u2208 rn. the learner must predict the clean example\nx from its corrupted version \u02dcx, or more generally predict the conditional\nprobability distribution p(x | \u02dcx).\n\n\u2022 density estimation\n\nor\n\nprobability mass function estimation\n:\n\nin the density\nestimation problem,\u00a0the machine learning algorithm is asked to learn a\nfunction pmodel : rn \u2192 r, where pmodel (x) can be interpreted as a probability\ndensity function (if x is continuous) or a probability mass function (if x is\ndiscrete) on the space that the examples were drawn from. to do such a task\nwell (we will specify exactly what that means when we discuss performance\nmeasures p ),\u00a0the\u00a0algorithm needs\u00a0to learn\u00a0the structure\u00a0of the\u00a0data it\nhas seen.\nit must know where examples cluster tightly and where they\nare unlikely to occur. most of the tasks described above require that the\nlearning algorithm has at\u00a0least implicitly captured\u00a0the structure\u00a0of the\nprobability distribution. density estimation allows us to explicitly capture\nthat distribution. in principle, we can then perform computations on that\ndistribution in order to solve the other tasks as well. for example, if we\nhave performed density estimation to obtain a probability distribution p(x),\nwe can use that distribution to solve the missing value imputation task. if\na value x i is missing and all of the other values, denoted x\u2212i, are given,\nthen we know the distribution over it is given by p(xi | x\u2212i). in practice,\ndensity estimation does not always allow us to solve all of these related tasks,\nbecause in many cases the required operations on p( x) are computationally\nintractable.\n\nof course, many other tasks and types of tasks are possible. the types of tasks\nwe list here are intended only to provide examples of what machine learning can\ndo, not to define a rigid taxonomy of tasks.\n\n5.1.2 the performance measure, p\n\nin order to evaluate the abilities of a machine learning algorithm, we must design\na quantitative measure of its performance. usually this performance measure p is\nspecific to the task\n\nbeing carried out by the system.\n\nt\n\nfor tasks such as classification, classification with missing inputs, and transcrip-\ntion, we often measure the accuracy of the model. accuracy is just the proportion\nof examples for which the model produces the correct output. we can also obtain\n\n103\n\n "}, {"Page_number": 119, "text": "chapter 5. machine learning basics\n\nequivalent information by measuring the error rate, the proportion of examples for\nwhich the model produces an incorrect output. we often refer to the error rate as\nthe expected 0-1 loss. the 0-1 loss on a particular example is 0 if it is correctly\nclassified and 1 if it is not. for tasks such as density estimation, it does not make\nsense to measure accuracy, error rate, or any other kind of 0-1 loss. instead, we\nmust use a different performance metric that gives the model a continuous-valued\nscore for each example. the most common approach is to report the average\nlog-probability the model assigns to some examples.\n\nusually we are interested in how well the machine learning algorithm performs\non data that it has not seen before, since this determines how well it will work when\ndeployed in the real world. we therefore evaluate these performance measures\nusing a\nof data that is separate from the data used for training the machine\nlearning system.\n\ntest set\n\nthe choice of performance measure may seem straightforward and objective,\nbut it is often difficult to choose a performance measure that corresponds well to\nthe desired behavior of the system.\n\nin some cases, this is because it is difficult to decide what should be measured.\nfor example, when performing a transcription task, should we measure the accuracy\nof the system at transcribing entire sequences, or should we use a more fine-grained\nperformance measure that gives partial credit for getting some elements of the\nsequence correct? when performing a regression task, should we penalize the\nsystem more if it frequently makes medium-sized mistakes or if it rarely makes\nvery large mistakes? these kinds of design choices depend on the application.\n\nin other cases, we know what quantity we would ideally like to measure, but\nmeasuring it is impractical. for example, this arises frequently in the context of\ndensity estimation. many of the best probabilistic models represent probability\ndistributions only implicitly. computing the actual probability value assigned to\na specific point in space in many such models is intractable. in these cases, one\nmust design an alternative criterion that still corresponds to the design objectives,\nor design a good approximation to the desired criterion.\n\n5.1.3 the experience, e\n\nsu-\nmachine learning algorithms can be broadly categorized as unsupervised\npervised by what kind of experience they are allowed to have during the learning\nprocess.\n\nor\n\nmost of the learning algorithms in this book can be understood as being allowed\n. a dataset is a collection of many examples, as\n\nto experience an entire\n\ndataset\n\n104\n\n "}, {"Page_number": 120, "text": "chapter 5. machine learning basics\n\ndefined in sec.\n\n5.1.1\n\n. sometimes we will also call examples\n\ndata points.\n\nfisher 1936\n\none of the oldest datasets studied by statisticians and machine learning re-\nsearchers is the iris dataset (\n). it is a collection of measurements of\ndifferent parts of 150 iris plants. each individual plant corresponds to one example.\nthe features within each example are the measurements of each of the parts of the\nplant: the sepal length, sepal width, petal length and petal width.\u00a0the dataset\nalso records which species each plant belonged to. three different species are\nrepresented in the dataset.\n\n,\n\nunsupervised learning algorithms experience a dataset containing many features,\nthen learn useful properties of the structure of this dataset. in the context of deep\nlearning, we usually want to learn the entire probability distribution that generated\na dataset, whether explicitly as in density estimation or implicitly for tasks like\nsynthesis or denoising. some other unsupervised learning algorithms perform other\nroles, like clustering, which consists of dividing the dataset into clusters of similar\nexamples.\n\nsupervised learning algorithms experience a dataset containing features, but\neach example is also associated with a label\n. for example, the iris dataset\nis annotated with the species of each iris plant. a supervised learning algorithm\ncan study the iris dataset and learn to classify iris plants into three different species\nbased on their measurements.\n\ntarget\n\nor\n\nroughly speaking, unsupervised learning involves observing several examples\nof a random vector x, and attempting to implicitly or explicitly learn the proba-\nbility distribution p(x), or some interesting properties of that distribution, while\nsupervised learning involves observing several examples of a random vector x and\nan associated value or vector y, and learning to predict y from x, usually by\nestimating p(y x|\n). the term supervised learning originates from the view of\nthe target y being provided by an instructor or teacher who shows the machine\nlearning system what to do. in unsupervised learning, there is no instructor or\nteacher, and the algorithm must learn to make sense of the data without this guide.\n\nunsupervised learning and supervised learning are not formally defined terms.\nthe lines between them are often blurred. many machine learning technologies can\nbe used to perform both tasks. for example, the chain rule of probability states\nthat for a vector x \u2208 rn, the joint distribution can be decomposed as\n\np( ) =x\n\np(xi | x1, . . . , xi\u22121).\n\n(5.1)\n\nnyi=1\n\nthis decomposition means that we can solve the ostensibly unsupervised problem of\nmodeling p(x) by splitting it into n supervised learning problems. alternatively, we\n\n105\n\n "}, {"Page_number": 121, "text": "chapter 5. machine learning basics\n\ncan solve the supervised learning problem of learning p(y | x) by using traditional\nunsupervised\u00a0learning technologies to\u00a0learn\u00a0the joint distribution p(x, y) and\ninferring\n\np y(\n\n| x) =\n\np , y\n(x )\n\n(x 0)\n\npy0 p , y\n\n.\n\n(5.2)\n\nthough unsupervised learning and supervised learning are not completely formal or\ndistinct concepts, they do help to roughly categorize some of the things we do with\nmachine learning algorithms. traditionally, people refer to regression, classification\nand structured output problems as supervised learning. density estimation in\nsupport of other tasks is usually considered unsupervised learning.\n\nother variants of the learning paradigm are possible. for example, in semi-\nsupervised learning, some examples include a supervision target but others do\nnot.\nin multi-instance learning, an entire collection of examples is labeled as\ncontaining or not containing an example of a class, but the individual members\nof the collection are not labeled. for a recent example of multi-instance learning\nwith deep models, see\n\nkotzias et al. 2015\n\n).\n\n(\n\nsome machine learning algorithms do not just experience a fixed dataset. for\nexample, reinforcement learning algorithms interact with an environment, so there\nis a feedback loop between the learning system and its experiences. such algorithms\nbertsekas\nare beyond the scope of this book. please see\nand tsitsiklis 1996\net al.\n(\n2013\n\n) for the deep learning approach to reinforcement learning.\n\n) for information about reinforcement learning, and\n\nsutton and barto 1998\n\nmnih\n\n) or\n\n(\n\n(\n\nmost machine learning algorithms simply experience a dataset. a dataset can\nbe described in many ways. in all cases, a dataset is a collection of examples,\nwhich are in turn collections of features.\n\none common way of describing a dataset is with a\n\n. a design\nmatrix is a matrix containing a different example in each row. each column of the\nmatrix corresponds to a different feature. for instance, the iris dataset contains\n150 examples with four features for each example. this means we can represent\nthe dataset with a design matrix x \u2208 r150 4\u00d7 , where xi,1 is the sepal length of\nplant i, xi,2 is the sepal width of plant i, etc. we will describe most of the learning\nalgorithms in this book in terms of how they operate on design matrix datasets.\n\ndesign matrix\n\nof course, to describe a dataset as a design matrix, it must be possible to\ndescribe each example as a vector, and each of these vectors must be the same size.\nthis is not always possible. for example, if you have a collection of photographs\nwith different widths and heights, then different photographs will contain different\nnumbers of pixels, so not all of the photographs may be described with the same\ndescribe how to handle different types\nlength of vector. sec.\n\nand chapter\n\n9.7\n\n10\n\n106\n\n "}, {"Page_number": 122, "text": "chapter 5. machine learning basics\n\nof such heterogeneous data. in cases like these, rather than describing the dataset\nas a matrix with m rows, we will describe it as a set containing m elements:\n{x(1), x(2), . . . , x(\n)m }. this notation does not imply that any two example vectors\nx( )i and x( )j have the same size.\n\nin the case of supervised learning, the example contains a label or target as\nwell as a collection of features. for example, if we want to use a learning algorithm\nto perform object recognition from photographs, we need to specify which object\nappears in each of the photos. we might do this with a numeric code, with 0\nsignifying a person, 1 signifying a car, 2 signifying a cat, etc. often when working\nwith a dataset containing a design matrix of feature observations x, we also\nprovide a vector of labels\n\nyi providing the label for example .i\n\n, with\n\ny\n\nof course, sometimes the label may be more than just a single number. for\nexample, if we want to train a speech recognition system to transcribe entire\nsentences, then the label for each example sentence is a sequence of words.\n\njust as there is no formal definition of supervised and unsupervised learning,\nthere is no rigid taxonomy of datasets or experiences. the structures described here\ncover most cases, but it is always possible to design new ones for new applications.\n\n5.1.4 example: linear regression\n\nour definition of a machine learning algorithm as an algorithm that is capable\nof improving a computer program\u2019s performance at some task via experience is\nsomewhat abstract. to make this more concrete, we present an example of a simple\nmachine learning algorithm:\nlinear regression. we will return to this example\nrepeatedly as we introduce more machine learning concepts that help to understand\nits behavior.\n\nas the name implies, linear regression solves a regression problem.\u00a0in other\nwords, the goal is to build a system that can take a vector x \u2208 rn as input and\npredict the value of a scalar y \u2208 r as its output. in the case of linear regression,\nthe output is a linear function of the input.\u00a0let \u02c6y be the value that our model\npredicts\n\nshould take on. we define the output to be\n\ny\n\n\u02c6y = w>x\n\n(5.3)\n\nwhere w \u2208 rn is a vector of parameters.\n\nparameters are values that control the behavior of the system. in this case, wi is\nthe coefficient that we multiply by feature xi before summing up the contributions\nfrom all the features. we can think of w as a set of\nthat determine how\neach feature affects the prediction. if a feature xi receives a positive weight wi ,\n\nweights\n\n107\n\n "}, {"Page_number": 123, "text": "chapter 5. machine learning basics\n\nthen increasing the value of that feature increases the value of our prediction \u02c6y.\nif a feature receives a negative weight, then increasing the value of that feature\ndecreases the value of our prediction. if a feature\u2019s weight is large in magnitude,\nthen it has a large effect on the prediction. if a feature\u2019s weight is zero, it has no\neffect on the prediction.\n\nwe thus have a definition of our task t :\u00a0to predict y from x by outputting\n\n\u02c6y = w >x. next we need a definition of our performance measure,\n\n.p\n\nsuppose that we have a design matrix of m example inputs that we will not\nuse for training, only for evaluating how well the model performs. we also have\na vector of regression targets providing the correct value of y for each of these\nexamples. because this dataset will only be used for evaluation, we call it the test\nset. we refer to the design matrix of inputs as x(\ntest and the vector of regression\ntargets as y (\n\n)\ntest .\n\n)\n\none way of measuring the performance of the model is to compute the mean\ntest gives the predictions of the\n\nsquared error of the model on the test set.\u00a0if \u02c6y (\nmodel on the test set, then the mean squared error is given by\n\n)\n\nmsetest =\n\n1\n\nmxi\n\n( \u02c6y (\n\n)\n\ntest \u2212 y(\n\n)\n\ntest )2\ni .\n\n(5.4)\n\nintuitively, one can see that this error measure decreases to 0 when \u02c6y (\nwe can also see that\n\n)\n\ntest = y(\n\n)\ntest .\n\nmse test =\n\n1\nm||\u02c6y (\n\n)\n\ntest \u2212 y(\n\n)\n\ntest ||2\n2 ,\n\n(5.5)\n\nso the error increases whenever the euclidean distance between the predictions\nand the targets increases.\n\nto make a machine learning algorithm, we need to design an algorithm that\nwill improve the weights w in a way that reduces msetest when the algorithm\nis allowed to gain experience by observing a training set (x(\ntrain ). one\nintuitive way of doing this (which we will justify later, in sec.\n) is just to\nminimize the mean squared error on the training set, msetrain.\n\ntrain , y(\n5.5.1\n\n)\n\n)\n\nto minimize msetrain, we can simply solve for where its gradient is\n\n:0\n\n\u2207w msetrain = 0\n1\nm|| \u02c6y (\n\u21d2 \u2207 w\n1\nm\u2207w||x(\n\ntrain \u2212 y(\ntrain w y\u2212 (\n\ntrain ||2\ntrain ||2\n\n)\n\n)\n\n)\n\n)\n\n\u21d2\n\n2 = 0\n\n2 = 0\n\n108\n\n(5.6)\n\n(5.7)\n\n(5.8)\n\n "}, {"Page_number": 124, "text": "chapter 5. machine learning basics\n\ny\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\u22122\n\u22123\n\nlinear regression example\n\n\u2212\n\u22121 0.\n\n0 5\n\n.\n\n0 5\n\n.\n\n1 0\n\n.\n\n.\n0 0\nx1\n\n)\nn\ni\na\nr\nt\n(\n\ne\ns\nm\n\n0 55.\n\n0 50.\n\n0 45.\n\n0 40.\n\n0 35.\n\n0 30.\n\n0 25.\n\n0 20.\n\noptimization of w\n\n0 5\n\n.\n\n.\n1 0\nw 1\n\n1 5\n\n.\n\nfigure 5.1: a linear regression problem, with a training set consisting of ten data points,\neach containing one feature. because there is only one feature, the weight vector w\ncontains only a single parameter to learn, w1. (left) observe that linear regression learns\nto set w1 such that the line y = w1x comes as close as possible to passing through all the\ntraining points.\nw 1 found by the normal\nequations, which we can see minimizes the mean squared error on the training set.\n\nthe plotted point indicates the value of\n\n(right)\n\n(5.9)\n\n)\n\ntrain (cid:31) = 0\n\n(5.10)\n(5.11)\n\n)\n\ntrain(cid:31)>(cid:30)x (\nw\u2212 2 >x (\ntrain w x\u2212 2\ntrain (cid:31)\u22121\n\n)\n\n)\n\n\u21d2 \u2207w(cid:30)x (\n\n)\n\ntrain w y\u2212 (\n\ntrain >x (\n\n)\n\ntrain w\n\n)\n\n)\n\ntrain w y\u2212 (\ntrain + y (\n\n)\n\n)\n\ntrain (cid:31) = 0\n\ntrain >y(\n\n)\n\ntrain >y (\n\n)\n\n\u21d2 \u2207w(cid:30)w>x(\n\ntrain >x (\n\n)\n\n\u21d2 2x (\n\u21d2 w =(cid:30)x (\n\ntrain > y(\n(\n\n)\n\ntrain = 0\n\n)\n\ntrain > x(\n\n)\n\nx (\n\ntrain >y (\n\n)\n\n)\ntrain\n\n(5.12)\n\nthe system of equations whose solution is given by eq.\n\nnormal equations.\u00a0evaluating eq.\nfor an example of the linear regression learning algorithm in action, see fig.\n\nis known as the\nconstitutes a simple learning algorithm.\n.5.1\n\n5.12\n\n5.12\n\nit is worth noting that the term linear regression is often used to refer to a\nslightly more sophisticated model with one additional parameter\u2014an intercept\nterm . in this model\n\nb\n\n\u02c6y = w >x + b\n\n(5.13)\n\nso the mapping from parameters to predictions is still a linear function but the\nmapping from features to predictions is now an affine function. this extension to\naffine functions means that the plot of the model\u2019s predictions still looks like a\nline, but it need not pass through the origin. instead of adding the bias parameter\nb, one can continue to use the model with only weights but augment x with an\n\n109\n\n "}, {"Page_number": 125, "text": "chapter 5. machine learning basics\n\nextra entry that is always set to . the weight corresponding to the extra\nentry\nplays the role of the bias parameter. we will frequently use the term \u201clinear\u201d when\nreferring to affine functions throughout this book.\n\n1\n\n1\n\nthe intercept term b is often called the\n\nparameter of the affine transfor-\nmation. this terminology derives from the point of view that the output of the\ntransformation is biased toward being b in the absence of any input.\u00a0this term\nis different from the idea of a statistical bias, in which a statistical estimation\nalgorithm\u2019s expected estimate of a quantity is not equal to the true quantity.\n\nbias\n\nlinear regression is of course an extremely simple and limited learning algorithm,\nbut it provides an example of how a learning algorithm can work. in the subsequent\nsections we will describe some of the basic principles underlying learning algorithm\ndesign and demonstrate how these principles can be used to build more complicated\nlearning algorithms.\n\n5.2 capacity, overfitting and underfitting\n\nthe central challenge in machine learning is that we must perform well on new,\npreviously unseen inputs\u2014not just those on which our model was trained. the\nability to perform well on previously unobserved inputs is called generalization.\n\ntypically, when training a machine learning model, we have access to a training\nset, we can compute some error measure on the training set called the training\nerror, and we reduce this training error. so far, what we have described is simply\nan optimization problem. what separates machine learning from optimization is\nthat we want the generalization error\n, to be low as well.\nthe generalization error is defined as the expected value of the error on a new\ninput. here the expectation is taken across different possible inputs, drawn from\nthe distribution of inputs we expect the system to encounter in practice.\n\n, also called the\n\ntest error\n\nwe typically estimate the generalization error of a machine learning model by\nof examples that were collected separately\n\ntest set\n\nmeasuring its performance on a\nfrom the training set.\n\nin our linear regression example, we trained the model by minimizing the\n\ntraining error,\n\n1\ntrain ||x (\n\n)\n\nm(\n\n)\n\ntrain w y\u2212 (\n\n)\n\ntrain ||2\n2,\n\n(5.14)\n\nbut we actually care about the test error,\n\n1\n\ntest ||x (\n\n)\n\n)\n\ntest w y\u2212 (\n\n)\n\ntest ||2\n2.\n\nm(\n\nhow can we affect performance on the test set when we get to observe only the\ntraining set? the field of statistical learning theory provides some answers. if the\n\n110\n\n "}, {"Page_number": 126, "text": "chapter 5. machine learning basics\n\ntraining and the test set are collected arbitrarily, there is indeed little we can do.\nif we are allowed to make some assumptions about how the training and test set\nare collected, then we can make some progress.\n\ni.i.d. assumptions\n\nthe train and test data are generated by a probability distribution over datasets\ncalled the data generating process. we typically make a set of assumptions known\ncollectively as the\nthese assumptions are that the examples\nin each dataset are independent from each other, and that the train set and test\nset are identically distributed, drawn from the same probability distribution as\neach other. this assumption allows us to describe the data generating process\nwith a probability distribution over a single example. the same distribution is\nthen used to generate every train example and every test example. we call that\nshared underlying distribution the data generating distribution, denoted pdata. this\nprobabilistic framework and the i.i.d. assumptions allow us to mathematically\nstudy the relationship between training error and test error.\n\none immediate connection we can observe between the training and test error\nis that the expected training error of a randomly selected model is equal to the\nexpected test error of that model. suppose we have a probability distribution\np(x, y ) and we sample from it repeatedly to generate the train set and the test set.\nfor some fixed value w, then the expected training set error is exactly the same as\nthe expected test set error, because both expectations are formed using the same\ndataset sampling process. the only difference between the two conditions is the\nname we assign to the dataset we sample.\n\nof course,\u00a0when\u00a0we use a machine learning algorithm,\u00a0we do\u00a0not fix the\nparameters ahead of time, then sample both datasets. we sample the training set,\nthen use it to choose the parameters to reduce training set error, then sample the\ntest set.\u00a0under this process, the expected test error is greater than or equal to\nthe expected value of training error. the factors determining how well a machine\nlearning algorithm will perform are its ability to:\n\n1.\u00a0make the training error small.\n\n2.\u00a0make the gap between training and test error small.\n\nthese two factors correspond to the two central challenges in machine learning:\n. underfitting occurs when the model is not able to\nunderfitting\nobtain a sufficiently low error value on the training set. overfitting occurs when\nthe gap between the training error and test error is too large.\n\noverfitting\n\nand\n\nwe can control whether a model is more likely to overfit or underfit by altering\nits capacity.\u00a0informally, a model\u2019s capacity is its ability to fit a wide variety of\n\n111\n\n "}, {"Page_number": 127, "text": "chapter 5. machine learning basics\n\nfunctions. models with low capacity may struggle to fit the training set. models\nwith high capacity can overfit by memorizing properties of the training set that do\nnot serve them well on the test set.\n\none way to control the capacity of a learning algorithm is by choosing its\nhypothesis space, the set of functions that the learning algorithm is allowed to\nselect as being the solution. for example, the linear regression algorithm has the\nset of all linear functions of its input as its hypothesis space. we can generalize\nlinear regression to include polynomials, rather than just linear functions, in its\nhypothesis space. doing so increases the model\u2019s capacity.\n\na polynomial of degree one gives us the linear regression model with which we\n\nare already familiar, with prediction\n\n\u02c6y\n\n=  +\n\nb wx.\n\n(5.15)\n\nby introducing x2 as another feature provided to the linear regression model, we\ncan learn a model that is quadratic as a function of\n\n:x\n\n\u02c6y\n\n=  + 1x w+ 2x2 .\n\nb w\n\n(5.16)\n\nthough this model implements a quadratic function of its input, the output is\nstill a linear function of the parameters, so we can still use the normal equations\nto train the model in closed form. we can continue to add more powers of x as\nadditional features, for example to obtain a polynomial of degree 9:\n\n\u02c6y\n\nb=  +\n\nwixi .\n\n9xi=1\n\n(5.17)\n\nmachine learning algorithms will generally perform best when their capacity\nis appropriate in regard to the true complexity of the task they need to perform\nand the amount of training data they are provided with. models with insufficient\ncapacity are unable to solve complex tasks. models with high capacity can solve\ncomplex tasks, but when their capacity is higher than needed to solve the present\ntask they may overfit.\n\n5.2\n\nfig.\n\nshows this principle in action. we compare a linear, quadratic and\ndegree-9 predictor attempting to fit a problem where the true underlying function\nis quadratic. the linear function is unable to capture the curvature in the true un-\nderlying problem, so it underfits. the degree-9 predictor is capable of representing\nthe correct function, but it is also capable of representing infinitely many other\nfunctions that pass exactly through the training points, because we have more\n\n112\n\n "}, {"Page_number": 128, "text": "chapter 5. machine learning basics\n\nparameters than training examples. we have little chance of choosing a solution\nthat generalizes well when so many wildly different solutions exist. in this example,\nthe quadratic model is perfectly matched to the true structure of the task so it\ngeneralizes well to new data.\n\nunderfitting\n\nappropriate\u00a0capacity\n\noverfitting\n\ny\n\ny\n\ny\n\nx\n\n0\n\nx\n\n0\n\nx\n\n0\n\nfigure 5.2: we fit three models to this example training set. the training data was\ngenerated synthetically, by randomly sampling x values and choosing y deterministically\nby evaluating a quadratic function. (left) a linear function fit to the data suffers from\nunderfitting\u2014it cannot capture the curvature that is present in the data. (\n) a\nquadratic function fit to the data generalizes well to unseen points. it does not suffer from\na significant amount of overfitting or underfitting. (\n) a polynomial of degree 9 fit to\nthe data suffers from overfitting. here we used the moore-penrose pseudoinverse to solve\nthe underdetermined normal equations. the solution passes through all of the training\npoints exactly, but we have not been lucky enough for it to extract the correct structure.\nit now has a deep valley in between two training points that does not appear in the true\nunderlying function. it also increases sharply on the left side of the data, while the true\nfunction decreases in this area.\n\ncenter\n\nright\n\nso far we have only described changing a model\u2019s capacity by changing the\nnumber\u00a0of input\u00a0features\u00a0it has\u00a0(and simultaneously adding\u00a0new parameters\nassociated with those features). there are in fact many ways of changing a model\u2019s\ncapacity. capacity is not determined only by the choice of model. the model\nspecifies which family of functions the learning algorithm can choose from when\nvarying the parameters in order to reduce a training objective. this is called the\nrepresentational capacity of the model. in many cases, finding the best function\nwithin this family is a very difficult optimization problem. in practice, the learning\nalgorithm does not actually find the best function, but merely one that significantly\nreduces the training error. these additional limitations, such as the imperfection\n\n113\n\n "}, {"Page_number": 129, "text": "chapter 5. machine learning basics\n\nof the optimization algorithm, mean that the learning algorithm\u2019s effective capacity\nmay be less than the representational capacity of the model family.\n\nour modern ideas about improving the generalization of machine learning\nmodels are refinements of thought dating back to philosophers at least as early\nas ptolemy. many early scholars invoke a principle of parsimony that is now\nmost widely known as occam\u2019s razor (c.\u00a01287-1347).\u00a0this principle states that\namong competing hypotheses that explain known observations equally well, one\nshould choose the \u201csimplest\u201d one. this idea was formalized and made more precise\nin the 20th century by the founders of statistical learning theory (vapnik and\nchervonenkis 1971 vapnik 1982 blumer\n\n1989 vapnik 1995\n\net al.,\n\n).\n\n,\n\n;\n\n,\n\n;\n\n;\n\n,\n\nstatistical learning theory provides various means of quantifying model capacity.\namong these, the most well-known is the vapnik-chervonenkis dimension, or vc\ndimension. the vc dimension measures the capacity of a binary classifier. the\nvc dimension is defined as being the largest possible value of m for which there\nexists a training set of m different x points that the classifier can label arbitrarily.\n\n;\n\n,\n\n;\n\nquantifying the capacity of the model allows statistical learning theory to\nmake quantitative predictions. the most important results in statistical learning\ntheory show that the discrepancy between training error and generalization error\nis bounded from above by a quantity that grows as the model capacity grows but\nshrinks as the number of training examples increases (vapnik and chervonenkis,\n1971 vapnik 1982 blumer\n). these bounds provide\nintellectual justification that machine learning algorithms can work, but they are\nrarely used in practice when working with deep learning algorithms. this is in\npart because the bounds are often quite loose and in part because it can be quite\ndifficult to determine the capacity of deep learning algorithms.\u00a0the problem of\ndetermining the capacity of a deep learning model is especially difficult because the\neffective capacity is limited by the capabilities of the optimization algorithm, and\nwe have little theoretical understanding of the very general non-convex optimization\nproblems involved in deep learning.\n\n1989 vapnik 1995\n\net al.,\n\n;\n\n,\n\nwe must remember that while simpler functions are more likely to generalize\n(to have a small gap between training and test error) we must still choose a\nsufficiently complex hypothesis to achieve low training error. typically, training\nerror decreases until it asymptotes to the minimum possible error value as model\ncapacity increases (assuming the error measure has a minimum value). typically,\ngeneralization error has a u-shaped curve as a function of model capacity. this is\nillustrated in fig.\n\n.5.3\n\nto reach the most extreme case of arbitrarily high capacity, we introduce\nthe concept of non-parametric models. so far,\u00a0we have seen only parametric\n\n114\n\n "}, {"Page_number": 130, "text": "chapter 5. machine learning basics\n\nunderfitting zone overfitting zone\n\ntraining error\ngeneralization error\n\nr\no\nr\nr\ne\n\n0\n\noptimal capacity\n\ncapacity\n\ngeneralization gap\n\nfigure 5.3: typical relationship between capacity and error. training and test error\nbehave differently.\u00a0at the left end of the graph, training error and generalization error\nare both high. this is the underfitting regime.\u00a0as we increase capacity, training error\ndecreases, but the gap between training and generalization error increases. eventually,\nthe size of this gap outweighs the decrease in training error, and we enter the overfitting\nregime, where capacity is too large, above the optimal capacity.\n\nmodels, such as linear regression. parametric models learn a function described\nby a parameter vector whose size is finite and fixed before any data is observed.\nnon-parametric models have no such limitation.\n\nsometimes, non-parametric models are just theoretical abstractions (such as\nan algorithm that searches over all possible probability distributions) that cannot\nbe implemented in practice. however, we can also design practical non-parametric\nmodels by making their complexity a function of the training set size. one example\nof such an algorithm is nearest neighbor regression. unlike linear regression, which\nhas a fixed-length vector of weights, the nearest neighbor regression model simply\nstores the x and y from the training set. when asked to classify a test point x,\nthe model looks up the nearest entry in the training set and returns the associated\nregression target. in other words, \u02c6y = yi where i = arg min||x i,: \u2212 ||x 2\n2. the\nalgorithm can also be generalized to distance metrics other than the l 2 norm, such\nas learned distance metrics (\n). if the algorithm is allowed\nto break ties by averaging the yi values for all xi,: that are tied for nearest, then\nthis algorithm is able to achieve the minimum possible training error (which might\nbe greater than zero, if two identical inputs are associated with different outputs)\non any regression dataset.\n\ngoldberger et al. 2005\n\n,\n\nfinally, we can also create a non-parametric learning algorithm by wrapping a\nparametric learning algorithm inside another algorithm that increases the number\n\n115\n\n "}, {"Page_number": 131, "text": "chapter 5. machine learning basics\n\nof parameters as needed. for example, we could imagine an outer loop of learning\nthat changes the degree of the polynomial learned by linear regression on top of a\npolynomial expansion of the input.\n\nthe ideal model is an oracle that simply knows the true probability distribution\nthat generates the data. even such a model will still incur some error on many\nproblems, because there may still be some noise in the distribution. in the case\nof supervised learning, the mapping from x to y may be inherently stochastic,\nor y may be a deterministic function that involves other variables besides those\nincluded in x. the error incurred by an oracle making predictions from the true\ndistribution\n\nbayes error.\n\nis called the\n\n(x )\n, y\n\np\n\ntraining and generalization error vary as the size of the training set varies.\nexpected generalization error can never increase as the number of training examples\nincreases. for non-parametric models, more data yields better generalization until\nthe best possible error is achieved. any fixed parametric model with less than\noptimal capacity will asymptote to an error value that exceeds the bayes error. see\nfig.\nfor an illustration. note that it is possible for the model to have optimal\ncapacity and yet still have a large gap between training and generalization error.\nin this situation, we may be able to reduce this gap by gathering more training\nexamples.\n\n5.4\n\n5.2.1 the no free lunch theorem\n\nlearning theory claims that a machine learning algorithm can generalize well from\na finite training set of examples. this seems to contradict some basic principles of\nlogic. inductive reasoning, or inferring general rules from a limited set of examples,\nis not logically valid.\u00a0to logically infer a rule describing every member of a set,\none must have information about every member of that set.\n\nin part, machine learning avoids this problem by offering only probabilistic rules,\nrather than the entirely certain rules used in purely logical reasoning.\u00a0machine\nlearning promises to find rules that are probably\nmembers of\nthe set they concern.\n\ncorrect about\n\nmost\n\nunfortunately, even this does not resolve the entire problem. the no free lunch\ntheorem for machine learning (wolpert 1996\n) states that, averaged over all possible\ndata generating distributions, every classification algorithm has the same error\nrate when classifying previously unobserved points. in other words, in some sense,\nno machine learning algorithm is universally any better than any other. the most\nsophisticated algorithm we can conceive of has the same average performance (over\nall possible tasks) as merely predicting that every point belongs to the same class.\n\n,\n\n116\n\n "}, {"Page_number": 132, "text": "chapter 5. machine learning basics\n\n)\ne\ns\nm\n\n(\n\u00a0\nr\no\nr\nr\ne\n\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n\n100\n\n)\ne\ne\nr\ng\ne\nd\n\u00a0\nl\na\ni\nm\no\nn\ny\nl\no\np\n(\n\u00a0\ny\nt\ni\nc\na\np\na\nc\n\u00a0\nl\na\nm\n\ni\nt\np\no\n\n20\n\n15\n\n10\n\n5\n\n0\n100\n\nbayes\u00a0error\ntrain\u00a0(quadratic)\ntest\u00a0(quadratic)\ntest\u00a0(optimal\u00a0capacity)\ntrain\u00a0(optimal\u00a0capacity)\n\n101\n\n102\n\n103\n\n104\n\n105\n\nnumber\u00a0of\u00a0training\u00a0examples\n\n101\n\n102\n\n103\n\n104\n\n105\n\nnumber\u00a0of\u00a0training\u00a0examples\n\nfigure 5.4: the effect of the training dataset size on the train and test error, as well as\non the optimal model capacity. we constructed a synthetic regression problem based on\nadding moderate amount of noise to a degree 5 polynomial, generated a single test set,\nand then generated several different sizes of training set. for each size, we generated 40\ndifferent training sets in order to plot error bars showing 95% confidence intervals. (top)\nthe mse on the train and test set for two different models:\u00a0a quadratic model, and a\nmodel with degree chosen to minimize the test error. both are fit in closed form. for\nthe quadratic model, the training error increases as the size of the training set increases.\nthis is because larger datasets are harder to fit. simultaneously, the test error decreases,\nbecause fewer incorrect hypotheses are consistent with the training data. the quadratic\nmodel does not have enough capacity to solve the task, so its test error asymptotes to\na high value. the test error at optimal capacity asymptotes to the bayes error. the\ntraining error can fall below the bayes error, due to the ability of the training algorithm\nto memorize specific instances of the training set. as the training size increases to infinity,\nthe training error of any fixed-capacity model (here, the quadratic model) must rise to at\nleast the bayes error.\nas the training set size increases, the optimal capacity\n(shown here as the degree of the optimal polynomial regressor) increases.\u00a0the optimal\ncapacity plateaus after reaching sufficient complexity to solve the task.\n\n(bottom)\n\n117\n\n "}, {"Page_number": 133, "text": "chapter 5. machine learning basics\n\nfortunately, these results hold only when we average over\n\npossible data\ngenerating distributions. if we make assumptions about the kinds of probability\ndistributions we encounter in real-world applications, then we can design learning\nalgorithms that perform well on these distributions.\n\nall\n\nthis means that the goal of machine learning research is not to seek a universal\nlearning algorithm or the absolute best learning algorithm. instead, our goal is to\nunderstand what kinds of distributions are relevant to the \u201creal world\u201d that an ai\nagent experiences, and what kinds of machine learning algorithms perform well on\ndata drawn from the kinds of data generating distributions we care about.\n\n5.2.2 regularization\n\nthe no free lunch theorem implies that we must design our machine learning\nalgorithms to perform well on a specific task. we do so by building a set of\npreferences into the learning algorithm. when these preferences are aligned with\nthe learning problems we ask the algorithm to solve, it performs better.\n\nso far, the only method of modifying a learning algorithm we have discussed is\nto increase or decrease the model\u2019s capacity by adding or removing functions from\nthe hypothesis space of solutions the learning algorithm is able to choose. we gave\nthe specific example of increasing or decreasing the degree of a polynomial for a\nregression problem. the view we have described so far is oversimplified.\n\nthe behavior of our algorithm is strongly affected not just by how large we\nmake the set of functions allowed in its hypothesis space, but by the specific identity\nof those functions. the learning algorithm we have studied so far, linear regression,\nhas a hypothesis space consisting of the set of linear functions of its input. these\nlinear functions can be very useful for problems where the relationship between\ninputs and outputs truly is close to linear. they are less useful for problems\nthat behave in a very nonlinear fashion. for example, linear regression would\nnot perform very well if we tried to use it to predict sin(x) from x. we can thus\ncontrol the performance of our algorithms by choosing what kind of functions we\nallow them to draw solutions from, as well as by controlling the amount of these\nfunctions.\n\nwe can also give a learning algorithm a preference for one solution in its\nhypothesis space to another. this means that both functions are eligible, but one\nis preferred.\u00a0the unpreferred solution be chosen only if it fits the training data\nsignificantly better than the preferred solution.\n\nfor example,\u00a0we can modify the training criterion for linear regression to\ninclude weight decay. to perform linear regression with weight decay, we minimize\n\n118\n\n "}, {"Page_number": 134, "text": "chapter 5. machine learning basics\n\na sum comprising both the mean squared error on the training and a criterion\nj (w) that expresses a preference for the weights to have smaller squared l2 norm.\nspecifically,\n\nj(\n\n) = w msetrain + \u03bbw>w,\n\n(5.18)\n\nwhere \u03bb is a value chosen ahead of time that controls the strength of our preference\nfor smaller weights. when \u03bb = 0, we impose no preference, and larger \u03bb forces the\nweights to become smaller. minimizing j (w ) results in a choice of weights that\nmake a tradeoff between fitting the training data and being small. this gives us\nsolutions that have a smaller slope, or put weight on fewer of the features. as an\nexample of how we can control a model\u2019s tendency to overfit or underfit via weight\ndecay, we can train a high-degree polynomial regression model with different values\nof\n\nfor the results.\n\n. see fig.\n\n5.5\n\n\u03bb\n\nunderfitting\n(excessive\u00a0\u00b8)\n\nappropriate\u00a0weight\u00a0decay\n\n(medium\u00a0\u00b8)\n\ny\n\ny\n\ny\n\noverfitting\n\n(\n\n\u00b8!\n\n0)\n\nx0\n\nx0\n\nx0\n\n5.2\n\nfigure 5.5: we fit a high-degree polynomial regression model to our example training set\nfrom fig.\n. the true function is quadratic, but here we use only models with degree 9.\nwe vary the amount of weight decay to prevent these high-degree models from overfitting.\n(left) with very large \u03bb, we can force the model to learn a function with no slope at\nall.\u00a0this underfits because it can only represent a constant function.\u00a0(\n) with a\nmedium value of\n, the learning algorithm recovers a curve with the right general shape.\neven though the model is capable of representing functions with much more complicated\nshape, weight decay has encouraged it to use a simpler function described by smaller\ncoefficients. (\n) with weight decay approaching zero (i.e., using the moore-penrose\npseudoinverse to solve the underdetermined problem with minimal regularization), the\ndegree-9 polynomial overfits significantly, as we saw in fig.\n\ncenter\n\nright\n\n.5.2\n\n\u03bb\n\nmore generally, we can regularize a model that learns a function f(x; \u03b8) by\nadding a penalty called a regularizer to the cost function.\u00a0in the case of weight\ndecay, the regularizer is \u03c9(w) = w>w. in chapter\n, we will see that many other\n7\n\n119\n\n "}, {"Page_number": 135, "text": "chapter 5. machine learning basics\n\nregularizers are possible.\n\nexpressing preferences for one function over another is a more general way\nof controlling a model\u2019s capacity than including or excluding members from the\nhypothesis space. we can think of excluding a function from a hypothesis space as\nexpressing an infinitely strong preference against that function.\n\nin our weight decay example, we expressed our preference for linear functions\ndefined with smaller weights explicitly,\u00a0via an extra term in the criterion we\nminimize. there are\u00a0many\u00a0other ways of\u00a0expressing preferences for\u00a0different\nsolutions, both implicitly and explicitly. together, these different approaches are\nknown as regularization. regularization is any modification we make to\na learning algorithm that is intended to reduce its generalization error\nbut not its training error. regularization is one of the central concerns of the\nfield of machine learning, rivaled in its importance only by optimization.\n\nthe no free lunch theorem has made it clear that there is no best machine\nlearning algorithm, and, in particular, no best form of regularization. instead\nwe must choose a form of regularization that is well-suited to the particular task\nwe want to solve. the philosophy of deep learning in general and this book in\nparticular is that a very wide range of tasks (such as all of the intellectual tasks\nthat people can do) may all be solved effectively using very general-purpose forms\nof regularization.\n\n5.3 hyperparameters and validation sets\n\nmost machine learning algorithms have several settings that we can use to control\nthe behavior of the learning algorithm. these settings are called hyperparameters.\nthe values of hyperparameters are not adapted by the learning algorithm itself\n(though we can design a nested learning procedure where one learning algorithm\nlearns the best hyperparameters for another learning algorithm).\n\nin the polynomial regression example we saw in fig.\n\n, there is a single hyper-\nparameter: the degree of the polynomial, which acts as a capacity hyperparameter.\nthe \u03bb value used to control the strength of weight decay is another example of a\nhyperparameter.\n\n5.2\n\nsometimes a setting is chosen to be a hyperparameter that the learning algo-\nrithm does not learn because it is difficult to optimize.\u00a0more frequently, we do\nnot learn the hyperparameter because it is not appropriate to learn that hyper-\nparameter on the training set. this applies to all hyperparameters that control\nmodel capacity. if learned on the training set, such hyperparameters would always\n\n120\n\n "}, {"Page_number": 136, "text": "chapter 5. machine learning basics\n\n5.3\n\nchoose the maximum possible model capacity, resulting in overfitting (refer to\nfig.\n). for example, we can always fit the training set better with a higher\ndegree polynomial and a weight decay setting of \u03bb = 0 than we could with a lower\ndegree polynomial and a positive weight decay setting.\n\nto solve this problem, we need a\n\nvalidation set\n\nof examples that the training\n\nalgorithm does not observe.\n\nearlier we discussed how a held-out test set, composed of examples coming from\nthe same distribution as the training set, can be used to estimate the generalization\nerror of a learner, after the learning process has completed. it is important that the\ntest examples are not used in any way to make choices about the model, including\nits hyperparameters.\u00a0for this reason, no example from the test set can be used\nin the validation set. therefore, we always construct the validation set from the\ntraining data. specifically, we split the training data into two disjoint subsets. one\nof these subsets is used to learn the parameters. the other subset is our validation\nset, used to estimate the generalization error during or after training, allowing\nfor the hyperparameters to be updated accordingly. the subset of data used to\nlearn the parameters is still typically called the training set, even though this\nmay be confused with the larger pool of data used for the entire training process.\nthe subset of data used to guide the selection of hyperparameters is called the\nvalidation set. typically, one uses about 80% of the training data for training and\n20% for validation. since the validation set is used to \u201ctrain\u201d the hyperparameters,\nthe validation set error will underestimate the generalization error, though typically\nby a smaller amount than the training error. after all hyperparameter optimization\nis complete, the generalization error may be estimated using the test set.\n\nin practice,\u00a0when the same test set has been used repeatedly to evaluate\nperformance of different algorithms over many years, and especially if we consider\nall the attempts from the scientific community at beating the reported state-of-\nthe-art performance on that test set, we end up having optimistic evaluations with\nthe test set as well. benchmarks can thus become stale and then do not reflect the\ntrue field performance of a trained system. thankfully, the community tends to\nmove on to new (and usually more ambitious and larger) benchmark datasets.\n\n5.3.1 cross-validation\n\ndividing the dataset into a fixed training set and a fixed test set can be problematic\nif it results in the test set being small. a small test set implies statistical uncertainty\naround the estimated average test error, making it difficult to claim that algorithm\na works better than algorithm on the given task.\n\nb\n\n121\n\n "}, {"Page_number": 137, "text": "chapter 5. machine learning basics\n\nwhen the dataset has hundreds of thousands of examples or more, this is not\na serious issue. when the dataset is too small, there are alternative procedures,\nwhich allow one to use all of the examples in the estimation of the mean test\nerror, at the price of increased computational cost. these procedures are based on\nthe idea of repeating the training and testing computation on different randomly\nchosen subsets or splits of the original dataset. the most common of these is the\nk-fold cross-validation procedure, shown in algorithm , in which a partition\nof the dataset is formed by splitting it into k non-overlapping subsets. the test\nerror may then be estimated by taking the average test error across k trials. on\ntrial i, the i-th subset of the data is used as the test set and the rest of the data is\nused as the training set. one problem is that there exist no unbiased estimators of\nthe variance of such average error estimators (bengio and grandvalet 2004\n), but\napproximations are typically used.\n\n5.1\n\n,\n\n5.4 estimators, bias and variance\n\nthe field of statistics gives us many tools that can be used to achieve the machine\nlearning goal of solving a task not only on the training set but also to generalize.\nfoundational concepts such as parameter estimation, bias and variance are useful\nto formally characterize notions of generalization, underfitting and overfitting.\n\n5.4.1 point estimation\n\npoint estimation is the attempt to provide the single \u201cbest\u201d prediction of some\nquantity of interest. in general the quantity of interest can be a single parameter\nor a vector of parameters in some parametric model, such as the weights in our\nlinear regression example in sec.\n\n, but it can also be a whole function.\n\n5.1.4\n\nin order to distinguish estimates of parameters from their true value,\u00a0our\n\nconvention will be to denote a point estimate of a parameter\n\nby\u03b8\n\n\u02c6\u03b8.\n\nlet {x(1), . . . , x(\n\n)m } be a set of m independent and identically distributed\n\n(i.i.d.) data points. a point estimator\n\nor\n\nstatistic\n\nis any function of the data:\n\n\u02c6\u03b8m =  (g x(1), . . . , x(\n\n)m ).\n\n(5.19)\n\nthe definition does not require that g return a value that is close to the true\n\u03b8 or even that the range of g is the same as the set of allowable values of \u03b8.\nthis definition of a point estimator is very general and allows the designer of an\nestimator great flexibility. while almost any function thus qualifies as an estimator,\n\n122\n\n "}, {"Page_number": 138, "text": "chapter 5. machine learning basics\n\nalgorithm 5.1 the k-fold cross-validation algorithm. it can be used to estimate\ngeneralization error of a learning algorithm a when the given dataset d is too\nsmall for a simple train/test or train/valid split to yield accurate estimation of\ngeneralization error, because the mean of a loss l on a small test set may have too\nhigh variance. the dataset d contains as elements the abstract examples z( )i (for\nthe i-th example), which could stand for an (input,target) pair z( )i = (x( )i , y( )i )\nin the case of supervised learning, or for just an input z( )i = x( )i\nin the case\nof unsupervised learning.\u00a0the algorithm returns the vector of errors e for each\nexample in d, whose mean is the estimated generalization error.\u00a0the errors on\nindividual examples can be used to compute a confidence interval around the mean\n(eq.\n). while these confidence intervals are not well-justified after the use of\ncross-validation, it is still common practice to use them to declare that algorithm a\nis better than algorithm b only if the confidence interval of the error of algorithm\na lies below and does not intersect the confidence interval of algorithm .b\n\n5.47\n\nd, a, l, k\n\ndefine kfoldxv(\nrequire: d, the given dataset, with elements z( )i\nrequire: a, the learning algorithm, seen as a function that takes a dataset as\n\n):\n\ninput and outputs a learned function\n\nrequire: l, the loss function, seen as a function from a learned function f and\n\ninto mutually exclusive subsets\n\ndi, whose union is\n\n.d\n\nrequire: k, the number of folds\n\nd to a scalar r\n\n\u2208\n\nan example z( )i \u2208\nsplit\nk\nd\nfor\nfrom to1\ni\nk\nf i =  (a d d\\ i)\nfor z ( )j\n\nin d i do\ne j =  (l fi, z ( )j )\n\ndo\n\nend for\n\nend for\nreturn e\n\n123\n\n "}, {"Page_number": 139, "text": "chapter 5. machine learning basics\n\na good estimator is a function whose output is close to the true underlying \u03b8 that\ngenerated the training data.\n\nfor now, we take the frequentist perspective on statistics. that is, we assume\nthat the true parameter value \u03b8 is fixed but unknown, while the point estimate\n\u02c6\u03b8 is a function of the data. since the data is drawn from a random process, any\nfunction of the data is random. therefore \u02c6\u03b8 is a random variable.\n\npoint estimation can also refer to the estimation of the relationship between\ninput and target variables. we refer to these types of point estimates as function\nestimators.\n\nfunction estimation as we mentioned above, sometimes we are interested in\nperforming function estimation (or function approximation). here we are trying to\npredict a variable y given an input vector x. we assume that there is a function\nf(x) that describes the approximate relationship between y and x. for example,\nwe may assume that y = f(x) + (cid:115), where (cid:115) stands for the part of y that is not\npredictable from x.\u00a0in function estimation, we are interested in approximating\nf with a model or estimate \u02c6f. function estimation is really just the same as\nestimating a parameter \u03b8; the function estimator \u02c6f is simply a point estimator in\nfunction space. the linear regression example (discussed above in sec.\n) and\nthe polynomial regression example (discussed in sec.\n) are both examples of\nscenarios that may be interpreted either as estimating a parameter w or estimating\na function \u02c6f\n\nmapping from tox\n\n5.1.4\n\n5.2\n\ny\n\n.\n\nwe now review the most commonly studied properties of point estimators and\n\ndiscuss what they tell us about these estimators.\n\n5.4.2 bias\n\nthe bias of an estimator is defined as:\n\nbias( \u02c6\u03b8m) =  (e \u02c6\u03b8m) \u2212 \u03b8\n\n(5.20)\n\nwhere the expectation is over the data (seen as samples from a random variable) and\n\u03b8 is the true underlying value of \u03b8 used to define the data generating distribution.\nan estimator \u02c6\u03b8m is said to be unbiased if bias( \u02c6\u03b8m) = 0, which implies that e( \u02c6\u03b8m) =\n\u03b8. an estimator \u02c6\u03b8m is said to be asymptotically unbiased if limm\u2192\u221e bias(\u02c6\u03b8m) = 0,\nwhich implies that limm\u2192\u221e e( \u02c6\u03b8m) = \u03b8.\n\n)m }\nexample: bernoulli distribution consider a set of samples{x (1), . . . , x(\nthat are independently and identically distributed according to a bernoulli distri-\n\n124\n\n "}, {"Page_number": 140, "text": "chapter 5. machine learning basics\n\nbution with mean :\u03b8\n\np x( ( )i ; ) = \n\n\u03b8\n\n\u03b8x ( )i\n\n(1\n\n)\u2212 \u03b8 (1\u2212x ( )i ).\n\n(5.21)\n\na common estimator for the \u03b8 parameter of this distribution is the mean of the\ntraining samples:\n\n\u02c6\u03b8m =\n\n1\nm\n\nx( )i .\n\nmxi=1\n\nto determine whether this estimator is biased, we can substitute eq.\n5.20:\n\nbias(\u02c6\u03b8m) = \n\n=\n\nm\n\n1\nm\n\n[e \u02c6\u03b8m] \u2212 \u03b8\nx ( )i #\u2212 \u03b8\n= e\" 1\nmxi=1\nehx( )ii \u2212 \u03b8\nmxi=1\n1xx( )i =0(cid:30)x( )i \u03b8x( )i\nmxi=1\nmxi=1\n\n( )\u03b8 \u2212 \u03b8\n= 0\n\n1\nm\n\u03b8\u2212\n\n1\nm\n\n= \n\u03b8\n\n=\n\n=\n\n(1\n\n)\u2212 \u03b8 (1\u2212x( )i )(cid:31) \u2212 \u03b8\n\n(5.22)\n\n5.22\n\ninto eq.\n\n(5.23)\n\n(5.24)\n\n(5.25)\n\n(5.26)\n\n(5.27)\n\n(5.28)\n\nsince bias(\u02c6\u03b8) = 0, we say that our estimator \u02c6\u03b8 is unbiased.\n\nexample: gaussian distribution estimator of the mean now, consider\na set of samples {x(1), . . . , x(\n)m } that are independently and identically distributed\naccording to a gaussian distribution p(x ( )i ) = n (x ( )i ; \u00b5, \u03c32 ), where i \u2208 {1, . . . , m}.\nrecall that the gaussian probability density function is given by\n\np x( ( )i ; \u00b5, \u03c32) =\n\n1\n\n\u221a2\u03c0\u03c32\n\nexp\u00a0 \u2212\n\n1\n2\n\n(x ( )i \u2212 \u00b5)2\n\n\u03c3 2\n\n!.\n\n(5.29)\n\na common estimator of the gaussian mean parameter is known as the sample\n\nmean:\n\n\u02c6\u00b5m =\n\nx( )i\n\n1\nm\n\nmxi=1\n\n125\n\n(5.30)\n\n "}, {"Page_number": 141, "text": "chapter 5. machine learning basics\n\nto determine the bias of the sample mean, we are again interested in calculating\nits expectation:\n\nbias(\u02c6\u00b5 m) = \n\n[\u02c6e \u00b5m] \u2212 \u00b5\n\nm\n\n= e\" 1\nx( )i# \u2212 \u00b5\nmxi=1\nehx( )i i!\u2212 \u00b5\n=\u00a0 1\nmxi=1\n\u00b5! \u2212 \u00b5\n=\u00a0 1\nmxi=1\n\nm\n\n= \n\n\u00b5\n\n= 0\n\nm\n\u00b5\u2212\n\n(5.31)\n\n(5.32)\n\n(5.33)\n\n(5.34)\n\n(5.35)\n\nthus we find that the sample mean is an unbiased estimator of gaussian mean\nparameter.\n\nexample: estimators of the variance of a gaussian distribution as an\nexample, we compare two different estimators of the variance parameter \u03c32 of a\ngaussian distribution. we are interested in knowing if either estimator is biased.\n\nthe first estimator of \u03c3 2 we consider is known as the sample variance:\n\n\u02c6\u03c32\nm =\n\n1\nm\n\nmxi=1(cid:30)x ( )i \u2212 \u02c6\u00b5m(cid:31)2\n\n,\n\n(5.36)\n\nwhere \u02c6\u00b5 m is the sample mean, defined above. more formally, we are interested in\ncomputing\n\nwe begin by evaluating the term e[\u02c6\u03c3 2\n\nm]:\n\nbias(\u02c6\u03c32\n\nm) = \n\n[\u02c6e \u03c3 2\n\nm] \u2212 \u03c32\n\ne[\u02c6\u03c32\n\nm] =e\" 1\n\nm\nm \u2212 1\nm\n\n=\n\n\u03c32\n\nmxi=1(cid:30)x ( )i \u2212 \u02c6\u00b5m(cid:31)2#\n\n(5.37)\n\n(5.38)\n\n(5.39)\n\nreturning to eq.\nsample variance is a biased estimator.\n\n5.37\n\n, we conclude that the bias of\n\n\u02c6\u03c32\nm is \u2212\u03c32/m. therefore, the\n\n126\n\n "}, {"Page_number": 142, "text": "chapter 5. machine learning basics\n\nthe unbiased sample variance estimator\n\n\u02dc\u03c3 2\nm =\n\n1\n\nm \u2212 1\n\nmxi=1(cid:30)x( )i \u2212 \u02c6\u00b5m(cid:31) 2\n\n(5.40)\n\nprovides an alternative approach. as the name suggests this estimator is unbiased.\nthat is, we find that e[\u02dc\u03c32\n\nm] = \u03c32:\n\nm] = e\"\n\ne[\u02dc\u03c32\n\n1\n\nmxi=1(cid:30)x( )i \u2212 \u02c6\u00b5m(cid:31)2#\n\u03c3 2(cid:33)\n\nm \u2212 1\nm\ne[\u02c6\u03c3 2\nm ]\nm \u2212 1\nm\n\nm \u2212 1(cid:32) m \u2212 1\n\nm\n\n=\n\n=\n\n= \u03c32.\n\n(5.41)\n\n(5.42)\n\n(5.43)\n\n(5.44)\n\nwe have two estimators: one is biased and the other is not. while unbiased\nestimators are clearly desirable, they are not always the \u201cbest\u201d estimators. as we\nwill see we often use biased estimators that possess other important properties.\n\n5.4.3 variance and standard error\n\nanother property of the estimator that we might want to consider is how much\nwe expect it to vary as a function of the data sample. just as we computed the\nexpectation of the estimator to determine its bias, we can compute its variance.\nthe variance of an estimator is simply the variance\n\nvar( \u02c6\u03b8)\n\n(5.45)\n\nwhere the random variable is the training set. alternately, the square root of the\nvariance is called the standard error, denoted se( \u02c6\u03b8).\n\nthe variance or the standard error of an estimator provides a measure of how\nwe would expect the estimate we compute from data to vary as we independently\nresample the dataset from the underlying data generating process. just as we\nmight like an estimator to exhibit low bias we would also like it to have relatively\nlow variance.\n\nwhen we compute any statistic using a finite number of samples, our estimate\nof the true underlying parameter is uncertain, in the sense that we could have\nobtained other samples from the same distribution and their statistics would have\n\n127\n\n "}, {"Page_number": 143, "text": "chapter 5. machine learning basics\n\nbeen different. the expected degree of variation in any estimator is a source of\nerror that we want to quantify.\n\nthe standard error of the mean is given by\n\nse(\u02c6\u00b5 m) =vuutvar[\n\n1\nm\n\nmxi=1\n\nx( )i ] =\n\n\u03c3\n\u221am\n\n,\n\n(5.46)\n\nwhere \u03c32 is the true variance of the samples x i. the standard error is often\nestimated by using an estimate of \u03c3. unfortunately, neither the square root of\nthe sample variance nor the square root of the unbiased estimator of the variance\nprovide an unbiased estimate of the standard deviation.\u00a0both approaches tend\nto underestimate the true standard deviation, but are still used in practice. the\nsquare root of the unbiased estimator of the variance is less of an underestimate.\nfor large\n\n, the approximation is quite reasonable.\n\nm\n\nthe standard error of the mean is very useful in machine learning experiments.\nwe often estimate the generalization error by computing the sample mean of the\nerror on the test set. the number of examples in the test set determines the\naccuracy of this estimate. taking advantage of the central limit theorem, which\ntells us that the mean will be approximately distributed with a normal distribution,\nwe can use the standard error to compute the probability that the true expectation\nfalls in any chosen interval. for example, the 95% confidence interval centered on\nthe mean is \u02c6\u00b5m is\n\n(\u02c6\u00b5 m \u2212 1 96se(\u02c6\n\n.\n\n\u00b5 m) \u02c6, \u00b5m + 1 96se(\u02c6\n\n\u00b5m)),\n\n.\n\n(5.47)\n\nunder the normal distribution with mean \u02c6\u00b5m and variance se( \u02c6\u00b5m )2 . in machine\nlearning experiments, it is common to say that algorithm a is better than algorithm\nb if the upper bound of the 95% confidence interval for the error of algorithm a is\nless than the lower bound of the 95% confidence interval for the error of algorithm\nb.\n\nexample:\u00a0bernoulli distribution we once again consider a set of samples\n{x(1), . . . , x(\n)m } drawn independently and identically from a bernoulli distribution\n(1 \u2212 \u03b8)(1\u2212x( )i ) ). this time we are interested in computing\n(recall p (x( )i ; \u03b8) = \u03b8 x( )i\nthe variance of the estimator \u02c6\u03b8m = 1\n\ni=1 x ( )i .\n\nmp m\nvar(cid:30)\u02c6\u03b8m(cid:31) = var\u00a0 1\n\nm\n\n128\n\nx( )i!\n\nmxi=1\n\n(5.48)\n\n "}, {"Page_number": 144, "text": "chapter 5. machine learning basics\n\nvar(cid:30)x( )i(cid:31)\n\n\u03b8\n\n(1 \u2212 )\n\u03b8\n\n1\nm2\n\n1\nm2\n\nmxi=1\nmxi=1\n\n1\nm2 m\u03b8\n1\nm\n\n(1 \u2212 )\n\u03b8\n(1 \u2212 )\n\u03b8\n\n\u03b8\n\n=\n\n=\n\n=\n\n=\n\n(5.49)\n\n(5.50)\n\n(5.51)\n\n(5.52)\n\nthe variance of the estimator decreases as a function of m, the number of examples\nin the dataset. this is a common property of popular estimators that we will\nreturn to when we discuss consistency (see sec.\n\n5.4.5\n\n).\n\n5.4.4 trading off bias and variance to minimize mean squared\n\nerror\n\nbias and variance measure two different sources of error in an estimator. bias\nmeasures the expected deviation from the true value of the function or parameter.\nvariance on the other hand, provides a measure of the deviation from the expected\nestimator value that any particular sampling of the data is likely to cause.\n\nwhat happens when we are given a choice between two estimators, one with\nmore bias and one with more variance? how do we choose between them? for\nexample, imagine that we are interested in approximating the function shown in\nfig.\nand we are only offered the choice between a model with large bias and\none that suffers from large variance. how do we choose between them?\n\n5.2\n\nthe most common way to negotiate this trade-off is to use cross-validation.\nempirically, cross-validation is highly successful on many real-world tasks. alter-\nnatively, we can also compare the mean squared error (mse) of the estimates:\n\nmse = \n\n[(e \u02c6\u03b8m \u2212 \u03b8) 2]\n\n= bias(\u02c6\u03b8m) 2 + var( \u02c6\u03b8m)\n\n(5.53)\n\n(5.54)\n\nthe mse measures the overall expected deviation\u2014in a squared error sense\u2014\nbetween the estimator and the true value of the parameter \u03b8. as is clear from\neq.\n, evaluating the mse incorporates both the bias and the variance. desirable\nestimators are those with small mse and these are estimators that manage to keep\nboth their bias and variance somewhat in check.\n\n5.54\n\nthe relationship between bias and variance is tightly linked to the machine\nlearning concepts of capacity, underfitting and overfitting. in the case where gen-\n\n129\n\n "}, {"Page_number": 145, "text": "chapter 5. machine learning basics\n\nunderfitting\u00a0zone\n\noverfitting\u00a0zone\n\nbias\n\ngeneralization\n\nerror\n\noptimal\ncapacity\n\nvariance\n\ncapacity\n\nfigure 5.6: as capacity increases (x-axis), bias (dotted) tends to decrease and variance\n(dashed) tends to increase, yielding another u-shaped curve for generalization error (bold\ncurve). if we vary capacity along one axis, there is an optimal capacity, with underfitting\nwhen the capacity is below this optimum and overfitting when it is above. this relationship\nis similar to the relationship between capacity, underfitting, and overfitting, discussed in\nsec.\n\nand fig.\n\n.\n5.3\n\n5.2\n\neralization error is measured by the mse (where bias and variance are meaningful\ncomponents of generalization error), increasing capacity tends to increase variance\nand decrease bias. this is illustrated in fig.\n, where we see again the u-shaped\ncurve of generalization error as a function of capacity.\n\n5.6\n\n5.4.5 consistency\n\nso far we have discussed the properties of various estimators for a training set of\nfixed size. usually, we are also concerned with the behavior of an estimator as the\namount of training data grows. in particular, we usually wish that, as the number\nof data points m in our dataset increases, our point estimates converge to the true\nvalue of the corresponding parameters. more formally, we would like that\n\nlim\nm\u2192\u221e\n\n\u02c6\u03b8m\n\np\n\n\u2192 \u03b8.\n\n(5.55)\n\np\n\nthe symbol\n\n\u2192 means that the convergence is in probability, i.e. for any (cid:115) > 0,\np (|\u02c6\u03b8m \u2212 |\u03b8 > (cid:115)) \u2192 0 as m \u2192 \u221e . the\u00a0condition described\u00a0by\u00a0eq.\nis\nit is sometimes referred to\u00a0as weak consistency,\u00a0with\nknown as\u00a0consistency.\nstrong consistency referring to the almost sure convergence of \u02c6\u03b8 to \u03b8. almost sure\n\n5.55\n\n130\n\n "}, {"Page_number": 146, "text": "chapter 5. machine learning basics\n\nx\n\nconvergence of a sequence of random variables x(1), x (2), . . . to a value x occurs\nwhen p(lim m\u2192\u221e x(\n\n)m =  ) = 1\n.\n\nconsistency ensures that\u00a0the bias\u00a0induced by the estimator is assured to\ndiminish as the number of data examples grows. however, the reverse is not\ntrue\u2014asymptotic unbiasedness does not imply consistency. for example, consider\nestimating the mean parameter \u00b5 of a normal distribution n(x ;\u00b5, \u03c3 2 ), with a\ndataset consisting of m samples: {x(1), . . . , x(\n)m }. we could use the first sample\nx(1) of the dataset as an unbiased estimator: \u02c6\u03b8 = x(1).\u00a0in that case, e(\u02c6\u03b8m) = \u03b8\nso the estimator is unbiased no matter how many data points are seen. this, of\ncourse, implies that the estimate is asymptotically unbiased. however, this is not\na consistent estimator as it is\n\nthe case that\n\n\u03b8 mas\n\nnot\n\n\u02c6\u03b8m \u2192\n\n\u2192 \u221e\n.\n\n5.5 maximum likelihood estimation\n\npreviously, we have seen some definitions of common estimators and analyzed\ntheir properties. but where did these estimators come from? rather than guessing\nthat some function might make a good estimator and then analyzing its bias and\nvariance, we would like to have some principle from which we can derive specific\nfunctions that are good estimators for different models.\n\nthe most common such principle is the maximum likelihood principle.\nconsider a set of m examples x = {x(1), . . . , x(\n\n)m } drawn independently from\n\nthe true but unknown data generating distribution pdata( )x .\n\nlet pmodel( x; \u03b8) be a parametric family of probability distributions over the\nsame space indexed by \u03b8. in other words, pmodel(x; \u03b8) maps any configuration x\nto a real number estimating the true probability pdata( )x .\n\nthe maximum likelihood estimator for\n\n\u03b8\n\nis then defined as\n\n\u03b8ml = arg max\n\npmodel( ; )x \u03b8\n\n\u03b8\n\n= arg max\n\n\u03b8\n\nmyi=1\n\npmodel(x ( )i ; )\u03b8\n\n(5.56)\n\n(5.57)\n\nthis product over many probabilities can be inconvenient for a variety of reasons.\nfor example, it is prone to numerical underflow. to obtain a more convenient\nbut equivalent optimization problem, we observe that taking the logarithm of the\nlikelihood does not change its arg max but does conveniently transform a product\n\n131\n\n "}, {"Page_number": 147, "text": "chapter 5. machine learning basics\n\ninto a sum:\n\n\u03b8ml = arg max\n\n\u03b8\n\nmxi=1\n\nlog pmodel(x( )i ; )\u03b8 .\n\n(5.58)\n\nbecause the argmax does not change when we rescale the cost function, we can\ndivide by m to obtain a version of the criterion that is expressed as an expectation\nwith respect to the empirical distribution \u02c6pdata defined by the training data:\n\n\u03b8ml = arg max\n\n\u03b8\n\nex\u223c\u02c6pdata log p model( ; )x \u03b8 .\n\n(5.59)\n\none way to interpret maximum likelihood estimation is to view it as minimizing\nthe dissimilarity between the empirical distribution \u02c6pdata defined by the training\nset and the model distribution, with the degree of dissimilarity between the two\nmeasured by the kl divergence. the kl divergence is given by\n\ndkl (\u02c6pdatakpmodel) = ex\u223c\u02c6p data [log \u02c6pdata ( )\n\nx \u2212\n\nlog\n\npmodel( )]x .\n\n(5.60)\n\nthe term on the left is a function only of the data generating process, not the\nmodel. this means when we train the model to minimize the kl divergence, we\nneed only minimize\n\n\u2212 ex\u223c\u02c6pdata [log p model ( )]x\nwhich is of course the same as the maximization in eq.\n\n5.59\n.\n\n(5.61)\n\nminimizing this kl divergence corresponds exactly to minimizing the cross-\nentropy between the distributions. many authors use the term \u201ccross-entropy\u201d to\nidentify specifically the negative log-likelihood of a bernoulli or softmax distribution,\nbut that is a misnomer. any loss consisting of a negative log-likelihood is a cross\nentropy between the empirical distribution defined by the training set and the\nmodel. for example, mean squared error is the cross-entropy between the empirical\ndistribution and a gaussian model.\n\nwe can thus see maximum likelihood as an attempt to make the model dis-\ntribution match the empirical distribution \u02c6pdata. ideally, we would like to match\nthe true data generating distribution pdata, but we have no direct access to this\ndistribution.\n\nwhile the optimal \u03b8 is the same regardless of whether we are maximizing the\nlikelihood or minimizing the kl divergence, the values of the objective functions\nare different. in software, we often phrase both as minimizing a cost function.\nmaximum likelihood thus becomes minimization of the negative log-likelihood\n(nll), or equivalently, minimization of the cross entropy. the perspective of\nmaximum likelihood as minimum kl divergence becomes helpful in this case\nbecause the kl divergence has a known minimum value of zero. the negative\nlog-likelihood can actually become negative when\n\nis real-valued.\n\nx\n\n132\n\n "}, {"Page_number": 148, "text": "chapter 5. machine learning basics\n\n5.5.1 conditional log-likelihood and mean squared error\n\nthe maximum likelihood estimator can readily be generalized to the case where\nour goal is to estimate a conditional probability p(y x|\n; \u03b8) in order to predict y\ngiven x. this is actually the most common situation because it forms the basis for\nmost supervised learning. if x represents all our inputs and y all our observed\ntargets, then the conditional maximum likelihood estimator is\n\n\u03b8 ml = arg max\n\np\n\n\u03b8\n\ny x|\n(\n\n; )\n.\n\u03b8\n\nif the examples are assumed to be i.i.d., then this can be decomposed into\n\n\u03b8ml = arg max\n\n\u03b8\n\nmxi=1\n\nlog (p y( )i\n\n| x( )i ; )\u03b8 .\n\n(5.62)\n\n(5.63)\n\n5.1.4\n\nexample: linear regression as maximum likelihood linear regression,\nintroduced earlier in sec.\n, may be justified as a maximum likelihood procedure.\npreviously, we motivated linear regression as an algorithm that learns to take an\ninput x and produce an output value \u02c6y.\u00a0the mapping from x to \u02c6y is chosen to\nminimize mean squared error, a criterion that we introduced more or less arbitrarily.\nwe now revisit linear regression from the point of view of maximum likelihood\nestimation. instead of producing a single prediction \u02c6y, we now think of the model\nas producing a conditional distribution p(y | x). we can imagine that with an\ninfinitely large training set, we might see several training examples with the same\ninput value x but different values of y . the goal of the learning algorithm is now to\nfit the distribution p(y | x) to all of those different y values that are all compatible\nwith x. to derive the same linear regression algorithm we obtained before, we\ndefine p(y | x) = n (y; \u02c6y(x; w), \u03c32). the function \u02c6y(x; w) gives the prediction of\nthe mean of the gaussian. in this example, we assume that the variance is fixed to\nsome constant \u03c32 chosen by the user. we will see that this choice of the functional\nform of p(y | x) causes the maximum likelihood estimation procedure to yield the\nsame learning algorithm as we developed before. since the examples are assumed\nto be i.i.d., the conditional log-likelihood (eq.\n\n) is given by\n\n5.63\n\nmxi=1\n\nlog (p y( )i\n\n| x ( )i ; )\u03b8\n\n=\n\nlog\n\n\u2212 m \u03c3 \u2212\n\nm\n2\n\nlog(2 )\u03c0 \u2212\n\nmxi=1\n\n|\u02c6y( )i \u2212 y ( )i ||2\n\n2\u03c32\n\n133\n\n(5.64)\n\n(5.65)\n\n "}, {"Page_number": 149, "text": "chapter 5. machine learning basics\n\nwhere \u02c6y ( )i\nis the output of the linear regression on the i-th input x( )i and m is the\nnumber of the training examples. comparing the log-likelihood with the mean\nsquared error,\n\nmse train =\n\n1\nm\n\nmxi=1\n\n||\u02c6y( )i \u2212 y( )i ||2,\n\n(5.66)\n\nwe immediately see that maximizing the log-likelihood with respect to w yields\nthe same estimate of the parameters w as does minimizing the mean squared error.\nthe two criteria have different values but the same location of the optimum. this\njustifies the use of the mse as a maximum likelihood estimation procedure. as we\nwill see, the maximum likelihood estimator has several desirable properties.\n\n5.5.2 properties of maximum likelihood\n\nthe main appeal of the maximum likelihood estimator is that it can be shown to\nbe the best estimator asymptotically, as the number of examples m \u2192 \u221e, in terms\nof its rate of convergence as\n\nincreases.\n\nm\n\nunder appropriate conditions, maximum likelihood estimator has the property\nabove), meaning that as the number of training\nof consistency (see sec.\nexamples approaches infinity, the maximum likelihood estimate of a parameter\nconverges to the true value of the parameter. these conditions are:\n\n5.4.5\n\n\u2022 the true distribution pdata must lie within the model family pmodel(\u00b7; \u03b8).\n\notherwise, no estimator can recover pdata .\n\n\u2022 the true distribution pdata must correspond to exactly one value of \u03b8. other-\nwise, maximum likelihood can recover the correct pdata, but will not be able\nto determine which value of was used by the data generating processing.\n\n\u03b8\n\nthere are other inductive principles besides the maximum likelihood estimator,\nmany of which share the property of being consistent estimators. however, consis-\ntent estimators can differ in their\n, meaning that one consistent\nestimator may obtain lower generalization error for a fixed number of samples m,\nor equivalently, may require fewer examples to obtain a fixed level of generalization\nerror.\n\nstatistic efficiency\n\nstatistical efficiency is typically studied in the parametric case (like in linear\nregression) where our goal is to estimate the value of a parameter (and assuming\nit is possible to identify the true parameter), not the value of a function. a way to\nmeasure how close we are to the true parameter is by the expected mean squared\nerror, computing the squared difference between the estimated and true parameter\n\n134\n\n "}, {"Page_number": 150, "text": "chapter 5. machine learning basics\n\nvalues, where the expectation is over m training samples from the data generating\ndistribution. that parametric mean squared error decreases as m increases, and\nfor m large, the cram\u00e9r-rao lower bound (\n) shows that no\nconsistent estimator has a lower mean squared error than the maximum likelihood\nestimator.\n\nrao 1945 cram\u00e9r 1946\n\n,\n\n;\n\n,\n\nfor these reasons (consistency and efficiency), maximum likelihood is often\nconsidered the preferred estimator to use for machine learning. when the number\nof examples is small enough to yield overfitting behavior, regularization strategies\nsuch as weight decay may be used to obtain a biased version of maximum likelihood\nthat has less variance when training data is limited.\n\n5.6 bayesian statistics\n\nso far we have discussed frequentist statistics and approaches based on estimating a\nsingle value of \u03b8, then making all predictions thereafter based on that one estimate.\nanother approach is to consider all possible values of \u03b8 when making a prediction.\nthe latter is the domain of\n\nbayesian statistics\n.\n\nas discussed in sec.\n\n, the frequentist perspective is that the true parameter\nvalue \u03b8 is fixed but unknown, while the point estimate \u02c6\u03b8 is a random variable on\naccount of it being a function of the dataset (which is seen as random).\n\n5.4.1\n\nthe bayesian perspective on statistics is quite different.\u00a0the bayesian uses\nprobability to reflect degrees of certainty of states of knowledge. the dataset is\ndirectly observed and so is not random. on the other hand, the true parameter \u03b8\nis unknown or uncertain and thus is represented as a random variable.\n\nbefore observing the data, we represent our knowledge of \u03b8 using the prior\nprobability distribution, p(\u03b8) (sometimes referred to as simply \u201cthe prior\u201d). gen-\nerally, the machine learning practitioner selects a prior distribution that is quite\nbroad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of\n\u03b8 before observing any data. for example, one might assume\n\u03b8 lies\nin some finite range or volume, with a uniform distribution. many priors instead\nreflect a preference for \u201csimpler\u201d solutions (such as smaller magnitude coefficients,\nor a function that is closer to being constant).\n\na priori\n\nthat\n\nnow consider that we have a set of data samples {x(1), . . . , x(\n\n)m }. we can\nrecover the effect of data on our belief about \u03b8 by combining the data likelihood\np x( (1), . . . , x(\n\n)m | \u03b8) with the prior via bayes\u2019 rule:\n\nx\n\n(1), . . . , x (\n\n)m ) =\n\n(\u03b8 |\np\n\np x( (1), . . . , x (\n\n)m | \u03b8\n)m )\np x( (1), . . . , x(\n\n\u03b8) (p )\n\n(5.67)\n\n135\n\n "}, {"Page_number": 151, "text": "chapter 5. machine learning basics\n\nin the scenarios where bayesian estimation is typically used, the prior begins as a\nrelatively uniform or gaussian distribution with high entropy, and the observation\nof the data usually causes the posterior to lose entropy and concentrate around a\nfew highly likely values of the parameters.\n\nrelative to maximum likelihood estimation, bayesian estimation offers two\nimportant differences. first, unlike the maximum likelihood approach that makes\npredictions using a point estimate of \u03b8, the bayesian approach is to make predictions\nusing a full distribution over \u03b8. for example, after observing m examples, the\npredicted distribution over the next data sample, x( +1)m , is given by\n\np x( ( +1)m\n\n| x(1), . . . , x(\n\n)m ) =z p x( ( +1)m\n\n\u03b8\n\n\u03b8) (p\n\n|\n\n|\n\nx(1), . . . , x(\n\n)m ) d .\u03b8\n\n(5.68)\n\nhere each value of \u03b8 with positive probability density contributes to the prediction\nof the next example, with the contribution weighted by the posterior density itself.\nafter having observed {x(1) , . . . , x(\n)m }, if we are still quite uncertain about the\nvalue of \u03b8, then this uncertainty is incorporated directly into any predictions we\nmight make.\n\n5.4\n\nin sec.\n\n, we discussed how the frequentist approach addresses the uncertainty\nin a given point estimate of \u03b8 by evaluating its variance. the variance of the\nestimator is an assessment of how the estimate might change with alternative\nsamplings of the observed data. the bayesian answer to the question of how to deal\nwith the uncertainty in the estimator is to simply integrate over it, which tends to\nprotect well against overfitting. this integral is of course just an application of\nthe laws of probability, making the bayesian approach simple to justify, while the\nfrequentist machinery for constructing an estimator is based on the rather ad hoc\ndecision to summarize all knowledge contained in the dataset with a single point\nestimate.\n\nthe second important difference between the bayesian approach to estimation\nand the maximum likelihood approach is due to the contribution of the bayesian\nprior distribution. the prior has an influence by shifting probability mass density\ntowards regions of the parameter space that are preferred\n. in practice,\nthe prior often expresses a preference for models that are simpler or more smooth.\ncritics of the bayesian approach identify the prior as a source of subjective human\njudgment impacting the predictions.\n\na priori\n\nbayesian methods typically generalize much better when limited training data\nis available, but typically suffer from high computational cost when the number of\ntraining examples is large.\n\n136\n\n "}, {"Page_number": 152, "text": "chapter 5. machine learning basics\n\nexample: bayesian linear regression here we consider the bayesian esti-\nmation approach to learning the linear regression parameters. in linear regression,\nwe learn a linear mapping from an input vector x \u2208 rn to predict the value of a\nscalar\n\n. the prediction is parametrized by the vector\n\nw \u2208 r n:\n\ny \u2208 r\n\n\u02c6y = w> x.\n\n(5.69)\n\ngiven a set of m training samples (x (\nof\n\nover the entire training set as:\n\ny\n\ntrain , y (\n\n)\n\ntrain ), we can express the prediction\n\n)\n\n\u02c6y (\n\ntrain = x (\n\n)\n\ntrain w.\n\n)\n\n(5.70)\n\nexpressed as a gaussian conditional distribution on y(\n\ntrain , we have\n\n)\n\np(y(\n\n)\n\ntrain | x(\n\ntrain , w\n\n)\n\n) = \n\ntrain ; x(\n\n)\n\ntrain w i, )\n\n)\n\n(n (\ny\n\u221d exp(cid:32)\u2212\n\n1\n2\n\n(y(\n\n)\n\ntrain \u2212 x (\n\ntrain w)> (y(\n\n)\n\n)\n\ntrain \u2212 x(\n\n(5.71)\n\n)\n\ntrain w)(cid:33) ,\n\n(5.72)\n\n)\n\n(\n\n.\n)\n\n)\ntrain )\n\nas simply x y,\n\nwhere we follow the standard mse formulation in assuming that the gaussian\nvariance on y is one. in what follows, to reduce the notational burden, we refer to\n(x(\n\ntrain , y (\nto determine the posterior distribution over the model parameter vector w, we\nfirst need to specify a prior distribution. the prior should reflect our naive belief\nabout the value of these parameters. while it is sometimes difficult or unnatural\nto express our prior beliefs in terms of the parameters of the model, in practice we\ntypically assume a fairly broad distribution expressing a high degree of uncertainty\nabout \u03b8.\u00a0for real-valued parameters it is common to use a gaussian as a prior\ndistribution:\n\np(\n\nw\n\n) = \n\n(\n\nn w \u00b50 , \u03bb0) \n\n;\n\nexp\u221d\n\n(cid:32)\u2212\n\n1\n2\n\n(w \u00b5\u2212 0 )> \u03bb\u22121\n\n0 (w \u00b5\u2212 0)(cid:33)\n\n(5.73)\n\nwhere \u00b50 and \u03bb0 are the prior distribution mean vector and covariance matrix\nrespectively.1\n\nwith the prior thus specified, we can now proceed in determining the posterior\n\ndistribution over the model parameters.\n\n,\n\n(w x|\np\n1unless there is a reason to assume a particular covariance structure, we typically assume a\n\ny) \u221d (y x|\n\n(5.74)\n\np\nw) (\n\n)w\n\np\n\n,\n\ndiagonal covariance matrix \u03bb0 = diag(\u03bb0 ).\n\n137\n\n "}, {"Page_number": 153, "text": "chapter 5. machine learning basics\n\n\u221d exp(cid:32)\u2212\n\u221d exp(cid:32)\u2212\n\n1\n2\n\ny xw\u2212\n(\n\n>(\n)\n\ny xw\u2212\n\n1\n\n2 (cid:30)\u22122y>xw w+ > x>xw w+ >\u03bb\u22121\n\n)\n\n1\n2\n\n0 w\n\n(5.75)\n\n\u00b5\u2212 2 >0 \u03bb\u22121\n\n(w \u00b5\u2212 0)>\u03bb\u22121\n\n(cid:33) exp(cid:32)\u2212\n\n0 (w \u00b5\u2212 0)(cid:33)\n0 w(cid:31)(cid:33) .\nand \u00b5m = \u03bbm(cid:28) x> y + \u03bb\u22121\n0 \u00b50(cid:29). using\nm \u00b5m(cid:33)\n\nm (w \u00b5\u2212 m) +\n\n\u00b5>m \u03bb\u22121\n\n(5.76)\n\n(5.77)\n\n1\n2\n\n(5.78)\n\nwe now define \u03bb m = (cid:28)x>x + \u03bb\u22121\n0 (cid:29)\u22121\n\nthese new variables, we find that the posterior may be rewritten as a gaussian\ndistribution:\n\n(w x|\np\n\n,\n\ny) \n\nexp\u221d\n\n(cid:32)\u2212\n\u221d exp(cid:32)\u2212\n\n1\n2\n1\n2\n\n(w \u00b5\u2212 m)> \u03bb\u22121\n(w \u00b5\u2212 m)> \u03bb\u22121\n\nm (w \u00b5\u2212 m)(cid:33) .\n\nall terms that do not include the parameter vector w have been omitted; they\nare implied by the fact that the distribution must be normalized to integrate to .1\neq.\n\nshows how to normalize a multivariate gaussian distribution.\n\n3.23\n\nexamining this posterior distribution allows us to gain some intuition for the\neffect of bayesian inference. in most situations, we set \u00b50 to 0. if we set \u03bb0 = 1\n\u03b1 i,\nthen \u00b5m gives the same estimate of w as does frequentist linear regression with\na weight decay penalty of \u03b1w>w.\u00a0one difference is that the bayesian estimate\nis undefined if alpha is set to zero\u2014-we are not allowed to begin the bayesian\nlearning process with an infinitely wide prior on w. the more important difference\nis that the bayesian estimate provides a covariance matrix, showing how likely all\nthe different values of\n\nare, rather than providing only the estimate\n\nw\n\n\u00b5 m.\n\n5.6.1 maximum\n\na posteriori\n\n(map) estimation\n\nwhile the most principled approach is to make predictions using the full bayesian\nposterior distribution over the parameter \u03b8, it is still often desirable to have a\nsingle point estimate.\u00a0one common reason for desiring a point estimate is that\nmost operations involving the bayesian posterior for most interesting models are\nintractable, and a point estimate offers a tractable approximation. rather than\nsimply returning to the maximum likelihood estimate, we can still gain some of\nthe benefit of the bayesian approach by allowing the prior to influence the choice\nof the point estimate.\u00a0one rational way to do this is to choose the maximum a\nposteriori (map) point estimate. the map estimate chooses the point of maximal\n\n138\n\n "}, {"Page_number": 154, "text": "chapter 5. machine learning basics\n\nposterior probability (or maximal probability density in the more common case of\ncontinuous\n\n):\u03b8\n\n\u03b8 map = arg max\n\n\u03b8\n\np(\n\n\u03b8 x|\n\n) = arg max\n\n\u03b8\n\nlog (\n\np x \u03b8|\n\n) + log ( )\np \u03b8 .\n\n(5.79)\n\nwe recognize, above on the right hand side, log p(x \u03b8|\nlikelihood term, and\n\n, corresponding to the prior distribution.\n\nlog ( )p \u03b8\n\n), i.e. the standard log-\n\nas an example, consider a linear regression model with a gaussian prior on the\nweights w. if this prior is given by n (w;0, 1\n\u03bb i2), then the log-prior term in eq.\n5.79 is proportional to the familiar \u03bbw>w weight decay penalty, plus a term that\ndoes not depend on w and does not affect the learning process. map bayesian\ninference with a gaussian prior on the weights thus corresponds to weight decay.\n\nas with full bayesian inference, map bayesian inference has the advantage of\nleveraging information that is brought by the prior and cannot be found in the\ntraining data. this additional information helps to reduce the variance in the\nmap point estimate (in comparison to the ml estimate). however, it does so at\nthe price of increased bias.\n\nmany regularized estimation strategies, such as maximum likelihood learning\nregularized with weight decay, can be interpreted as making the map approxima-\ntion to bayesian inference. this view applies when the regularization consists of\nadding an extra term to the objective function that corresponds to log p(\u03b8 ). not\nall regularization penalties correspond to map bayesian inference. for example,\nsome regularizer terms may not be the logarithm of a probability distribution.\nother regularization terms depend on the data, which of course a prior probability\ndistribution is not allowed to do.\n\nmap bayesian inference provides a straightforward way to design complicated\nyet interpretable regularization terms. for example, a more complicated penalty\nterm can be derived by using a mixture of gaussians, rather than a single gaussian\ndistribution, as the prior (nowlan and hinton 1992\n\n).\n\n,\n\n5.7 supervised learning algorithms\n\n5.1.3\n\nrecall from sec.\nthat supervised learning algorithms are, roughly speaking,\nlearning algorithms that learn to associate some input with some output, given a\ntraining set of examples of inputs x and outputs y .\u00a0in many cases the outputs\ny may be difficult to collect automatically and must be provided by a human\n\u201csupervisor,\u201d but the term still applies even when the training set targets were\ncollected automatically.\n\n139\n\n "}, {"Page_number": 155, "text": "chapter 5. machine learning basics\n\n5.7.1 probabilistic supervised learning\n\nmost\u00a0supervised\u00a0learning\u00a0algorithms\u00a0in this\u00a0book\u00a0are\u00a0based\u00a0on estimating\u00a0a\nprobability distribution p(y | x). we can do this simply by using maximum\nlikelihood estimation to find the best parameter vector \u03b8 for a parametric family\nof distributions\n\np y(\n\n| x \u03b8; )\n.\n\nwe have already seen that linear regression corresponds to the family\n\np y\n(\n\n|\n\nn\nx \u03b8; ) = \n\n( ; \u03b8>x i, ).\ny\n\n(5.80)\n\nwe can generalize linear regression to the classification scenario by defining a\ndifferent family of probability distributions. if we have two classes, class 0 and\nclass 1, then we need only specify the probability of one of these classes. the\nprobability of class 1 determines the probability of class 0, because these two values\nmust add up to 1.\n\nthe normal distribution over real-valued numbers that we used for linear\nregression is parametrized in terms of a mean. any value we supply for this mean\nis valid. a distribution over a binary variable is slightly more complicated, because\nits mean must always be between 0 and 1. one way to solve this problem is to use\nthe logistic sigmoid function to squash the output of the linear function into the\ninterval (0, 1) and interpret that value as a probability:\n\np y\n\n( = 1 \n\n; ) = \n\n\u03c3\n\n| x \u03b8\n\n(\u03b8>x).\n\n(5.81)\n\nthis approach is known as logistic regression (a somewhat strange name since we\nuse the model for classification rather than regression).\n\nin the case of linear regression, we were able to find the optimal weights by\nsolving the normal equations. logistic regression is somewhat more difficult. there\nis no closed-form solution for its optimal weights. instead, we must search for\nthem by maximizing the log-likelihood. we can do this by minimizing the negative\nlog-likelihood (nll) using gradient descent.\n\nthis same strategy can be applied to essentially any supervised learning problem,\nby writing down a parametric family of conditional probability distributions over\nthe right kind of input and output variables.\n\n5.7.2 support vector machines\n\none of the most influential approaches to supervised learning is the support vector\nmachine (\n). this model is similar to\nlogistic regression in that it is driven by a linear function w>x + b. unlike logistic\n\nboser et al. 1992 cortes and vapnik 1995\n\n,\n\n,\n\n;\n\n140\n\n "}, {"Page_number": 156, "text": "chapter 5. machine learning basics\n\nregression, the support vector machine does not provide probabilities, but only\noutputs a class identity. the svm predicts that the positive class is present when\nw>x + b is positive. likewise, it predicts that the negative class is present when\nw>x + b is negative.\n\none key innovation associated with support vector machines is the\n\nkernel trick\n.\nthe kernel trick consists of observing that many machine learning algorithms can\nbe written exclusively in terms of dot products between examples. for example, it\ncan be shown that the linear function used by the support vector machine can be\nre-written as\n\nw>x + =  +\n\nb\n\nb\n\n\u03b1i x>x( )i\n\n(5.82)\n\nmxi=1\n\nkernel\n\nwhere x( )i\nis a training example and \u03b1 is a vector of coefficients. rewriting the\nlearning algorithm this way allows us to replace x by the output of a given feature\n( )i ) = \u03c6(x)\u00b7 \u03c6 (x( )i ) called\nfunction \u03c6(x) and the dot product with a function k(x x,\n\u00b7 operator represents an inner product analogous to \u03c6(x)>\u03c6 (x( )i ).\na\nfor some feature spaces, we may not use literally the vector inner product. in\nsome infinite dimensional spaces, we need to use other kinds of inner products, for\nexample, inner products based on integration rather than summation. a complete\ndevelopment of these kinds of inner products is beyond the scope of this book.\n\n. the\n\nafter replacing dot products with kernel evaluations, we can make predictions\n\nusing the function\n\nf\n\n( ) = \nb\nx\n\n\u03b1 ik\n\n,(x x( )i ).\n\n(5.83)\n\n+xi\n\nthis function is nonlinear with respect to x, but the relationship between \u03c6( x)\nand f(x) is linear. also, the relationship between \u03b1 and f (x) is linear. the\nkernel-based function is exactly equivalent to preprocessing the data by applying\n\u03c6( )x to all inputs, then learning a linear model in the new transformed space.\n\nthe kernel trick is powerful for two reasons. first, it allows us to learn models\nthat are nonlinear as a function of x using convex optimization techniques that are\nguaranteed to converge efficiently. this is possible because we consider \u03c6 fixed and\noptimize only \u03b1, i.e., the optimization algorithm can view the decision function\nas being linear in a different space. second, the kernel function k often admits\nan implementation that is significantly more computational efficient than naively\nconstructing two\n\nvectors and explicitly taking their dot product.\n\n\u03c6( )x\n\nin some cases, \u03c6(x) can even be infinite dimensional, which would result in\nan infinite computational cost for the naive, explicit approach. in many cases,\n0 ) is a nonlinear, tractable function of x even when \u03c6(x) is intractable. as\nk(x x,\n\n141\n\n "}, {"Page_number": 157, "text": "chapter 5. machine learning basics\n\nan example of an infinite-dimensional feature space with a tractable kernel, we\nconstruct a feature mapping \u03c6(x) over the non-negative integers x. suppose that\nthis mapping returns a vector containing x ones followed by infinitely many zeros.\nwe can write a kernel function k(x, x( )i ) = min(x, x( )i ) that is exactly equivalent\nto the corresponding infinite-dimensional dot product.\n\nthe most commonly used kernel is the gaussian kernel\n\nk\n\n(u v\n\n,\n\n) = \n\nu v\n\n(n \u2212 ; 0 2i)\n\n, \u03c3\n\n(5.84)\n\nwhere n(x; \u00b5, \u03c3) is the standard normal density. this kernel is also known as\nthe radial basis function (rbf) kernel, because its value decreases along lines in\nv space radiating outward from u . the gaussian kernel corresponds to a dot\nproduct in an infinite-dimensional space, but the derivation of this space is less\nstraightforward than in our example of the\n\nkernel over the integers.\n\nmin\n\nwe can think of the gaussian kernel as performing a kind of\n\ntemplate matching\n.\na training example x associated with training label y becomes a template for class\ny. when a test point x0 is near x according to euclidean distance, the gaussian\nkernel has a large response, indicating that x0 is very similar to the x template.\nthe model then puts a large weight on the associated training label y.\u00a0overall,\nthe prediction will combine many such training labels weighted by the similarity\nof the corresponding training examples.\n\nsupport vector machines are not the only algorithm that can be enhanced\nusing the kernel trick. many other linear models can be enhanced in this way. the\ncategory of algorithms that employ the kernel trick is known as kernel machines\nor kernel methods (\n\nwilliams and rasmussen 1996 sch\u00f6lkopf\n\net al.,\n\n1999\n\n).\n\n,\n\n;\n\na major drawback to kernel machines is that the cost of evaluating the decision\nfunction is linear in the number of training examples, because the i-th example\n( )i ) to the decision function. support vector machines\ncontributes a term \u03b1 ik(x x,\nare able to mitigate this by learning an \u03b1 vector that contains mostly zeros.\nclassifying a new example then requires evaluating the kernel function only for\nthe training examples that have non-zero \u03b1 i. these training examples are known\nas support vectors.\n\nkernel machines also suffer from a high computational cost of training when\nthe dataset is large. we will revisit this idea in sec.\u00a0\n. kernel machines with\ngeneric kernels struggle to generalize well. we will explain why in sec.\n. the\nmodern incarnation of deep learning was designed to overcome these limitations of\nkernel machines. the current deep learning renaissance began when hinton et al.\n(\n) demonstrated that a neural network could outperform the rbf kernel svm\n2006\non the mnist benchmark.\n\n5.11\n\n5.9\n\n142\n\n "}, {"Page_number": 158, "text": "chapter 5. machine learning basics\n\n5.7.3 other simple supervised learning algorithms\n\nwe have already briefly encountered another non-probabilistic supervised learning\nalgorithm, nearest neighbor regression. more generally, k-nearest neighbors is\na family of techniques that can be used for classification or regression. as a\nnon-parametric learning algorithm, k-nearest neighbors is not restricted to a fixed\nnumber of parameters. we usually think of the k-nearest neighbors algorithm\nas not having any parameters, but rather implementing a simple function of the\ntraining data. in fact, there is not even really a training stage or learning process.\ninstead, at test time, when we want to produce an output y for a new test input x,\nwe find the k-nearest neighbors to x in the training data x. we then return the\naverage of the corresponding y values in the training set. this works for essentially\nany kind of supervised learning where we can define an average over y values. in\nthe case of classification, we can average over one-hot code vectors c with cy = 1\nand ci = 0 for all other values of i. we can then interpret the average over these\none-hot codes as giving a probability distribution over classes. as a non-parametric\nlearning algorithm, k-nearest neighbor can achieve very high capacity. for example,\nsuppose we have a multiclass classification task and measure performance with 0-1\nloss. in this setting,\n-nearest neighbor converges to double the bayes error as the\n1\nnumber of training examples approaches infinity. the error in excess of the bayes\nerror results from choosing a single neighbor by breaking ties between equally\ndistant neighbors randomly. when there is infinite training data, all test points x\nwill have infinitely many training set neighbors at distance zero. if we allow the\nalgorithm to use all of these neighbors to vote, rather than randomly choosing one\nof them, the procedure converges to the bayes error rate.\u00a0the high capacity of\nk-nearest neighbors allows it to obtain high accuracy given a large training set.\nhowever, it does so at high computational cost, and it may generalize very badly\ngiven a small, finite training set. one weakness of k-nearest neighbors is that it\ncannot learn that one feature is more discriminative than another. for example,\nimagine we have a regression task with x \u2208 r100 drawn from an isotropic gaussian\ndistribution, but only a single variable x1 is relevant to the output. suppose\nfurther that this feature simply encodes the output directly, i.e. that y = x1 in all\ncases. nearest neighbor regression will not be able to detect this simple pattern.\nthe nearest neighbor of most points x will be determined by the large number of\nfeatures x2 through x100, not by the lone feature x1.\u00a0thus the output on small\ntraining sets will essentially be random.\n\n143\n\n "}, {"Page_number": 159, "text": "chapter 5. machine learning basics\n\n0\n\n1\n\n00\n\n01\n\n10\n\n11\n\n010\n\n011\n\n110\n\n111\n\n1110\n\n1111\n\n00\n\n0\n\n010\n\n01\n\n011\n\n1\n\n110\n\n11\n\n10\n\n1110\n\n111\n\n1111\n\nfigure 5.7: diagrams describing how a decision tree works. (top) each node of the tree\nchooses to send the input example to the child node on the left (0) or or the child node on\nthe right (1). internal nodes are drawn as circles and leaf nodes as squares. each node is\ndisplayed with a binary string identifier corresponding to its position in the tree, obtained\nby appending a bit to its parent identifier (0=choose left or top, 1=choose right or bottom).\n(bottom) the tree divides space into regions.\u00a0the 2d plane shows how a decision tree\nmight divide r2. the nodes of the tree are plotted in this plane, with each internal node\ndrawn along the dividing line it uses to categorize examples, and leaf nodes drawn in the\ncenter of the region of examples they receive. the result is a piecewise-constant function,\nwith one piece per leaf. each leaf requires at least one training example to define, so it is\nnot possible for the decision tree to learn a function that has more local maxima than the\nnumber of training examples.\n\n144\n\n "}, {"Page_number": 160, "text": "chapter 5. machine learning basics\n\n5.7\n\nanother type of learning algorithm that also breaks the input space into regions\nbreiman et al.\n,\nand has separate parameters for each region is the decision tree (\n1984) and its many variants. as shown in fig.\n, each node of the decision tree\nis associated with a region in the input space, and internal nodes break that region\ninto one sub-region for each child of the node (typically using an axis-aligned\ncut).\u00a0space is thus sub-divided into non-overlapping regions, with a one-to-one\ncorrespondence between leaf nodes and input regions. each leaf node usually maps\nevery point in its input region to the same output. decision trees are usually\ntrained with specialized algorithms that are beyond the scope of this book. the\nlearning algorithm can be considered non-parametric if it is allowed to learn a tree\nof arbitrary size, though decision trees are usually regularized with size constraints\nthat turn them into parametric models in practice. decision trees as they are\ntypically used, with axis-aligned splits and constant outputs within each node,\nstruggle to solve some problems that are easy even for logistic regression. for\nexample, if we have a two-class problem and the positive class occurs wherever\nx2 > x1, the decision boundary is not axis-aligned. the decision tree will thus\nneed to approximate the decision boundary with many nodes, implementing a step\nfunction that constantly walks back and forth across the true decision function\nwith axis-aligned steps.\n\nas we have seen, nearest neighbor predictors and decision trees have many\nlimitations. nonetheless, they are useful learning algorithms when computational\nresources are constrained. we can also build intuition for more sophisticated\nlearning algorithms by thinking about the similarities and differences between\nsophisticated algorithms and -nn or decision tree baselines.\n\nk\n\nsee\n\nmurphy 2012 bishop 2006 hastie et al. 2001\n\n) or other machine\nlearning textbooks for more material on traditional supervised learning algorithms.\n\n),\u00a0\n\n),\n\n(\n\n(\n\n(\n\n5.8 unsupervised learning algorithms\n\n5.1.3\n\nrecall from sec.\nthat unsupervised algorithms are those that experience only\n\u201cfeatures\u201d but not a supervision signal. the distinction between supervised and\nunsupervised algorithms is not formally and rigidly defined because there is no\nobjective test for distinguishing whether a value is a feature or a target provided by\na supervisor. informally, unsupervised learning refers to most attempts to extract\ninformation from a distribution that do not require human labor to annotate\nexamples. the term is usually associated with density estimation, learning to\ndraw samples from a distribution, learning to denoise data from some distribution,\nfinding a manifold that the data lies near, or clustering the data into groups of\n\n145\n\n "}, {"Page_number": 161, "text": "chapter 5. machine learning basics\n\nrelated examples.\n\na classic unsupervised learning task is to find the \u201cbest\u201d representation of the\ndata. by \u2018best\u2019 we can mean different things, but generally speaking we are looking\nfor a representation that preserves as much information about x as possible while\nobeying some penalty or constraint aimed at keeping the representation\nor\nmore accessible than\n\nsimpler\n\nitself.\n\nx\n\n,\n\n,\n\nsimpler\u00a0\n\nthere are multiple ways of defining a\n\nrepresentation. three of the\nmost common include lower dimensional representations, sparse representations\nand independent representations. low-dimensional representations attempt to\ncompress as much information about x as possible in a smaller representation.\nsparse representations (\nbarlow 1989 olshausen and field 1996 hinton and\nghahramani 1997\n) embed the dataset into a representation whose entries are\nmostly zeroes for most inputs. the use of sparse representations typically requires\nincreasing the dimensionality of the representation, so that the representation\nbecoming mostly zeroes does not discard too much information. this results in an\noverall structure of the representation that tends to distribute data along the axes\nof the representation space. independent representations attempt to disentangle\nthe sources of variation underlying the data distribution such that the dimensions\nof the representation are statistically independent.\n\n;\n\n,\n\n;\n\nof\u00a0course these\u00a0three\u00a0criteria are\u00a0certainly\u00a0not mutually exclusive. low-\ndimensional representations often yield elements that have fewer or weaker de-\npendencies than the original high-dimensional data. this is because one way to\nreduce the size of a representation is to find and remove redundancies. identifying\nand removing more redundancy allows the dimensionality reduction algorithm to\nachieve more compression while discarding less information.\n\nthe notion of representation is one of the central themes of deep learning and\ntherefore one of the central themes in this book. in this section, we develop some\nsimple examples of representation learning algorithms. together, these example\nalgorithms show how to operationalize all three of the criteria above. most of the\nremaining chapters introduce additional representation learning algorithms that\ndevelop these criteria in different ways or introduce other criteria.\n\n5.8.1 principal components analysis\n\n2.12\n\nin sec.\n, we saw that the principal components analysis algorithm provides a\nmeans of compressing data. we can also view pca as an unsupervised learning\nalgorithm that learns a representation of data. this representation is based on\ntwo of the criteria for a simple representation described above. pca learns a\n\n146\n\n "}, {"Page_number": 162, "text": "chapter 5. machine learning basics\n\n20\n\n10\n\n2\nx\n\n0\n\n\u221210\n\n\u221220\n\n20\n\n10\n\n2\nz\n\n0\n\n\u221210\n\n\u221220\n\n\u2212\n\u221220\n\n10\n\n0\nx 1\n\n10\n\n20\n\n\u2212\n\u221220\n\n10\n\n0\nz1\n\n10\n\n20\n\nfigure 5.8: pca learns a linear projection that aligns the direction of greatest variance\nwith the axes of the new space. (left) the original data consists of samples of x. in this\nspace, the variance might occur along directions that are not axis-aligned.\nthe\ntransformed data z = x>w now varies most along the axis z1. the direction of second\nmost variance is now along z2.\n\n(right)\n\nrepresentation that has lower dimensionality than the original input. it also learns\na representation whose elements have no linear correlation with each other. this\nis a first step toward the criterion of learning representations whose elements are\nstatistically independent. to achieve full independence, a representation learning\nalgorithm must also remove the nonlinear relationships between variables.\n\n5.8\n\n. in sec.\n\npca learns an orthogonal, linear transformation of the data that projects an\n, we saw that we\ninput x to a representation z as shown in fig.\ncould learn a one-dimensional representation that best reconstructs the original\ndata (in the sense of mean squared error) and that this representation actually\ncorresponds to the first principal component of the data. thus we can use pca\nas a simple and effective dimensionality reduction method that preserves as much\nof the information in the data as possible (again, as measured by least-squares\nreconstruction error). in the following, we will study how the pca representation\ndecorrelates the original data representation\n\n2.12\n\n.x\n\nlet us consider the m n\u00d7 -dimensional design matrix x. we will assume that\nthe data has a mean of zero, e[ x] = 0. if this is not the case, the data can easily\nbe centered by subtracting the mean from all examples in a preprocessing step.\n\nthe unbiased sample covariance matrix associated with\n\nx\n\nis given by:\n\nvar[\n\n] =x\n\nx>x.\n\n1\n\nm \u2212 1\n\n147\n\n(5.85)\n\n "}, {"Page_number": 163, "text": "chapter 5. machine learning basics\n\npca finds a representation (through linear transformation) z = x>w where\nvar[\n\n]z is diagonal.\n\nin sec.\n\n2.12\n\n, we saw that the principal components of a design matrix\n\nx are\n\ngiven by the eigenvectors of x>x. from this view,\n\nx>x w w\n\n=  \u03bb >.\n\n(5.86)\n\nin this section, we exploit an alternative derivation of the principal components. the\nprincipal components may also be obtained via the singular value decomposition.\nspecifically, they are the right singular vectors of x . to see this, let w be the\nright singular vectors in the decomposition x = u w\u03c2 >.\u00a0we then recover the\noriginal eigenvector equation with\n\nas the eigenvector basis:\n\nw\n\nx >x =(cid:30)u w\u03c2 >(cid:31)> u w\u03c2 > = w \u03c32w >.\n\n(5.87)\n\nthe svd is helpful to show that pca results in a diagonal var[z]. using the\n\nsvd of\n\nx\n\n, we can express the variance of\n\nx\n\nas:\n\nvar[\n\n] =x\n\n=\n\n=\n\n=\n\n1\n\n1\n\n1\n\nm \u2212 1\nm \u2212 1\nm \u2212 1\nm \u2212 1\n\n1\n\nx>x\n\n(u w\u03c2 > )>u w\u03c2 >\n\nw \u03c3> u>u w\u03c2 >\n\nw \u03c32 w>,\n\n(5.88)\n\n(5.89)\n\n(5.90)\n\n(5.91)\n\nwhere we use the fact that u>u = i because the u matrix of the singular value\ndefinition is defined to be orthonormal. this shows that if we take z = x> w, we\ncan ensure that the covariance of\n\nis diagonal as required:\n\nz\n\nvar[\n\n] =z\n\n=\n\n=\n\n=\n\n1\n\n1\n\n1\n\nm \u2212 1\nm \u2212 1\nm \u2212 1\nm \u2212 1\n\n1\n\nz>z\n\nw>x>xw\n\nw w >\u03c3 2w w >\n\n\u03c32,\n\n(5.92)\n\n(5.93)\n\n(5.94)\n\n(5.95)\n\nwhere this time we use the fact that w w> = i, again from the definition of the\nsvd.\n\n148\n\n "}, {"Page_number": 164, "text": "chapter 5. machine learning basics\n\nthe above analysis shows that when we project the data x to z, via the linear\ntransformation w , the resulting representation has a diagonal covariance matrix\n(as given by \u03c32) which immediately implies that the individual elements of z are\nmutually uncorrelated.\n\nthis ability of pca to transform data into a representation where the elements\nare mutually uncorrelated is a very important property of pca. it is a simple\nexample of a representation that attempt to disentangle the unknown factors\nof variation underlying the data.\u00a0in the case of pca, this disentangling takes\nthe form of finding a rotation of the input space (described by w ) that aligns the\nprincipal axes of variance with the basis of the new representation space associated\nwith .z\n\nwhile correlation is an important category of dependency between elements of\nthe data, we are also interested in learning representations that disentangle more\ncomplicated forms of feature dependencies. for this, we will need more than what\ncan be done with a simple linear transformation.\n\n5.8.2\n\nk\n\n-means clustering\n\nanother example of a simple representation learning algorithm is k -means clustering.\nthe k-means clustering algorithm divides the training set into k different clusters\nof examples that are near each other. we can thus think of the algorithm as\nproviding a k-dimensional one-hot code vector h representing an input x. if x\nbelongs to cluster i , then hi = 1 and all other entries of the representation h are\nzero.\n\nthe one-hot code provided by k-means clustering is an example of a sparse\nrepresentation, because the majority of its entries are zero for every input. later,\nwe will develop other algorithms that learn more flexible sparse representations,\nwhere more than one entry can be non-zero for each input x . one-hot codes\nare an extreme example of sparse representations that lose many of the benefits\nof a distributed representation. the one-hot code still confers some statistical\nadvantages (it naturally conveys the idea that all examples in the same cluster are\nsimilar to each other) and it confers the computational advantage that the entire\nrepresentation may be captured by a single integer.\n\nthe k-means algorithm works by initializing k different centroids {\u00b5(1), . . . , \u00b5( )k }\nto different values, then alternating between two different steps until convergence.\nin one step, each training example is assigned to cluster i, where i is the index of\nthe nearest centroid \u00b5( )i . in the other step, each centroid \u00b5( )i\nis updated to the\nmean of all training examples x( )j assigned to cluster .i\n\n149\n\n "}, {"Page_number": 165, "text": "chapter 5. machine learning basics\n\none difficulty pertaining to clustering is that the clustering problem is inherently\nill-posed, in the sense that there is no single criterion that measures how well a\nclustering of the data corresponds to the real world. we can measure properties of\nthe clustering such as the average euclidean distance from a cluster centroid to the\nmembers of the cluster. this allows us to tell how well we are able to reconstruct\nthe training data from the cluster assignments. we do not know how well the\ncluster assignments correspond to properties of the real world.\u00a0moreover, there\nmay be many different clusterings that all correspond well to some property of\nthe real world. we may hope to find a clustering that relates to one feature but\nobtain a different, equally valid clustering that is not relevant to our task. for\nexample, suppose that we run two clustering algorithms on a dataset consisting of\nimages of red trucks, images of red cars, images of gray trucks, and images of gray\ncars. if we ask each clustering algorithm to find two clusters, one algorithm may\nfind a cluster of cars and a cluster of trucks, while another may find a cluster of\nred vehicles and a cluster of gray vehicles. suppose we also run a third clustering\nalgorithm, which is allowed to determine the number of clusters. this may assign\nthe examples to four clusters, red cars, red trucks, gray cars, and gray trucks. this\nnew clustering now at least captures information about both attributes, but it has\nlost information about similarity. red cars are in a different cluster from gray\ncars, just as they are in a different cluster from gray trucks.\u00a0the output of the\nclustering algorithm does not tell us that red cars are more similar to gray cars\nthan they are to gray trucks. they are different from both things, and that is all\nwe know.\n\nthese issues illustrate some of the reasons that we may prefer a distributed\nrepresentation to a one-hot representation. a distributed representation could have\ntwo attributes for each vehicle\u2014one representing its color and one representing\nwhether it is a car or a truck.\nit is still not entirely clear what the optimal\ndistributed representation is (how can the learning algorithm know whether the\ntwo attributes we are interested in are color and car-versus-truck rather than\nmanufacturer and age?) but having many attributes reduces the burden on the\nalgorithm to guess which single attribute we care about, and allows us to measure\nsimilarity between objects in a fine-grained way by comparing many attributes\ninstead of just testing whether one attribute matches.\n\n5.9 stochastic gradient descent\n\nnearly all of deep learning is powered by one very important algorithm: stochastic\n. stochastic gradient descent is an extension of the gradient\ngradient descent\n\nsgd\n\nor\n\n150\n\n "}, {"Page_number": 166, "text": "chapter 5. machine learning basics\n\ndescent algorithm introduced in sec.\n\n.4.3\n\na recurring problem in machine learning is that large training sets are necessary\nfor good generalization, but large training sets are also more computationally\nexpensive.\n\nthe cost function used by a machine learning algorithm often decomposes as a\nsum over training examples of some per-example loss function. for example, the\nnegative conditional log-likelihood of the training data can be written as\n\nj( ) = \n\n\u03b8\n\nex,y\u223c\u02c6pdata l , y,\n\n(x\n\n\u03b8) =\n\n1\nm\n\nmxi=1\n\nl(x( )i , y( )i , \u03b8)\n\n(5.96)\n\nwhere\n\nl\n\nis the per-example loss\n\nl , y,\n\n(x\n\n\u03b8) = \n\nlog\u2212\n\np y\n(\n\n| x \u03b8; )\n.\n\nfor these additive cost functions, gradient descent requires computing\n\n\u2207\u03b8j( ) =\u03b8\n\n1\nm\n\nmxi=1\n\n\u2207 \u03b8l(x( )i , y( )i ,\n\n.\u03b8)\n\n(5.97)\n\nthe computational cost of this operation is o(m). as the training set size grows to\nbillions of examples, the time to take a single gradient step becomes prohibitively\nlong.\n\nthe insight of stochastic gradient descent is that the gradient is an expectation.\nthe expectation may be approximately estimated using a small set of samples.\nspecifically, on each step of the algorithm, we can sample a minibatch of examples\n\nb = {x(1), . . . , x(m0)} drawn uniformly from the training set. the minibatch size\nm0 is typically chosen to be a relatively small number of examples, ranging from\n1 to a few hundred. crucially, m0 is usually held fixed as the training set size m\ngrows. we may fit a training set with billions of examples using updates computed\non only a hundred examples.\n\nthe estimate of the gradient is formed as\n\ng =\n\n1\nm0\u2207\u03b8\n\nm0xi=1\n\nl(x( )i , y( )i ,\n\n.\u03b8)\n\n(5.98)\n\nusing examples from the minibatch . the stochastic gradient descent algorithm\nthen follows the estimated gradient downhill:\n\nb\n\nwhere\n\n(cid:115)\n\nis the learning rate.\n\n\u03b8\n\n\u03b8\n\n\u2190 \u2212 (cid:115) ,\ng\n\n151\n\n(5.99)\n\n "}, {"Page_number": 167, "text": "chapter 5. machine learning basics\n\ngradient descent in general has often been regarded as slow or unreliable. in\nthe past, the application of gradient descent to non-convex optimization problems\nwas regarded as foolhardy or unprincipled. today, we know that the machine\nlearning models described in part work very well when trained with gradient\ndescent. the optimization algorithm may not be guaranteed to arrive at even a\nlocal minimum in a reasonable amount of time, but it often finds a very low value\nof the cost function quickly enough to be useful.\n\nii\n\nstochastic gradient descent has many important uses outside the context of\ndeep learning.\nit is the main way to train large linear models on very large\ndatasets. for a fixed model size, the cost per sgd update does not depend on the\ntraining set size m. in practice, we often use a larger model as the training set size\nincreases, but we are not forced to do so. the number of updates required to reach\nconvergence usually increases with training set size.\u00a0however, as m approaches\ninfinity, the model will eventually converge to its best possible test error before\nsgd has sampled every example in the training set. increasing m further will not\nextend the amount of training time needed to reach the model\u2019s best possible test\nerror. from this point of view, one can argue that the asymptotic cost of training\na model with sgd is\n\nas a function of\n\no(1)\n\n.\nm\n\nprior to the advent of deep learning, the main way to learn nonlinear models\nwas to use the kernel trick in combination with a linear model. many kernel learning\nalgorithms require constructing an m m\u00d7 matrix gi,j = k(x( )i , x( )j ). constructing\nthis matrix has computational cost o (m2), which is clearly undesirable for datasets\nwith\u00a0billions of\u00a0examples.\nin\u00a0academia,\u00a0starting\u00a0in 2006, deep\u00a0learning was\ninitially interesting because it was able to generalize to new examples better\nthan competing algorithms when trained on medium-sized datasets with tens of\nthousands of examples. soon after, deep learning garnered additional interest in\nindustry, because it provided a scalable way of training nonlinear models on large\ndatasets.\n\nstochastic gradient descent and many enhancements to it are described further\n\nin chapter\n\n.8\n\n5.10 building a machine learning algorithm\n\nnearly all deep learning algorithms can be described as particular instances of\na fairly simple recipe: combine a specification of a dataset, a cost function, an\noptimization procedure and a model.\n\nfor example, the linear regression algorithm combines a dataset consisting of\n\n152\n\n "}, {"Page_number": 168, "text": "chapter 5. machine learning basics\n\nx\n\nand , the cost function\n\ny\n\nj\n\n, b\n\n(w ) = \u2212ex,y\u223c\u02c6p data log pmodel(\n\ny | x ,\n)\n\n(5.100)\n\nthe model specification pmodel (y | x) = n (y; x> w + b, 1), and, in most cases, the\noptimization algorithm defined by solving for where the gradient of the cost is zero\nusing the normal equations.\n\nby realizing that we can replace any of these components mostly independently\n\nfrom the others, we can obtain a very wide variety of algorithms.\n\nthe cost function typically includes at least one term that causes the learning\nprocess to perform statistical estimation. the most common cost function is the\nnegative log-likelihood, so that minimizing the cost function causes maximum\nlikelihood estimation.\n\nthe cost function may also include additional terms, such as regularization\nterms. for example, we can add weight decay to the linear regression cost function\nto obtain\n\n2 \u2212 ex,\nthis still allows closed-form optimization.\n\n(w ) = \n\n||w 2\n||\n\u03bb\n\n, b\n\nj\n\npy\u223c data log pmodel(\n\ny | x .\n)\n\n(5.101)\n\nif we change the model to be nonlinear, then most cost functions can no longer\nbe optimized in closed form. this requires us to choose an iterative numerical\noptimization procedure, such as gradient descent.\n\nthe recipe for constructing a learning algorithm by combining models, costs, and\noptimization algorithms supports both supervised and unsupervised learning. the\nlinear regression example shows how to support supervised learning. unsupervised\nlearning can be supported by defining a dataset that contains only x and providing\nan appropriate unsupervised cost and model. for example, we can obtain the first\npca vector by specifying that our loss function is\n\nj(\n\n) = w\n\nex\u223cpdata|| \u2212\n\nx r( ;\n\n)x w 2\n||\n2\n\n(5.102)\n\nwhile our model is defined to have w with norm one and reconstruction function\nr( ) = \n\nw>xw.\n\nx\n\nin some cases, the cost function may be a function that we cannot actually\nevaluate, for computational reasons. in these cases, we can still approximately\nminimize it using iterative numerical optimization so long as we have some way of\napproximating its gradients.\n\nmost machine learning algorithms make use of this recipe, though it may not\nimmediately be obvious. if a machine learning algorithm seems especially unique or\n\n153\n\n "}, {"Page_number": 169, "text": "chapter 5. machine learning basics\n\nhand-designed, it can usually be understood as using a special-case optimizer. some\nmodels such as decision trees or k-means require special-case optimizers because\ntheir cost functions have flat regions that make them inappropriate for minimization\nby gradient-based optimizers. recognizing that most machine learning algorithms\ncan be described using this recipe helps to see the different algorithms as part of a\ntaxonomy of methods for doing related tasks that work for similar reasons, rather\nthan as a long list of algorithms that each have separate justifications.\n\n5.11 challenges motivating deep learning\n\nthe simple machine learning algorithms described in this chapter work very well on\na wide variety of important problems. however, they have not succeeded in solving\nthe central problems in ai, such as recognizing speech or recognizing objects.\n\nthe development of deep learning was motivated in part by the failure of\n\ntraditional algorithms to generalize well on such ai tasks.\n\nthis section is about how the challenge of generalizing to new examples becomes\nexponentially more difficult when working with high-dimensional data, and how\nthe mechanisms used to achieve generalization in traditional machine learning\nare insufficient to learn complicated functions in high-dimensional spaces. such\nspaces also often impose high computational costs. deep learning was designed to\novercome these and other obstacles.\n\n5.11.1 the curse of dimensionality\n\nmany machine learning problems become exceedingly difficult when the number\nof\u00a0dimensions in\u00a0the data\u00a0is high. this\u00a0phenomenon is\u00a0known as\u00a0the curse\nof dimensionality.\u00a0of particular concern is that the number of possible distinct\nconfigurations of a set of variables increases exponentially as the number of variables\nincreases.\n\n154\n\n "}, {"Page_number": 170, "text": "chapter 5. machine learning basics\n\nfigure 5.9: as the number of relevant dimensions of the data increases (from left to\nright), the number of configurations of interest may grow exponentially. (left) in this\none-dimensional example, we have one variable for which we only care to distinguish 10\nregions of interest. with enough examples falling within each of these regions (each region\ncorresponds to a cell in the illustration), learning algorithms can easily generalize correctly.\na straightforward way to generalize is to estimate the value of the target function within\neach region (and possibly interpolate between neighboring regions).\nwith 2\ndimensions (center) it is more difficult to distinguish 10 different values of each variable.\nwe need to keep track of up to 10\u00d710=100 regions, and we need at least that many\n103 = 1000\nexamples to cover all those regions.\nregions and at least that many examples. for d dimensions and v values to be distinguished\nalong each axis, we seem to need o( vd ) regions and examples. this is an instance of the\ncurse of dimensionality. figure graciously provided by nicolas chapados.\n\nwith 3 dimensions this grows to\n\n(center)\n\n(right)\n\nthe curse of dimensionality arises in many places in computer science, and\n\nespecially so in machine learning.\n\n5.9\n\none challenge posed by the curse of dimensionality is a statistical challenge.\n, a statistical challenge arises because the number of\nas illustrated in fig.\npossible configurations of x is much larger than the number of training examples.\nto understand the issue, let us consider that the input space is organized into a\ngrid, like in the figure. in low dimensions we can describe this space with a low\nnumber of grid cells that are mostly occupied by the data. when generalizing to a\nnew data point, we can usually tell what to do simply by inspecting the training\nexamples that lie in the same cell as the new input.\u00a0for example, if estimating\nthe probability density at some point x, we can just return the number of training\nexamples in the same unit volume cell as x, divided by the total number of training\nexamples. if we wish to classify an example, we can return the most common class\nof training examples in the same cell. if we are doing regression we can average\nthe target values observed over the examples in that cell. but what about the\ncells for which we have seen no example? because in high-dimensional spaces the\nnumber of configurations is going to be huge, much larger than our number of\nexamples, most configurations will have no training example associated with it.\n\n155\n\n "}, {"Page_number": 171, "text": "chapter 5. machine learning basics\n\nhow could we possibly say something meaningful about these new configurations?\nmany traditional machine learning algorithms simply assume that the output at a\nnew point should be approximately the same as the output at the nearest training\npoint.\n\n5.11.2 local constancy and smoothness regularization\n\nin order to generalize well, machine learning algorithms need to be guided by prior\nbeliefs about what kind of function they should learn. previously, we have seen\nthese priors incorporated as explicit beliefs in the form of probability distributions\nover parameters of the model. more informally, we may also discuss prior beliefs as\ndirectly influencing the\nitself and only indirectly acting on the parameters\nvia their effect on the function. additionally, we informally discuss prior beliefs as\nbeing expressed implicitly, by choosing algorithms that are biased toward choosing\nsome class of functions over another, even though these biases may not be expressed\n(or even possible to express) in terms of a probability distribution representing our\ndegree of belief in various functions.\n\nfunction\n\namong the most widely used of these implicit \u201cpriors\u201d is the smoothness prior\nor local constancy prior. this prior states that the function we learn should not\nchange very much within a small region.\n\nmany simpler algorithms rely exclusively on this prior to generalize well, and\nas a result they fail to scale to the statistical challenges involved in solving ai-\nlevel tasks. throughout this book, we will describe how deep learning introduces\nadditional (explicit\u00a0and implicit) priors in order to\u00a0reduce the generalization\nerror on sophisticated tasks. here, we explain why the smoothness prior alone is\ninsufficient for these tasks.\n\nthere are many different ways to implicitly or explicitly express a prior belief\nthat the learned function should be smooth or locally constant. all of these different\nmethods are designed to encourage the learning process to learn a function f\u2217 that\nsatisfies the condition\n\nf \u2217 ( ) x \u2248 f\u2217( + )\nx (cid:115)\n\n(5.103)\n\nfor most configurations x and small change (cid:115). in other words, if we know a good\nanswer for an input x (for example, if x is a labeled training example) then that\nanswer is probably good in the neighborhood of x. if we have several good answers\nin some neighborhood we would combine them (by some form of averaging or\ninterpolation) to produce an answer that agrees with as many of them as much as\npossible.\n\nan extreme example of the local constancy approach is the k -nearest neighbors\n\n156\n\n "}, {"Page_number": 172, "text": "chapter 5. machine learning basics\n\nfamily of learning algorithms. these predictors are literally constant over each\nregion containing all the points x that have the same set of k nearest neighbors in\nthe training set. for k = 1, the number of distinguishable regions cannot be more\nthan the number of training examples.\n\nwhile the k-nearest neighbors algorithm copies the output from nearby training\nexamples, most kernel machines interpolate between training set outputs associated\nwith nearby training examples. an important class of kernels is the family of local\nkernels where k(u v,\n) is large when u = v and decreases as u and v grow farther\napart from each other. a local kernel can be thought of as a similarity function\nthat performs template matching, by measuring how closely a test example x\nresembles each training example x( )i .\u00a0much of the modern motivation for deep\nlearning is derived from studying the limitations of local template matching and\nhow deep models are able to succeed in cases where local template matching fails\n(\nbengio et al. 2006b\n\n).\n\n,\n\ndecision trees also suffer from the limitations of exclusively smoothness-based\nlearning because they break the input space into as many regions as there are\nleaves and use a separate parameter (or sometimes many parameters for extensions\nof decision trees) in each region. if the target function requires a tree with at\nleast n leaves to be represented accurately, then at least n training examples are\nrequired to fit the tree. a multiple of n is needed to achieve some level of statistical\nconfidence in the predicted output.\n\nin general, to distinguish o( k) regions in input space, all of these methods\nrequire o(k) examples. typically there are o( k) parameters, with o(1) parameters\nassociated with each of the o(k) regions. the case of a nearest neighbor scenario,\nwhere each training example can be used to define at most one region, is illustrated\nin fig.\n\n.\n5.10\n\nis there a way to represent a complex function that has many more regions\nto be distinguished than the number of training examples? clearly, assuming\nonly smoothness of the underlying function will not allow a learner to do that.\nfor\u00a0example,\u00a0imagine that\u00a0the target function is a kind of checkerboard. a\ncheckerboard contains many variations but there is a simple structure to them.\nimagine what happens when the number of training examples is substantially\nsmaller than the number of black and white squares on the checkerboard. based\non only local generalization and the smoothness or local constancy prior, we would\nbe guaranteed to correctly guess the color of a new point if it lies within the same\ncheckerboard square as a training example. there is no guarantee that the learner\ncould correctly extend the checkerboard pattern to points lying in squares that do\nnot contain training examples. with this prior alone, the only information that an\n\n157\n\n "}, {"Page_number": 173, "text": "chapter 5. machine learning basics\n\nfigure 5.10:\u00a0illustration of how the nearest neighbor algorithm breaks up the input space\ninto regions.\u00a0an example (represented here by a circle) within each region defines the\nregion boundary (represented here by the lines). the y value associated with each example\ndefines what the output should be for all points within the corresponding region.\u00a0the\nregions defined by nearest neighbor matching form a geometric pattern called a voronoi\ndiagram. the number of these contiguous regions cannot grow faster than the number\nof training examples. while this figure illustrates the behavior of the nearest neighbor\nalgorithm specifically, other machine learning algorithms that rely exclusively on the\nlocal smoothness prior for generalization exhibit similar behaviors: each training example\nonly informs the learner about how to generalize in some neighborhood immediately\nsurrounding that example.\n\n158\n\n "}, {"Page_number": 174, "text": "chapter 5. machine learning basics\n\nexample tells us is the color of its square, and the only way to get the colors of the\nentire checkerboard right is to cover each of its cells with at least one example.\n\nthe smoothness assumption and the associated non-parametric learning algo-\nrithms work extremely well so long as there are enough examples for the learning\nalgorithm to observe high points on most peaks and low points on most valleys\nof the true underlying function to be learned. this is generally true when the\nfunction to be learned is smooth enough and varies in few enough dimensions.\nin high dimensions, even a very smooth function can change smoothly but in a\ndifferent way along each dimension. if the function additionally behaves differently\nin different regions, it can become extremely complicated to describe with a set of\ntraining examples. if the function is complicated (we want to distinguish a huge\nnumber of regions compared to the number of examples), is there any hope to\ngeneralize well?\n\nthe answer to both of these questions is yes. the key insight is that a very\nlarge number of regions, e.g., o(2 k), can be defined with o(k) examples, so long\nas we introduce some dependencies between the regions via additional assumptions\nabout the underlying data generating distribution. in this way, we can actually\ngeneralize non-locally (\n). many\ndifferent deep learning algorithms provide implicit or explicit assumptions that are\nreasonable for a broad range of ai tasks in order to capture these advantages.\n\nbengio and monperrus 2005 bengio et al. 2006c\n\n,\n\n;\n\n,\n\nother approaches to machine learning often make stronger, task-specific as-\nsumptions. for example, we could easily solve the checkerboard task by providing\nthe assumption that the target function is periodic. usually we do not include such\nstrong, task-specific assumptions into neural networks so that they can generalize\nto a much wider variety of structures. ai tasks have structure that is much too\ncomplex to be limited to simple, manually specified properties such as periodicity,\nso we want learning algorithms that embody more general-purpose assumptions.\nthe core idea in deep learning is that we assume that the data was generated\nby the composition of factors or features, potentially at multiple levels in a\nhierarchy. many other similarly generic assumptions can further improve deep\nlearning algorithms. these apparently mild assumptions allow an exponential gain\nin the relationship between the number of examples and the number of regions\nthat can be distinguished. these exponential gains are described more precisely in\nsec.\n. the exponential advantages conferred by the\nuse of deep, distributed representations counter the exponential challenges posed\nby the curse of dimensionality.\n\n, and sec.\n\n, sec.\n\n6.4.1\n\n15.5\n\n15.4\n\n159\n\n "}, {"Page_number": 175, "text": "chapter 5. machine learning basics\n\n5.11.3 manifold learning\n\nan important concept underlying many ideas in machine learning is that of a\nmanifold.\n\na\n\nmanifold\n\nis a connected region. mathematically, it is a set of points, associated\nwith a neighborhood around each point. from any given point, the manifold locally\nappears to be a euclidean space. in everyday life, we experience the surface of the\nworld as a 2-d plane, but it is in fact a spherical manifold in 3-d space.\n\nthe definition of a neighborhood surrounding each point implies the existence\nof transformations that can be applied to move on the manifold from one position\nto a neighboring one. in the example of the world\u2019s surface as a manifold, one can\nwalk north, south, east, or west.\n\nalthough there is a formal mathematical meaning to the term \u201cmanifold,\u201d\nin machine learning it tends to be used more loosely to designate a connected\nset of points that can be approximated well by considering only a small number\nof degrees of freedom, or dimensions, embedded in a higher-dimensional space.\neach dimension corresponds to a local direction of variation. see fig.\nfor an\nexample of training data lying near a one-dimensional manifold embedded in two-\ndimensional space. in the context of machine learning, we allow the dimensionality\nof the manifold to vary from one point to another. this often happens when a\nmanifold intersects itself. for example, a figure eight is a manifold that has a single\ndimension in most places but two dimensions at the intersection at the center.\n\n5.11\n\n2 5.\n\n2 0.\n\n1 5.\n\n1 0.\n\n0 5.\n\n0 0.\n\n\u22120 5.\n\u22121 0.\n\n0 5\n\n.\n\n1 0\n\n.\n\n1 5\n\n.\n\n2 0\n\n.\n\n2 5\n\n.\n\n3 0\n\n.\n\n3 5\n\n.\n\n4 0\n\n.\n\nfigure 5.11: data sampled from a distribution in a two-dimensional space that is actually\nconcentrated near a one-dimensional manifold, like a twisted string. the solid line indicates\nthe underlying manifold that the learner should infer.\n\n160\n\n "}, {"Page_number": 176, "text": "chapter 5. machine learning basics\n\nmany machine learning problems seem hopeless if we expect the machine\nlearning algorithm to\u00a0learn functions\u00a0with interesting variations\u00a0across all\u00a0of\nrn. manifold learning algorithms surmount this obstacle by assuming that most\nof rn consists of invalid inputs,\u00a0and that interesting inputs occur only along\na collection of manifolds containing a small subset of points, with interesting\nvariations in the output of the learned function occurring only along directions\nthat lie on the manifold, or with interesting variations happening only when we\nmove from one manifold to another. manifold learning was introduced in the case\nof continuous-valued data and the unsupervised learning setting, although this\nprobability concentration idea can be generalized to both discrete data and the\nsupervised learning setting: the key assumption remains that probability mass is\nhighly concentrated.\n\nthe assumption that the data lies along a low-dimensional manifold may not\nalways be correct or useful. we argue that in the context of ai tasks, such as\nthose that involve processing images, sounds, or text, the manifold assumption is\nat least approximately correct. the evidence in favor of this assumption consists\nof two categories of observations.\n\n5.12\n\nthe first observation in favor of the manifold hypothesis is that the probability\ndistribution over images, text strings, and sounds that occur in real life is highly\nconcentrated.\u00a0uniform noise essentially never resembles structured inputs from\nthese domains. fig.\nshows how, instead, uniformly sampled points look like the\npatterns of static that appear on analog television sets when no signal is available.\nsimilarly, if you generate a document by picking letters uniformly at random, what\nis the probability that you will get a meaningful english-language text? almost\nzero, again, because most of the long sequences of letters do not correspond to a\nnatural language sequence: the distribution of natural language sequences occupies\na very small volume in the total space of sequences of letters.\n\n161\n\n "}, {"Page_number": 177, "text": "chapter 5. machine learning basics\n\nfigure 5.12: sampling images uniformly at random (by randomly picking each pixel\naccording to a uniform distribution) gives rise to noisy images. although there is a non-\nzero probability to generate an image of a face or any other object frequently encountered\nin ai applications, we never actually observe this happening in practice. this suggests\nthat the images encountered in ai applications occupy a negligible proportion of the\nvolume of image space.\n\nof course, concentrated probability distributions are not sufficient to show\nthat the data lies on a reasonably small number of manifolds. we must also\nestablish that the examples we encounter are connected to each other by other\n\n162\n\n "}, {"Page_number": 178, "text": "chapter 5. machine learning basics\n\nexamples, with each example surrounded by other highly similar examples that\nmay be reached by applying transformations to traverse the manifold. the second\nargument in favor of the manifold hypothesis is that we can also imagine such\nneighborhoods and transformations, at least informally. in the case of images, we\ncan certainly think of many possible transformations that allow us to trace out a\nmanifold in image space: we can gradually dim or brighten the lights, gradually\nmove or rotate objects in the image, gradually alter the colors on the surfaces of\nobjects, etc. it remains likely that there are multiple manifolds involved in most\napplications. for example, the manifold of images of human faces may not be\nconnected to the manifold of images of cat faces.\n\nthese thought experiments supporting the manifold hypotheses convey some in-\ntuitive reasons supporting it. more rigorous experiments\u00a0(cayton 2005 narayanan\nand mitter 2010 sch\u00f6lkopf\net al.,\n2000 brand 2003 belkin and niyogi 2003 donoho and grimes 2003 weinberger\nand saul 2004\n) clearly support the hypothesis for a large class of datasets of\ninterest in ai.\n\n1998 roweis and saul 2000 tenenbaum\n\net al.,\n\n;\n,\n\n,\n\n,\n\n;\n\n,\n\n;\n\n;\n\n,\n\n,\n\n;\n\n,\n\n;\n\n;\n\n;\n\nwhen the data lies on a low-dimensional manifold, it can be most natural\nfor machine learning algorithms to represent the data in terms of coordinates on\nthe manifold, rather than in terms of coordinates in rn. in everyday life, we can\nthink of roads as 1-d manifolds embedded in 3-d space. we give directions to\nspecific addresses in terms of address numbers along these 1-d roads, not in terms\nof coordinates in 3-d space. extracting these manifold coordinates is challenging,\nbut holds the promise to improve many machine learning algorithms. this general\nprinciple is applied in many contexts. fig.\nshows the manifold structure of a\ndataset consisting of faces. by the end of this book, we will have developed the\nmethods necessary to learn such a manifold structure.\u00a0in fig.\n, we will see\nhow a machine learning algorithm can successfully accomplish this goal.\n\n20.6\n\n5.13\n\nthis concludes part , which has provided the basic concepts in mathematics\nand machine learning which are employed throughout the remaining parts of the\nbook. you are now prepared to embark upon your study of deep learning.\n\ni\n\n163\n\n "}, {"Page_number": 179, "text": "chapter 5. machine learning basics\n\nfigure 5.13: training examples from the qmul multiview face dataset (\n)\ngong et al. 2000\nfor which the subjects were asked to move in such a way as to cover the two-dimensional\nmanifold corresponding to two angles of rotation. we would like learning algorithms to\nbe able to discover and disentangle such manifold coordinates. fig.\nillustrates such a\nfeat.\n\n20.6\n\n,\n\n164\n\n "}, {"Page_number": 180, "text": "part ii\n\ndeep networks: modern\n\npractices\n\n165\n\n "}, {"Page_number": 181, "text": "this part of the book summarizes the state of modern deep learning as it is\n\nused to solve practical applications.\n\ndeep learning has a long history and many aspirations. several approaches\nhave been proposed that have yet to entirely bear fruit. several ambitious goals\nhave yet to be realized. these less-developed branches of deep learning appear in\nthe final part of the book.\n\nthis part focuses only on those approaches that are essentially working tech-\n\nnologies that are already used heavily in industry.\n\nmodern\u00a0deep learning\u00a0provides\u00a0a very\u00a0powerful\u00a0framework\u00a0for supervised\nlearning. by adding more layers and more units within a layer, a deep network can\nrepresent functions of increasing complexity. most tasks that consist of mapping an\ninput vector to an output vector, and that are easy for a person to do rapidly, can\nbe accomplished via deep learning, given sufficiently large models and sufficiently\nlarge datasets of labeled training examples. other tasks, that can not be described\nas associating one vector to another, or that are difficult enough that a person\nwould require time to think and reflect in order to accomplish the task, remain\nbeyond the scope of deep learning for now.\n\nthis part of the book describes the core parametric function approximation\ntechnology that is behind nearly all modern practical applications of deep learning.\nwe\u00a0begin\u00a0by\u00a0describing the\u00a0feedforward\u00a0deep network model that is used to\nrepresent these functions. next, we present advanced techniques for regularization\nand optimization of such models. scaling these models to large inputs such as high\nresolution images or long temporal sequences requires specialization. we introduce\nthe convolutional network for scaling to large images and the recurrent neural\nnetwork for processing temporal sequences. finally, we present general guidelines\nfor the practical methodology involved in designing, building, and configuring an\napplication involving deep learning, and review some of the applications of deep\nlearning.\n\nthese chapters are the most important for a practitioner\u2014someone who wants\nto begin implementing and using deep learning algorithms to solve real-world\nproblems today.\n\n166\n\n "}, {"Page_number": 182, "text": "chapter 6\n\ndeep feedforward networks\n\n(\n\n, also often called\n\nmulti-\ndeep feedforward networks\n), are the quintessential deep learning models. the goal\nlayer perceptrons mlps\nof a feedforward network is to approximate some function f\u2217.\u00a0for example, for\na classifier, y = f\u2217(x) maps an input x to a category y. a feedforward network\ndefines a mapping y = f (x; \u03b8) and learns the value of the parameters \u03b8 that result\nin the best function approximation.\n\nfeedforward neural networks\n\n, or\n\nthese models are called feedforward because information flows through the\nfunction being evaluated from x, through the intermediate computations used to\ndefine f, and finally to the output y. there are no feedback connections in which\noutputs of the model are fed back into itself. when feedforward neural networks\nare extended to include feedback connections, they are called recurrent neural\nnetworks, presented in chapter\n\n.10\n\nfeedforward networks are of extreme importance to machine learning practi-\ntioners. they form the basis of many important commercial applications. for\nexample, the convolutional networks used for object recognition from photos are a\nspecialized kind of feedforward network. feedforward networks are a conceptual\nstepping stone on the path to recurrent networks, which power many natural\nlanguage applications.\n\nnetworks\n\nfeedforward neural networks are called\n\nbecause they are typically rep-\nresented by composing together many different functions. the model is associated\nwith a directed acyclic graph describing how the functions are composed together.\nfor example, we might have three functions f (1), f (2), and f (3) connected in a\nchain, to form f(x) = f (3) (f (2)(f (1)(x))). these chain structures are the most\ncommonly used structures of neural networks. in this case, f (1) is called the first\nlayer of the network, f (2) is called the second layer, and so on. the overall length\n\n167\n\n "}, {"Page_number": 183, "text": "chapter 6. deep feedforward networks\n\ndepth\n\nof the chain gives the\nof the model. it is from this terminology that the\nname \u201cdeep learning\u201d arises. the final layer of a feedforward network is called the\noutput layer. during neural network training, we drive f(x) to match f\u2217(x). the\ntraining data provides us with noisy, approximate examples of f \u2217(x) evaluated at\nf\u2248 \u2217(x).\ndifferent training points.\u00a0each example x is accompanied by a label y\nthe training examples specify directly what the output layer must do at each point\nx; it must produce a value that is close to y. the behavior of the other layers is\nnot directly specified by the training data. the learning algorithm must decide\nhow to use those layers to produce the desired output, but the training data does\nnot say what each individual layer should do. instead, the learning algorithm must\ndecide how to use these layers to best implement an approximation of f \u2217. because\nthe training data does not show the desired output for each of these layers, these\nlayers are called\n\nhidden layers\n.\n\nwidth\n\nfinally, these networks are called neural because they are loosely inspired by\nneuroscience. each hidden layer of the network is typically vector-valued. the\ndimensionality of these hidden layers determines the\nof the model. each\nelement of the vector may be interpreted as playing a role analogous to a neuron.\nrather than thinking of the layer as representing a single vector-to-vector function,\nwe can also think of the layer as consisting of many\nthat act in parallel,\neach representing a vector-to-scalar function. each unit resembles a neuron in\nthe sense that it receives input from many other units and computes its own\nactivation value.\u00a0the idea of using many layers of vector-valued representation\nis drawn from neuroscience. the choice of the functions f( )i (x) used to compute\nthese representations is also loosely guided by neuroscientific observations about\nthe functions that biological neurons compute. however, modern neural network\nresearch is guided by many mathematical and engineering disciplines, and the\ngoal of neural networks is not to perfectly model the brain. it is best to think of\nfeedforward networks as function approximation machines that are designed to\nachieve statistical generalization, occasionally drawing some insights from what we\nknow about the brain, rather than as models of brain function.\n\nunits\n\none way to understand feedforward networks is to begin with linear models\nand consider how to overcome their limitations.\u00a0linear models, such as logistic\nregression and linear regression, are appealing because they may be fit efficiently\nand reliably, either in closed form or with convex optimization. linear models also\nhave the obvious defect that the model capacity is limited to linear functions, so\nthe model cannot understand the interaction between any two input variables.\n\nto extend linear models to represent nonlinear functions of x, we can apply\nthe linear model not to x itself but to a transformed input \u03c6(x), where \u03c6 is a\n\n168\n\n "}, {"Page_number": 184, "text": "chapter 6. deep feedforward networks\n\n5.7.2\n\nnonlinear transformation. equivalently, we can apply the kernel trick described in\nsec.\n, to obtain a nonlinear learning algorithm based on implicitly applying\nthe \u03c6 mapping. we can think of \u03c6 as providing a set of features describing x, or\nas providing a new representation for\n\n.x\n\nthe question is then how to choose the mapping .\u03c6\n\n1. one option is to use a very generic \u03c6, such as the infinite-dimensional \u03c6 that\nis implicitly used by kernel machines based on the rbf kernel.\u00a0if \u03c6(x) is\nof high enough dimension, we can always have enough capacity to fit the\ntraining set, but generalization to the test set often remains poor. very\ngeneric feature mappings are usually based only on the principle of local\nsmoothness and do not encode enough prior information to solve advanced\nproblems.\n\n2. another option is to manually engineer \u03c6. until the advent of deep learning,\nthis was the dominant approach. this approach requires decades of human\neffort for\u00a0each\u00a0separate task,\u00a0with\u00a0practitioners specializing in different\ndomains such as\u00a0speech recognition\u00a0or computer\u00a0vision,\u00a0and with little\ntransfer between domains.\n\n3. the strategy of deep learning is to learn \u03c6. in this approach, we have a model\n) = \u03c6(x; \u03b8)>w. we now have parameters \u03b8 that we use to learn\ny = f(x; \u03b8 w,\n\u03c6 from a broad class of functions, and parameters w that map from \u03c6(x) to\nthe desired output. this is an example of a deep feedforward network, with\n\u03c6 defining a hidden layer. this approach is the only one of the three that\ngives up on the convexity of the training problem, but the benefits outweigh\nthe harms. in this approach, we parametrize the representation as \u03c6(x; \u03b8)\nand use the optimization algorithm to find the \u03b8 that corresponds to a good\nrepresentation. if we wish, this approach can capture the benefit of the first\napproach by being highly generic\u2014we do so by using a very broad family\n\u03c6(x; \u03b8). this approach can also capture the benefit of the second approach.\nhuman practitioners can encode their knowledge to help generalization by\ndesigning families \u03c6(x ;\u03b8) that they expect will perform well. the advantage\nis that the human designer only needs to find the right general function\nfamily rather than finding precisely the right function.\n\nthis general principle of improving models by learning features extends beyond\nthe feedforward networks described in this chapter. it is a recurring theme of deep\nlearning that applies to all of the kinds of models described throughout this book.\nfeedforward networks are the application of this principle to learning deterministic\n\n169\n\n "}, {"Page_number": 185, "text": "chapter 6. deep feedforward networks\n\nmappings from x to y that lack feedback connections. other models presented\nlater will apply these principles to learning stochastic mappings, learning functions\nwith feedback, and learning probability distributions over a single vector.\n\nwe begin this chapter with a simple example of a feedforward network. next,\nwe address each of the design decisions needed to deploy a feedforward network.\nfirst, training a feedforward network requires making many of the same design\ndecisions as are necessary for a linear model: choosing the optimizer, the cost\nfunction, and the form of the output units. we review these basics of gradient-based\nlearning, then proceed to confront some of the design decisions that are unique\nto feedforward networks. feedforward networks have introduced the concept of a\nhidden layer, and this requires us to choose the\nthat will be\nused to compute the hidden layer values. we must also design the architecture of\nthe network, including how many layers the network should contain, how these\nnetworks should be connected to each other, and how many units should be in\neach layer. learning in deep neural networks requires computing the gradients of\ncomplicated functions. we present the back-propagation algorithm and its modern\ngeneralizations, which can be used to efficiently compute these gradients. finally,\nwe close with some historical perspective.\n\nactivation functions\n\n6.1 example: learning xor\n\nto make the idea of a feedforward network more concrete, we begin with an\nexample of a fully functioning feedforward network on a very simple task: learning\nthe xor function.\n\nthe xor function (\u201cexclusive or\u201d) is an operation on two binary values, x1\nand x2. when exactly one of these binary values is equal to , the xor function\nreturns\n. otherwise, it returns 0. the xor function provides the target function\n1\ny = f\u2217(x) that we want to learn. our model provides a function y = f(x;\u03b8) and\nour learning algorithm will adapt the parameters \u03b8 to make f as similar as possible\nto f\u2217 .\n\n1\n\nin this simple example, we will not be concerned with statistical generalization.\nwe want our network to perform correctly on the four points x = {[0,0]>, [0, 1]> ,\n[1, 0]>, and [1, 1]>}.\u00a0we will train the network on all four of these points. the\nonly challenge is to fit the training set.\n\nwe can treat this problem as a regression problem and use a mean squared error\nloss function. we choose this loss function to simplify the math for this example\nas much as possible. we will see later that there are other, more appropriate\n\n170\n\n "}, {"Page_number": 186, "text": "chapter 6. deep feedforward networks\n\napproaches for modeling binary data.\n\nevaluated on our whole training set, the mse loss function is\n\nj( ) =\u03b8\n\n1\n\n4xx\u2208x\n\n(f\u2217 ( )\n\nx \u2212 f x \u03b8 2 .\n( ; ))\n\n(6.1)\n\nnow we must choose the form of our model, f(x; \u03b8 ). suppose that we choose\n\na linear model, with\n\n\u03b8\n\nconsisting of\n\nw\n\nand . our model is defined to be\n\nb\n\nf\n\n( ;x w ) = x>w + b.\n\n, b\n\n(6.2)\n\nwe can minimize j(\u03b8 ) in closed form with respect to w and b using the normal\nequations.\n\nafter solving the normal equations, we obtain w = 0 and b = 1\n2.\u00a0the linear\nmodel simply outputs 0.5 everywhere. why does this happen? fig.\n6.1\nshows how\na linear model is not able to represent the xor function. one way to solve this\nproblem is to use a model that learns a different feature space in which a linear\nmodel is able to represent the solution.\n\nspecifically, we will introduce a very simple feedforward network with one\nhidden layer containing two hidden units. see fig.\nfor an illustration of\nthis model. this feedforward network has a vector of hidden units h that are\ncomputed by a function f(1)(x; w c, ). the values of these hidden units are then\nused as the input for a second layer. the second layer is the output layer of the\nnetwork. the output layer is still just a linear regression model, but now it is\napplied to h rather than to x. the network now contains two functions chained\ntogether: h = f (1) (x; w c, ) and y = f (2)(h; w, b), with the complete model being\nf\n\n( ;x w c w ) = \nf\n\n(2)(f (1)( ))x .\n\n6.2\n\n, b\n\n,\n\n,\n\nwhat function should f (1) compute? linear models have served us well so far,\nand it may be tempting to make f (1) be linear as well. unfortunately, if f(1) were\nlinear, then the feedforward network as a whole would remain a linear function of\nits input. ignoring the intercept terms for the moment, suppose f (1)(x) = w >x\nand f (2)(h) = h> w. then f( x) = w> w>x. we could represent this function as\nf ( ) = \n\nx>w 0 where w0 = w w.\n\nx\n\nclearly, we must use a nonlinear function to describe the features. most neural\nnetworks do so using an affine transformation controlled by learned parameters,\nfollowed by a fixed, nonlinear function called an activation function. we use that\nstrategy here, by defining h = g(w >x + c), where w provides the weights of a\nlinear transformation and c the biases. previously, to describe a linear regression\nmodel, we used a vector of weights and a scalar bias parameter to describe an\n\n171\n\n "}, {"Page_number": 187, "text": "chapter 6. deep feedforward networks\n\noriginal\n\nx\n\nspace\n\nlearned\n\nh\n\nspace\n\n2\nx\n\n1\n\n0\n\n2\nh\n\n1\n\n0\n\n0\n\n1\n\n0\n\nx1\n\n1\nh1\n\n2\n\nfigure 6.1: solving the xor problem by learning a representation. the bold numbers\nprinted on the plot indicate the value that the learned function must output at each point.\n(left) a linear model applied directly to the original input cannot implement the xor\nfunction. when x1 = 0, the model\u2019s output must increase as x 2 increases. when x1 = 1,\nthe model\u2019s output must decrease as x2 increases. a linear model must apply a fixed\ncoefficient w2 to x2. the linear model therefore cannot use the value of x1 to change\nthe coefficient on x2 and cannot solve this problem. (right) in the transformed space\nrepresented by the features extracted by a neural network, a linear model can now solve\nthe problem. in our example solution, the two points that must have output\nhave been\ncollapsed into a single point in feature space. in other words, the nonlinear features have\nmapped both x = [1, 0]> and x = [0, 1]> to a single point in feature space, h = [1 ,0]>.\nthe linear model can now describe the function as increasing in h1 and decreasing in h2.\nin this example, the motivation for learning the feature space is only to make the model\ncapacity greater so that it can fit the training set. in more realistic applications, learned\nrepresentations can also help the model to generalize.\n\n1\n\n172\n\n "}, {"Page_number": 188, "text": "chapter 6. deep feedforward networks\n\nyy\n\nh1h1\n\nx1x1\n\nh2h2\n\nx2x2\n\nyy\n\nhh\n\nxx\n\nw\n\nw\n\nfigure 6.2: an example of a feedforward network, drawn in two different styles. specifically,\nthis is the feedforward network we use to solve the xor example. it has a single hidden\nlayer containing two units. (left) in this style, we draw every unit as a node in the\ngraph. this style is very explicit and unambiguous but for networks larger than this\nexample it can consume too much space. (right) in this style, we draw a node in the\ngraph for each entire vector representing a layer\u2019s activations. this style is much more\ncompact. sometimes we annotate the edges in this graph with the name of the parameters\nthat describe the relationship between two layers.\u00a0here, we indicate that a matrix w\ndescribes the mapping from x to h, and a vector w describes the mapping from h to y.\nwe typically omit the intercept parameters associated with each layer when labeling this\nkind of drawing.\n\naffine transformation from an input vector to an output scalar. now, we describe\nan affine transformation from a vector x to a vector h, so an entire vector of bias\nparameters is needed. the activation function g is typically chosen to be a function\nthat is applied element-wise, with h i = g(x>w :,i + ci). in modern neural networks,\nthe default recommendation is to use the rectified linear unit or relu (jarrett\net al.,\n) defined by the activation\nfunction\n\n2009 nair and hinton 2010 glorot\ndepicted in fig.\n\net al.,\n.\n6.3\n\n2011a\n\n, z\n\n,\n\n;\n\n;\n( ) = max 0{\ng z\n\n}\n\nwe can now specify our complete network as\n\nf\n\n( ;x w c w ) = w> max 0{ , w >x c+ } + b.\n\n, b\n\n,\n\n,\n\nwe can now specify a solution to the xor problem. let\n\nw =(cid:30) 1 1\n1 1 (cid:31) ,\nc =(cid:30) 0\n\u22121 (cid:31) ,\nw =(cid:30) 1\n\u22122 (cid:31),\n\n173\n\n(6.3)\n\n(6.4)\n\n(6.5)\n\n(6.6)\n\n "}, {"Page_number": 189, "text": "chapter 6. deep feedforward networks\n\nthe rectified linear activation function\n\n}\nz\n,\n0\n{\nx\na\nm\n=\n\n \n\n \n)\nz\n(\ng\n\n0\n\n0\n\nz\n\nfigure 6.3:\u00a0the rectified linear activation function. this activation function is the default\nactivation function recommended for use with most feedforward neural networks. applying\nthis function to the output of a linear transformation yields a nonlinear transformation.\nhowever, the function remains very close to linear, in the sense that is a piecewise linear\nfunction with two linear pieces. because rectified linear units are nearly linear, they\npreserve many of the properties that make linear models easy to optimize with gradient-\nbased methods. they also preserve many of the properties that make linear models\ngeneralize well. a common principle throughout computer science is that we can build\ncomplicated systems from minimal components.\u00a0much as a turing machine\u2019s memory\nneeds only to be able to store 0 or 1 states, we can build a universal function approximator\nfrom rectified linear functions.\n\n174\n\n "}, {"Page_number": 190, "text": "chapter 6. deep feedforward networks\n\nand\n\n.\nb = 0\n\nwe can now walk through the way that the model processes a batch of inputs.\nlet x be the design matrix containing all four points in the binary input space,\nwith one example per row:\n\nthe first step in the neural network is to multiply the input matrix by the first\nlayer\u2019s weight matrix:\n\nnext, we add the bias vector\n\nc\n\n, to obtain\n\nin this space, all of the examples lie along a line with slope . as we move along\nthis line, the output needs to begin at\n, then rise to , then drop back down to .\n0\n0\na linear model cannot implement such a function. to finish computing the value\nof\n\nfor each example, we apply the rectified linear transformation:\n\nh\n\n1\n\n1\n\n(cid:39)(cid:40)(cid:40)(cid:41) .\n(cid:39)(cid:40)(cid:40)(cid:41) .\n\nx =(cid:34)(cid:35)(cid:35)(cid:36)\nxw =(cid:34)(cid:35)(cid:35)(cid:36)\n(cid:34)(cid:35)(cid:35)(cid:36)\n\n0\n1\n1\n2\n\n0 0\n0 1\n1 0\n1 1\n\n0 0\n1 1\n1 1\n2 2\n\n1\u2212\n0\n0\n1\n\n(cid:39)(cid:40)(cid:40)(cid:41).\n\n0 0\n1 0\n1 0\n2 1\n\n(cid:34)(cid:35)(cid:35)(cid:36)\n\n(cid:39)(cid:40)(cid:40)(cid:41) .\n\n(6.7)\n\n(6.8)\n\n(6.9)\n\n(6.10)\n\nthis transformation has changed the relationship between the examples. they no\nlonger lie on a single line. as shown in fig.\n, they now lie in a space where a\nlinear model can solve the problem.\n\n6.1\n\nwe finish by multiplying by the weight vector\n\n:w\n\n(cid:34)(cid:35)(cid:35)(cid:36)\n\n(cid:39)(cid:40)(cid:40)(cid:41) .\n\n0\n1\n1\n0\n\n175\n\n(6.11)\n\n "}, {"Page_number": 191, "text": "chapter 6. deep feedforward networks\n\nthe neural network has obtained the correct answer for every example in the batch.\n\nin this example, we simply specified the solution, then showed that it obtained\nzero error.\u00a0in a real situation, there might be billions of model parameters and\nbillions of training examples, so one cannot simply guess the solution as we did\nhere. instead, a gradient-based optimization algorithm can find parameters that\nproduce very little error. the solution we described to the xor problem is at a\nglobal minimum of the loss function, so gradient descent could converge to this\npoint. there are other equivalent solutions to the xor problem that gradient\ndescent could also find. the convergence point of gradient descent depends on the\ninitial values of the parameters. in practice, gradient descent would usually not\nfind clean, easily understood, integer-valued solutions like the one we presented\nhere.\n\n6.2 gradient-based learning\n\ndesigning and training a neural network is not much different from training any\nother machine learning model with gradient descent. in sec.\n, we described\nhow to build a machine learning algorithm by specifying an optimization procedure,\na cost function, and a model family.\n\n5.10\n\nthe largest difference between the linear models we have seen so far and neural\nnetworks is that the nonlinearity of a neural network causes most interesting loss\nfunctions to become non-convex. this means that neural networks are usually\ntrained by using iterative, gradient-based optimizers that merely drive the cost\nfunction to a very low value, rather than the linear equation solvers used to train\nlinear regression models or the convex optimization algorithms with global conver-\ngence guarantees used to train logistic regression or svms. convex optimization\nconverges starting from any initial parameters (in theory\u2014in practice it is very\nrobust but can encounter numerical problems). stochastic gradient descent applied\nto non-convex loss functions has no such convergence guarantee, and is sensitive\nto the values of the initial parameters. for feedforward neural networks, it is\nimportant to initialize all weights to small random values. the biases may be\ninitialized to zero or to small positive values.\u00a0the iterative gradient-based opti-\nmization algorithms used to train feedforward networks and almost all other deep\nmodels will be described in detail in chapter\n, with parameter initialization in\nparticular discussed in sec.\u00a0\n. for the moment, it suffices to understand that\nthe training algorithm is almost always based on using the gradient to descend the\ncost function in one way or another.\u00a0the specific algorithms are improvements\n, and,\nand refinements on the ideas of gradient descent, introduced in sec.\n\n8.4\n\n4.3\n\n8\n\n176\n\n "}, {"Page_number": 192, "text": "chapter 6. deep feedforward networks\n\nmore specifically, are most often improvements of the stochastic gradient descent\nalgorithm, introduced in sec.\n\n.5.9\n\nwe can of course, train models such as linear regression and support vector\nmachines with gradient descent too, and in fact this is common when the training\nset is extremely large. from this point of view, training a neural network is not\nmuch different from training any other model. computing the gradient is slightly\nmore complicated for a neural network, but can still be done efficiently and exactly.\nsec.\nwill describe how to obtain the gradient using the back-propagation\nalgorithm and modern generalizations of the back-propagation algorithm.\n\n6.5\n\nas with other machine learning models, to apply gradient-based learning we\nmust choose a cost function, and we must choose how to represent the output of\nthe model. we now revisit these design considerations with special emphasis on\nthe neural networks scenario.\n\n6.2.1 cost functions\n\nan important aspect of the design of a deep neural network is the choice of the\ncost function. fortunately, the cost functions for neural networks are more or less\nthe same as those for other parametric models, such as linear models.\n\nin most cases, our parametric model defines a distribution p(y x|\n\n;\u03b8 ) and\nwe simply use the principle of maximum likelihood. this means we use the\ncross-entropy between the training data and the model\u2019s predictions as the cost\nfunction.\n\nsometimes, we take a simpler approach, where rather than predicting a complete\nprobability distribution over y, we merely predict some statistic of y conditioned\non . specialized loss functions allow us to train a predictor of these estimates.\n\nx\n\nthe total cost function used to train a neural network will often combine one\nof the primary cost functions described here with a regularization term. we have\nalready seen some simple examples of regularization applied to linear models in sec.\n5.2.2. the weight decay approach used for linear models is also directly applicable\nto deep neural networks and is among the most popular regularization strategies.\nmore advanced regularization strategies for neural networks will be described in\nchapter\n\n.7\n\n6.2.1.1 learning conditional distributions with maximum likelihood\n\nmost modern neural networks are trained using maximum likelihood. this means\nthat the cost function is simply the negative log-likelihood, equivalently described\n\n177\n\n "}, {"Page_number": 193, "text": "chapter 6. deep feedforward networks\n\nas the cross-entropy between the training data and the model distribution. this\ncost function is given by\n\nj( ) = \n\n\u03b8\n\n\u2212ex y, \u223c \u02c6pdata log pmodel(\n\ny x|\n)\n.\n\n(6.12)\n\nthe specific form of the cost function changes from model to model, depending\non the specific form of log pmodel. the expansion of the above equation typically\nyields some terms that do not depend on the model parameters and may be\n) = n (y;f (x; \u03b8), i),\ndiscarded. for example, as we saw in sec.\n5.5.1\nthen we recover the mean squared error cost,\n\npmodel (y x|\n\n, if\n\nj \u03b8( ) =\n\n1\n2\n\ne x y, \u223c \u02c6pdata|| \u2212\n\ny\n\nf ( ; )x \u03b8 2 + const,\n\n||\n\n(6.13)\n\nup to a scaling factor of 1\n2 and a term that does not depend on . the discarded\nconstant is based on the variance of the gaussian distribution, which in this case\nwe chose not to parametrize. previously, we saw that the equivalence between\nmaximum likelihood estimation with an output distribution and minimization of\nmean squared error holds for a linear model, but in fact, the equivalence holds\nregardless of the\n\nused to predict the mean of the gaussian.\n\nf ( ; )x \u03b8\n\n\u03b8\n\nan advantage of this approach of deriving the cost function from maximum\nlikelihood is that it removes the burden of designing cost functions for each model.\nspecifying a model p(y x|\n) automatically determines a cost function log p(y x|\n).\none recurring theme throughout neural network design is that the gradient of\nthe cost function must be large and predictable enough to serve as a good guide\nfor the learning algorithm. functions that saturate (become very flat) undermine\nthis objective because they make the gradient become very small. in many cases\nthis happens because the activation functions used to produce the output of the\nhidden units or the output units saturate. the negative log-likelihood helps to\navoid this problem for many models. many output units involve an exp function\nthat can saturate when its argument is very negative. the log function in the\nnegative log-likelihood cost function undoes the exp of some output units. we will\ndiscuss the interaction between the cost function and the choice of output unit in\nsec.\n\n6.2.2\n.\n\none unusual property of the cross-entropy cost used to perform maximum\nlikelihood estimation is that it usually does not have a minimum value when applied\nto the models commonly used in practice. for discrete output variables, most\nmodels are parametrized in such a way that they cannot represent a probability\nof zero or one, but can come arbitrarily close to doing so. logistic regression\nis an example of such a model. for real-valued output variables, if the model\n\n178\n\n "}, {"Page_number": 194, "text": "chapter 6. deep feedforward networks\n\ncan control the density of the output distribution (for example, by learning the\nvariance parameter of a gaussian output distribution) then it becomes possible\nto assign extremely high density to the correct training set outputs, resulting in\ncross-entropy approaching negative infinity. regularization techniques described\nin chapter\nprovide several different ways of modifying the learning problem so\nthat the model cannot reap unlimited reward in this way.\n\n7\n\n6.2.1.2 learning conditional statistics\n\ninstead of learning a full probability distribution p(y x|\njust one conditional statistic of\n\ngiven .\nx\n\ny\n\n;\u03b8) we often want to learn\n\nfor example, we may have a predictor f (x;\u03b8) that we wish to predict the mean\n.y\n\nof\n\nfunctional\n\nif we use a sufficiently powerful neural network, we can think of the neural\nnetwork as being able to represent any function f from a wide class of functions,\nwith this class being limited only by features such as continuity and boundedness\nrather than by having a specific parametric form. from this point of view, we\ncan view the cost function as being a\nrather than just a function. a\nfunctional is a mapping from functions to real numbers. we can thus think of\nlearning as choosing a function rather than merely choosing a set of parameters.\nwe can design our cost functional to have its minimum occur at some specific\nfunction we desire. for example, we can design the cost functional to have its\nminimum lie on the function that maps x to the expected value of y given x.\nsolving an optimization problem with respect to a function requires a mathematical\ntool called calculus of variations, described in sec.\n. it is not necessary to\nunderstand calculus of variations to understand the content of this chapter.\u00a0at\nthe moment, it is only necessary to understand that calculus of variations may be\nused to derive the following two results.\n\n19.4.2\n\nour first result derived using calculus of variations is that solving the optimiza-\n\ntion problem\n\nyields\n\nf\u2217 = arg min\n\nf\n\nex y, \u223cp data|| \u2212\n\ny\n\nf ( )x 2\n||\n\nf \u2217( ) = \n\nx\n\ne\ny\u223cp data (\n\n[\n]y ,\n)y x|\n\n(6.14)\n\n(6.15)\n\nso long as this function lies within the class we optimize over. in other words, if we\ncould train on infinitely many samples from the true data-generating distribution,\nminimizing the mean squared error cost function gives a function that predicts the\nmean of\n\nfor each value of\n\n.\nx\n\ny\n\n179\n\n "}, {"Page_number": 195, "text": "chapter 6. deep feedforward networks\n\ndifferent cost functions give different statistics. a second result derived using\n\ncalculus of variations is that\n\nf\u2217 = arg min\n\nf\n\nex y, \u223cp data|| \u2212\n\ny\n\n||\nf ( )x 1\n\n(6.16)\n\nyields a function that predicts the median value of y for each x, so long as such a\nfunction may be described by the family of functions we optimize over. this cost\nfunction is commonly called mean absolute error.\n\nunfortunately, mean squared error and mean absolute error often lead to poor\nresults when used with gradient-based optimization. some output units that\nsaturate produce very small gradients when combined with these cost functions.\nthis is one reason that the cross-entropy cost function is more popular than mean\nsquared error or mean absolute error, even when it is not necessary to estimate an\nentire distribution\n\np(\n\ny x|\n.\n)\n\n6.2.2 output units\n\nthe choice of cost function is tightly coupled with the choice of output unit. most\nof the time, we simply use the cross-entropy between the data distribution and the\nmodel distribution.\u00a0the choice of how to represent the output then determines\nthe form of the cross-entropy function.\n\nany kind of neural network unit that may be used as an output can also be\nused as a hidden unit. here, we focus on the use of these units as outputs of the\nmodel, but in principle they can be used internally as well. we revisit these units\nwith additional detail about their use as hidden units in sec.\n\n.6.3\n\nthroughout this section, we suppose that the feedforward network provides a\nset of hidden features defined by h = f (x; \u03b8). the role of the output layer is then\nto provide some additional transformation from the features to complete the task\nthat the network must perform.\n\n6.2.2.1 linear units for gaussian output distributions\n\none simple kind of output unit is an output unit based on an affine transformation\nwith no nonlinearity. these are often just called linear units.\n\ngiven features h, a layer of linear output units produces a vector \u02c6y = w> h+ b.\nlinear output layers are often used to produce the mean of a conditional\n\ngaussian distribution:\n\np(\n\ny x|\n\n) = \n\n( ;\n\nn y \u02c6y i, ).\n\n180\n\n(6.17)\n\n "}, {"Page_number": 196, "text": "chapter 6. deep feedforward networks\n\nmaximizing the log-likelihood is then equivalent to minimizing the mean squared\nerror.\n\nthe maximum likelihood framework makes it straightforward to learn the\ncovariance of the gaussian too, or to make the covariance of the gaussian be a\nfunction of the input. however, the covariance must be constrained to be a positive\ndefinite matrix for all inputs. it is difficult to satisfy such constraints with a linear\noutput layer, so typically other output units are used to parametrize the covariance.\napproaches to modeling the covariance are described shortly, in sec.\n\n.\n6.2.2.4\n\nbecause linear units do not saturate, they pose little difficulty for gradient-\nbased optimization algorithms and may be used with a wide variety of optimization\nalgorithms.\n\n6.2.2.2 sigmoid units for bernoulli output distributions\n\nmany tasks require predicting the value of a binary variable y . classification\nproblems with two classes can be cast in this form.\n\nthe maximum-likelihood approach is to define a bernoulli distribution over y\n\nconditioned on .x\n\na bernoulli distribution is defined by just a single number. the neural net\nneeds to predict only p (y = 1 | x). for this number to be a valid probability, it\nmust lie in the interval [0, 1].\n\nsatisfying this constraint requires some careful design effort. suppose we were\n\nto use a linear unit, and threshold its value to obtain a valid probability:\n\np y( = 1 \n\n) = max\n\n| x\n\n,\n\nn0 min\n\nn1, w>h + boo .\n\n(6.18)\n\nthis would indeed define a valid conditional distribution, but we would not be able\nto train it very effectively with gradient descent. any time that w>h + b strayed\noutside the unit interval, the gradient of the output of the model with respect to\nits parameters would be 0. a gradient of 0 is typically problematic because the\nlearning algorithm no longer has a guide for how to improve the corresponding\nparameters.\n\ninstead, it is better to use a different approach that ensures there is always a\nstrong gradient whenever the model has the wrong answer. this approach is based\non using sigmoid output units combined with maximum likelihood.\n\na sigmoid output unit is defined by\n\n\u02c6y\n\n\u03c3=  (cid:26)w>h + b(cid:27)\n\n181\n\n(6.19)\n\n "}, {"Page_number": 197, "text": "chapter 6. deep feedforward networks\n\nwhere\n\n\u03c3\n\nis the logistic sigmoid function described in sec.\n\n3.10\n.\n\nwe can think of the sigmoid output unit as having two components. first, it\nuses a linear layer to compute z = w> h + b. next, it uses the sigmoid activation\nfunction to convert\n\ninto a probability.\n\nz\n\nwe omit the dependence on x for the moment to discuss how to define a\nprobability distribution over y using the value z. the sigmoid can be motivated\nby constructing an unnormalized probability distribution \u02dcp(y ), which does not\nsum to 1. we can then divide by an appropriate constant to obtain a valid\nprobability distribution. if we begin with the assumption that the unnormalized log\nprobabilities are linear in y and z , we can exponentiate to obtain the unnormalized\nprobabilities. we then normalize to see that this yields a bernoulli distribution\ncontrolled by a sigmoidal transformation of\n\n:z\n\nlog \u02dcp y\n\u02dcp y\n\n( ) = \n\nyz\n\n( ) = exp(\n\n)yz\n\nyz\n)\nexp(\ny0=0 exp(y0z)\n((2 \u2212 1) )\nz .\n\np1\n\ny\n\np y( ) =\n\np y\n\n( ) = \n\n\u03c3\n\n(6.20)\n\n(6.21)\n\n(6.22)\n\n(6.23)\n\nprobability distributions based on exponentiation and normalization are common\nthroughout the statistical modeling literature. the z variable defining such a\ndistribution over binary variables is called a logit.\n\nthis approach to predicting the probabilities in log-space is natural to use\nwith maximum likelihood learning. because the cost function used with maximum\nlikelihood is \u2212 log p( y | x), the log in the cost function undoes the exp of the\nsigmoid. without this effect, the saturation of the sigmoid could prevent gradient-\nbased\u00a0learning from\u00a0making\u00a0good\u00a0progress. the\u00a0loss function\u00a0for\u00a0maximum\nlikelihood learning of a bernoulli parametrized by a sigmoid is\n\nj\n\nlog\n\n\u2212\n( ) = \n\u03b8\n\u2212\n= \n\u03b6\n=  ((1\n\np y\n(\n\n| x)\ny \u2212 z\n1) )\n\nlog ((2\n\n\u03c3\n\u2212 y z .\n2 ) )\n\n(6.24)\n\n(6.25)\n(6.26)\n\nthis derivation makes use of some properties from sec.\n\n. by rewriting\nthe loss in terms of the softplus function, we can see that it saturates only when\n(1 \u2212 2y)z is very negative. saturation thus occurs only when the model already\nhas the right answer\u2014when y = 1 and z is very positive, or y = 0 and z is very\nnegative. when z has the wrong sign, the argument to the softplus function,\n\n3.10\n\n182\n\n "}, {"Page_number": 198, "text": "chapter 6. deep feedforward networks\n\n(1\u2212 2y )z, may be simplified to | |z . as | |z becomes large while z has the wrong sign,\nthe softplus function asymptotes toward simply returning its argument | |z . the\nderivative with respect to z asymptotes to sign(z), so, in the limit of extremely\nincorrect z, the softplus function does not shrink the gradient at all. this property\nis very useful because it means that gradient-based learning can act to quickly\ncorrect a mistaken .z\n\nwhen we use other loss functions, such as mean squared error, the loss can\nsaturate anytime \u03c3(z ) saturates. the sigmoid activation function saturates to 0\nwhen z becomes very negative and saturates to when\nz becomes very positive.\nthe gradient can shrink too small to be useful for learning whenever this happens,\nwhether the model has the correct answer or the incorrect answer. for this reason,\nmaximum likelihood is almost always the preferred approach to training sigmoid\noutput units.\n\n1\n\nanalytically, the logarithm of the sigmoid is always defined and finite, because\nthe sigmoid returns values restricted to the open interval (0, 1), rather than using\nthe entire closed interval of valid probabilities [0,1]. in software implementations,\nto avoid numerical problems, it is best to write the negative log-likelihood as a\nfunction of z, rather than as a function of \u02c6y = \u03c3 (z). if the sigmoid function\nunderflows to zero, then taking the logarithm of \u02c6y yields negative infinity.\n\n6.2.2.3 softmax units for multinoulli output distributions\n\nany time we wish to represent a probability distribution over a discrete variable\nwith n possible values, we may use the softmax function. this can be seen as a\ngeneralization of the sigmoid function which was used to represent a probability\ndistribution over a binary variable.\n\nsoftmax functions are most often used as the output of a classifier, to represent\nthe probability distribution over n different classes. more rarely, softmax functions\ncan be used inside the model itself, if we wish the model to choose between one of\nn different options for some internal variable.\n\nin the case of binary variables, we wished to produce a single number\n\n\u02c6y\n\n= \n\np y\n\n( = 1 \n\n| x\n)\n.\nbecause this number needed to lie between\nand , and because we wanted the\n0\nlogarithm of the number to be well-behaved for gradient-based optimization of\nthe log-likelihood, we chose to instead predict a number z = log \u02dcp(y = 1 | x).\nexponentiating and normalizing gave us a bernoulli distribution controlled by the\nsigmoid function.\n\n(6.27)\n\n1\n\n183\n\n "}, {"Page_number": 199, "text": "chapter 6. deep feedforward networks\n\nto generalize to the case of a discrete variable with n values, we now need\nto produce a vector \u02c6y, with \u02c6yi = p (y = i | x). we require not only that each\nelement of \u02c6yi be between\nso that\nit represents a valid probability distribution. the same approach that worked for\nthe bernoulli distribution generalizes to the multinoulli distribution. first, a linear\nlayer predicts unnormalized log probabilities:\n\nand , but also that the entire vector sums to\n\n0\n\n1\n\n1\n\n(6.28)\nwhere z i = log \u02dcp (y = i | x). the softmax function can then exponentiate and\n\u02c6y. formally, the softmax function is given by\nnormalize\n\nto obtain the desired\n\nz w= \n\nz\n\n> h b+ ,\n\nsoftmax( )z i =\n\n.\n\n(6.29)\n\nexp(zi)\n\npj exp(zj )\n\nas with the logistic sigmoid, the use of the exp function works very well when\ntraining the softmax to output a target value y using maximum log-likelihood. in\nthis case, we wish to maximize log p (y = i ; z) = log softmax(z )i. defining the\nsoftmax in terms of exp is natural because the log in the log-likelihood can undo\nthe\n\nof the softmax:\n\nexp\n\nlog softmax( )z i = zi \u2212 logxj\n\nexp(zj ).\n\n(6.30)\n\n6.30\n\nshows that the input\n\nthe first term of eq.\n\nzi always has a direct con-\ntribution to the cost function. because this term cannot saturate, we know that\nlearning can proceed, even if the contribution of zi to the second term of eq. 6.30\nbecomes very small. when maximizing the log-likelihood, the first term encourages\nzi to be pushed up, while the second term encourages all of z to be pushed down.\n\nto gain some intuition for the second term, logpj exp(zj), observe that this term\n\ncan be roughly approximated by maxj z j. this approximation is based on the idea\nthat exp(zk) is insignificant for any z k that is noticeably less than max j z j. the\nintuition we can gain from this approximation is that the negative log-likelihood\ncost function always strongly penalizes the most active incorrect prediction. if the\ncorrect answer already has the largest input to the softmax, then the \u2212zi term\nwill then contribute little to the overall training cost, which will be dominated by\nother examples that are not yet correctly classified.\n\nand the logpj exp(zj ) \u2248 maxj zj = zi terms will roughly cancel.\u00a0this example\n\nso far we have discussed only a single example. overall, unregularized maximum\nlikelihood will drive the model to learn parameters that drive the softmax to predict\n\n184\n\n "}, {"Page_number": 200, "text": "chapter 6. deep feedforward networks\n\nthe fraction of counts of each outcome observed in the training set:\n\nsoftmax( ( ; ))\n\n.\n\n(6.31)\n\nz x \u03b8 i \u2248 pm\npm\n\nj=1 1y( )j =i,x ( )j =x\n\nj=1 1x( )j =x\n\nbecause maximum likelihood is a consistent estimator, this is guaranteed to happen\nso long as the model family is capable of representing the training distribution. in\npractice, limited model capacity and imperfect optimization will mean that the\nmodel is only able to approximate these fractions.\n\nmany objective functions other than the log-likelihood do not work as well\nwith the softmax function. specifically, objective functions that do not use a log to\nundo the exp of the softmax fail to learn when the argument to the exp becomes\nvery negative, causing the gradient to vanish. in particular, squared error is a\npoor loss function for softmax units, and can fail to train the model to change its\noutput, even when the model makes highly confident incorrect predictions (\nbridle\n,\n1990). to understand why these other loss functions can fail, we need to examine\nthe softmax function itself.\n\nlike the sigmoid, the softmax activation can saturate. the sigmoid function has\na single output that saturates when its input is extremely negative or extremely\npositive. in the case of the softmax, there are multiple output values. these\noutput values can saturate when the differences between input values become\nextreme. when the softmax saturates, many cost functions based on the softmax\nalso saturate, unless they are able to invert the saturating activating function.\n\nto see that the softmax function responds to the difference between its inputs,\nobserve that the softmax output is invariant to adding the same scalar to all of its\ninputs:\n\nsoftmax( ) = softmax( + )\nc .\n\nz\n\nz\n\n(6.32)\n\nusing this property, we can derive a numerically stable variant of the softmax:\n\nsoftmax( ) = softmax(\n\nz\n\nz \u2212 i\nmax\n\nzi).\n\n(6.33)\n\nthe reformulated version allows us to evaluate softmax with only small numerical\nerrors even when z contains extremely large or extremely negative numbers. ex-\namining the numerically stable variant, we see that the softmax function is driven\nby the amount that its arguments deviate from maxi zi.\n\nan output softmax(z)i saturates to when the corresponding input is maximal\n(zi = maxi zi ) and zi is much greater than all of the other inputs. the output\nsoftmax( z)i can also saturate to when\nzi is not maximal and the maximum is\nmuch greater. this is a generalization of the way that sigmoid units saturate, and\n\n1 \n\n0\n\n185\n\n "}, {"Page_number": 201, "text": "chapter 6. deep feedforward networks\n\ncan cause similar difficulties for learning if the loss function is not designed to\ncompensate for it.\n\n1\n\nthe argument z to the softmax function can be produced in two different ways.\nthe most common is simply to have an earlier layer of the neural network output\nevery element of z, as described above using the linear layer z = w> h +b. while\nstraightforward, this approach actually overparametrizes the distribution. the\nn\u2212 1 parameters are\nconstraint that the n outputs must sum to means that only\nnecessary; the probability of the n -th value may be obtained by subtracting the\nfirst n\u2212 1 \nprobabilities from . we can thus impose a requirement that one element\nof z be fixed. for example, we can require that zn = 0. indeed, this is exactly\nwhat the sigmoid unit does. defining p (y = 1 | x) = \u03c3(z) is equivalent to defining\np (y = 1 | x) = softmax(z )1 with a two-dimensional z and z1 = 0. both the n \u2212 1\nargument and the n argument approaches to the softmax can describe the same\nset of probability distributions, but have different learning dynamics. in practice,\nthere is rarely much difference between using the overparametrized version or the\nrestricted version, and it is simpler to implement the overparametrized version.\n\n1\n\nfrom a neuroscientific point of view, it is interesting to think of the softmax as\na way to create a form of competition between the units that participate in it: the\nsoftmax outputs always sum to 1 so an increase in the value of one unit necessarily\ncorresponds to a decrease in the value of others. this is analogous to the lateral\ninhibition that is believed to exist between nearby neurons in the cortex. at the\nextreme (when the difference between the maximal ai and the others is large in\nmagnitude) it becomes a form of\n(one of the outputs is nearly 1\nand the others are nearly 0).\n\nwinner-take-all\n\nthe name \u201csoftmax\u201d can be somewhat confusing. the function is more closely\nrelated to the argmax function than the max function. the term \u201csoft\u201d derives\nfrom the fact that the softmax function is continuous and differentiable. the\nargmax function, with its result represented as a one-hot vector, is not continuous\nor differentiable. the softmax function thus provides a \u201csoftened\u201d version of the\nargmax. the corresponding soft version of the maximum function is softmax(z)>z.\nit would perhaps be better to call the softmax function \u201csoftargmax,\u201d\u00a0but the\ncurrent name is an entrenched convention.\n\n6.2.2.4 other output types\n\nthe linear,\u00a0sigmoid,\u00a0and softmax output units described above are the most\ncommon. neural networks can generalize to almost any kind of output layer that\nwe wish. the principle of maximum likelihood provides a guide for how to design\n\n186\n\n "}, {"Page_number": 202, "text": "chapter 6. deep feedforward networks\n\na good cost function for nearly any kind of output layer.\n\nin general, if we define a conditional distribution p(y x|\n\nmaximum likelihood suggests we use\n\nlog (p y x \u03b8; )\n\nas our cost function.\n\n; \u03b8 ), the principle of\n\n\u2212\n\n|\n\nin general, we can think of the neural network as representing a function f(x; \u03b8).\nthe outputs of this function are not direct predictions of the value y. instead,\nf (x;\u03b8) = \u03c9 provides the parameters for a distribution over y. our loss function\ncan then be interpreted as\n\np y \u03c9 x\n\n.\n( ))\n\n\u2212 log ( ;\n\nfor example, we may wish to learn the variance of a conditional gaussian for\ny , given x. in the simple case, where the variance \u03c32 is a constant, there is a\nclosed form expression because the maximum likelihood estimator of variance is\nsimply the empirical mean of the squared difference between observations y and\ntheir expected value. a computationally more expensive approach that does not\nrequire writing special-case code is to simply include the variance as one of the\nproperties of the distribution p(y | x) that is controlled by \u03c9 = f(x ; \u03b8). the\nnegative log-likelihood \u2212 log p (y; \u03c9(x)) will then provide a cost function with the\nappropriate terms necessary to make our optimization procedure incrementally\nlearn the variance. in the simple case where the standard deviation does not depend\non the input, we can make a new parameter in the network that is copied directly\ninto \u03c9. this new parameter might be \u03c3 itself or could be a parameter v representing\n\u03c3 2 or it could be a parameter \u03b2 representing 1\n\u03c3 2, depending on how we choose to\nparametrize the distribution. we may wish our model to predict a different amount\nof variance in y for different values of x.\u00a0this is called a heteroscedastic model.\nin the heteroscedastic case, we simply make the specification of the variance be\none of the values output by f (x; \u03b8). a typical way to do this is to formulate the\ngaussian distribution using precision, rather than variance, as described in eq.\n3.22. in the multivariate case it is most common to use a diagonal precision matrix\n\ndiag\n\n( )\u03b2 .\n\n(6.34)\n\nthis formulation works well with gradient descent because the formula for the\nlog-likelihood of the gaussian distribution parametrized by \u03b2 involves only mul-\ntiplication by \u03b2i and addition of log \u03b2i . the gradient of multiplication, addition,\nand logarithm operations is well-behaved. by comparison, if we parametrized the\noutput in terms of variance, we would need to use division. the division function\nbecomes arbitrarily steep near zero. while large gradients can help learning,\narbitrarily large gradients usually result in instability. if we parametrized the\noutput in terms of standard deviation, the log-likelihood would still involve division,\nand would also involve squaring. the gradient through the squaring operation\ncan vanish near zero, making it difficult to learn parameters that are squared.\n\n187\n\n "}, {"Page_number": 203, "text": "chapter 6. deep feedforward networks\n\nregardless of whether we use standard deviation, variance, or precision, we must\nensure that the covariance matrix of the gaussian is positive definite. because\nthe eigenvalues of the precision matrix are the reciprocals of the eigenvalues of\nthe covariance matrix, this is equivalent to ensuring that the precision matrix is\npositive definite. if we use a diagonal matrix, or a scalar times the diagonal matrix,\nthen the only condition we need to enforce on the output of the model is positivity.\nif we suppose that a is the raw activation of the model used to determine the\ndiagonal precision, we can use the softplus function to obtain a positive precision\nvector: \u03b2 = \u03b6( a). this same strategy applies equally if using variance or standard\ndeviation rather than precision or if using a scalar times identity rather than\ndiagonal matrix.\n\nit is rare to learn a covariance or precision matrix with richer structure than\ndiagonal.\u00a0if the covariance is full and conditional, then a parametrization must\nbe chosen that guarantees positive-definiteness of the predicted covariance matrix.\nx b x b> ( )x , where b is an unconstrained\nthis can be achieved by writing \u03c3( ) = \nsquare matrix. one practical issue if the matrix is full rank is that computing the\nd\u00d7 matrix requiring o(d3) computation for the\nlikelihood is expensive, with a d\ndeterminant and inverse of \u03c3(x) (or equivalently, and more commonly done, its\neigendecomposition or that of\n\nb x( )\n\n( )\n\n).\n\nwe often want to perform multimodal regression, that is, to predict real values\nthat come from a conditional distribution p(y x|\n) that can have several different\npeaks in y space for the same value of x. in this case, a gaussian mixture is\na natural representation for the output (\nneural networks with gaussian mixtures as their output are often called mixture\ndensity networks.\u00a0a gaussian mixture output with n components is defined by\nthe conditional probability distribution\n\njacobs et al. 1991 bishop 1994\n\n).\n\n,\n\n;\n\n,\n\np(\n\ny x|\n\n) =\n\nnxi=1\n\np\n\nc\n( = \ni\n\n| nx)\n\n( ;y \u00b5( )i ( )x , \u03c3( )i ( ))x .\n\n(6.35)\n\nthe neural network must have three outputs: a vector defining p(c = i | x), a\nmatrix providing \u00b5( )i (x) for all i, and a tensor providing \u03c3( )i (x) for all i. these\noutputs must satisfy different constraints:\n\n1. mixture components p( c = i | x): these form a multinoulli distribution\nover the n different components associated with latent variable 1 c, and can\n\n1we consider c to be latent because we do not observe it in the data: given input x and target\ny, it is not possible to know with certainty which gaussian component was responsible for y, but\nwe can imagine that y was generated by picking one of them, and make that unobserved choice a\nrandom variable.\n\n188\n\n "}, {"Page_number": 204, "text": "chapter 6. deep feedforward networks\n\ntypically be obtained by a softmax over an n-dimensional vector, to guarantee\nthat these outputs are positive and sum to 1.\n\n2. means \u00b5( )i ( x): these indicate the center or mean associated with the i-th\ngaussian component, and are unconstrained (typically with no nonlinearity\nat all for these output units). if y is a d-vector, then the network must output\nd\u00d7 matrix containing all n of these d-dimensional vectors.\u00a0learning\nan n\nthese means with maximum likelihood is slightly more complicated than\nlearning the means of a distribution with only one output mode. we only\nwant to update the mean for the component that actually produced the\nobservation. in practice, we do not know which component produced each\nobservation. the expression for the negative log-likelihood naturally weights\neach example\u2019s contribution to the loss for each component by the probability\nthat the component produced the example.\n\n3. covariances \u03c3( )i (x): these specify the covariance matrix for each component\ni. as when learning a single gaussian component, we typically use a diagonal\nmatrix to avoid needing to compute determinants. as with learning the means\nof the mixture, maximum likelihood is complicated by needing to assign\npartial responsibility for each point to each mixture component. gradient\ndescent will automatically follow the correct process if given the correct\nspecification of the negative log-likelihood under the mixture model.\n\nit has been reported that gradient-based optimization of conditional gaussian\nmixtures (on the output of neural networks) can be unreliable, in part because one\ngets divisions (by the variance) which can be numerically unstable (when some\nvariance gets to be small for a particular example, yielding very large gradients).\none solution is to clip gradients (see sec.\n) while another is to scale the\ngradients heuristically (\n\nmurray and larochelle 2014\n\n10.11.1\n\n).\n\n,\n\n,\n\n) or movements of physical objects (graves 2013\n\ngaussian mixture outputs are particularly effective in generative models of\nspeech (schuster 1999\n). the\nmixture density strategy gives a way for the network to represent multiple output\nmodes and to control the variance of its output, which is crucial for obtaining\na high degree of quality in these real-valued domains. an example of a mixture\ndensity network is shown in fig.\n\n.6.4\n\n,\n\nin general, we may wish to continue to model larger vectors y containing more\nvariables, and to impose richer and richer structures on these output variables. for\nexample, we may wish for our neural network to output a sequence of characters\nthat forms\u00a0a sentence.\nin these cases,\u00a0we may continue to use the\u00a0principle\nof maximum likelihood applied to our model p(y; \u03c9( x)), but the model we use\n\n189\n\n "}, {"Page_number": 205, "text": "chapter 6. deep feedforward networks\n\ny\n\nx\n\nfigure 6.4: samples drawn from a neural network with a mixture density output layer.\nthe input x is sampled from a uniform distribution and the output y is sampled from\n). the neural network is able to learn nonlinear mappings from the input to\npmodel (y x|\nthe parameters of the output distribution. these parameters include the probabilities\ngoverning which of three mixture components will generate the output as well as the\nparameters for each mixture component. each mixture component is gaussian with\npredicted mean and variance. all of these aspects of the output distribution are able to\nvary with respect to the input\n\n, and to do so in nonlinear ways.\n\nx\n\nto describe y becomes complex enough to be beyond the scope of this chapter.\nchapter\ndescribes how to use recurrent neural networks to define such models\nover sequences, and part\ndescribes advanced techniques for modeling arbitrary\nprobability distributions.\n\niii\n\n10\n\n6.3 hidden units\n\nso far we have focused our discussion on design choices for neural networks that\nare common to most parametric machine learning models trained with gradient-\nbased optimization. now we turn to an issue that is unique to feedforward neural\nnetworks: how to choose the type of hidden unit to use in the hidden layers of the\nmodel.\n\nthe design of hidden units is an extremely active area of research and does not\n\nyet have many definitive guiding theoretical principles.\n\nrectified linear units are an excellent default choice of hidden unit. many other\ntypes of hidden units are available. it can be difficult to determine when to use\nwhich kind (though rectified linear units are usually an acceptable choice). we\n\n190\n\n "}, {"Page_number": 206, "text": "chapter 6. deep feedforward networks\n\ndescribe here some of the basic intuitions motivating each type of hidden units.\nthese intuitions can be used to suggest when to try out each of these units. it is\nusually impossible to predict in advance which will work best. the design process\nconsists of trial and error, intuiting that a kind of hidden unit may work well,\nand then training a network with that kind of hidden unit and evaluating its\nperformance on a validation set.\n\n8\n\n4.3\n\n. these ideas will be described further in chapter\n\nsome of the hidden units included in this list are not actually differentiable at\nall input points. for example, the rectified linear function g(z) = max{0, z} is not\ndifferentiable at z = 0. this may seem like it invalidates g for use with a gradient-\nbased learning algorithm. in practice, gradient descent still performs well enough\nfor these models to be used for machine learning tasks.\u00a0this is in part because\nneural network training algorithms do not usually arrive at a local minimum of\nthe cost function, but instead merely reduce its value significantly, as shown in\nfig.\n. because we do not\nexpect training to actually reach a point where the gradient is 0 , it is acceptable\nfor the minima of the cost function to correspond to points with undefined gradient.\nhidden units that are not differentiable are usually non-differentiable at only a\nsmall number of points. in general, a function g(z ) has a left derivative defined\nby the slope of the function immediately to the left of z and a right derivative\ndefined by the slope of the function immediately to the right of z . a function\nis differentiable at z only if both the left derivative and the right derivative are\ndefined and equal to each other. the functions used in the context of neural\nnetworks usually have defined left derivatives and defined right derivatives. in the\ncase of g(z) = max{0, z}, the left derivative at z = 0\nand the right derivative\nis\n. software implementations of neural network training usually return one of\nthe one-sided derivatives rather than reporting that the derivative is undefined or\nraising an error. this may be heuristically justified by observing that gradient-\nbased optimization on a digital computer is subject to numerical error anyway.\nwhen a function is asked to evaluate g(0), it is very unlikely that the underlying\nvalue truly was\n(cid:115) that was rounded\nto . in some contexts, more theoretically pleasing justifications are available, but\nthese usually do not apply to neural network training. the important point is that\nin practice one can safely disregard the non-differentiability of the hidden unit\nactivation functions described below.\n\n. instead, it was likely to be some small value\n0\n\n0is\n\n1\n\n0\n\nunless indicated otherwise, most hidden units can be described as accepting\na vector of inputs x, computing an affine transformation z = w >x + b, and\nthen applying an element-wise nonlinear function g( z). most hidden units are\ndistinguished from each other only by the choice of the form of the activation\nfunction\n\n.\ng( )z\n\n191\n\n "}, {"Page_number": 207, "text": "chapter 6. deep feedforward networks\n\n6.3.1 rectified linear units and their generalizations\n\nrectified linear units use the activation function\n\n( ) = max 0{\ng z\n\n, z\n\n}\n.\n\nrectified linear units are easy to optimize because they are so similar to linear\nunits. the only difference between a linear unit and a rectified linear unit is\nthat a rectified linear unit outputs zero across half its domain.\u00a0this makes the\nderivatives through a rectified linear unit remain large whenever the unit is active.\nthe gradients are not only large but also consistent. the second derivative of the\nrectifying operation is\nalmost everywhere, and the derivative of the rectifying\neverywhere that the unit is active.\u00a0this means that the gradient\noperation is\ndirection is far more useful for learning than it would be with activation functions\nthat introduce second-order effects.\n\n0\n\n1\n\nrectified linear units are typically used on top of an affine transformation:\n\nh\n\nw=  (g\n\n>x b+ ).\n\n(6.36)\n\nwhen initializing the parameters of the affine transformation, it can be a good\npractice to set all elements of b to a small, positive value, such as 0.1. this makes\nit very likely that the rectified linear units will be initially active for most inputs\nin the training set and allow the derivatives to pass through.\n\nseveral generalizations of rectified linear units exist. most of these general-\nizations perform comparably to rectified linear units and occasionally perform\nbetter.\n\none drawback to rectified linear units is that they cannot learn via gradient-\nbased\u00a0methods\u00a0on examples\u00a0for which their\u00a0activation\u00a0is zero. a\u00a0variety of\ngeneralizations of rectified linear units guarantee that they receive gradient every-\nwhere.\n\n,\n\njarrett et al. 2009\n\nthree generalizations of rectified linear units are based on using a non-zero\nslope \u03b1i when zi < 0: hi = g (z \u03b1,\n)i = max(0, zi) + \u03b1 i min(0, zi).\u00a0absolute value\nrectification fixes \u03b1i = \u22121 to obtain g(z) = | |z .\u00a0it is used for object recognition\n), where it makes sense to seek features that are\nfrom images (\ninvariant under a polarity reversal of the input illumination. other generalizations\nmaas et al.\nof rectified linear units are more broadly applicable. a leaky relu (\n,\n2013) fixes \u03b1i to a small value like 0.01 while a parametric relu\ntreats\n\u03b1i as a learnable parameter (\nmaxout units (goodfellow\n\n) generalize rectified linear units further.\ninstead of applying an element-wise function g(z), maxout units divide z into\ngroups of k values. each maxout unit then outputs the maximum element of one\n\nhe et al. 2015\n\nprelu\n\net al.,\n\n2013a\n\nor\n\n).\n\n,\n\n192\n\n "}, {"Page_number": 208, "text": "chapter 6. deep feedforward networks\n\nof these groups:\n\ng( )z i = max\nj\u2208g( )i\n\nzj\n\n(6.37)\n\nwhere g( )i\nis the indices of the inputs for group i , { (i \u2212 1)k + 1, . . . , ik} . this\nprovides a way of learning a piecewise linear function that responds to multiple\ndirections in the input\n\nspace.\n\nx\n\na maxout unit can learn a piecewise linear, convex function with up to k pieces.\nmaxout units can thus be seen as learning the activation function itself rather\nthan just the relationship between units. with large enough k, a maxout unit can\nlearn to approximate any convex function with arbitrary fidelity. in particular,\na maxout layer with two pieces can learn to implement the same function of the\ninput x as a traditional layer using the rectified linear activation function, absolute\nvalue rectification function, or the leaky or parametric relu, or can learn to\nimplement a totally different function altogether. the maxout layer will of course\nbe parametrized differently from any of these other layer types, so the learning\ndynamics will be different even in the cases where maxout learns to implement the\nsame function of\n\nas one of the other layer types.\n\nx\n\neach maxout unit is now parametrized by k weight vectors instead of just one,\nso maxout units typically need more regularization than rectified linear units. they\ncan work well without regularization if the training set is large and the number of\npieces per unit is kept low (\n\ncai et al. 2013\n\n).\n\n,\n\nmaxout units have a few other benefits. in some cases, one can gain some sta-\ntistical and computational advantages by requiring fewer parameters. specifically,\nif the features captured by n different linear filters can be summarized without\nlosing information by taking the max over each group of k features, then the next\nlayer can get by with\n\ntimes fewer weights.\n\nk\n\nbecause each unit is driven by multiple filters, maxout units have some redun-\ndancy that helps them to resist a phenomenon called catastrophic forgetting in\nwhich neural networks forget how to perform tasks that they were trained on in\nthe past (goodfellow\n\net al.,\n\n2014a\n\n).\n\nrectified linear units and all of these generalizations of them are based on the\nprinciple that models are easier to optimize if their behavior is closer to linear.\nthis same general principle of using linear behavior to obtain easier optimization\nalso applies in other contexts besides deep linear networks. recurrent networks can\nlearn from sequences and produce a sequence of states and outputs. when training\nthem, one needs to propagate information through several time steps, which is much\neasier when some linear computations (with some directional derivatives being of\nmagnitude near 1) are involved. one of the best-performing recurrent network\n\n193\n\n "}, {"Page_number": 209, "text": "chapter 6. deep feedforward networks\n\narchitectures, the lstm, propagates information through time via summation\u2014a\nparticular straightforward kind of such linear activation. this is discussed further\nin sec.\n\n.\n10.10\n\n6.3.2 logistic sigmoid and hyperbolic tangent\n\nprior to the introduction of rectified linear units, most neural networks used the\nlogistic sigmoid activation function\n\n(6.38)\n\n(6.39)\n\n.\n1\n\nor the hyperbolic tangent activation function\n\ng z\n\n( ) =  ( )\n\u03c3 z\n\ng z\n\n( ) = tanh( )\nz .\n\nthese activation functions are closely related because\n\ntanh( ) = 2 (2 )\n\nz\n\n\u03c3 z \u2212\n\n1\n\nwe\u00a0have already\u00a0seen sigmoid units\u00a0as output\u00a0units,\u00a0used\u00a0to predict\u00a0the\nprobability that a binary variable is\n. unlike piecewise linear units, sigmoidal\nunits saturate across most of their domain\u2014they saturate to a high value when\nz is very positive, saturate to a low value when z is very negative, and are only\nstrongly sensitive to their input when z is near 0. the widespread saturation of\nsigmoidal units can make gradient-based learning very difficult. for this reason,\ntheir use as hidden units in feedforward networks is now discouraged. their use\nas output units is compatible with the use of gradient-based learning when an\nappropriate cost function can undo the saturation of the sigmoid in the output\nlayer.\n\nwhen a sigmoidal activation function must be used, the hyperbolic tangent\nactivation function typically performs better than the logistic sigmoid. it resembles\nthe identity function more closely, in the sense that tanh(0) = 0 while \u03c3(0) = 1\n2 .\nbecause tanh is similar to identity near\n\u02c6y =\nw> tanh(u> tanh(v >x)) resembles training a linear model \u02c6y = w >u >v >x so\nlong as the activations of the network can be kept small. this makes training the\ntanh network easier.\n\n, training a deep neural network\n\n0\n\nsigmoidal activation functions are more common in settings other than feed-\nforward networks. recurrent networks, many probabilistic models, and some\nautoencoders have additional requirements that rule out the use of piecewise\nlinear activation functions and make sigmoidal units more appealing despite the\ndrawbacks of saturation.\n\n194\n\n "}, {"Page_number": 210, "text": "chapter 6. deep feedforward networks\n\n6.3.3 other hidden units\n\nmany other types of hidden units are possible, but are used less frequently.\n\nin general, a wide variety of differentiable functions perform perfectly well.\nmany unpublished activation functions perform just as well as the popular ones.\nto provide a concrete example, the authors tested a feedforward network using\nh = cos(w x + b) on the mnist dataset and obtained an error rate of less than\n1%, which is competitive with results obtained using more conventional activation\nfunctions. during research and development of new techniques, it is common\nto test many different activation functions and find that several variations on\nstandard practice perform comparably. this means that usually new hidden unit\ntypes are published only if they are clearly demonstrated to provide a significant\nimprovement. new hidden unit types that perform roughly comparably to known\ntypes are so common as to be uninteresting.\n\nit would be impractical to list all of the hidden unit types that have appeared\n\nin the literature. we highlight a few especially useful and distinctive ones.\n\none possibility is to not have an activation g(z) at all. one can also think of\nthis as using the identity function as the activation function. we have already\nseen that a linear unit can be useful as the output of a neural network.\u00a0it may\nalso be used as a hidden unit. if every layer of the neural network consists of only\nlinear transformations, then the network as a whole will be linear. however, it\nis acceptable for some layers of the neural network to be purely linear. consider\na neural network layer with n inputs and p outputs, h = g(w> x + b). we may\nreplace this with two layers, with one layer using weight matrix u and the other\nusing weight matrix v . if the first layer has no activation function, then we have\nessentially factored the weight matrix of the original layer based on w . the\nfactored approach is to compute h = g(v > u> x + b). if u produces q outputs,\nthen u and v together contain only (n + p)q parameters, while w contains np\nparameters. for small q, this can be a considerable saving in parameters.\nit\ncomes at the cost of constraining the linear transformation to be low-rank, but\nthese low-rank relationships are often sufficient. linear hidden units thus offer an\neffective way of reducing the number of parameters in a network.\n\n6.2.2.3\n\nsoftmax units are another kind of unit that is usually used as an output (as\n) but may sometimes be used as a hidden unit. softmax\ndescribed in sec.\nunits naturally represent a probability distribution over a discrete variable with k\npossible values, so they may be used as a kind of switch. these kinds of hidden\nunits are usually only used in more advanced architectures that explicitly learn to\nmanipulate memory, described in sec.\n\n10.12\n.\n\n195\n\n "}, {"Page_number": 211, "text": "chapter 6. deep feedforward networks\n\na few other reasonably common hidden unit types include:\n\n\u2022 radial\u00a0basis\u00a0function\n\nor\u00a0\n\nrbf\n\nfunction becomes more active as x approaches a template w:,i. because it\nsaturates to\n\n, it can be difficult to optimize.\n\nfor most\n\nx\n\n0\n\nunit: hi = exp(cid:26)\u2212 1\n\ni ||w:,i \u2212 ||x 2(cid:27). this\n\n\u03c3 2\n\n(\n\n(\n\ndugas et al. 2001\n\nglorot et al. 2011a\n\n) for function approximation and by\n\n\u2022 softplus: g( a) = \u03b6(a) = log(1 + ea). this is a smooth version of the rectifier,\nnair\nintroduced by\n) for the conditional distributions of undirected probabilistic\nand hinton 2010\n) compared the softplus and rectifier and found\nmodels.\nbetter results with the latter. the use of the softplus is generally discouraged.\nthe softplus demonstrates that the performance of hidden unit types can\nbe very counterintuitive\u2014one might expect it to have an advantage over\nthe rectifier due to being differentiable everywhere or due to saturating less\ncompletely, but empirically it does not.\n\n(\n\n\u2022 hard tanh: this is shaped similarly to the tanh and the rectifier but unlike\nit was introduced\n\nthe latter, it is bounded, g(a) = max (\u2212 1, min(1, a)).\nby\n\ncollobert 2004\n\n).\n\n(\n\nhidden unit design remains an active area of research and many useful hidden\n\nunit types remain to be discovered.\n\n6.4 architecture design\n\nanother key design consideration for neural networks is determining the architecture.\nthe word architecture refers to the overall structure of the network:\u00a0how many\nunits it should have and how these units should be connected to each other.\n\nmost neural networks are organized into groups of units called layers.\u00a0most\nneural network architectures arrange these layers in a chain structure, with each\nlayer being a function of the layer that preceded it. in this structure, the first layer\nis given by\n\nthe second layer is given by\n\nand so on.\n\nh(1) = g(1)(cid:26)w (1)> x b+ (1)(cid:27) ,\nh(2) = g(2)(cid:26)w (2)>h(1) + b(2)(cid:27) ,\n\n196\n\n(6.40)\n\n(6.41)\n\n "}, {"Page_number": 212, "text": "chapter 6. deep feedforward networks\n\nin these chain-based architectures, the main architectural considerations are\nto choose the depth of the network and the width of each layer.\u00a0as we will see,\na network with even one hidden layer is sufficient to fit the training set. deeper\nnetworks often are able to use far fewer units per layer and far fewer parameters\nand often generalize to the test set, but are also often harder to optimize.\u00a0the\nideal network architecture for a task must be found via experimentation guided by\nmonitoring the validation set error.\n\n6.4.1 universal approximation properties and depth\n\na linear model, mapping from features to outputs via matrix multiplication, can\nby definition represent only linear functions. it has the advantage of being easy to\ntrain because many loss functions result in convex optimization problems when\napplied to linear models. unfortunately, we often want to learn nonlinear functions.\n\n,\n\n;\n\nat first glance, we might presume that learning a nonlinear function requires\ndesigning a specialized model family for the kind of nonlinearity we want to learn.\nfortunately, feedforward networks with hidden layers provide a universal approxi-\nmation framework. specifically, the universal approximation theorem (\nhornik et al.\n,\n) states that a feedforward network with a linear output layer\n1989 cybenko 1989\nand at least one hidden layer with any \u201csquashing\u201d activation function (such as\nthe logistic sigmoid activation function) can approximate any borel measurable\nfunction from one finite-dimensional space to another with any desired non-zero\namount of error, provided that the network is given enough hidden units. the\nderivatives of the feedforward network can also approximate the derivatives of the\nfunction arbitrarily well (\n). the concept of borel measurability\nis beyond the scope of this book; for our purposes it suffices to say that any\ncontinuous function on a closed and bounded subset of rn is borel measurable\nand therefore may be approximated by a neural network. a neural network may\nalso approximate any function mapping from any finite dimensional discrete space\nto another. while the original theorems were first stated in terms of units with\nactivation functions that saturate both for very negative and for very positive\narguments, universal approximation theorems have also been proven for a wider\nclass of activation functions, which includes the now commonly used rectified linear\nunit (\n\nleshno et al. 1993\n\nhornik et al. 1990\n\n).\n\n,\n\n,\n\nthe universal approximation theorem means that regardless of what function\nwe are trying to learn, we know that a large mlp will be able to represent this\nfunction. however, we are not guaranteed that the training algorithm will be able\nto learn that function. even if the mlp is able to represent the function, learning\ncan fail for two different reasons. first, the optimization algorithm used for training\n\n197\n\n "}, {"Page_number": 213, "text": "chapter 6. deep feedforward networks\n\nmay not be able to find the value of the parameters that corresponds to the desired\nfunction. second, the training algorithm might choose the wrong function due to\noverfitting. recall from sec.\nthat the \u201cno free lunch\u201d theorem shows that\nthere is no universally superior machine learning algorithm. feedforward networks\nprovide a universal system for representing functions, in the sense that, given a\nfunction, there exists a feedforward network that approximates the function. there\nis no universal procedure for examining a training set of specific examples and\nchoosing a function that will generalize to points not in the training set.\n\n5.2.1\n\nbarron 1993\n\nthe universal approximation theorem says that there exists a network large\nenough to achieve any degree of accuracy we desire, but the theorem does not\nsay how large this network will be.\n) provides some bounds on the\nsize of a single-layer network needed to approximate a broad class of functions.\nunfortunately, in the worse case, an exponential number of hidden units (possibly\nwith one hidden unit corresponding to each input configuration that needs to be\ndistinguished) may be required. this is easiest to see in the binary case: the\nnumber of possible binary functions on vectors v \u2208 {0, 1}n is 22 n\nand selecting\none such function requires 2n bits, which will in general require o(2 n) degrees of\nfreedom.\n\n(\n\nin summary, a feedforward network with a single layer is sufficient to represent\nany function, but the layer may be infeasibly large and may fail to learn and\ngeneralize correctly. in many circumstances, using deeper models can reduce the\nnumber of units required to represent the desired function and can reduce the\namount of generalization error.\n\nthere exist families of functions which can be approximated efficiently by an\narchitecture with depth greater than some value d, but which require a much larger\nmodel if depth is restricted to be less than or equal to d. in many cases, the number\nof hidden units required by the shallow model is exponential in n.\u00a0such results\nwere first proven for models that do not resemble the continuous, differentiable\nneural networks used for machine learning, but have since been extended to these\nmodels. the first results were for circuits of logic gates (\n). later\nwork extended these results to linear threshold units with non-negative weights\n), and then to networks with\n(\n;\nh\u00e5stad and goldmann 1991 hajnal et al. 1993\n).\u00a0many modern\ncontinuous-valued activations (\nneural networks use rectified linear units.\n) demonstrated\nthat shallow networks with a broad family of non-polynomial activation functions,\nincluding rectified linear units, have universal approximation properties, but these\nresults do not address the questions of depth or efficiency\u2014they specify only that\na sufficiently wide rectifier network could represent any function. pascanu et al.\n\nmaass 1992 maass et al. 1994\n\nleshno et al. 1993\n\nh\u00e5stad 1986\n\n(\n\n,\n\n,\n\n,\n\n,\n\n;\n\n,\n\n198\n\n "}, {"Page_number": 214, "text": "chapter 6. deep feedforward networks\n\n(\n\n) and\n\nmontufar et al. 2014\n\n(\n2013b\n) showed that functions representable with a\ndeep rectifier net can require an exponential number of hidden units with a shallow\n(one hidden layer) network. more precisely, they showed that piecewise linear\nnetworks (which can be obtained from rectifier nonlinearities or maxout units) can\nrepresent functions with a number of regions that is exponential in the depth of the\nnetwork. fig.\nillustrates how a network with absolute value rectification creates\nmirror images of the function computed on top of some hidden unit, with respect\nto the input of that hidden unit. each hidden unit specifies where to fold the\ninput space in order to create mirror responses (on both sides of the absolute value\nnonlinearity). by composing these folding operations, we obtain an exponentially\nlarge number of piecewise linear regions which can capture all kinds of regular\n(e.g., repeating) patterns.\n\n6.5\n\net al. (\n\nfigure 6.5: an intuitive, geometric explanation of the exponential advantage of deeper\nrectifier networks formally shown by pascanu\nmontufar et al. 2014\n).\n(left) an absolute value rectification unit has the same output for every pair of mirror\npoints in its input. the mirror axis of symmetry is given by the hyperplane defined by the\nweights and bias of the unit. a function computed on top of that unit (the green decision\nsurface) will be a mirror image of a simpler pattern across that axis of symmetry. (center)\nthe function can be obtained by folding the space around the axis of symmetry. (right)\nanother repeating pattern can be folded on top of the first (by another downstream unit)\nto obtain another symmetry (which is now repeated four times, with two hidden layers).\n\n) and by\n\n2014a\n\n(\n\nmore precisely, the main theorem in\n\n) states that the\nnumber of linear regions carved out by a deep rectifier network with d inputs,\ndepth , and\n\nunits per hidden layer, is\n\nmontufar et al. 2014\n\nn\n\n(\n\nl\n\no\u00a0 (cid:28)n\n\nd(cid:29)d l( \u22121)\n\nnd!,\n\n(6.42)\n\ni.e., exponential in the depth . in the case of maxout networks with\nunit, the number of linear regions is\n\nl\n\nk\n\nfilters per\n\no(cid:26)k(\n\n1)+\n\nl\u2212\n\n199\n\nd(cid:27) .\n\n(6.43)\n\n "}, {"Page_number": 215, "text": "chapter 6. deep feedforward networks\n\nof course, there is no guarantee that the kinds of functions we want to learn in\napplications of machine learning (and in particular for ai) share such a property.\n\nwe may also want to choose a deep model for statistical reasons.\u00a0any time\nwe choose a specific machine learning algorithm, we are implicitly stating some\nset of prior beliefs we have about what kind of function the algorithm should\nlearn. choosing a deep model encodes a very general belief that the function we\nwant to learn should involve composition of several simpler functions. this can be\ninterpreted from a representation learning point of view as saying that we believe\nthe learning problem consists of discovering a set of underlying factors of variation\nthat can in turn be described in terms of other, simpler underlying factors of\nvariation. alternately, we can interpret the use of a deep architecture as expressing\na belief that the function we want to learn is a computer program consisting of\nmultiple steps, where each step makes use of the previous step\u2019s output.\u00a0these\nintermediate outputs are not necessarily factors of variation, but can instead be\nanalogous to counters or pointers that the network uses to organize its internal\nprocessing. empirically, greater depth does seem to result in better generalization\nbengio et al. 2007 erhan et al. 2009 bengio 2009\nfor a wide variety of tasks (\n;\n2011 ciresan\n;\nmesnil\net al.,\net al.,\net al.,\n;\n2013 farabet\net al.,\n;\n2013 goodfellow\n2013 couprie\net al.\nfor examples of some\net al.\n2014d szegedy\n,\n,\nof these empirical results. this suggests that using deep architectures does indeed\nexpress a useful prior over the space of functions the model learns.\n\n,\n,\n2012 sermanet\n\n;\n2013 kahou\nand fig.\n\n2012 krizhevsky\n\net al.,\n6.7\n\n). see fig.\n\net al.,\n\net al.,\n\n2014a\n\n6.6\n\n;\n\n,\n\n;\n\n;\n\n;\n\n;\n\n;\n\n6.4.2 other architectural considerations\n\nso far we have described neural networks as being simple chains of layers, with the\nmain considerations being the depth of the network and the width of each layer.\nin practice, neural networks show considerably more diversity.\n\nmany neural network architectures have been developed for specific tasks.\nspecialized architectures for computer vision called convolutional networks are\ndescribed in chapter\n. feedforward networks may also be generalized to the\nrecurrent neural networks for sequence processing, described in chapter\n, which\nhave their own architectural considerations.\n\n10\n\n9\n\nin general, the layers need not be connected in a chain, even though this is the\nmost common practice. many architectures build a main chain but then add extra\narchitectural features to it, such as skip connections going from layer i to layer\ni + 2 or higher. these skip connections make it easier for the gradient to flow from\noutput layers to layers nearer the input.\n\n200\n\n "}, {"Page_number": 216, "text": "chapter 6. deep feedforward networks\n\neffect\u00a0of\u00a0depth\n\n96.5\n\n96.0\n\n95.5\n\n95.0\n\n94.5\n\n94.0\n\n93.5\n\n93.0\n\n92.5\n\n)\n\n%\n\n(\n\u00a0\ny\nc\na\nr\nu\nc\nc\na\n\u00a0\nt\ns\ne\nt\n\n92.0\n\n3\n\n4\n\n5\n\n6\n8\nnumber\u00a0of\u00a0hidden\u00a0layers\n\n7\n\n9\n\n10\n\n11\n\nfigure 6.6: empirical results showing that deeper networks generalize better when used\nto transcribe multi-digit numbers from photographs of addresses. data from goodfellow\net al. (\n).\u00a0the test set accuracy consistently increases with increasing depth.\u00a0see\nfig.\n6.7\nfor a control experiment demonstrating that other increases to the model size do\nnot yield the same effect.\n\n2014d\n\n201\n\n "}, {"Page_number": 217, "text": "chapter 6. deep feedforward networks\n\n97\n\n96\n\n95\n\n94\n\n93\n\n92\n\n)\n\n%\n\n(\n\ny\nc\na\nr\nu\nc\nc\na\n\nt\ns\ne\nt\n\n91\n\n0 0\n\n.\n\neffect of number of parameters\n\n3, convolutional\n3, fully connected\n11, convolutional\n\n0 2\n\n.\n\n0 4\n\n.\n\n0 6\n\n.\n\n0 8\n\n.\n\n1 0\n\n.\n\nnumber of parameters\n\n\u00d7108\n\n(\n\ngoodfellow et al. 2014d\n\nfigure 6.7: deeper models tend to perform better. this is not merely because the model is\nlarger. this experiment from\n) shows that increasing the number\nof parameters in layers of convolutional networks without increasing their depth is not\nnearly as effective at increasing test set performance. the legend indicates the depth of\nnetwork used to make each curve and whether the curve represents variation in the size of\nthe convolutional or the fully connected layers. we observe that shallow models in this\ncontext overfit at around 20 million parameters while deep ones can benefit from having\nover 60 million. this suggests that using a deep model expresses a useful preference over\nthe space of functions the model can learn. specifically, it expresses a belief that the\nfunction should consist of many simpler functions composed together. this could result\neither in learning a representation that is composed in turn of simpler representations (e.g.,\ncorners defined in terms of edges) or in learning a program with sequentially dependent\nsteps (e.g., first locate a set of objects, then segment them from each other, then recognize\nthem).\n\n202\n\n "}, {"Page_number": 218, "text": "chapter 6. deep feedforward networks\n\nanother key consideration of architecture design is exactly how to connect a\npair of layers to each other. in the default neural network layer described by a linear\ntransformation via a matrix w , every input unit is connected to every output\nunit. many specialized networks in the chapters ahead have fewer connections, so\nthat each unit in the input layer is connected to only a small subset of units in\nthe output layer. these strategies for reducing the number of connections reduce\nthe number of parameters and the amount of computation required to evaluate\nthe network, but are often highly problem-dependent. for example, convolutional\nnetworks, described in chapter\n, use specialized patterns of sparse connections\nthat are very effective for computer vision problems. in this chapter, it is difficult\nto give much more specific advice concerning the architecture of a generic neural\nnetwork. subsequent chapters develop the particular architectural strategies that\nhave been found to work well for different application domains.\n\n9\n\n6.5 back-propagation and other differentiation algo-\n\nrithms\n\nwhen we use a feedforward neural network to accept an input x and produce an\noutput \u02c6y , information flows forward through the network. the inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces \u02c6y. this is called forward propagation. during\u00a0training,\nforward propagation can continue onward until it produces a scalar cost j (\u03b8). the\nback-propagation\nbackprop\n,\nallows the information from the cost to then flow backwards through the network,\nin order to compute the gradient.\n\nrumelhart et al. 1986a\n\n), often simply called\n\nalgorithm (\n\n,\n\ncomputing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. the\nback-propagation algorithm does so using a simple and inexpensive procedure.\n\nthe term back-propagation is\u00a0often misunderstood as\u00a0meaning the whole\nlearning algorithm for multi-layer neural networks. actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nfurthermore, back-propagation is often misunderstood as being specific to multi-\nlayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). specifically, we will describe how to compute the gradient\n\u2207x f(x y, ) for an arbitrary function f, where x is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\n\n203\n\n "}, {"Page_number": 219, "text": "chapter 6. deep feedforward networks\n\nbut whose derivatives are not required. in learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters,\n\u2207\u03b8 j(\u03b8). many machine learning tasks involve computing other derivatives, either\nas part of the learning process,\u00a0or to analyze the learned model. the back-\npropagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. the\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the jacobian of a function\nf with multiple outputs. we restrict our description here to the most commonly\nused case where\n\nhas a single output.\n\nf\n\n6.5.1 computational graphs\n\nso far we have discussed neural networks with a relatively informal graph language.\nto describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\n\nmany ways of formalizing computation as graphs are possible.\n\nhere, we use each node in the graph to indicate a variable. the variable may\n\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\n\nto formalize our graphs, we also need to introduce the idea of an operation.\nan operation is a simple function of one or more variables. our graph language\nis accompanied by a set of allowable operations. functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\n\nwithout loss of generality,\u00a0we define an operation to return only a single\noutput variable. this does not lose generality because the output variable can have\nmultiple entries, such as a vector. software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\n\nif a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y .\u00a0we sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context.\n\nexamples of computational graphs are shown in fig.\n\n.6.8\n\n204\n\n "}, {"Page_number": 220, "text": "chapter 6. deep feedforward networks\n\n\u02c6y\u02c6y\n\n(cid:114)\n\nu(2)u(2)\n\n+\n\nu(1)u(1)\n\ndot\n\nzz\n\n\u21e5\n\nxx\n\nyy\n\nxx\n\nww\n\nbb\n\n(a)\n\nhh\n\nrelu\n\nu (1)u (1)\n\nu (2)u (2)\n\n+\n\nmatmul\n\nxx\n\nww\n\nbb\n\n(c)\n\n(b)\n\nu(2)u(2)\n\nu(1)u(1)\n\nsum\n\nsqr\n\ndot\n\nww\n\n(d)\n\n\u02c6y\u02c6y\n\nxx\n\nu(3)u(3)\n\n\u21e5\n\n(cid:114)(cid:114)\n\n(b)\n\n(a)\n\nthe graph using the\n\nthe graph for the logistic regression prediction\n\nfigure 6.8: examples of computational graphs.\n\u00d7 operation to\ncompute z = xy.\u00a0\nsome of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. we simply name the i-th such variable u( )i .\nthe\ncomputational graph for the expression h = max{0, xw + b}, which computes a design\nmatrix of rectified linear unit activations h given a design matrix containing a minibatch\nof inputs x .\nexamples a\u2013c applied at most one operation to each variable, but it\nis possible to apply more than one operation. here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. the\ni .\n\nweights are used to make the both the prediction \u02c6y and the weight decay penalty \u03bbp i w2\n\n\u02c6y = \u03c3(cid:24)x> w + b(cid:25).\n\n(d)\n\n(c)\n\n205\n\n "}, {"Page_number": 221, "text": "chapter 6. deep feedforward networks\n\n6.5.2 chain rule of calculus\n\nthe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient.\n\nlet x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. suppose that y = g (x) and z = f(g(x)) = f (y). then\nthe chain rule states that\n\ndz\ndx\n\n=\n\ndz\ndy\n\ndy\ndx\n\n.\n\n(6.44)\n\nwe can generalize this beyond the scalar case. suppose that x \u2208 r m, y \u2208 rn,\ng maps from rm to rn, and f maps from rn to r. if y = g(x ) and z = f (y), then\n\n\u2202z\n\u2202xi\n\n=xj\n\n\u2202z\n\u2202yj\n\n\u2202yj\n\u2202xi\n\n.\n\nin vector notation, this may be equivalently written as\n\n\u2207xz =(cid:28) \u2202y\n\u2202x(cid:29)>\n\n\u2207y z,\n\n(6.45)\n\n(6.46)\n\nwhere \u2202y\n\n\u2202x is the\n\njacobian matrix of\n\nn m\u00d7\nfrom this we see that the gradient of a variable x can be obtained by multiplying\na jacobian matrix \u2202y\n\u2202x by a gradient \u2207y z. the back-propagation algorithm consists\nof performing such a jacobian-gradient product for each operation in the graph.\n\ng\n\n.\n\nusually we do not apply the back-propagation algorithm merely to vectors,\nbut rather to tensors of arbitrary dimensionality. conceptually, this is exactly the\nsame as back-propagation with vectors. the only difference is how the numbers\nare arranged in a grid to form a tensor. we could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor.\nin this rearranged view,\nback-propagation is still just multiplying jacobians by gradients.\n\nto denote the gradient of a value z with respect to a tensor x, we write \u2207x z,\njust as if x were a vector. the indices into x now have multiple coordinates\u2014for\nexample, a 3-d tensor is indexed by three coordinates. we can abstract this away\nby using a single variable i to represent the complete tuple of indices. for all\npossible index tuples i, (\u2207x z)i gives \u2202z\n. this is exactly the same as how for all\n\n\u2202xi\n\n206\n\n "}, {"Page_number": 222, "text": "chapter 6. deep feedforward networks\n\npossible integer indices i into a vector, (\u2207xz )i gives \u2202z\ncan write the chain rule as it applies to tensors. if\n\ny\n\n. using this notation, we\n, then\n\nf=  ( )y\n\n\u2202xi\nx=  (g\n)\n\nand\n\nz\n\n\u2207 x z =xj\n\n(\u2207xy j)\n\n\u2202z\n\u2202yj\n\n.\n\n(6.47)\n\n6.5.3 recursively applying the chain rule to obtain backprop\n\nusing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. however, actually evaluating that expression in a computer\nintroduces some extra considerations.\n\nspecifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. any procedure that computes the gradient\nwill need to choose whether to store these subexpressions or to recompute them\nseveral times. an example of how these repeated subexpressions arise is given in\nfig.\u00a0\n.\u00a0in some cases, computing the same subexpression twice would simply\nbe wasteful.\u00a0for complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nin other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\n\n6.9\n\n6.2\n\nalong with algorithm\n\nwe first begin by a version of the back-propagation algorithm that specifies\nthe actual gradient computation directly (algorithm\n6.1\nfor the associated forward computation), in the order it will actually be done and\naccording to the recursive application of chain rule. one could either directly\nperform these computations or view the description of the algorithm as a symbolic\nspecification of the computational graph for computing the back-propagation. how-\never, this formulation does not make explicit the manipulation and the construction\nof the symbolic graph that performs the gradient computation. such a formulation\nis presented below in sec.\n, with algorithm , where we also generalize to\nnodes that contain arbitrary tensors.\n\n6.5.6\n\n6.5\n\nfirst consider a computational graph describing how to compute a single scalar\nu( )n (say the loss on a training example). this scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u(1) to u(ni) .\u00a0in\nother words we wish to compute \u2202u ( )n\nfor all i \u2208 {1 ,2, . . . , ni} . in the application\n\u2202u( )i\nof back-propagation to computing gradients for gradient descent over parameters,\nu( )n will be the cost associated with an example or a minibatch, while u(1) to u (n i)\ncorrespond to the parameters of the model.\n\n207\n\n "}, {"Page_number": 223, "text": "chapter 6. deep feedforward networks\n\nwe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u(ni+1) and\ngoing up to u( )n . as defined in algorithm , each node\nis associated with an\noperation f ( )i and is computed by evaluating the function\n\nu( )i\n\n6.1\n\nwhere a ( )i\n\nis the set of all nodes that are parents of u ( )i .\n\nu( )i =  (f a( )i )\n\n(6.48)\n\nalgorithm 6.1 a procedure that performs the computations mapping ni inputs\nu(1) to u(ni) to an output u ( )n . this defines a computational graph where each node\ncomputes numerical value u( )i by applying a function f ( )i to the set of arguments\n(u( )i ). the\na( )i that comprises the values of previous nodes u( )j , j < i, with j p a\u2208\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu(1) to u(ni) . the output of the computational graph is read off the last (output)\nnode u( )n .\nfor i\n= 1\nu( )i \u2190 xi\nend for\nfor i\na( )i \u2190 {u( )j | \u2208j\nu( )i \u2190 f ( )i (a( )i )\nend for\nreturn u( )n\n\nn=  i + 1, . . . , n do\n\np a u( ( )i )}\n\n, . . . , n\n\ni do\n\nthat algorithm specifies the forward propagation computation, which we could\nput in a graph g .\nin order to perform back-propagation, we can construct a\ncomputational graph that depends on g and adds to it an extra set of nodes. these\nform a subgraph b with one node per node of g. computation in b proceeds in\nexactly the reverse of the order of computation in g, and each node of b computes\nthe derivative \u2202u( )n\n\u2202u ( )i associated with the forward graph node u( )i . this is done\nusing the chain rule with respect to scalar output u( )n :\n\n\u2202u( )n\n\u2202u( )j\n\n= xi j p a u\n\n\u2202u( )n\n\u2202u( )i\n\n\u2202u ( )i\n\u2202u( )j\n\n: \u2208 ( ( )i )\nb contains exactly one edge for each\nas specified by algorithm . the subgraph\nedge from node u( )j to node u( )i of g. the edge from u( )j to u( )i\nis associated with\nthe computation of \u2202u( )i\n\u2202u( )j . in addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u( )i that are children\n\n6.2\n\n(6.49)\n\n208\n\n "}, {"Page_number": 224, "text": "chapter 6. deep feedforward networks\n\nof u ( )j and the vector containing the partial derivatives \u2202u( )i\nfor the same children\n\u2202u( )j\nnodes u( )i . to summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in g, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations.\n\nalgorithm 6.2 simplified version of the back-propagation algorithm for computing\nthe derivatives of u( )n with respect to the variables in the graph. this example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u(1), . . . , u(n i).\nthis simplified version computes the derivatives of all nodes in the graph.\u00a0the\ncomputational cost of this algorithm is proportional to the number of edges in\nthe graph, assuming that the partial derivative associated with each edge requires\na constant time. this is of the same order as the number of computations for\nthe forward propagation. each \u2202u( )i\nis a function of the parents u( )j of u( )i , thus\n\u2202u( )j\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\n\n6.1\n\nfor this example) to obtain the activa-\n\nrun forward propagation (algorithm\ntions of the network\ninitialize grad_table, a data structure that will store the derivatives that have\n[u( )i ] will store the computed value of\nbeen computed. the entry grad table\n\u2202u( )n\n\u2202u( )i .\n_\ngrad table\nn=  \u2212 1 down to 1 do\nfor j\n\n[\u2202u( )n ] \n\n1\u2190\n\n_\n\nthe next line computes \u2202u( )n\n\ngrad table\n\n_\nend for\nreturn {grad table\n\n_\n\n[u( )j ] \u2190pi j p a u\n\n[u( )i ]  = 1\n\n: \u2208 ( ( )i )\n\n\u2202u( )j =pi j p a u\n\n: \u2208 ( ( )i ) grad table\n| i\n\n, . . . , ni}\n\n_\n\n\u2202u( )i\n\u2202u ( )j using stored values:\n\n\u2202u ( )n\n\u2202u( )i\n[u( )i ] \u2202u ( )i\n\u2202u( )j\n\nthe back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. specifically, it performs on the order\nof one jacobian product per node in the graph.\u00a0this can be seen from the fact\nto node u( )i of\nin algorithm\nthe graph exactly once in order to obtain the associated partial derivative \u2202u( )j\n\u2202u ( )i .\nback-propagation thus avoids the exponential explosion in repeated subexpressions.\n\nthat backprop visits each edge from node\n\nu( )j\n\n6.2\n\n209\n\n "}, {"Page_number": 225, "text": "chapter 6. deep feedforward networks\n\nhowever, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. we will revisit these ideas\nafter describing the back-propagation algorithm itself.\n\n6.5.4 back-propagation computation in fully-connected mlp\n\nto clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer mlp.\n\n6.3\n\nalgorithm\n\nfirst shows the forward propagation, which maps parameters to\nthe supervised loss l( \u02c6y y, ) associated with a single (input,target) training example\n(\n)x y,\n\n, with \u02c6y the output of the neural network when\n\nis provided in input.\n\nx\n\nalgorithm\u00a0\n\n6.4\n\nthen\u00a0shows\u00a0the corresponding\u00a0computation to\u00a0be\u00a0done\u00a0for\n\napplying the back-propagation algorithm to this graph.\n\n6.3\n\nalgorithm\n\nare demonstrations that are chosen to be\nsimple and straightforward to understand. however, they are specialized to one\nspecific problem.\n\nand algorithm\n\n6.4\n\nmodern software implementations are based on the generalized form of back-\npropagation described in sec.\nbelow, which can accommodate any computa-\ntional graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\n\n6.5.6\n\n6.5.5 symbol-to-symbol derivatives\n\nalgebraic expressions and\u00a0computational graphs both operate on\u00a0symbols,\u00a0or\nvariables\u00a0that do\u00a0not have\u00a0specific values. these\u00a0algebraic and\u00a0graph-based\nrepresentations are called symbolic representations. when we actually use or\ntrain a neural network, we must assign specific values to these symbols. we\nreplace a symbolic input to the network x with a specific\nvalue, such as\n[1 2 3 765\n\nnumeric\n\n. ,\n\n.\n\n1 8]\n\n,\u2212 . > .\n\nsome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. we call this approach \u201csymbol-\nto-number\u201d differentiation. this is the approach used by libraries such as torch\n(\ncollobert et al. 2011b\n\n) and caffe (\n\njia 2013\n\n).\n\n,\n\n,\n\nanother approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. this\n\n210\n\n "}, {"Page_number": 226, "text": "chapter 6. deep feedforward networks\n\nf\n\nf\n\nf\n\nzz\n\nyy\n\nxx\n\nww\n\nfigure 6.9: a computational graph that results in repeated subexpressions when computing\nthe gradient. let w \u2208 r be the input to the graph. we use the same function f : r\nr\u2192\nas the operation that we apply at every step of a chain: x = f( w), y = f( x), z = f(y).\nto compute \u2202z\n\nand obtain:\n\n6.44\n\n\u2202w , we apply eq.\n\n\u2202z\n\u2202w\n\u2202z\n\u2202y\n\n=\n\n\u2202y\n\u2202x\n\n\u2202x\n\u2202w\n\n=f 0( )y f 0( )x f 0 ( )w\n=f 0( ( ( )))\n\nf f w f 0( ( ))\n\nf w f 0( )w\n\n(6.50)\n\n(6.51)\n\n(6.52)\n(6.53)\n\n6.52\n\nsuggests an implementation in which we compute the value of\n\neq.\nf (w) only once\nand store it in the variable x. this is the approach taken by the back-propagation\nalgorithm. an alternative approach is suggested by eq.\n, where the subexpression\nf(w) appears more than once. in the alternative approach, f(w) is recomputed each time\nit is needed.\u00a0when the memory required to store the value of these expressions is low,\nthe back-propagation approach of eq.\u00a0\nis clearly preferable because of its reduced\nruntime. however, eq.\nis also a valid implementation of the chain rule, and is useful\nwhen memory is limited.\n\n6.53\n\n6.52\n\n6.53\n\n211\n\n "}, {"Page_number": 227, "text": "chapter 6. deep feedforward networks\n\n6.2.1.1\n\nalgorithm 6.3 forward propagation through a typical deep neural network and\nthe computation of the cost function. the loss l( \u02c6y y,\n) depends on the output \u02c6y\nand on the target y (see sec.\nfor examples of loss functions). to obtain the\ntotal cost j, the loss may be added to a regularizer \u03c9(\u03b8), where \u03b8 contains all the\nparameters (weights and biases). algorithm\nshows how to compute gradients\nof j with respect to parameters w and b. for simplicity, this demonstration uses\nonly a single input example x. practical applications should use a minibatch. see\nsec.\nrequire: network depth, l\nrequire: w ( )i , i\nrequire: b( )i , i\nrequire: x, the input to process\nrequire: y, the target output\n\n} the weight matrices of the model\n} the bias parameters of the model\n\nfor a more realistic demonstration.\n\n\u2208 {1\n\u2208 {1\n\n, . . . , l ,\n\n, . . . , l ,\n\n6.5.7\n\n6.4\n\nh(0) = x\nfor\n= 1\n\nk\n\n, . . . , l\n\ndo\n\na ( )k = b( )k + w ( )k h(\nh( )k =  (f a ( )k )\n\n1)\n\nk\u2212\n\nend for\nh=  ( )l\n\u02c6y\nj\nl=  (\u02c6y y,\n\n) + \u03c9( )\n\u03bb \u03b8\n\n,\n\n)\nbergstra et al. 2010 bastien et al. 2012\nis the approach taken by theano (\n). an example of how this approach works\nand tensorflow (\nabadi et al. 2015\nis\u00a0illustrated in\u00a0fig.\n. the\u00a0primary\u00a0advantage of\u00a0this approach\u00a0is that\nthe derivatives are described in the same language as the original expression.\nbecause the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. computation of higher-order derivatives is described in sec.\n\n6.5.10\n.\n\n6.10\n\n,\n\n;\n\n,\n\nwe will use the latter approach and describe the back-propagation algorithm in\nterms of constructing a computational graph for the derivatives. any subset of the\ngraph may then be evaluated using specific numerical values at a later time. this\nallows us to avoid specifying exactly when each operation should be computed.\ninstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents\u2019 values are available.\n\nthe description of the symbol-to-symbol based approach subsumes the symbol-\nto-number approach. the symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. the key difference is that the symbol-to-number\n\n212\n\n "}, {"Page_number": 228, "text": "chapter 6. deep feedforward networks\n\n6.3\n\nalgorithm 6.4 backward computation\u00a0for the\u00a0deep neural\u00a0network of\u00a0algo-\nrithm , which uses in addition to the input\nx a target y. this computation\nyields the gradients on the activations a( )k\nfor each layer k, starting from the\noutput layer and going backwards to the first hidden layer. from these gradients,\nwhich can be interpreted as an indication of how each layer\u2019s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. the\ngradients on weights and biases can be immediately used as part of a stochas-\ntic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods.\n\nafter the forward computation, compute the gradient on the output layer:\ng \u2190 \u2207 \u02c6yj = \u2207\u02c6y l( \u02c6y, y)\ndo\nfor\n1\n\n, . . . ,\n\nl, l\n\nk\n\n=  \u2212 1\n\nf\n\nf\n\n= g (cid:129) 0(a( )k )\n\nconvert the\u00a0gradient\u00a0on\u00a0the layer\u2019s\u00a0output into\u00a0a gradient into the\u00a0pre-\nnonlinearity activation (element-wise multiplication if\ng \u2190 \u2207a( )k j\ncompute gradients on weights and biases (including the regularization term,\nwhere needed):\n\u2207b ( )k \u03c9( )\u03b8\n\u2207 b( )k j\n\u03bb\n=  +g\nk\u2212 > + \u03bb\u2207w ( )k \u03c9( )\u03b8\n\u2207 w( )k j = g h(\npropagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations:\ng \u2190 \u2207h(\n\nk\u2212 j = w ( )k > g\n\nis element-wise):\n\n1)\n\n1)\n\nend for\n\n213\n\n "}, {"Page_number": 229, "text": "chapter 6. deep feedforward networks\n\nf\n\nf\n\nf\n\nzz\n\nyy\n\nxx\n\nww\n\nf\n\nf\n\nf\n\nzz\n\nyy\n\nxx\n\nww\n\nf 0\n\nf 0\n\nf 0\n\ndz\ndz\ndy\ndy\n\ndy\ndy\ndx\ndx\n\ndx\ndx\ndw\ndw\n\n\u21e5\n\n\u21e5\n\ndz\ndz\ndx\ndx\n\ndz\ndz\ndw\ndw\n\nfigure 6.10:\u00a0an example of the symbol-to-symbol approach to computing derivatives. in\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. instead, it adds nodes to a computational graph describing how\nto compute these derivatives. a generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (left) in this example, we begin with a graph\nwe run the back-propagation algorithm, instructing\nrepresenting z = f( f(f(w))).\nit to construct the graph for the expression corresponding to dz\ndw . in this example, we do\nnot explain how the back-propagation algorithm works. the purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\n\n(right)\n\n214\n\n "}, {"Page_number": 230, "text": "chapter 6. deep feedforward networks\n\napproach does not expose the graph.\n\n6.5.6 general back-propagation\n\nthe back-propagation algorithm is very simple. to compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by dz\ndz = 1. we can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the jacobian of the operation that produced z. we continue\nmultiplying by jacobians traveling backwards through the graph in this way until\nwe reach x. for any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\n\nmore formally, each node in the graph g corresponds to a variable. to achieve\nmaximum generality, we describe this variable as being a tensor v.\u00a0tensor can\nin general have any number of dimensions, and subsume scalars, vectors, and\nmatrices.\n\nwe assume that each variable\n\nv\n\nis associated with the following subroutines:\n\n_\n\n\u2022 get operation\n\n(v): this returns the operation that computes v, repre-\nsented by the edges coming into v in the computational graph. for example,\nthere may be a python or c++ class representing the matrix multiplication\noperation, and the get_operation function. suppose we have a variable that\nis created by matrix multiplication, c = ab . then get operation\n(v)\nreturns a pointer to an instance of the corresponding c++ class.\n\n_\n\n\u2022 get consumers\n\n_\n\n(v,g): this returns the list of variables that are children of\n\nv in the computational graph .g\n\nget inputs\n\n_\n\n(v,\n\n): this returns the list of variables that are parents of v\n\n\u2022\n\ng\n\nin the computational graph .g\n\neach operation op is also associated with a bprop operation. this bprop\noperation can compute a jacobian-vector product as described by eq.\n. this\nis how the back-propagation algorithm is able to achieve great generality. each\noperation is responsible for knowing how to back-propagate through the edges in\nthe graph that it participates in. for example, we might use a matrix multiplication\noperation to create a variable c = ab. suppose that the gradient of a scalar z with\nrespect to c is given by g. the matrix multiplication operation is responsible for\ndefining two back-propagation rules, one for each of its input arguments. if we call\n\n6.47\n\n215\n\n "}, {"Page_number": 231, "text": "chapter 6. deep feedforward networks\n\nthe bprop method to request the gradient with respect to a given that the gradient\non the output is g , then the bprop method of the matrix multiplication operation\nmust state that the gradient with respect to a is given by gb >. likewise, if we\ncall the bprop method to request the gradient with respect to b, then the matrix\noperation is responsible for implementing the bprop method and specifying that\nthe desired gradient is given by a>g. the back-propagation algorithm itself does\nnot need to know any differentiation rules. it only needs to call each operation\u2019s\nbprop rules with the right arguments. formally, op bprop\n,x g) must\nreturn\n\n(inputs,\n\n.\n\n(\u2207xop f inputs\n\n. (\n\n)i) gi,\n\n(6.54)\n\nxi\n\nwhich\u00a0is just\u00a0an implementation\u00a0of the\u00a0chain\u00a0rule as\u00a0expressed in\u00a0eq.\n6.47\n.\nhere, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements, x is the input whose gradient\nwe wish to compute, and\n\nis the gradient on the output of the operation.\n\ng\n\nthe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. for example, if the mul operator is passed\ntwo copies of x to compute x2, the op.bprop method should still return x as the\nderivative with respect to both inputs. the back-propagation algorithm will later\nadd both of these arguments together to obtain 2 x, which is the correct total\nderivative on .x\n\nsoftware implementations of back-propagation usually provide both the opera-\ntions and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually.\n\nthe back-propagation algorithm is formally described in algorithm .6.5\n\n6.5.2\n\nin sec.\n\n, we motivated back-propagation as a strategy for avoiding comput-\ning the same subexpression in the chain rule multiple times. the naive algorithm\ncould have exponential runtime due to these repeated subexpressions. now that\nwe have specified the back-propagation algorithm, we can understand its com-\nputational cost. if we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed.\u00a0keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\n\n216\n\n "}, {"Page_number": 232, "text": "chapter 6. deep feedforward networks\n\nbuild_grad\n\nalgorithm 6.5 the outermost skeleton of the back-propagation algorithm. this\nportion does simple setup and cleanup work. most of the important work happens\nin the\n.\nrequire: t, the target set of variables whose gradients must be computed.\nrequire: g, the computational graph\nrequire: z, the variable to be differentiated\n\nsubroutine of algorithm\n\n6.6\n\nlet g0 be g pruned to contain only nodes that are ancestors of z and descendents\nof nodes in .t\ninitialize\ngrad table\nfor\n\n, a data structure associating tensors to their gradients\n\ngrad_table\n\n1\n\nz \u2190\n[ ] \ndo\n(v,\nbuild grad\n\n_\nv in t\n_\n\nend for\nreturn\n\ngrad_table\n\n_\n\n,g g 0, grad table\n)\nrestricted to\n\nt\n\nmultiplication as a single operation). computing a gradient in a graph with n nodes\nwill never execute more than o( n2) operations or store the output of more than\no(n2) operations. here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. for example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. we can see that computing the gradient requires as\nmost o(n2 ) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). the back-propagation algorithm\nadds one jacobian-vector product, which should be expressed with o(1) nodes, per\nedge in the original graph. because the computational graph is a directed acyclic\ngraph it has at most o (n2) edges. for the kinds of graphs that are commonly used\nin practice, the situation is even better. most neural network cost functions are\nroughly chain-structured, causing back-propagation to have o(n) cost. this is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. this potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (eq.\n\n) non-recursively:\n\n6.49\n\n\u2202u ( )n\n\u2202u( )j =\n\nx\n\npath (u (\u03c01 ),u (\u03c02) ,...,u(\u03c0 t )),\n\nfrom \u03c01 = toj\n\n\u03c0 t=n\n\n\u2202u (\u03c0k)\n\u2202u(\u03c0k\u22121) .\n\ntyk=2\n\n(6.55)\n\nsince the number of paths from node j to node n can grow up to exponentially in the\n\n217\n\n "}, {"Page_number": 233, "text": "chapter 6. deep feedforward networks\n\n_\n\n(v,\n\n,g g 0, grad table\n\nalgorithm 6.6 the inner loop subroutine build grad\n) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm .6.5\nrequire: v, the variable whose gradient should be added to\nrequire: g, the graph to modify.\nrequire: g0, the restriction of\ng\nto nodes that participate in the gradient.\nrequire: grad_table, a data structure mapping nodes to their gradients\nthen\n]v\n\nv is in grad_table\nreturn\n\ngrad table[\n\n.\ngrad_table\n\nand\n\ng\n\n_\n\n_\n\nif\n\nget operation\n( ,\nc\n\n_get consumers( ,g0) do\n_\n0 )\ng\n(c,\n,\n\nv\n( )c\n,g g0, grad table\n)\n_\n\nop bprop get inputs\n\n)\n,v d\n\nend if\ni \u2190 1\nfor c\nop\nd\n\n_\n\nin\n\u2190 _\n\u2190 build grad\ng( )i \u2190\n(\n.\ni\u2190 + 1\ni\nend for\ng \u2190p i\ng ( )i\n\n_\n[\n] = v\ngrad table\ninsert\nreturn g\n\ng\n\ng\n\nand the operations creating it into\n\ng\n\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. this large cost would be incurred because the same computation for\n\u2202u( )i\n\u2202u( )j would be redone many times.\u00a0to avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results \u2202u( )n\n\u2202u( )i . each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. by filling in these table entries in order,\nback-propagation avoids repeating many common subexpressions. this table-filling\nstrategy is sometimes called dynamic programming.\n\n6.5.7 example: back-propagation for mlp training\n\nas an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\n\nhere we develop a very simple multilayer perception with a single hidden\nlayer. to train this model, we will use minibatch stochastic gradient descent.\n\n218\n\n "}, {"Page_number": 234, "text": "chapter 6. deep feedforward networks\n\nthe back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. specifically, we use a minibatch of examples from the training\nset formatted as a design matrix x and a vector of associated class labels y.\nthe network computes a layer of hidden features h = max{0 , xw (1)}. to\nsimplify the presentation we do not use biases in this model. we assume that our\ngraph language includes a relu operation that can compute max{ 0, z} element-\nwise. the predictions of the unnormalized log probabilities over classes are then\ngiven by hw (2). we assume that our graph language includes a cross_entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. the resulting cross-\nentropy defines the cost jmle. minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. however, to make this example more realistic,\nwe also include a regularization term. the total cost\n\nj\n\nj=  mle + \u03bb(cid:32)(cid:33)xi,j (cid:26)w (1)\ni,j (cid:27)2\n\ni,j(cid:27)2(cid:37)(cid:38)\n+xi,j (cid:26)w (2)\n\n(6.56)\n\nconsists of the cross-entropy and a weight decay term with coefficient \u03bb. the\ncomputational graph is illustrated in fig.\n\n.\n6.11\n\nthe computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. this demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually.\n\nwe can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in fig.\n. to train, we wish\nto compute both \u2207w (1)j and \u2207w (2)j . there are two different paths leading\nbackward from j to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. the weight decay cost is relatively simple; it will\nalways contribute 2\u03bbw ( )i to the gradient on w ( )i .\n\n6.11\n\nthe other path through the cross-entropy cost is slightly more complicated.\nlet g be the gradient on the unnormalized log probabilities u (2) provided by\nthe cross_entropy operation. the back-propagation algorithm now needs to\nexplore two different branches. on the shorter branch,\u00a0it adds h >g to the\ngradient on w (2), using the back-propagation rule for the second argument to\nthe matrix multiplication operation. the other branch corresponds to the longer\nchain descending further along the network. first, the back-propagation algorithm\ncomputes \u2207hj = gw (2)> using the back-propagation rule for the first argument\nto the matrix multiplication operation. next, the relu operation uses its back-\n\n219\n\n "}, {"Page_number": 235, "text": "chapter 6. deep feedforward networks\n\nj mle\nj mle\n\ncross_entropy\n\njj\n\n+\n\nu (2)u (2)\n\nyy\n\nmatmul\n\nrelu\n\nhh\n\nw (2)w (2)\n\nsqr\n\nu (5)u (5)\n\nsum\n\nu(6)u(6)\n\nu(7)u(7)\n\n(cid:114)(cid:114)\n\n+\n\nu(8)u(8)\n\n\u21e5\n\nu (1)u (1)\n\nmatmul\n\nxx\n\nw (1)w (1)\n\nsqr\n\nu (3)u (3)\n\nsum\n\nu(4)u(4)\n\nfigure 6.11: the computational graph used to compute the cost used to train our example\nof a single-layer mlp using the cross-entropy loss and weight decay.\n\npropagation rule to zero out components of the gradient corresponding to entries\ng0. the last step of the\nof u (1) that were less than . let the result be called\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the\n\nx >g0 to the gradient on w (1).\n\noperation to add\n\nmatmul\n\n0\n\nafter these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters.\n\nfor the mlp, the computational cost is dominated by the cost of matrix\nmultiplication. during the forward propagation stage, we multiply by each weight\nmatrix, resulting in o(w) multiply-adds, where w is the number of weights. during\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost.\u00a0the main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\nthis value is stored from the time it is computed until the backward pass has\nreturned to the same point. the memory cost is thus o(mnh ), where m is the\nnumber of examples in the minibatch and n h is the number of hidden units.\n\n220\n\n "}, {"Page_number": 236, "text": "chapter 6. deep feedforward networks\n\n6.5.8 complications\n\nour description of the back-propagation algorithm here is simpler than the imple-\nmentations actually used in practice.\n\nas noted above, we have restricted the definition of an operation to be a\nfunction that returns a single tensor. most software implementations need to\nsupport operations that can return more than one tensor. for example, if we wish\nto compute both the maximum value in a tensor and the index of that value, it is\nbest to compute both in a single pass through memory, so it is most efficient to\nimplement this procedure as a single operation with two outputs.\n\nwe\u00a0have\u00a0not described\u00a0how to control the\u00a0memory consumption\u00a0of back-\npropagation. back-propagation often involves summation of many tensors together.\nin the naive approach, each of these tensors would be computed separately, then\nall of them would be added in a second step. the naive approach has an overly\nhigh memory bottleneck that can be avoided by maintaining a single buffer and\nadding each value to that buffer as it is computed.\n\nreal-world implementations of back-propagation also need to handle various\ndata types, such as 32-bit floating point, 64-bit floating point, and integer values.\nthe policy for handling each of these types takes special care to design.\n\nsome operations have undefined gradients, and it is important to track these\n\ncases and determine whether the gradient requested by the user is undefined.\n\nvarious other technicalities make real-world differentiation more complicated.\nthese technicalities are not insurmountable, and this chapter has described the key\nintellectual tools needed to compute derivatives, but it is important to be aware\nthat many more subtleties exist.\n\n6.5.9 differentiation outside the deep learning community\n\nthe\u00a0deep learning\u00a0community has\u00a0been\u00a0somewhat isolated\u00a0from the\u00a0broader\ncomputer science community and has largely developed its own cultural attitudes\nconcerning how to perform differentiation. more generally, the field of automatic\ndifferentiation is concerned with how to compute derivatives algorithmically. the\nback-propagation algorithm described here is only one approach to automatic\ndifferentiation. it is a special case of a broader class of techniques called reverse\nmode accumulation. other approaches evaluate the subexpressions of the chain rule\nin different orders. in general, determining the order of evaluation that results in\nthe lowest computational cost is a difficult problem. finding the optimal sequence\n), in the\nof operations to compute the gradient is np-complete (\n\nnaumann 2008\n\n,\n\n221\n\n "}, {"Page_number": 237, "text": "exp(zi)\n\np i exp(zi)\n\nchapter 6. deep feedforward networks\n\nsense that it may require simplifying algebraic expressions into their least expensive\nform.\n\nfor example, suppose we have variables p 1, p 2, . . . , pn representing probabilities\nand variables z1, z2 , . . . , zn representing unnormalized log probabilities. suppose\nwe define\n\nqi =\n\n,\n\n(6.57)\n\nwhere we build the softmax function out of exponentiation, summation and division\n\noperations,\u00a0and\u00a0construct\u00a0a cross-entropy\u00a0loss j = \u2212pi p i log qi. a human\n\nmathematician can observe that the derivative of j with respect to zi takes a very\nsimple form: qi \u2212 pi. the back-propagation algorithm is not capable of simplifying\nthe gradient this way, and will instead explicitly propagate gradients through all of\nthe logarithm and exponentiation operations in the original graph. some software\nlibraries such as theano (\n) are able to\nperform some kinds of algebraic substitution to improve over the graph proposed\nby the pure back-propagation algorithm.\n\nbergstra et al. 2010 bastien et al. 2012\n\n,\n\n;\n\n,\n\n6.2\n\n6.49\n\nbecause each local partial derivative\n\nwhen the forward graph g has a single output node and each partial derivative\n\u2202u( )i\n\u2202u( )j can be computed with a constant amount of computation, back-propagation\nguarantees that the number of computations for the gradient computation is of\nthe same order as the number of computations for the forward computation: this\n\u2202u( )i\ncan be seen in algorithm\n\u2202u( )j needs\nto be computed only once along with an associated multiplication and addition\nfor the recursive chain-rule formulation (eq.\n). the overall computation is\ntherefore o(# edges). however, it can potentially be reduced by simplifying the\ncomputational graph constructed by back-propagation, and this is an np-complete\ntask.\u00a0implementations such as theano and tensorflow use heuristics based on\nmatching known simplification patterns in order to iteratively attempt to simplify\nthe graph. we defined back-propagation only for the computation of a gradient of a\nscalar output but back-propagation can be extended to compute a jacobian (either\nof k different scalar nodes in the graph, or of a tensor-valued node containing k\nvalues). a naive implementation may then need k times more computation: for\neach scalar internal node in the original forward graph, the naive implementation\ncomputes k gradients instead of a single gradient. when the number of outputs\nof the graph is larger than the number of inputs, it is sometimes preferable to\nuse another form of automatic differentiation called forward mode accumulation.\nforward mode computation has been proposed for obtaining real-time computation\nof gradients in recurrent networks, for example (\n). this\nalso avoids the need to store the values and gradients for the whole graph, trading\noff computational efficiency for memory. the relationship between forward mode\n\nwilliams and zipser 1989\n\n,\n\n222\n\n "}, {"Page_number": 238, "text": "chapter 6. deep feedforward networks\n\nand backward mode is analogous to the relationship between left-multiplying versus\nright-multiplying a sequence of matrices, such as\n\nabcd,\n\n(6.58)\n\nwhere the matrices can be thought of as jacobian matrices. for example, if d\nis a column vector while a has many rows, this corresponds to a graph with a\nsingle output and many inputs, and starting the multiplications from the end\nand going backwards only requires matrix-vector products. this corresponds to\nthe backward mode. instead, starting to multiply from the left would involve a\nseries of matrix-matrix products, which makes the whole computation much more\nexpensive. however, if a has fewer rows than d has columns, it is cheaper to run\nthe multiplications left-to-right, corresponding to the forward mode.\n\nin many communities outside of machine learning,\u00a0it is more\u00a0common to\nimplement differentiation software that acts directly on traditional programming\nlanguage code, such as python or c code, and automatically generates programs\nthat different functions written in these languages. in the deep learning community,\ncomputational graphs are usually represented by explicit data structures created by\nspecialized libraries. the specialized approach has the drawback of requiring the\nlibrary developer to define the bprop methods for every operation and limiting the\nuser of the library to only those operations that have been defined. however, the\nspecialized approach also has the benefit of allowing customized back-propagation\nrules to be developed for each operation, allowing the developer to improve speed\nor stability in non-obvious ways that an automatic procedure would presumably\nbe unable to replicate.\n\nback-propagation is therefore not the only way or the optimal way of computing\nthe gradient, but it is a very practical method that continues to serve the deep\nlearning community very well. in the future, differentiation technology for deep\nnetworks may improve as deep learning practitioners become more aware of advances\nin the broader field of automatic differentiation.\n\n6.5.10 higher-order derivatives\n\nsome software frameworks support the use of higher-order derivatives. among the\ndeep learning software frameworks, this includes at least theano and tensorflow.\nthese libraries use the same kind of data structure to describe the expressions for\nderivatives as they use to describe the original function being differentiated. this\nmeans that the symbolic differentiation machinery can be applied to derivatives.\n\nin the context of deep learning, it is rare to compute a single second derivative\nof a scalar function. instead, we are usually interested in properties of the hessian\n\n223\n\n "}, {"Page_number": 239, "text": "chapter 6. deep feedforward networks\n\nmatrix. if we have a function f : rn \u2192 r, then the hessian matrix is of size n n\u00d7 .\nin typical deep learning applications, n will be the number of parameters in the\nmodel, which could easily number in the billions. the entire hessian matrix is\nthus infeasible to even represent.\n\ninstead of explicitly computing the hessian, the typical deep learning approach\nis to use krylov methods. krylov methods are a set of iterative techniques for\nperforming various operations like approximately inverting a matrix or finding\napproximations to its eigenvectors or eigenvalues, without using any operation\nother than matrix-vector products.\n\nin order to use krylov methods on the hessian, we only need to be able to\ncompute the product between the hessian matrix h and an arbitrary vector v. a\nstraightforward technique (\n\n) for doing so is to compute\n\nchristianson 1992\n\n,\n\nhv = \u2207xh(\u2207x f x( ))> vi .\n\n(6.59)\n\nboth of the gradient computations in this expression may be computed automati-\ncally by the appropriate software library. note that the outer gradient expression\ntakes the gradient of a function of the inner gradient expression.\n\nif v is itself a vector produced by a computational graph, it is important to\nspecify that the automatic differentiation software should not differentiate through\nthe graph that produced .v\n\nwhile computing the hessian is usually not advisable, it is possible to do with\nfor all i = 1, . . . , n, where\n\nhessian vector products. one simply computes he( )i\ne( )i\n\nis the one-hot vector with e ( )i\n\ni = 1 and all other entries equal to 0.\n\n6.6 historical notes\n\nfeedforward networks can be seen as efficient nonlinear function approximators\nbased on using gradient descent to minimize the error in a function approximation.\nfrom this point of view, the modern feedforward network is the culmination of\ncenturies of progress on the general function approximation task.\n\nthe chain rule that underlies the back-propagation algorithm was invented\nin the 17th century (\n). calculus and algebra have\nlong been used to solve optimization problems in closed form, but gradient descent\nwas not introduced as a technique for iteratively approximating the solution to\noptimization problems until the 19th century (cauchy 1847\n\nleibniz 1676 l\u2019h\u00f4pital 1696\n\n).\n\n,\n\n;\n\n,\n\n,\n\nbeginning in the 1940s, these function approximation techniques were used to\nmotivate machine learning models such as the perceptron. however, the earliest\n\n224\n\n "}, {"Page_number": 240, "text": "chapter 6. deep feedforward networks\n\nmodels were based on linear models. critics including marvin minsky pointed\nout several of the flaws of the linear model family, such as it inability to learn the\nxor function, which led to a backlash against the entire neural network approach.\n\n;\n\n;\n\n,\n\n;\n\n,\n\n;\n\n,\n\n;\n\n,\n\n,\n\n(\n\n).\n\net al.,\n\nlinnainmaa 1976 werbos 1981\n\nlearning nonlinear functions required the development of a multilayer per-\nceptron and a means of computing the gradient through such a model. efficient\napplications of the chain rule based on dynamic programming began to appear in the\n1960s and 1970s, mostly for control applications (\nkelley 1960 bryson and denham\n,\n1961 dreyfus 1962 bryson and ho 1969 dreyfus 1973\n) but also for sensitivity\nanalysis (\n) proposed applying these techniques\nto training artificial neural networks. the idea was finally developed in practice\nafter being independently rediscovered in different ways (\nlecun 1985 parker,\n1985 rumelhart\n). the book parallel distributed processing presented\nthe results of some of the first successful experiments with back-propagation in a\nchapter (\n) that contributed greatly to the popularization\nof back-propagation and initiated a very active period of research in multi-layer\nneural networks. however, the ideas put forward by the authors of that book\nand in particular by rumelhart and hinton go much beyond back-propagation.\nthey include crucial ideas about the possible computational implementation of\nseveral central aspects of cognition and learning, which came under the name of\n\u201cconnectionism\u201d because of the importance given the connections between neurons\nas the locus of learning and memory. in particular, these ideas include the notion\nof distributed representation (hinton\n\nrumelhart et al. 1986b\n\net al.,\n\n1986a\n\n1986\n\n).\n\n,\n\n;\n\n,\n\nfollowing the success of back-propagation, neural network research gained pop-\nularity and reached a peak in the early 1990s. afterwards, other machine learning\ntechniques became more popular until the modern deep learning renaissance that\nbegan in 2006.\n\nthe core ideas behind modern feedforward networks have not changed sub-\nstantially since the 1980s. the same back-propagation algorithm and the same\napproaches to gradient descent are still in use. most of the improvement in neural\nnetwork performance from 1986 to 2015 can be attributed to two factors. first,\nlarger datasets have reduced the degree to which statistical generalization is a\nchallenge for neural networks. second, neural networks have become much larger,\ndue to more powerful computers, and better software infrastructure. however, a\nsmall number of algorithmic changes have improved the performance of neural\nnetworks noticeably.\n\none of these algorithmic changes was the replacement of mean squared error\nwith the cross-entropy family of loss functions. mean squared error was popular in\nthe 1980s and 1990s, but was gradually replaced by cross-entropy losses and the\n\n225\n\n "}, {"Page_number": 241, "text": "chapter 6. deep feedforward networks\n\nprinciple of maximum likelihood as ideas spread between the statistics community\nand the machine learning community. the use of cross-entropy losses greatly\nimproved the performance of models with sigmoid and softmax outputs, which\nhad previously suffered from saturation and slow learning when using the mean\nsquared error loss.\n\nthe other major algorithmic change that has greatly improved the performance\nof feedforward networks was the replacement of sigmoid hidden units with piecewise\nlinear hidden units, such as rectified linear units. rectification using the max{0, z}\nfunction was introduced in early neural network models and dates back at least\nas far as the cognitron and neocognitron (fukushima 1975 1980\n). these early\nmodels did not\u00a0use rectified\u00a0linear units,\u00a0but\u00a0instead applied\u00a0rectification to\nnonlinear functions. despite the early popularity of rectification, rectification was\nlargely replaced by sigmoids in the 1980s, perhaps because sigmoids perform better\nwhen neural networks are very small. as of the early 2000s, rectified linear units\nwere avoided due to a somewhat superstitious belief that activation functions with\nnon-differentiable points must be avoided. this began to change in about 2009.\njarrett\n) observed that \u201cusing a rectifying nonlinearity is the single most\nimportant factor in improving the performance of a recognition system\u201d among\nseveral different factors of neural network architecture design.\n\net al. (\n\n2009\n\n,\n\n,\n\nfor small datasets,\n\njarrett et al. 2009\n\n) observed that using rectifying non-\nlinearities is even more important than learning the weights of the hidden layers.\nrandom weights are sufficient to propagate useful information through a rectified\nlinear network, allowing the classifier layer at the top to learn how to map different\nfeature vectors to class identities.\n\n(\n\nwhen more data is available, learning begins to extract enough useful knowledge\nto exceed the performance of randomly chosen parameters.\nglorot et al. 2011a\n)\nshowed that learning is far easier in deep rectified linear networks than in deep\nnetworks that have curvature or two-sided saturation in their activation functions.\n\n(\n\nglorot et al. 2011a\n\nrectified linear units are also of historical interest because they show that\nneuroscience has continued to\u00a0have an influence on the development of\u00a0deep\nlearning algorithms.\n) motivate rectified linear units from\nbiological considerations. the half-rectifying nonlinearity was intended to capture\nthese properties of biological neurons: 1) for some inputs, biological neurons are\ncompletely inactive. 2) for some inputs, a biological neuron\u2019s output is proportional\nto its input. 3) most of the time, biological neurons operate in the regime where\nthey are inactive (i.e., they should have sparse activations).\n\n(\n\nwhen the modern resurgence of deep learning began in 2006, feedforward\nnetworks continued to have a bad reputation. from about 2006-2012, it was widely\n\n226\n\n "}, {"Page_number": 242, "text": "chapter 6. deep feedforward networks\n\nbelieved that feedforward networks would not perform well unless they were assisted\nby other models, such as probabilistic models. today, it is now known that with the\nright resources and engineering practices, feedforward networks perform very well.\ntoday, gradient-based learning in feedforward networks is used as a tool to develop\nprobabilistic models, such as the variational autoencoder and generative adversarial\nnetworks, described in chapter\n. rather than being viewed as an unreliable\ntechnology that must be supported by other techniques, gradient-based learning in\nfeedforward networks has been viewed since 2012 as a powerful technology that\nmay be applied to many other machine learning tasks. in 2006, the community\nused unsupervised learning to support supervised learning, and now, ironically, it\nis more common to use supervised learning to support unsupervised learning.\n\n20\n\nfeedforward networks continue to have unfulfilled potential. in the future, we\nexpect they will be applied to many more tasks, and that advances in optimization\nalgorithms and model design will improve their performance even further. this\nchapter has primarily described the neural network family of models.\nin the\nsubsequent chapters, we turn to how to use these models\u2014how to regularize and\ntrain them.\n\n227\n\n "}, {"Page_number": 243, "text": "chapter 7\n\nregularization for deep learning\n\na central problem in machine learning is how to make an algorithm that will\nperform well not just on the training data, but also on new inputs. many strategies\nused in machine learning are explicitly designed to reduce the test error, possibly\nat the expense of increased training error. these strategies are known collectively\nas regularization.\u00a0as we will see there are a great many forms of regularization\navailable to the deep learning practitioner.\nin fact,\u00a0developing more effective\nregularization strategies has been one of the major research efforts in the field.\n\n5\n\nchapter\n\nintroduced the basic concepts of generalization, underfitting, overfit-\nting, bias, variance and regularization. if you are not already familiar with these\nnotions, please refer to that chapter before continuing with this one.\n\nin this chapter, we describe regularization in more detail, focusing on regular-\nization strategies for deep models or models that may be used as building blocks\nto form deep models.\n\nsome sections of this chapter deal with standard concepts in machine learning.\nif you are already familiar with these concepts,\u00a0feel free to skip the relevant\nsections. however, most of this chapter is concerned with the extension of these\nbasic concepts to the particular case of neural networks.\n\n5.2.2\n\nin sec.\n\n, we defined regularization as \u201cany modification we make to a\nlearning algorithm that is intended to reduce its generalization error but not\nits training error.\u201d there are many regularization strategies. some put extra\nconstraints\u00a0on a\u00a0machine\u00a0learning\u00a0model, such\u00a0as adding\u00a0restrictions on\u00a0the\nparameter values. some add extra terms in the objective function that can be\nthought of as corresponding to a soft constraint on the parameter values. if chosen\ncarefully, these extra constraints and penalties can lead to improved performance\n\n228\n\n "}, {"Page_number": 244, "text": "chapter 7. regularization for deep learning\n\non the test set. sometimes these constraints and penalties are designed to encode\nspecific kinds of prior knowledge. other times, these constraints and penalties\nare designed to express a generic preference for a simpler model class in order to\npromote generalization. sometimes penalties and constraints are necessary to make\nan underdetermined problem determined. other forms of regularization, known as\nensemble methods, combine multiple hypotheses that explain the training data.\n\nin the context of deep learning, most regularization strategies are based on\nregularizing estimators. regularization of an estimator works by trading increased\nbias for reduced variance. an effective regularizer is one that makes a profitable\ntrade, reducing variance significantly while not overly increasing the bias. when\nwe discussed generalization and overfitting in chapter\n, we focused on three\nsituations, where the model family being trained either (1) excluded the true\ndata generating process\u2014corresponding to underfitting and inducing bias, or (2)\nmatched the true data generating process, or (3) included the generating process\nbut also many other possible generating processes\u2014the overfitting regime where\nvariance rather than bias dominates the estimation error. the goal of regularization\nis to take a model from the third regime into the second regime.\n\n5\n\nin practice, an overly complex model family does not necessarily include the\ntarget function or the true data generating process, or even a close approximation\nof either. we almost never have access to the true data generating process so\nwe can never know for sure if the model family being estimated includes the\ngenerating process or not. however, most applications of deep learning algorithms\nare to domains where the true data generating process is almost certainly outside\nthe model family. deep learning algorithms are typically applied to extremely\ncomplicated domains such as images, audio sequences and text, for which the true\ngeneration process essentially involves simulating the entire universe. to some\nextent, we are always trying to fit a square peg (the data generating process) into\na round hole (our model family).\n\nwhat this means is that controlling the complexity of the model is not a\nsimple matter of finding the model of the right size, with the right number of\nparameters. instead, we might find\u2014and indeed in practical deep learning scenarios,\nwe almost always do find\u2014that the best fitting model (in the sense of minimizing\ngeneralization error) is a large model that has been regularized appropriately.\n\nwe now review several strategies for how to create such a large, deep, regularized\n\nmodel.\n\n229\n\n "}, {"Page_number": 245, "text": "chapter 7. regularization for deep learning\n\n7.1 parameter norm penalties\n\nregularization has been used for decades prior to the advent of deep learning. linear\nmodels such as linear regression and logistic regression allow simple, straightforward,\nand effective regularization strategies.\n\nmany regularization approaches are based on limiting the capacity of models,\nsuch as neural networks, linear regression, or logistic regression, by adding a pa-\nrameter norm penalty \u03c9(\u03b8) to the objective function j . we denote the regularized\nobjective function by \u02dcj:\n\n\u02dcj\n\n( ;\u03b8 x y) =  ( ;\u03b8 x y) + \u03c9( )\u03b8\n\n\u03b1\n\nj\n\n,\n\n,\n\n(7.1)\n\nwhere \u03b1 \u2208 [0,\u221e) is a hyperparameter that weights the relative contribution of\nthe norm penalty term,\nj (x; \u03b8).\nsetting \u03b1 to 0 results in no regularization. larger values of \u03b1 correspond to more\nregularization.\n\n, relative to the standard objective function\n\n\u03c9\n\nwhen our training algorithm minimizes the regularized objective function \u02dcj it\nwill decrease both the original objective j on the training data and some measure\nof the size of the parameters \u03b8 (or some subset of the parameters). different\nchoices for the parameter norm can result in different solutions being preferred.\nin this section, we discuss the effects of the various norms when used as penalties\non the model parameters.\n\n\u03c9\n\n\u03c9\n\nbefore delving into the regularization behavior of different norms, we note that\nfor neural networks, we typically choose to use a parameter norm penalty\nthat\npenalizes only the weights of the affine transformation at each layer and leaves\nthe biases unregularized. the biases typically require less data to fit accurately\nthan the weights.\u00a0each weight specifies how two variables interact.\u00a0fitting the\nweight well requires observing both variables in a variety of conditions. each\nbias controls only a single variable. this means that we do not induce too much\nvariance by leaving the biases unregularized. also, regularizing the bias parameters\ncan introduce a significant amount of underfitting. we therefore use the vector w\nto indicate all of the weights that should be affected by a norm penalty, while the\nvector \u03b8 denotes all of the parameters, including both w and the unregularized\nparameters.\n\nin the context of neural networks, it is sometimes desirable to use a separate\npenalty with a different \u03b1 coefficient for each layer of the network. because it can\nbe expensive to search for the correct value of multiple hyperparameters, it is still\nreasonable to use the same weight decay at all layers just to reduce the search\nspace.\n\n230\n\n "}, {"Page_number": 246, "text": "chapter 7. regularization for deep learning\n\n7.1.1 l2 parameter regularization\n\n5.2.2\n\nwe have already seen, in sec.\n, one of the simplest and most common kinds\nof parameter norm penalty: the l2 parameter norm penalty commonly known as\nweight decay. this regularization strategy drives the weights closer to the origin1\nby adding a regularization term \u03c9(\u03b8) = 1\nin\nother academic communities, l2 regularization is also known as ridge regression or\ntikhonov regularization.\n\n2 to the objective function.\n\n2k kw 2\n\nwe can gain some insight into the behavior of weight decay regularization\nby studying the gradient of the regularized objective function. to simplify the\npresentation, we assume no bias parameter, so \u03b8 is just w. such a model has the\nfollowing total objective function:\n\n\u02dcj\n(\n;w x y) =\n\n,\n\n\u03b1\n2\n\nw>w\n\nwith the corresponding parameter gradient\n\n+ (j\n\nw x y\n\n;\n\n,\n\n),\n\n\u2207w \u02dcj\n;w x y) =  w + \u2207wj\n(\n\n\u03b1\n\n,\n\n(\n;w x y)\n.\n\n,\n\n(7.2)\n\n(7.3)\n\nto take a single gradient step to update the weights, we perform this update:\n\n\u2190 \u2212 (cid:115) \u03b1(\nw\nwritten another way, the update is:\n\nw w\n\n+ \u2207wj\n\n(\n;w x y))\n\n,\n\n.\n\nw\n\n\u2190 \u2212(1\n\nw\n\n(cid:115)\u03b1) \u2212 \u2207(cid:115) wj\n\n(\n;w x y)\n.\n\n,\n\n(7.4)\n\n(7.5)\n\nwe can see that the addition of the weight decay term has modified the learning\nrule to multiplicatively shrink the weight vector by a constant factor on each step,\njust before performing the usual gradient update. this describes what happens in\na single step. but what happens over the entire course of training?\n\nwe will further simplify the analysis by making a quadratic approximation\nto the objective function in the neighborhood of the value of the weights that\nobtains minimal unregularized training cost, w\u2217 = arg minw j(w). if the objective\nfunction is truly quadratic, as in the case of fitting a linear regression model with\nmean squared error, then the approximation is perfect.\n\n\u02c6j\n\n( ) = \n\u03b8\n\nj\n\n(w\u2217) +\n\n1\n2\n\n(w w\u2212 \u2217)> h w w\n( \u2212 \u2217)\n\n(7.6)\n\n1more generally, we could regularize the parameters to be near any specific point in space\nand, surprisingly, still get a regularization effect, but better results will be obtained for a value\ncloser to the true one, with zero being a default value that makes sense when we do not know if\nthe correct value should be positive or negative. since it is far more common to regularize the\nmodel parameters towards zero, we will focus on this special case in our exposition.\n\n231\n\n "}, {"Page_number": 247, "text": "chapter 7. regularization for deep learning\n\nwhere h is the hessian matrix of j with respect to w evaluated at w\u2217. there is\nno first-order term in this quadratic approximation, because w\u2217 is defined to be a\nminimum, where the gradient vanishes. likewise, because w\u2217 is the location of a\nminimum of\n\nis positive semidefinite.\n\n, we can conclude that\n\nh\n\nj\n\nthe minimum of \u02c6j occurs where its gradient\n\n\u2207w \u02c6j(\n\nw h w w\u2212 \u2217)\n) = \n\n(\n\n(7.7)\n\nis equal to .0\n\nto study the effect of weight decay, we modify eq.\n\nby adding the weight\ndecay gradient. we can now solve for the minimum of the regularized version of \u02c6j.\nwe use the variable \u02dcw to represent the location of the minimum.\n\n7.7\n\n\u03b1 \u02dcw h+ ( \u02dcw w\u2212 \u2217) = 0\n\u2217\nh \u03b1i \u02dcw hw= \n( + )\n= ( + \u03b1 )\u22121 hw \u2217.\n\n\u02dcw h i\n\n(7.8)\n\n(7.9)\n\n(7.10)\n\nas \u03b1 approaches 0, the regularized solution \u02dcw approaches w\u2217. but what\nhappens as \u03b1 grows? because h is real and symmetric, we can decompose it\ninto a diagonal matrix \u03bb and an orthonormal basis of eigenvectors, q, such that\nh q q\n\n=  \u03bb >. applying the decomposition to eq.\n\n, we obtain:\n\n7.10\n\n\u02dcw q q\n\n= ( \u03bb > + )\u03b1i \u22121q q\u03bb >w\u2217\n\n=hq\n\n= \n\n( +\u03bb \u03b1 ) >i\u22121\n\ni q\n\nq \u03bb \u03b1i \u22121\u03bbq>w\u2217.\n\n( + )\n\nq q\u03bb > w\u2217\n\n(7.11)\n\n(7.12)\n\n(7.13)\n\nwe see that the effect of weight decay is to rescale w \u2217 along the axes defined by\nthe eigenvectors of h . specifically, the component of w\u2217 that is aligned with the\n\u03bbi\ni-th eigenvector of h is rescaled by a factor of\n\u03bbi+\u03b1. (you may wish to review\nhow this kind of scaling works, first explained in fig.\n\n2.3\n\n).\n\nalong the directions where the eigenvalues of h are relatively large, for example,\nwhere \u03bbi (cid:130) \u03b1, the effect of regularization is relatively small. however, components\nwith \u03bbi (cid:129) \u03b1 will be shrunk to have nearly zero magnitude. this effect is illustrated\nin fig.\n\n.7.1\n\nonly directions along which the parameters contribute significantly to reducing\nthe objective function are preserved relatively intact. in directions that do not\ncontribute to reducing the objective function, a small eigenvalue of the hessian\ntells us that movement in this direction will not significantly increase the gradient.\n\n232\n\n "}, {"Page_number": 248, "text": "chapter 7. regularization for deep learning\n\nw\u2217\n\n2\nw\n\n\u02dcw\n\nw1\n\nfigure 7.1: an illustration of the effect of l2 (or weight decay) regularization on the value\nof the optimal w. the solid ellipses represent contours of equal value of the unregularized\nobjective. the dotted circles represent contours of equal value of the l2 regularizer. at\nthe point \u02dcw, these competing objectives reach an equilibrium. in the first dimension, the\neigenvalue of the hessian of j is small.\u00a0the objective function does not increase much\nwhen moving horizontally away from w \u2217. because the objective function does not express\na strong preference along this direction, the regularizer has a strong effect on this axis.\nthe regularizer pulls w1 close to zero. in the second dimension, the objective function\nis very sensitive to movements away from w\u2217 . the corresponding eigenvalue is large,\nindicating high curvature. as a result, weight decay affects the position of w2 relatively\nlittle.\n\n233\n\n "}, {"Page_number": 249, "text": "chapter 7. regularization for deep learning\n\ncomponents of the weight vector corresponding to such unimportant directions\nare decayed away through the use of the regularization throughout training.\n\nso far we have discussed weight decay in terms of its effect on the optimization\nof an abstract, general, quadratic cost function. how do these effects relate to\nmachine learning in particular? we can find out by studying linear regression, a\nmodel for which the true cost function is quadratic and therefore amenable to the\nsame kind of analysis we have used so far. applying the analysis again, we will\nbe able to obtain a special case of the same results, but with the solution now\nphrased in terms of the training data. for linear regression, the cost function is\nthe sum of squared errors:\n\nwhen we add l2 regularization, the objective function changes to\n\nxw y\u2212 >(\n(\n\n)\n\nxw y\u2212 .\n)\n\nxw y\u2212 >(\n(\n\n)\n\nxw y\u2212\n\n) +\n\n1\n2\n\n\u03b1w> w.\n\nthis changes the normal equations for the solution from\n\nw x= ( > x)\u22121x>y\n\nto\n\n(7.14)\n\n(7.15)\n\n(7.16)\n\nw x= ( >x\n\ni+ \u03b1 )\u22121x >y.\n\n7.16\n\nthe matrix x>x in eq.\n\nis proportional to the covariance matrix\n\nusing l 2 regularization replaces this matrix with(cid:29) x>x\n\n(7.17)\n1\nm x>x.\n7.17\n.\nthe new matrix is the same as the original one, but with the addition of \u03b1 to the\ndiagonal. the diagonal entries of this matrix correspond to the variance of each\ninput feature. we can see that l2 regularization causes the learning algorithm\nto \u201cperceive\u201d the input x as having higher variance, which makes it shrink the\nweights on features whose covariance with the output target is low compared to\nthis added variance.\n\ni+ \u03b1 (cid:30)\u22121\n\nin eq.\n\n7.1.2 l1 regularization\n\nwhile l 2 weight decay is the most common form of weight decay, there are other\nways to penalize the size of the model parameters.\u00a0another option is to use l1\nregularization.\n\nformally, l1 regularization on the model parameter\n\nw\n\nis defined as:\n\n\u03b8\n\n||\n\u03c9( ) = \n\n||w 1 =xi\n\n234\n\n|wi|,\n\n(7.18)\n\n "}, {"Page_number": 250, "text": "chapter 7. regularization for deep learning\n\nthat is, as the sum of absolute values of the individual parameters.2 we will\nnow discuss the effect of l1 regularization on the simple linear regression model,\nwith no bias parameter, that we studied in our analysis of l2 regularization. in\nparticular, we are interested in delineating the differences between l1 and l2 forms\nof regularization. as with l 2 weight decay, l1 weight decay controls the strength\nof the regularization by scaling the penalty\n\u03b1.\n\u03c9\nthus, the regularized objective function \u02dcj\n;w x y) is given by\n(\n\nusing a positive hyperparameter\n\n,\n\n\u02dcj\n\n(\n;w x y) = \n\n,\n\n||\n||w 1 + (\n\u03b1\n\nj w x y,\n\n;\n\n)\n,\n\n(7.19)\n\nwith the corresponding gradient (actually, sub-gradient):\n\n\u2207w \u02dcj\n(\n;w x y) =  sign(\n\n\u03b1\n\n,\n\n) +w \u2207wj\n\n(x y w;\n\n,\n\n)\n\n(7.20)\n\nwhere\n\nsign(\n\n)w\n\nis simply the sign of\n\nw\n\napplied element-wise.\n\n7.20\n\nby inspecting eq.\n\n, we can see immediately that the effect of\n\nl 1 regu-\nlarization is quite different from that of l2 regularization. specifically, we can\nsee that the regularization contribution to the gradient no longer scales linearly\nwith each wi; instead it is a constant factor with a sign equal to sign(w i). one\nconsequence of this form of the gradient is that we will not necessarily see clean\n; w) as we did for l2\nalgebraic solutions to quadratic approximations of j (x y,\nregularization.\n\nour simple linear model has a quadratic cost function that we can represent\nvia its taylor series. alternately, we could imagine that this is a truncated taylor\nseries approximating the cost function of a more sophisticated model. the gradient\nin this setting is given by\n\nwhere, again,\n\nh\n\nis the hessian matrix of with respect to\n\nj\n\nw\n\nevaluated at\n\n\u2207w \u02c6j(\n\nw h w w\u2212 \u2217),\n) = \n\n(\n\n(7.21)\n\nw\u2217.\n\nbecause the l1 penalty does not admit clean algebraic expressions in the case\nof a fully general hessian, we will also make the further simplifying assumption\nthat the hessian is diagonal, h = diag([h1 1, , . . . , hn,n]), where each hi,i > 0.\nthis assumption holds if the data for the linear regression problem has been\npreprocessed to remove all correlation between the input features, which may be\naccomplished using pca.\n\n2as with l 2 regularization, we could regularize the parameters towards a value that is not\nzero, but instead towards some parameter value w ( )o . in that case the l 1 regularization would\nintroduce the term \u03c9( ) = \n\n\u03b8\n\n|| \u2212w w( )o ||1 = p i |wi \u2212 w ( )o\n|.\n\ni\n\n235\n\n "}, {"Page_number": 251, "text": "chapter 7. regularization for deep learning\n\nour quadratic approximation of the l1 regularized objective function decom-\n\nposes into a sum over the parameters:\n\n\u02c6j\n;w x y) =  (w\u2217;\n(\n\nj\n\n,\n\n) +x y, xi (cid:37)1\n\n2\n\nhi,i (wi \u2212 w\u2217i )2 + \u03b1|wi|(cid:38) .\n\n(7.22)\n\nthe problem of minimizing this approximate cost function has an analytical solution\n(for each dimension ), with the following form:\n\ni\n\nwi = sign(w\u2217i ) max(cid:39)|w\u2217i | \u2212\n\n\u03b1\nhi,i\n\n, 0(cid:40) .\n\n(7.23)\n\nconsider the situation where w\u2217i >\n\n0 for all\n\n. there are two possible outcomes:\ni\n\n1.\u00a0w\u2217i \u2264 \u03b1\n\nhi,i\n\n. here the optimal value of wi under the regularized objective is\n) to the\n) is overwhelmed\u2014in direction i\u2014by the l1\n\nsimply wi = 0.\u00a0this occurs because the contribution of j(w ;x y,\nregularized objective \u02dcj (w; x y,\nregularization which pushes the value of w i to zero.\n\n2.\u00a0w\u2217i > \u03b1\nhi,i\n\n, here the regularization does not move the optimal value of wi to\nzero but instead it just shifts it in that direction by a distance equal to \u03b1\n.\nhi,i\n\na similar process happens when w\u2217i < 0, but with the l1 penalty making wi less\nnegative by \u03b1\nhi,i\n\n, or 0.\n\nin comparison to l2 regularization, l 1 regularization results in a solution that\nis more sparse.\u00a0sparsity in this context refers to the fact that some parameters\nhave an optimal value of zero. the sparsity of l1 regularization is a qualitatively\ndifferent behavior than arises with l2 regularization. eq.\n\u02dcw\nfor l2 regularization. if we revisit that equation using the assumption of a diagonal\nhessian h that we introduced for our analysis of l1 regularization, we find that\n\u02dcwi = hi,i\nhi,i+\u03b1w\u2217i . if w\u2217i was nonzero, then \u02dcwi remains nonzero. this demonstrates\nthat l2 regularization does not cause the parameters to become sparse, while l1\nregularization may do so for large enough .\u03b1\n\ngave the solution\n\n7.13\n\nthe sparsity property induced by l1 regularization has been used extensively\nas a feature selection mechanism. feature selection simplifies a machine learning\nproblem by choosing which subset of the available features should be used. in\nparticular, the well known lasso (\n) (least absolute shrinkage and\nselection operator) model integrates an l1 penalty with a linear model and a least\nsquares cost function. the l1 penalty causes a subset of the weights to become\nzero, suggesting that the corresponding features may safely be discarded.\n\ntibshirani 1995\n\n,\n\n236\n\n "}, {"Page_number": 252, "text": "chapter 7. regularization for deep learning\n\n5.6.1\n\nin sec.\n\n, we saw that many regularization strategies can be interpreted as\nmap bayesian inference, and that in particular, l2 regularization is equivalent\nto map bayesian inference with a gaussian prior on the weights.\u00a0for l1 regu-\n\nlarization, the penalty \u03b1\u03c9(w) = \u03b1pi |w i| used to regularize a cost function is\n\nequivalent to the log-prior term that is maximized by map bayesian inference\nwhen the prior is an isotropic laplace distribution (eq.\n\n) over\n\n3.26\n\n:\nw\n\nlog (\n\n) =\n\np w xi\n\nlog laplace(wi; 0,\n\n1\n\u03b1\n\n) = \u2212\u03b1xi\n\n|wi|\n\n+ log \u03b1 log 2\n\n\u2212\n\n(7.24)\n\nfrom the point of view of learning via minimization with respect to w, we can\nignore the\n\nterms because they do not depend on\n\nlog 2\n\nlog\n\nw\n\n.\n\n\u03b1 \u2212\n\n7.2 norm penalties as constrained optimization\n\nconsider the cost function regularized by a parameter norm penalty:\n\n\u02dcj\n\n( ;\u03b8 x y) =  ( ;\u03b8 x y) + \u03c9( )\u03b8\n.\n\n\u03b1\n\nj\n\n,\n\n,\n\n(7.25)\n\n4.4\n\nrecall from sec.\n\nthat we can minimize a function subject to constraints by\nconstructing a generalized lagrange function, consisting of the original objective\nfunction plus a set of penalties. each penalty is a product between a coefficient,\ncalled a karush\u2013kuhn\u2013tucker (kkt) multiplier, and a function representing\nwhether the constraint is satisfied. if we wanted to constrain \u03c9( \u03b8) to be less than\nsome constant\n\n, we could construct a generalized lagrange function\n\nk\n\nl\n(\n\u03b8, \u03b1 x y,\n\n;\n\n) =  ( ;\n\nj \u03b8 x y,\n\n) + (\u03c9( )\n\u03b8\n\n\u03b1\n\nk .)\n\n\u2212\n\nthe solution to the constrained problem is given by\n\n\u03b8\u2217 = arg min\n\n\u03b8\n\nmax\n\n\u03b1,\u03b1\u22650l(\n\n)\u03b8, \u03b1 .\n\n(7.26)\n\n(7.27)\n\n4.5\n\n4.4\n\nas described in sec.\n\n, solving this problem requires modifying both\n\nprovides a worked example of linear regression with an\n\n\u03b8 and\nl2 constraint.\n\u03b1. sec.\nmany different procedures are possible\u2014some may use gradient descent, while\nothers may use analytical solutions for where the gradient is zero\u2014but in all\nprocedures \u03b1 must increase whenever \u03c9(\u03b8 ) > k and decrease whenever \u03c9( \u03b8) < k .\nall positive \u03b1 encourage \u03c9(\u03b8) to shrink. the optimal value \u03b1\u2217 will encourage \u03c9(\u03b8)\nto shrink, but not so strongly to make\n\nbecome less than .\nk\n\n\u03c9( )\u03b8\n\n237\n\n "}, {"Page_number": 253, "text": "chapter 7. regularization for deep learning\n\nto gain some insight into the effect of the constraint, we can fix \u03b1\u2217 and view\n\nthe problem as just a function of\n\n:\u03b8\n\n\u03b8\u2217 = arg min\n\n\u03b8\n\nl(\u03b8, \u03b1\u2217) = arg min\n\n\u03b8\n\nj\n\n( ;\u03b8 x y) + \u2217\u03c9( )\u03b8 .\n\n\u03b1\n\n,\n\n(7.28)\n\n\u03c9\n\nis the\n\nthis is exactly the same as the regularized training problem of minimizing \u02dcj.\nwe can thus think of a parameter norm penalty as imposing a constraint on the\nl2 norm, then the weights are constrained to lie in a l2 ball.\nweights. if\n\u03c9\nl1 norm, then the weights are constrained to lie in a region of limited\nif\nis the\nl1 norm. usually we do not know the size of the constraint region that we impose\nby using weight decay with coefficient \u03b1\u2217 because the value of \u03b1\u2217 does not directly\ntell us the value of k. in principle, one can solve for k , but the relationship between\nk and \u03b1\u2217 depends on the form of j . while we do not know the exact size of the\nconstraint region, we can control it roughly by increasing or decreasing \u03b1 in order\nto grow or shrink the constraint region. larger \u03b1 will result in a smaller constraint\nregion. smaller will result in a larger constraint region.\n\n\u03b1\n\n4.4\n\nsometimes we may wish to use explicit constraints rather than penalties. as\ndescribed in sec.\n, we can modify algorithms such as stochastic gradient descent\nto take a step downhill on j (\u03b8) and then project \u03b8 back to the nearest point\nthat satisfies \u03c9(\u03b8 ) < k. this can be useful if we have an idea of what value of k\nis appropriate and do not want to spend time searching for the value of \u03b1 that\ncorresponds to this\n\n.k\n\nanother reason to use explicit constraints and reprojection rather than enforcing\nconstraints with penalties is that penalties can cause non-convex optimization\nprocedures to get stuck in local minima corresponding to small \u03b8. when training\nneural networks, this usually manifests as neural networks that train with several\n\u201cdead units.\u201d these are units that do not contribute much to the behavior of the\nfunction learned by the network because the weights going into or out of them are\nall very small.\u00a0when training with a penalty on the norm of the weights, these\nconfigurations can be locally optimal, even if it is possible to significantly reduce\nj by making the weights larger. explicit constraints implemented by re-projection\ncan work much better in these cases because they do not encourage the weights\nto approach the origin. explicit constraints implemented by re-projection only\nhave an effect when the weights become large and attempt to leave the constraint\nregion.\n\nfinally, explicit constraints with reprojection can be useful because they impose\nsome stability on the optimization procedure. when using high learning rates, it\nis possible to encounter a positive feedback loop in which large weights induce\nlarge gradients which then induce a large update to the weights. if these updates\n\n238\n\n "}, {"Page_number": 254, "text": "chapter 7. regularization for deep learning\n\nconsistently increase the size of the weights, then \u03b8 rapidly moves away from\nthe origin until numerical overflow occurs. explicit constraints with reprojection\nallow us to terminate this feedback loop after the weights have reached a certain\nmagnitude. hinton\n) recommend using constraints combined with a\nhigh learning rate to allow rapid exploration of parameter space while maintaining\nsome stability.\n\net al. (\n\n2012c\n\n(\n\n2012c\n\net al. (\n\nin particular, hinton\n\n) recommend constraining the norm of each\ncolumn of the weight matrix of a neural net layer, rather than constraining the\nfrobenius norm of the entire weight matrix, a strategy introduced by srebro and\nshraibman 2005\n). constraining the norm of each column separately prevents any\none hidden unit from having very large weights. if we converted this constraint\ninto a penalty in a lagrange function, it would be similar to l2 weight decay\nbut with a separate kkt multiplier for the weights of each hidden unit. each of\nthese kkt multipliers would be dynamically updated separately to make each\nhidden unit obey the constraint.\u00a0in practice, column norm limitation is always\nimplemented as an explicit constraint with reprojection.\n\n7.3 regularization and under-constrained problems\n\nin some cases, regularization is necessary for machine learning problems to be\nproperly defined. many linear models in machine learning, including linear re-\ngression and pca, depend on inverting the matrix x>x . this is not possible\nwhenever x >x is singular. this matrix can be singular whenever the data truly\nhas no variance in some direction, or when there are fewer examples (rows of x)\nthan input features (columns of x).\u00a0in this case, many forms of regularization\ncorrespond to inverting x> x + \u03b1i instead. this regularized matrix is guaranteed\nto be invertible.\n\nthese linear problems have closed form solutions when the relevant matrix\nis invertible. it is also possible for a problem with no closed form solution to be\nunderdetermined. an example is logistic regression applied to a problem where\nthe classes are linearly separable. if a weight vector w is able to achieve perfect\nclassification, then 2w will also achieve perfect classification and higher likelihood.\nan iterative optimization procedure like stochastic gradient descent will continually\nincrease the magnitude of w and, in theory, will never halt. in practice, a numerical\nimplementation of gradient descent will eventually reach sufficiently large weights\nto cause numerical overflow, at which point its behavior will depend on how the\nprogrammer has decided to handle values that are not real numbers.\n\nmost forms of regularization are able to guarantee the convergence of iterative\n\n239\n\n "}, {"Page_number": 255, "text": "chapter 7. regularization for deep learning\n\nmethods applied to underdetermined problems.\u00a0for example, weight decay will\ncause gradient descent to quit increasing the magnitude of the weights when the\nslope of the likelihood is equal to the weight decay coefficient.\n\nthe idea of using regularization to solve underdetermined problems extends\nbeyond machine learning. the same idea is useful for several basic linear algebra\nproblems.\n\nas we saw in sec.\n\n, we can solve underdetermined linear equations using\nthe moore-penrose pseudoinverse. recall that one definition of the pseudoinverse\nx + of a matrix\n\nisx\n\n2.9\n\n(x> x\n\nx+ = lim\n\u03b1&0\nas performing linear regression with weight decay.\nwe can now recognize eq.\nspecifically, eq.\nas the regularization coefficient shrinks\nto zero. we can thus interpret the pseudoinverse as stabilizing underdetermined\nproblems using regularization.\n\nis the limit of eq.\n\ni+ \u03b1 )\u22121x>.\n\n(7.29)\n\n7.29\n\n7.17\n\n7.29\n\n7.4 dataset augmentation\n\nthe best way to make a machine learning model generalize better is to train it on\nmore data. of course, in practice, the amount of data we have is limited. one way\nto get around this problem is to create fake data and add it to the training set.\nfor some machine learning tasks, it is reasonably straightforward to create new\nfake data.\n\nthis approach is easiest for classification. a classifier needs to take a compli-\ncated, high dimensional input x and summarize it with a single category identity y.\nthis means that the main task facing a classifier is to be invariant to a wide variety\nof transformations. we can generate new (x, y) pairs easily just by transforming\nthe\n\ninputs in our training set.\n\nx\n\nthis approach is not as readily applicable to many other tasks. for example, it\nis difficult to generate new fake data for a density estimation task unless we have\nalready solved the density estimation problem.\n\ndataset augmentation has been a particularly effective technique for a specific\nclassification problem: object recognition. images are high dimensional and include\nan enormous variety of factors of variation, many of which can be easily simulated.\noperations like translating the training images a few pixels in each direction can\noften greatly improve generalization, even if the model has already been designed to\nbe partially translation invariant by using the convolution and pooling techniques\n\n240\n\n "}, {"Page_number": 256, "text": "chapter 7. regularization for deep learning\n\ndescribed in chapter\nscaling the image have also proven quite effective.\n\n9\n\n. many other operations such as rotating the image or\n\none must be careful not to apply transformations that would change the correct\nclass. for example, optical character recognition tasks require recognizing the\ndifference between \u2019b\u2019 and \u2019d\u2019 and the difference between \u20196\u2019 and \u20199\u2019, so horizontal\nflips and 180\u25e6 rotations are not appropriate ways of augmenting datasets for these\ntasks.\n\nthere are also transformations that we would like our classifiers to be invariant\nto, but which are not easy to perform. for example, out-of-plane rotation can not\nbe implemented as a simple geometric operation on the input pixels.\n\ndataset augmentation is effect for speech recognition tasks as well (jaitly and\n\nhinton 2013\n\n,\n\n).\n\n,\n\n,\n\ninjecting noise in the input to a neural network (sietsma and dow 1991\n)\ncan also be seen as a form of data augmentation. for many classification and\neven some regression tasks, the task should still be possible to solve even if small\nrandom noise is added to the input. neural networks prove not to be very robust\nto noise, however (tang and eliasmith 2010\n). one way to improve the robustness\nof neural networks is simply to train them with random noise applied to their\ninputs. input noise injection is part of some unsupervised learning algorithms such\nas the denoising autoencoder (vincent\n). noise injection also works\nwhen the noise is applied to the hidden units, which can be seen as doing dataset\naugmentation at multiple levels of abstraction. poole\n) recently showed\nthat this approach can be highly effective provided that the magnitude of the\nnoise is carefully tuned. dropout, a powerful regularization strategy that will be\ndescribed in sec.\n, can be seen as a process of constructing new inputs by\nmultiplying by noise.\n\net al. (\n\net al.,\n\n2008\n\n2014\n\n7.12\n\nwhen comparing machine learning benchmark results, it is important to take\nthe effect of dataset augmentation into account. often, hand-designed dataset\naugmentation schemes can dramatically reduce the generalization error of a machine\nlearning technique. to compare the performance of one machine learning algorithm\nto another, it is necessary to perform controlled experiments. when comparing\nmachine learning algorithm a and machine learning algorithm b, it is necessary\nto make sure that both algorithms were evaluated using the same hand-designed\ndataset augmentation schemes. suppose that algorithm a performs poorly with\nno dataset augmentation and algorithm b performs well when combined with\nnumerous synthetic transformations of the input. in such a case it is likely the\nsynthetic transformations caused the improved performance, rather than the use\nof machine learning algorithm b. sometimes deciding whether an experiment\n\n241\n\n "}, {"Page_number": 257, "text": "chapter 7. regularization for deep learning\n\nhas been properly controlled requires subjective judgment. for example, machine\nlearning algorithms that inject noise into the input are performing a form of dataset\naugmentation. usually, operations that are generally applicable (such as adding\ngaussian noise to the input) are considered part of the machine learning algorithm,\nwhile operations that are specific to one application domain (such as randomly\ncropping an image) are considered to be separate pre-processing steps.\n\n7.5 noise robustness\n\n7.4\n\nsec.\nhas motivated the use of noise applied to the inputs as a dataset aug-\nmentation strategy. for some models, the addition of noise with infinitesimal\nvariance at the input of the model is equivalent to imposing a penalty on the\nnorm of the weights (\n, ). in the general case, it is important to\nremember that noise injection can be much more powerful than simply shrinking\nthe parameters, especially when the noise is added to the hidden units. noise\napplied to the hidden units is such an important topic as to merit its own separate\ndiscussion; the dropout algorithm described in sec.\nis the main development\nof that approach.\n\nbishop 1995a b\n\n7.12\n\n,\n\nanother way that noise has been used in the service of regularizing models\nis by adding it to the weights. this technique has been used primarily in the\ncontext of recurrent neural networks (\n).\u00a0this can\nbe interpreted as a stochastic implementation of a bayesian inference over the\nweights.\u00a0the bayesian treatment of learning would consider the model weights\nto be uncertain and representable via a probability distribution that reflects this\nuncertainty. adding noise to the weights is a practical, stochastic way to reflect\nthis uncertainty (graves 2011\n\njim et al. 1996 graves 2011\n\n).\n\n,\n\n;\n\n,\n\n,\n\nthis can also be interpreted as equivalent (under some assumptions) to a\nmore traditional form of regularization. adding noise to the weights has been\nshown to be an effective regularization strategy in the context of recurrent neural\nnetworks (\n). in the following, we will present an\nanalysis of the effect of weight noise on a standard feedforward neural network (as\nintroduced in chapter\n\njim et al. 1996 graves 2011\n\n).6\n\n,\n\n;\n\n,\n\nwe study the regression setting, where we wish to train a function \u02c6y(x) that\nmaps a set of features x to a scalar using the least-squares cost function between\nthe model predictions \u02c6y( )x and the true values\n\n:y\n\nthe training set consists of\n\nm\n\nlabeled examples\n\n, . . . , x (\n\n(\n\n)m , y(\n\nj = e\n\np x,y(\n\n)(cid:31)(\u02c6y( )\n\nx \u2212 y 2(cid:32) .\n\n)\n{(x (1), y (1))\n\n242\n\n(7.30)\n)m )}.\n\n "}, {"Page_number": 258, "text": "chapter 7. regularization for deep learning\n\nwe now assume that with each input presentation we also include a random\nperturbation (cid:115)w \u223c n ((cid:115); 0, \u03b7i) of the network weights. let us imagine that we\nhave a standard l-layer mlp. we denote the perturbed model as \u02c6y (cid:32)w (x). despite\nthe injection of noise, we are still interested in minimizing the squared error of the\noutput of the network. the objective function thus becomes:\n\n\u02dcjw = ep\n\n= ep\n\nx \u2212 y 2i\n\n)\n\n,y,\n\n(x (cid:32)w )h(\u02c6y (cid:32)w ( )\n(x (cid:32)w )(cid:31)\u02c6y 2\n\n(cid:32)w ( )\n\n,y,\n\nx \u2212 yy (cid:32)w ( ) +x\n\n2 \u02c6\n\ny2(cid:32) .\n\n(7.31)\n\n(7.32)\n\n,y(x )(cid:31)k\u2207w \u02c6y( )x k2(cid:32). this form of regularization encourages the parameters to\n\nfor small \u03b7, the minimization of j with added weight noise (with covariance\n\u03b7i) is equivalent to minimization of j with an additional regularization\u00a0term:\n\u03b7e\np\ngo to regions of parameter space where small perturbations of the weights have\na relatively small influence on the output. in other words, it pushes the model\ninto regions where the model is relatively insensitive to small variations in the\nweights, finding points that are not merely minima, but minima surrounded by\nflat regions (hochreiter and schmidhuber 1995\n). in the simplified case of linear\nregression (where, for instance, \u02c6y(x) = w>x +b ), this regularization term collapses\n\ninto \u03b7ep( )x (cid:31)k kx 2(cid:32), which is not a function of parameters and therefore does not\n\ncontribute to the gradient of \u02dcjw with respect to the model parameters.\n\n,\n\n7.5.1\n\ninjecting noise at the output targets\n\nmost datasets have some amount of mistakes in the y labels. it can be harmful\nto maximize log p(y | x) when y is a mistake. one way to prevent this is to\nexplicitly model the noise on the labels. for example, we can assume that for some\nsmall constant (cid:115), the training set label y is correct with probability 1 \u2212 (cid:115), and\notherwise any of the other possible labels might be correct.\u00a0this assumption is\neasy to incorporate into the cost function analytically, rather than by explicitly\ndrawing noise samples.\u00a0for example, label smoothing regularizes a model based\non a softmax with k output values by replacing the hard\nclassification\ntargets with targets of (cid:115)\nk (cid:115), respectively. the standard cross-entropy\nloss may then be used with these soft targets. maximum likelihood learning with a\nsoftmax classifier and hard targets may actually never converge\u2014the softmax can\nnever predict a probability of exactly\nor exactly , so it will continue to learn\nlarger and larger weights, making more extreme predictions forever. it is possible\nto prevent this scenario using other regularization strategies like weight decay.\nlabel smoothing has the advantage of preventing the pursuit of hard probabilities\nwithout discouraging correct classification. this strategy has been used since\n\nk and 1 \u2212 k\u22121\n\nand\n\n0\n\n1\n\n0\n\n1\n\n243\n\n "}, {"Page_number": 259, "text": "chapter 7. regularization for deep learning\n\nthe 1980s and continues to be featured prominently in modern neural networks\n(\nszegedy et al. 2015\n\n).\n\n,\n\n7.6 semi-supervised learning\n\nin the paradigm of semi-supervised learning, both unlabeled examples from p (x)\nand labeled examples from p(x y,\n) or predict y from\nx.\n\n) are used to estimate p (y x|\n\nin the context of deep learning, semi-supervised learning usually refers to\nlearning a representation h = f(x). the goal is to learn a representation so\nthat examples from the same class have similar representations. unsupervised\nlearning can provide useful cues for how to group examples in representation\nspace. examples that cluster tightly in the input space should be mapped to\nsimilar representations. a linear classifier in the new space may achieve better\ngeneralization in many cases (belkin and niyogi 2002 chapelle\n). a\nlong-standing variant of this approach is the application of principal components\nanalysis as a pre-processing step before applying a classifier (on the projected\ndata).\n\net al.,\n\n2003\n\n,\n\n;\n\n) shares parameters with a discriminative model of p(y x|\n\ninstead of having separate unsupervised and supervised components in the\nmodel, one can construct models in which a generative model of either p (x) or\n). one can\np(x y,\nthen trade-off the supervised criterion \u2212 log p (y x|\n) with the unsupervised or\ngenerative one (such as \u2212 log p(x) or \u2212 log p(x y,\n)). the generative criterion then\nexpresses a particular form of prior belief about the solution to the supervised\nlearning problem (\np(x) is\nlasserre et al. 2006\nconnected to the structure of p (y x|\n) in a way that is captured by the shared\nparametrization. by controlling how much of the generative criterion is included\nin the total criterion, one can find a better trade-off than with a purely generative\nor a purely discriminative training criterion (\nlasserre et al. 2006 larochelle and\nbengio 2008\n\n), namely that the structure of\n\n).\n\n,\n\n,\n\n,\n\n;\n\nin the context of scarcity of labeled data (and abundance of unlabeled data),\ndeep architectures have shown promise as well. salakhutdinov and hinton 2008\n)\ndescribe a method for learning the kernel function of a kernel machine used for\nregression, in which the usage of unlabeled examples for modeling p (x) improves\np (\n\nquite significantly.\n\n(\n\nchapelle et al. 2006\n\n(\n\n) for more information about semi-supervised learning.\n\ny x|\n)\nsee\n\n244\n\n "}, {"Page_number": 260, "text": "chapter 7. regularization for deep learning\n\n7.7 multi-task learning\n\n,\n\ncaruana 1993\n\nmulti-task learning (\n) is a way to improve generalization by pooling\nthe examples (which can be seen as soft constraints imposed on the parameters)\narising out of several tasks.\u00a0in the same way that additional training examples\nput more pressure on the parameters of the model towards values that generalize\nwell, when part of a model is shared across tasks, that part of the model is more\nconstrained towards good values (assuming the sharing is justified), often yielding\nbetter generalization.\n\ny(1)y(1)\n\ny(2)y(2)\n\nh(1)h(1)\n\nh(2)h(2)\n\nh(3)h(3)\n\nh(shared)\nh(shared)\n\nxx\n\nfigure 7.2: multi-task learning can be cast in several ways in deep learning frameworks\nand this figure illustrates the common situation where the tasks share a common input but\ninvolve different target random variables. the lower layers of a deep network (whether it\nis supervised and feedforward or includes a generative component with downward arrows)\ncan be shared across such tasks, while task-specific parameters (associated respectively\nwith the weights into and from h(1) and h(2)) can be learned on top of those yielding a\nshared representation h (shared). the underlying assumption is that there exists a common\npool of factors that explain the variations in the input x , while each task is associated\nwith a subset of these factors. in this example, it is additionally assumed that top-level\nhidden units h(1) and h(2) are specialized to each task (respectively predicting y(1) and\ny (2)) while some intermediate-level representation h(shared) is shared across all tasks. in\nthe unsupervised learning context, it makes sense for some of the top-level factors to be\nassociated with none of the output tasks (h(3)): these are the factors that explain some of\nthe input variations but are not relevant for predicting y(1) or y(2).\n\n7.2\n\nfig.\n\nillustrates a very common form of multi-task learning, in which different\nsupervised tasks (predicting y( )i given x) share the same input x, as well as some\nintermediate-level representation h(shared) capturing a common pool of factors. the\n\n245\n\n "}, {"Page_number": 261, "text": "chapter 7. regularization for deep learning\n\nmodel can generally be divided into two kinds of parts and associated parameters:\n\n1. task-specific parameters (which only benefit from the examples of their task\nto achieve good generalization). these are the upper layers of the neural\nnetwork in fig.\n\n.7.2\n\n2. generic parameters, shared across all the tasks (which benefit from the\npooled data of all the tasks). these are the lower layers of the neural network\nin fig.\n\n.7.2\n\n) can be\nimproved generalization and generalization error bounds (\nachieved because of the shared parameters, for which statistical strength can be\ngreatly improved (in proportion with the increased number of examples for the\nshared parameters, compared to the scenario of single-task models). of course this\nwill happen only if some assumptions about the statistical relationship between\nthe different tasks are valid, meaning that there is something shared across some\nof the tasks.\n\nbaxter 1995\n\n,\n\nfrom the point of view of deep learning, the underlying prior belief is the\nfollowing: among the factors that explain the variations observed in the\ndata associated with the different tasks, some are shared across two or\nmore tasks.\n\n7.8 early stopping\n\nwhen training large models with sufficient representational capacity to overfit\nthe task, we often observe that training error decreases steadily over time, but\nvalidation set error begins to rise again. see fig.\nfor an example of this behavior.\nthis behavior occurs very reliably.\n\n7.3\n\nthis means we can obtain a model with better validation set error (and thus,\nhopefully better test set error) by returning to the parameter setting at the point\nin time with the lowest validation set error. instead of running our optimization\nalgorithm until we reach a (local) minimum of validation error, we run it until the\nerror on the validation set has not improved for some amount of time. every time\nthe error on the validation set improves, we store a copy of the model parameters.\nwhen the training algorithm terminates, we return these parameters, rather than\nthe latest parameters. this procedure is specified more formally in algorithm .7.1\n\nthis strategy is known as early stopping. it is probably the most commonly\nused form of regularization in deep learning. its popularity is due both to its\neffectiveness and its simplicity.\n\n246\n\n "}, {"Page_number": 262, "text": "chapter 7. regularization for deep learning\n\n)\nd\no\no\nh\n\ni\nl\ne\nk\ni\nl\n\u00a0\ng\no\nl\n\u00a0\ne\nv\ni\nt\na\ng\ne\nn\n(\n\u00a0\ns\ns\no\nl\n\n0.20\n\n0.15\n\n0.10\n\n0.05\n\n0.00\n\n0\n\nlearning\u00a0curves\n\ntraining\u00a0set\u00a0loss\nvalidation\u00a0set\u00a0loss\n\n50\n\n100\n\n150\n\n200\n\n250\n\ntime\u00a0(epochs)\n\nfigure 7.3: learning curves showing how the negative log-likelihood loss changes over\ntime (indicated as number of training iterations over the dataset, or epochs). in this\nexample, we train a maxout network on mnist. observe that the training objective\ndecreases consistently over time, but the validation set average loss eventually begins to\nincrease again, forming an asymmetric u-shaped curve.\n\n5.3\n\n7.3\n\none way to think of early stopping is as a very efficient hyperparameter selection\nalgorithm. in this view, the number of training steps is just another hyperparameter.\nwe can see in fig.\nthat this hyperparameter has a u-shaped validation set\nperformance curve. most hyperparameters that control model capacity have such a\nu-shaped validation set performance curve, as illustrated in fig.\n. in the case of\nearly stopping, we are controlling the effective capacity of the model by determining\nhow many steps it can take to fit the training set. most hyperparameters must be\nchosen using an expensive guess and check process, where we set a hyperparameter\nat the start of training, then run training for several steps to see its effect. the\n\u201ctraining time\u201d\u00a0hyperparameter is unique in that by definition a single run of\ntraining tries out many values of the hyperparameter. the only significant cost\nto choosing this hyperparameter automatically via early stopping is running the\nvalidation set evaluation periodically during training.\nideally, this is done in\nparallel to the training process on a separate machine, separate cpu, or separate\ngpu from the main training process. if such resources are not available, then the\ncost of these periodic evaluations may be reduced by using a validation set that is\nsmall compared to the training set or by evaluating the validation set error less\nfrequently and obtaining a lower resolution estimate of the optimal training time.\n\nan additional cost to early stopping is the need to maintain a copy of the\nbest parameters. this cost is generally negligible, because it is acceptable to store\nthese parameters in a slower and larger form of memory (for example, training in\n\n247\n\n "}, {"Page_number": 263, "text": "chapter 7. regularization for deep learning\n\ngpu memory, but storing the optimal parameters in host memory or on a disk\ndrive). since the best parameters are written to infrequently and never read during\ntraining, these occasional slow writes have little effect on the total training time.\n\nearly stopping is a very unobtrusive form of regularization, in that it requires\nalmost no change in the underlying training procedure, the objective function,\nor the set of allowable parameter values. this means that it is easy to use early\nstopping without damaging the learning dynamics. this is in contrast to weight\ndecay, where one must be careful not to use too much weight decay and trap the\nnetwork in a bad local minimum corresponding to a solution with pathologically\nsmall weights.\n\nearly stopping may be used either alone or in conjunction with other regulariza-\ntion strategies. even when using regularization strategies that modify the objective\nfunction to encourage better generalization, it is rare for the best generalization to\noccur at a local minimum of the training objective.\n\nearly stopping requires a validation set, which means some training data is not\nfed to the model. to best exploit this extra data, one can perform extra training\nafter the initial training with early stopping has completed. in the second, extra\ntraining step, all of the training data is included. there are two basic strategies\none can use for this second training procedure.\n\n7.2\n\none strategy (algorithm ) is to initialize the model again and retrain on all\nof the data. in this second training pass, we train for the same number of steps as\nthe early stopping procedure determined was optimal in the first pass. there are\nsome subtleties associated with this procedure. for example, there is not a good\nway of knowing whether to retrain for the same number of parameter updates or\nthe same number of passes through the dataset. on the second round of training,\neach pass through the dataset will require more parameter updates because the\ntraining set is bigger.\n\nanother strategy for using all of the data is to keep the parameters obtained\nfrom the first round of training and then continue training but now using all of\nthe data. at this stage, we now no longer have a guide for when to stop in terms\nof a number of steps.\u00a0instead, we can monitor the average loss function on the\nvalidation set, and continue training until it falls below the value of the training\nset objective at which the early stopping procedure halted. this strategy avoids\nthe high cost of retraining the model from scratch, but is not as well-behaved. for\nexample, there is not any guarantee that the objective on the validation set will\never reach the target value, so this strategy is not even guaranteed to terminate.\nthis procedure is presented more formally in algorithm .7.3\n\nearly stopping is also useful because it reduces the computational cost of the\n\n248\n\n "}, {"Page_number": 264, "text": "chapter 7. regularization for deep learning\n\nw\u2217\n\nw\u2217\n\n2\nw\n\n\u02dcw\n\n2\nw\n\n\u02dcw\n\nw1\n\nw 1\n\nfigure 7.4: an illustration of the effect of early stopping. (left) the solid contour\nlines indicate the contours of the negative log-likelihood. the dashed line indicates the\ntrajectory taken by sgd beginning from the origin. rather than stopping at the point\nw\u2217 that minimizes the cost, early stopping results in the trajectory stopping at an earlier\npoint \u02dcw.\nl2 regularization for comparison. the\ndashed circles indicate the contours of the l2 penalty, which causes the minimum of the\ntotal cost to lie nearer the origin than the minimum of the unregularized cost.\n\nan illustration of the effect of\n\n(right)\n\ntraining procedure. besides the obvious reduction in cost due to limiting the number\nof training iterations, it also has the benefit of providing regularization without\nrequiring the addition of penalty terms to the cost function or the computation of\nthe gradients of such additional terms.\n\nis\n\n) and\n\nsj\u00f6berg and ljung 1995\n\nhow early stopping acts as a regularizer: so far we have stated that early\nstopping\na regularization strategy, but we have supported this claim only by\nshowing learning curves where the validation set error has a u-shaped curve. what\nis the actual mechanism by which early stopping regularizes the model? bishop\n(\n) argued that early stopping has the effect of\n1995a\nrestricting the optimization procedure to a relatively small volume of parameter\nspace in the neighborhood of the initial parameter value \u03b8 o. more specifically,\nimagine taking \u03c4 optimization steps (corresponding to \u03c4 training iterations) and\nwith learning rate (cid:115). we can view the product (cid:115)\u03c4 as a measure of effective capacity.\nassuming the gradient is bounded, restricting both the number of iterations and\nthe learning rate limits the volume of parameter space reachable from \u03b8o. in this\nsense, (cid:115)\u03c4 behaves as if it were the reciprocal of the coefficient used for weight decay.\n\n(\n\nindeed, we can show how\u2014in the case of a simple linear model with a quadratic\nerror function and simple gradient descent\u2014early stopping is equivalent to l2\n\n249\n\n "}, {"Page_number": 265, "text": "chapter 7. regularization for deep learning\n\nregularization.\n\nin order to compare with classical l2 regularization, we examine a simple\nsetting where the only parameters are linear weights (\u03b8 = w). we can model\nthe cost function j with a quadratic approximation in the neighborhood of the\nempirically optimal value of the weights w\u2217:\n\n\u02c6j\n\n( ) = \n\u03b8\n\nj\n\n(w\u2217) +\n\n1\n2\n\n(w w\u2212 \u2217)>h w w\n\n( \u2212 \u2217),\n\n(7.33)\n\nwhere h is the hessian matrix of j with respect to w evaluated at w\u2217. given the\nassumption that w\u2217 is a minimum of j (w), we know that h is positive semidefinite.\nunder a local taylor series approximation, the gradient is given by:\n\n\u2207w \u02c6j(\n\nw h w w\u2212 \u2217).\n) = \n\n(\n\n(7.34)\n\nwe are going to study the trajectory followed by the parameter vector during\ntraining. for simplicity, let us set the initial parameter vector to the origin,3 that\nis w(0) = 0. let us suppose that we update the parameters via gradient descent:\n\n1)\n\nw( )\u03c4 = w (\n= w (\nw( )\u03c4 \u2212 w\u2217 = (\n\n\u03c4\u2212 \u2212 \u2207(cid:115) wj(w(\n\u03c4\u2212 \u2212 (cid:115)h w( (\ni h\u2212 (cid:115)\n\nw (\n\n)(\n\n1)\n\n1)\n\n\u03c4\u2212 \u2212 w\u2217)\n\n\u03c4\u2212 )\n1)\n1)\n\n\u03c4\u2212 \u2212 w\u2217 )\n\n(7.35)\n\n(7.36)\n\n(7.37)\n\nlet us now rewrite this expression in the space of the eigenvectors of h , exploiting\nthe eigendecomposition of h: h = q q\u03bb > , where \u03bb is a diagonal matrix and q\nis an orthonormal basis of eigenvectors.\n\nw ( )\u03c4 \u2212 w\u2217 = (i q q\n\n1)\n\n\u2212 (cid:115) \u03bb >)(w (\ni \u2212 (cid:115)\u03bb q>(w(\n\n\u03c4\u2212 \u2212 w\u2217)\n\u03c4\u2212 \u2212 w\u2217)\n\n1)\n\n)\n\nq>(w( )\u03c4 \u2212 w\u2217) = (\n\n(7.38)\n\n(7.39)\n\nassuming that w(0) = 0 and that (cid:115) is chosen to be small enough to guarantee\n|1 \u2212 (cid:115)\u03bbi| < 1, the parameter trajectory during training after \u03c4 parameter updates\nis as follows:\n\nnow, the expression for q> \u02dcw in eq.\nas:\n\nq >w( )\u03c4 = [\n\n)\n\n(\n\ni \u2212 i \u2212 (cid:115)\u03bb \u03c4]q>w\u2217 .\n7.13\n\nfor\n\nl2 regularization can be rearranged\n\n(7.40)\n\nq> \u02dcw\n\n= ( +\u03bb \u03b1 )\u22121\u03bbq>w\u2217\n\ni\n\n(7.41)\n\n3for neural networks, to obtain symmetry breaking between hidden units, we cannot initialize\n. however, the argument holds for any other\n\n6.2\n\nall the parameters to 0, as discussed in sec.\ninitial value w(0).\n\n250\n\n "}, {"Page_number": 266, "text": "chapter 7. regularization for deep learning\n\nq> \u02dcw i\n\n= [ \u2212 ( +\u03bb \u03b1 )\u22121 \u03b1]q>w\u2217\n7.42\n\ni\n\n, we see that if the hyperparameters\n\ncomparing eq.\n7.40\nare chosen such that\n\nand eq.\n\ni \u2212 (cid:115)\u03bb \u03c4 = ( + )\n(\n\n\u03bb \u03b1i \u22121\u03b1,\n\n)\n\n(7.42)\n\n(cid:115), \u03b1, and \u03c4\n\n(7.43)\n\nthen l2 regularization and early stopping can be seen to be equivalent (at least\nunder the quadratic approximation of the objective function). going even further,\nby taking logarithms and using the series expansion for log(1 + x), we can conclude\nthat if all \u03bbi are small (that is, (cid:115)\u03bbi (cid:129) 1 and \u03bbi/\u03b1 (cid:129) 1) then\n\n1\n(cid:115)\u03b1\n1\n\u03c4(cid:115)\n\n,\n\n.\n\n\u03c4 \u2248\n\u03b1 \u2248\n\n(7.44)\n\n(7.45)\n\nthat is, under these assumptions, the number of training iterations \u03c4 plays a role\ninversely proportional to the l2 regularization parameter, and the inverse of \u03c4(cid:115)\nplays the role of the weight decay coefficient.\n\nparameter values corresponding to directions of significant curvature (of the\nobjective function) are regularized less than directions of less curvature. of course,\nin the context of early stopping, this really means that parameters that correspond\nto directions of significant curvature tend to learn early relative to parameters\ncorresponding to directions of less curvature.\n\nthe derivations in this section have shown that a trajectory of length \u03c4 ends\nat a point that corresponds to a minimum of the l 2-regularized objective. early\nstopping is of course more than the mere restriction of the trajectory length;\ninstead, early stopping typically involves monitoring the validation set error in\norder to stop the trajectory at a particularly good point in space. early stopping\ntherefore has the advantage over weight decay that early stopping automatically\ndetermines the correct amount of regularization while weight decay requires many\ntraining experiments with different values of its hyperparameter.\n\n7.9 parameter tying and parameter sharing\n\nthus far, in this chapter, when we have discussed adding constraints or penalties\nto the parameters, we have always done so with respect to a fixed region or point.\nfor example, l2 regularization (or weight decay) penalizes model parameters for\ndeviating from the fixed value of zero. however, sometimes we may need other\nways to express our prior knowledge about suitable values of the model parameters.\n\n251\n\n "}, {"Page_number": 267, "text": "chapter 7. regularization for deep learning\n\nsometimes we might not know precisely what values the parameters should take\nbut we know, from knowledge of the domain and model architecture, that there\nshould be some dependencies between the model parameters.\n\na common type of dependency that we often want to express is that certain\nparameters should be close to one another. consider the following scenario: we\nhave two models performing the same classification task (with the same set of\nclasses) but with somewhat different input distributions. formally, we have model\na with parameters w( )a and model b with parameters w(\n)b . the two models\nmap the input\u00a0to two\u00a0different,\u00a0but\u00a0related outputs: \u02c6y ( )a = f(w( )a , x) and\n\u02c6y (\n\n)b =  (g w (\n\n)b , x).\n\nlet us imagine that the tasks are similar enough (perhaps with similar input\nand output distributions) that we believe the model parameters should be close\nto each other: \u2200i, w( )a\n. we can leverage this information\nthrough regularization. specifically, we can use a parameter norm penalty of the\nform: \u03c9(w ( )a , w(\n2.\u00a0here we used an l2 penalty, but other\nchoices are also possible.\n\n)b ) = kw( )a \u2212 w(\n\nshould be close to w(\ni\n\n)b k2\n\n)b\n\ni\n\nthis kind of approach was proposed by\n\n), who regularized\nthe parameters of one model, trained as a classifier in a supervised paradigm, to\nbe close to the parameters of another model, trained in an unsupervised paradigm\n(to capture the distribution of the observed input data). the architectures were\nconstructed such that many of the parameters in the classifier model could be\npaired to corresponding parameters in the unsupervised model.\n\nlasserre et al. 2006\n\n(\n\nwhile a parameter norm penalty is one way to regularize parameters to be\nclose to one another, the more popular way is to use constraints: to force sets of\nparameters to be equal. this method of regularization is often referred to as\nparameter sharing, where we interpret the various models or model components as\nsharing a unique set of parameters. a significant advantage of parameter sharing\nover regularizing the parameters to be close (via a norm penalty) is that only a\nsubset of the parameters (the unique set) need to be stored in memory. in certain\nmodels\u2014such as the convolutional neural network\u2014this can lead to significant\nreduction in the memory footprint of the model.\n\nconvolutional neural networks by far the most popular and extensive use\nof parameter sharing occurs in convolutional neural networks (cnns) applied to\ncomputer vision.\n\nnatural images have many statistical properties that are invariant to translation.\nfor example, a photo of a cat remains a photo of a cat if it is translated one pixel\n\n252\n\n "}, {"Page_number": 268, "text": "chapter 7. regularization for deep learning\n\nto the right. cnns take this property into account by sharing parameters across\nmultiple image locations. the same feature (a hidden unit with the same weights)\nis computed over different locations in the input. this means that we can find a\ncat with the same cat detector whether the cat appears at column i or column\ni + 1 in the image.\n\nparameter sharing has allowed cnns to dramatically lower the number of unique\nmodel parameters and to significantly increase network sizes without requiring a\ncorresponding increase in training data.\u00a0it remains one of the best examples of\nhow to effectively incorporate domain knowledge into the network architecture.\n\ncnns will be discussed in more detail in chapter\n\n.9\n\n7.10 sparse representations\n\nweight decay acts by placing a penalty directly on the model parameters. another\nstrategy is to place a penalty on the activations of the units in a neural network,\nencouraging their activations to be sparse. this indirectly imposes a complicated\npenalty on the model parameters.\n\n7.1.2\n\nwe have already discussed (in sec.\n\nl1 penalization induces a sparse\nparametrization\u2014meaning that many of the parameters become zero (or close to\nzero). representational sparsity, on the other hand, describes a representation\nwhere many of the elements of the representation are zero (or close to zero).\na simplified view of this distinction can be illustrated in the context of linear\nregression:\n\n) how\n\n(cid:48)(cid:49)(cid:49)(cid:49)(cid:49)(cid:50)\n\n(cid:43)(cid:44)(cid:44)(cid:44)(cid:44)(cid:45)\n\n18\n5\n15\n\u22129\n\u22123\ny \u2208 rm\n\n=\n\n4 0\n0 0\n0 5\n1 0\n1 0\n\n(cid:43)(cid:44)(cid:44)(cid:44)(cid:44)(cid:45)\n\n0\n0\n0\n\u2212\n4\n0\n\n0\n3\n0\n0\n\u2212\n5\n\n\u2212\n2\n0\n0\n\u2212\n1\n0\n\n0\n\u2212\n1\n0\n0\n0\na \u2208 rm n\u00d7\n\n\u221214\n1\n1\n2\n23\n\n(cid:43)(cid:44)(cid:44)(cid:44)(cid:44)(cid:45)\n\n(cid:48)(cid:49)(cid:49)(cid:49)(cid:49)(cid:50)\n\ny \u2208 rm\n\n=\n\n(cid:43)(cid:44)(cid:44)(cid:44)(cid:44)(cid:45)\n\n3\n4\n\u2212\n1\n3\n\u2212\n5\n\n4\n1\n1\n3\n\u2212 \u2212\n2\n3\n\u2212\n3\n0\n\u2212 \u2212\n5\n1\n\n\u2212\n1\n2\n5\n1\n4\n\n\u2212\n5\n2\n\u2212 \u2212\n1\n3\n2\n4\n\u2212\n3\n2\n\u2212\n2\n2\nb \u2208 rm n\u00d7\n\n253\n\n(cid:48)(cid:49)(cid:49)(cid:49)(cid:49)(cid:50)\n(cid:48)(cid:49)(cid:49)(cid:49)(cid:49)(cid:50)\n\n2\n3\n\u22122\n\u22125\n1\n4\n\n(cid:43)(cid:44)(cid:44)(cid:44)(cid:44)(cid:44)(cid:44)(cid:45)\n(cid:43)(cid:44)(cid:44)(cid:44)(cid:44)(cid:44)(cid:44)(cid:45)\n\n(cid:48)(cid:49)(cid:49)(cid:49)(cid:49)(cid:49)(cid:49)(cid:50)\n(cid:48)(cid:49)(cid:49)(cid:49)(cid:49)(cid:49)(cid:49)(cid:50)\n\nx \u2208 rn\n\n0\n2\n0\n0\n\u22123\n0\nh \u2208 rn\n\n(7.46)\n\n(7.47)\n\n "}, {"Page_number": 269, "text": "chapter 7. regularization for deep learning\n\nin the first expression, we have an example of a sparsely parametrized linear\nregression model. in the second, we have linear regression with a sparse representa-\ntion h of the data x. that is, h is a function of x that, in some sense, represents\nthe information present in , but does so with a sparse vector.\n\nx\n\nrepresentational regularization is accomplished by the same sorts of mechanisms\n\nthat we have used in parameter regularization.\n\nnorm penalty regularization of representations is performed by adding to the\nloss function j a norm penalty on the representation, denoted \u03c9(h). as before, we\ndenote the regularized loss function by \u02dcj:\n\n\u02dcj\n\n( ;\u03b8 x y) =  ( ;\u03b8 x y) + \u03c9( )h\n\n\u03b1\n\nj\n\n,\n\n,\n\n(7.48)\n\nwhere \u03b1 \u2208 [0,\u221e) weights the relative contribution of the norm penalty term, with\nlarger values of\n\ncorresponding to more regularization.\n\n\u03b1\n\n||h 1 =pi |hi|.\u00a0of course, the l1 penalty is only one choice of penalty\n\njust as an l1 penalty on the parameters induces parameter sparsity, an l1\npenalty on the elements of the representation induces representational sparsity:\n\u03c9(h) = ||\nthat can result in a sparse representation. others include the penalty derived from\na student-t prior on the representation (\nolshausen and field 1996 bergstra 2011\n)\nand kl divergence penalties (\n) that are especially\nuseful for representations with elements constrained to lie on the unit interval.\nlee\n) both provide examples of strategies\nbased on regularizing the average activation across several examples, 1\nbe near some target value, such as a vector with .01 for each entry.\n\n,\nlarochelle and bengio 2008\n\ngoodfellow\n\net al. (\n\net al. (\n\n) and\n\n2009\n\n2008\n\n,\n\n;\n\n,\n\nmpi h( )i , to\n\nother approaches obtain representational sparsity with a hard constraint on\nthe activation\u00a0values. for example,\u00a0orthogonal\u00a0matching pursuit (\npati et al.\n,\n1993) encodes an input x with the representation h that solves the constrained\noptimization problem\n\narg min\n\nh h,k k0 <k k \u2212\n\nx w h 2,\nk\n\n(7.49)\n\nwhere k kh 0 is the number of non-zero entries of h .\u00a0this problem can be solved\nefficiently when w is constrained to be orthogonal. this method is often called\nomp-k with the value of k specified to indicate the number of non-zero features\nallowed.\n) demonstrated that omp- can be a very effective\nfeature extractor for deep architectures.\n\ncoates and ng 2011\n\n1\n\n(\n\nessentially any model that has hidden units can be made sparse. throughout\nthis book, we will see many examples of sparsity regularization used in a variety of\ncontexts.\n\n254\n\n "}, {"Page_number": 270, "text": "chapter 7. regularization for deep learning\n\n7.11 bagging and other ensemble methods\n\n(short for\n\nbootstrap aggregating\n\nbagging\n) is a technique for reducing generalization\nerror by combining several models (\n). the idea is to train several\ndifferent models separately, then have all of the models vote on the output for test\nexamples. this is an example of a general strategy in machine learning called model\naveraging. techniques employing this strategy are known as ensemble methods.\n\nbreiman 1994\n\n,\n\nthe reason that model averaging works is that different models will usually\n\nnot make all the same errors on the test set.\n\nconsider for example a set of k regression models. suppose that each model\nmakes an error (cid:115)i on each example,\u00a0with the errors drawn from a zero-mean\nmultivariate normal distribution with variances e[(cid:115)2\ni ] = v and covariances e[(cid:115)i(cid:115)j] =\nc.\u00a0then the error made by the average prediction of all the ensemble models is\n1\n\nkpi (cid:115)i. the expected squared error of the ensemble predictor is\n(cid:115)i(cid:115)j(cid:46)(cid:47)(cid:48)(cid:50)\n\ne(cid:43)(cid:45)\u00a0 1\nkxi\n\n(cid:41)(cid:42)(cid:115)2\ni +xj\n\n(cid:115)i!2(cid:48)(cid:50) =\n\ne(cid:43)(cid:45)xi\n\n1\nk2\n\n6=\ni\n\n(7.50)\n\n(7.51)\n\n=\n\nv +\n\n1\nk\n\nk \u2212 1\nk\n\nc.\n\nin the case where the errors are perfectly correlated and c = v, the mean squared\nerror reduces to v, so the model averaging does not help at all. in the case where\nthe errors are perfectly uncorrelated and c = 0, the expected squared error of the\nensemble is only 1\nk v. this means that the expected squared error of the ensemble\ndecreases linearly with the ensemble size. in other words, on average, the ensemble\nwill perform at least as well as any of its members, and if the members make\nindependent errors, the ensemble will perform significantly better than its members.\n\ndifferent ensemble methods construct the ensemble of models in different ways.\nfor example, each member of the ensemble could be formed by training a completely\ndifferent kind of model using a different algorithm or objective function. bagging\nis a method that allows the same kind of model, training algorithm and objective\nfunction to be reused several times.\n\nspecifically, bagging involves constructing k different datasets. each dataset\nhas the same number of examples as the original dataset, but each dataset is\nconstructed by sampling with replacement from the original dataset. this means\nthat, with high probability, each dataset is missing some of the examples from the\noriginal dataset and also contains several duplicate examples (on average around\n2/3 of the examples from the original dataset are found in the resulting training\n\n255\n\n "}, {"Page_number": 271, "text": "chapter 7. regularization for deep learning\n\noriginal\u00a0dataset\n\nfirst\u00a0resampled\u00a0dataset\n\nfirst\u00a0ensemble\u00a0member\n\nsecond\u00a0resampled\u00a0dataset\n\nsecond\u00a0ensemble\u00a0member\n\n8\n\n8\n\nfigure 7.5: a cartoon depiction of how bagging works. suppose we train an \u20188\u2019 detector\non the dataset depicted above, containing an \u20188\u2019, a \u20186\u2019 and a \u20189\u2019. suppose we make two\ndifferent resampled datasets. the bagging training procedure is to construct each of these\ndatasets by sampling with replacement. the first dataset omits the \u20189\u2019 and repeats the \u20188\u2019.\non this dataset, the detector learns that a loop on top of the digit corresponds to an \u20188\u2019.\non the second dataset, we repeat the \u20189\u2019 and omit the \u20186\u2019. in this case, the detector learns\nthat a loop on the bottom of the digit corresponds to an \u20188\u2019. each of these individual\nclassification rules is brittle, but if we average their output then the detector is robust,\nachieving maximal confidence only when both loops of the \u20188\u2019 are present.\n\nset, if it has the same size as the original). model i is then trained on dataset\ni. the differences between which examples are included in each dataset result in\ndifferences between the trained models. see fig.\n\nfor an example.\n\n7.5\n\nneural networks reach a wide enough variety of solution points that they can\noften benefit from model averaging even if all of the models are trained on the same\ndataset. differences in random initialization, random selection of minibatches,\ndifferences in hyperparameters, or different outcomes of non-deterministic imple-\nmentations of neural networks are often enough to cause different members of the\nensemble to make partially independent errors.\n\nmodel averaging is an extremely powerful and reliable method for reducing\ngeneralization error. its use is usually discouraged when benchmarking algorithms\nfor scientific papers, because any machine learning algorithm can benefit substan-\ntially from model averaging at the price of increased computation and memory.\nfor this reason, benchmark comparisons are usually made using a single model.\n\nmachine learning contests are usually won by methods using model averag-\ning over dozens of models. a recent prominent example is the netflix grand\nprize (koren 2009\n\n).\n\n,\n\n256\n\n "}, {"Page_number": 272, "text": "chapter 7. regularization for deep learning\n\nnot all techniques for constructing ensembles are designed to make the ensemble\nmore regularized than the individual models. for example, a technique called\nboosting (freund and schapire 1996b a\n, ) constructs an ensemble with higher capacity\nthan the individual models. boosting has been applied to build ensembles of neural\nnetworks (schwenk and bengio 1998\n) by incrementally adding neural networks to\nthe ensemble.\u00a0boosting has also been applied interpreting an individual neural\nnetwork as an ensemble (\n), incrementally adding hidden units\nto the neural network.\n\nbengio et al. 2006a\n\n,\n\n,\n\n,\n\n7.12 dropout\n\n2014\n\net al.,\u00a0\n\n) provides a computationally\u00a0inexpensive but\ndropout (srivastava\u00a0\npowerful method of regularizing a broad family of models. to a first approximation,\ndropout can be thought of as a method of making bagging practical for ensembles\nof very many large neural networks. bagging involves training multiple models,\nand evaluating multiple models on each test example. this seems impractical\nwhen each model is a large neural network, since training and evaluating such\nnetworks is costly in terms of runtime and memory. it is common to use ensembles\nof five to ten neural networks\u2014\n) used six to win the ilsvrc\u2014\nbut more than this rapidly becomes unwieldy. dropout provides an inexpensive\napproximation to training and evaluating a bagged ensemble of exponentially many\nneural networks.\n\nszegedy et al. 2014a\n\n(\n\n7.6\n\nspecifically, dropout trains the ensemble consisting of all sub-networks that\ncan be formed by removing non-output units from an underlying base network,\nas illustrated in fig.\n. in most modern neural networks, based on a series of\naffine transformations and nonlinearities, we can effectively remove a unit from a\nnetwork by multiplying its output value by zero.\u00a0this procedure requires some\nslight modification for models such as radial basis function networks, which take\nthe difference between the unit\u2019s state and some reference value. here, we present\nthe dropout algorithm in terms of multiplication by zero for simplicity, but it can\nbe trivially modified to work with other operations that remove a unit from the\nnetwork.\n\nrecall that to learn with bagging, we define k different models, construct k\ndifferent datasets by sampling from the training set with replacement, and then\ntrain model i on dataset i. dropout aims to approximate this process, but with an\nexponentially large number of neural networks. specifically, to train with dropout,\nwe use a minibatch-based learning algorithm that makes small steps, such as\nstochastic gradient descent. each time we load an example into a minibatch, we\n\n257\n\n "}, {"Page_number": 273, "text": "chapter 7. regularization for deep learning\n\nyy\n\nh 1h 1\n\nx 1x 1\n\nh 2h 2\n\nx 2x 2\n\nbase\u00a0network\n\nh 1h 1\n\nx 1x 1\n\nh 1h 1\n\nx 1x 1\n\nh 1h 1\n\nx 1x 1\n\nyy\n\nyy\n\nyy\n\nyy\n\nyy\n\nyy\n\nh 2h 2\n\nh 1h 1\n\nh 2h 2\n\nh 1h 1\n\nh 2h 2\n\nx 2x 2\n\nx 2x 2\n\nx 1x 1\n\nx 1x 1\n\nyy\n\nyy\n\nh 1h 1\n\nh 2h 2\n\nx 2x 2\n\nx 1x 1\n\nx 2x 2\n\nyy\n\nyy\n\nh 2h 2\n\nx 1x 1\n\nh 1h 1\n\nyy\n\nyy\n\nx 2x 2\n\nx 1x 1\n\nh 2h 2\n\nh 1h 1\n\nh 2h 2\n\nx 2x 2\n\nh 2h 2\n\nx 2x 2\n\nyy\n\nyy\n\nyy\n\nyy\n\nx 2x 2\n\nensemble\u00a0of\u00a0sub-networks\n\nfigure\u00a07.6: dropout\u00a0trains an\u00a0ensemble\u00a0consisting of\u00a0all sub-networks\u00a0that\u00a0can be\nconstructed by removing non-output units from an underlying base network. here, we\nbegin with a base network with two visible units and two hidden units. there are sixteen\npossible subsets of these four units. we show all sixteen subnetworks that may be formed\nby dropping out different subsets of units from the original network. in this small example,\na large proportion of the resulting networks have no input units or no path connecting\nthe input to the output. this problem becomes insignificant for networks with wider\nlayers, where the probability of dropping all possible paths from inputs to outputs becomes\nsmaller.\n\n258\n\n "}, {"Page_number": 274, "text": "chapter 7. regularization for deep learning\n\nh 2h 2\n\nx 2x 2\n\nh 1h 1\n\nx 1x 1\n\nyy\n\nyy\n\n\u02c6h 1\n\u02c6h 1\n\n\u02c6h 2\n\u02c6h 2\n\n\u00b5h1\u00b5h1\n\nh 1h 1\n\nh 2h 2\n\n\u00b5 h2\u00b5 h2\n\n\u02c6x1\u02c6x1\n\n\u02c6x2\u02c6x2\n\n\u00b5x1\u00b5x1\n\nx 1x 1\n\nx 2x 2\n\n\u00b5 x2\u00b5 x2\n\nfigure 7.7: an example of forward propagation through a feedforward network using\ndropout. (top) in this example, we use a feedforward network with two input units, one\nhidden layer with two hidden units, and one output unit.\nto perform forward\npropagation with dropout, we randomly sample a vector \u00b5 with one entry for each input\nor hidden unit in the network. the entries of \u00b5 are binary and are sampled independently\nfrom each other. the probability of each entry being\n0.5\nfor the hidden layers and 0.8 for the input. each unit in the network is multiplied by\nthe corresponding mask, and then forward propagation continues through the rest of the\nnetwork as usual. this is equivalent to randomly selecting one of the sub-networks from\nfig.\n\nand running forward propagation through it.\n\nis a hyperparameter, usually\n\n(bottom)\n\n7.6\n\n1\n\n259\n\n "}, {"Page_number": 275, "text": "chapter 7. regularization for deep learning\n\nrandomly sample a different binary mask to apply to all of the input and hidden\nunits in the network. the mask for each unit is sampled independently from all of\nthe others. the probability of sampling a mask value of one (causing a unit to be\nincluded) is a hyperparameter fixed before training begins.\u00a0it is not a function\nof the current value of the model parameters or the input example. typically,\nan input unit is included with probability 0.8 and a hidden unit is included with\nprobability 0.5. we then run forward propagation, back-propagation, and the\nlearning update as usual. fig.\nillustrates how to run forward propagation with\ndropout.\n\n7.7\n\nmore formally, suppose that a mask vector \u00b5 specifies which units to include,\nand j (\u03b8 \u00b5, ) defines the cost of the model defined by parameters \u03b8 and mask \u00b5.\nthen dropout training consists in minimizing e\u00b5j(\u03b8 \u00b5,\n). the expectation contains\nexponentially many terms but we can obtain an unbiased estimate of its gradient\nby sampling values of\n\n.\u00b5\n\ndropout training is not quite the same as bagging training. in the case of\nbagging, the models are all independent. in the case of dropout, the models share\nparameters, with each model inheriting a different subset of parameters from the\nparent neural network. this parameter sharing makes it possible to represent an\nexponential number of models with a tractable amount of memory. in the case of\nbagging, each model is trained to convergence on its respective training set. in the\ncase of dropout, typically most models are not explicitly trained at all\u2014usually,\nthe model is large enough that it would be infeasible to sample all possible sub-\nnetworks within the lifetime of the universe. instead, a tiny fraction of the possible\nsub-networks are each trained for a single step, and the parameter sharing causes\nthe remaining sub-networks to arrive at good settings of the parameters. these\nare the only differences. beyond these, dropout follows the bagging algorithm. for\nexample, the training set encountered by each sub-network is indeed a subset of\nthe original training set sampled with replacement.\n\nto make a prediction, a bagged ensemble must accumulate votes from all of\nits members. we refer to this process as inference in this context. so far, our\ndescription of bagging and dropout has not required that the model be explicitly\nprobabilistic. now, we assume that the model\u2019s role is to output a probability\ndistribution. in the case of bagging, each model i produces a probability distribution\np( )i (y | x). the prediction of the ensemble is given by the arithmetic mean of all\nof these distributions,\n\np( )i (\n\ny | x .\n)\n\n(7.52)\n\nin the case of dropout, each sub-model defined by mask vector \u00b5 defines a prob-\n\n1\nk\n\nkxi=1\n\n260\n\n "}, {"Page_number": 276, "text": "chapter 7. regularization for deep learning\n\nability distribution p(y\nby\n\n,\n\n| x \u00b5 ). the arithmetic mean over all masks is given\n\np\n\np y\n( )\u00b5 (\n\n,\n\n| x \u00b5)\n\n(7.53)\n\nx\u00b5\n\nwhere p(\u00b5) is the probability distribution that was used to sample \u00b5 at training\ntime.\n\nbecause this sum includes an exponential number of terms, it is intractable\nto evaluate except in cases where the structure of the model permits some form\nof simplification. so far, deep neural nets are not known to permit any tractable\nsimplification.\ninstead,\u00a0we can\u00a0approximate the\u00a0inference with sampling,\u00a0by\naveraging together the output from many masks. even 10-20 masks are often\nsufficient to obtain good performance.\n\nhowever, there is an even better approach, that allows us to obtain a good\napproximation to the predictions of the entire ensemble, at the cost of only one\nforward propagation. to do so, we change to using the geometric mean rather than\nthe arithmetic mean of the ensemble members\u2019 predicted distributions. warde-\nfarley\n) present arguments and empirical evidence that the geometric\nmean performs comparably to the arithmetic mean in this context.\n\net al. (\n\n2014\n\nthe geometric mean of multiple probability distributions is not guaranteed to be\na probability distribution. to guarantee that the result is a probability distribution,\nwe impose the requirement that none of the sub-models assigns probability 0 to any\nevent, and we renormalize the resulting distribution. the unnormalized probability\ndistribution defined directly by the geometric mean is given by\n\n\u02dcpensemble(\n\n) =\n\ny | x\n\n2dsy\u00b5\n\np y\n(\n\n,\n\n| x \u00b5)\n\n(7.54)\n\nwhere d is the number of units that may be dropped. here we use a uniform\ndistribution over \u00b5 to simplify the presentation, but non-uniform distributions are\nalso possible. to make predictions we must re-normalize the ensemble:\n\npensemble(\n\n) =\n\ny | x\n\ny | x\n)\n\n\u02dcpensemble(\n\npy0 \u02dcpensemble(y0 | x)\n\n.\n\n(7.55)\n\n,\n\na key insight (\n\nhinton et al. 2012c\n\n) involved in dropout is that we can approxi-\nmate pensemble by evaluating p(y | x) in one model: the model with all units, but\nwith the weights going out of unit i multiplied by the probability of including unit\ni. the motivation for this modification is to capture the right expected value of\nthe output from that unit. we call this approach the weight scaling inference rule.\n\n261\n\n "}, {"Page_number": 277, "text": "chapter 7. regularization for deep learning\n\nbecause we usually use an inclusion probability of 1\n\nthere is not yet any theoretical argument for the accuracy of this approximate\ninference rule in deep nonlinear networks, but empirically it performs very well.\n2 , the weight scaling rule\nusually amounts to dividing the weights by\nat the end of training, and then using\nthe model as usual. another way to achieve the same result is to multiply the\nstates of the units by\nduring training. either way, the goal is to make sure that\nthe expected total input to a unit at test time is roughly the same as the expected\ntotal input to that unit at train time, even though half the units at train time are\nmissing on average.\n\n2 \n\n2\n\nfor many classes of models that do not have nonlinear hidden units, the weight\nscaling inference rule is exact. for a simple example, consider a softmax regression\nclassifier with\n\ninput variables represented by the vector\n\nv\n\nn\n\n:\n\np\n\n( = y\ny\n\n| v) = softmax(cid:33)w>v + b(cid:34)y\n\n.\n\n(7.56)\n\nwe can index into the family of sub-models by element-wise multiplication of the\ninput with a binary vector\n\n:d\n\np\n\n( = y\ny\n\n| v;\n\nd\n\n) = \n\nsoftmax(cid:33)w >(\n\nd (cid:128) v\n\n) +\n\nb(cid:34)y\n\n.\n\n(7.57)\n\nthe ensemble predictor is defined by re-normalizing the geometric mean over all\nensemble members\u2019 predictions:\n\npensemble( = \n\ny\n\ny | v\n\n) =\n\nwhere\n\n\u02dcpensemble( = \n\ny\n\ny | v\n\n) =\n\n\u02dcpensemble( = \n\ny\n\n\u02dcpensemble( = y\n\n)\n\ny | v\ny0 | v)\n\np\n\n( = y\ny\n\n.\n)d\n\n| v;\n\nto see that the weight scaling rule is exact, we can simplify \u02dcpensemble:\n\n\u02dcpensemble ( = \n\ny\n\ny | v\n\n) =\n\np\n\n( = y\ny\n\n)d\n\n| v;\n\npy0\n2 ns yd\u2208{\n2 ns yd\u2208{\n\n}0 1, n\n\n}0 1, n\n\n}0 1,\n\n= 2ns yd\u2208{\n= 2nvuut yd\u2208{\n\n}0 1, n\n\nsoftmax (w> (\n\nd (cid:128) v\n\n) + )\nb y\n\nn\n\nexp(cid:29)w>y,:(\ny0 exp(cid:33)w >y0,:(\n\n262\n\nd (cid:128) v\n\n) +\nd (cid:128) v\n\nb(cid:30)\n\n) +\n\nb(cid:34)\n\np\n\n(7.58)\n\n(7.59)\n\n(7.60)\n\n(7.61)\n\n(7.62)\n\n "}, {"Page_number": 278, "text": "chapter 7. regularization for deep learning\n\nbecause \u02dcp will be normalized, we can safely ignore multiplication by factors that\nare constant with respect to :y\n\n\u02dcpensemble( = \n\ny\n\n=\n\n(7.63)\n\nd (cid:128) v\n\n) +\n\nd (cid:128) v\n\n) +\n\nb(cid:30)\n\nb(cid:34)\n\n2nqq d\u2208{\n}0 1, n exp(cid:29)w>y,: (\n2nrqd\u2208{\n}0 1, npy0 exp(cid:33)w>y 0,:(\ny | v \u221d 2ns yd\u2208{\n= exp(cid:41)(cid:42) 1\n2n xd\u2208{\n= exp(cid:35)1\n\nw>y,:v + b(cid:36)\n\nw>y,:(\n\n}0 1, n\n\n}0 1, n\n\n) \n\n2\n\nd (cid:128) v\n\nexp(cid:29)w >y,:(\nb(cid:46)(cid:47)\n\nd (cid:128) v\n\n) +\n\n) +\n\nb(cid:30)\n\n(7.64)\n\n(7.65)\n\n(7.66)\n\nsubstituting this back into eq.\n1\n2w .\n\n7.58\n\nwe obtain a softmax classifier with weights\n\n2013a\n\nthe weight scaling rule is also exact in other settings, including regression\nnetworks with conditionally normal outputs, and deep networks that have hidden\nlayers without nonlinearities. however, the weight scaling rule is only an approxi-\nmation for deep models that have nonlinearities. though the approximation has\nnot been theoretically characterized, it often works well, empirically. goodfellow\net al. (\n) found experimentally that the weight scaling approximation can work\nbetter (in terms of classification accuracy) than monte carlo approximations to the\nensemble predictor. this held true even when the monte carlo approximation was\nallowed to sample up to 1,000 sub-networks.\n) found\nthat some models obtain better classification accuracy using twenty samples and\nthe monte carlo approximation. it appears that the optimal choice of inference\napproximation is problem-dependent.\n\ngal and ghahramani 2015\n\n(\n\n2014\n\net al. (\n\nsrivastava\n\n) showed that dropout is more effective than other\nstandard computationally inexpensive regularizers, such as weight decay, filter\nnorm constraints and sparse activity regularization. dropout may also be combined\nwith other forms of regularization to yield a further improvement.\n\none advantage of dropout is that it is very computationally cheap. using\ndropout during training requires only o(n) computation per example per update,\nto generate n random binary numbers and multiply them by the state. depending\non the implementation, it may also require o (n) memory to store these binary\nnumbers until the back-propagation stage. running inference in the trained model\n\n263\n\n "}, {"Page_number": 279, "text": "chapter 7. regularization for deep learning\n\nhas the same cost per-example as if dropout were not used, though we must pay\nthe cost of dividing the weights by 2 once before beginning to run inference on\nexamples.\n\nanother significant advantage of dropout is that it does not significantly limit\nthe type of model or training procedure that can be used. it works well with nearly\nany model that uses a distributed representation and can be trained with stochastic\ngradient descent. this includes feedforward neural networks, probabilistic models\n), and recurrent\nsuch as restricted boltzmann machines (srivastava\nneural networks (bayer and osendorfer 2014 pascanu\n). many other\nregularization strategies of comparable power impose more severe restrictions on\nthe architecture of the model.\n\net al.,\n\net al.,\n\n2014a\n\n2014\n\n,\n\n;\n\nthough the cost per-step of applying dropout to a specific model is negligible,\nthe cost of using dropout in a complete system can be significant. because dropout\nis a regularization technique, it reduces the effective capacity of a model. to offset\nthis effect, we must increase the size of the model. typically the optimal validation\nset error is much lower when using dropout, but this comes at the cost of a much\nlarger model and many more iterations of the training algorithm. for very large\ndatasets, regularization confers little reduction in generalization error.\u00a0in these\ncases, the computational cost of using dropout and larger models may outweigh\nthe benefit of regularization.\n\nwhen extremely few labeled training examples are available, dropout is less\n) outperform\u00a0dropout\u00a0on the\n) where fewer than 5,000 examples\n). when additional unlabeled data is available,\n\neffective. bayesian neural\u00a0networks (\nalternative splicing dataset (\nare available (srivastava\net al.,\nunsupervised feature learning can gain an advantage over dropout.\n\n,\u00a0\nxiong et al. 2011\n\nneal 1996\n\n2014\n\n,\n\n2013\n\nwager\n\net al. (\n\n) showed that, when applied to linear regression, dropout\nis equivalent to l2 weight decay, with a different weight decay coefficient for\neach input feature. the magnitude of each feature\u2019s weight decay coefficient is\ndetermined by its variance. similar results hold for other linear models. for deep\nmodels, dropout is not equivalent to weight decay.\n\nthe stochasticity used while training with dropout is not necessary for the\napproach\u2019s success. it is just a means of approximating the sum over all sub-\nmodels. wang and manning 2013\n) derived analytical approximations to this\nmarginalization. their approximation, known as fast dropout resulted in faster\nconvergence time due to the reduced stochasticity in the computation of the\ngradient. this method can also be applied at test time, as a more principled\n(but also more computationally expensive) approximation to the average over all\nsub-networks than the weight scaling approximation. fast dropout has been used\n\n(\n\n264\n\n "}, {"Page_number": 280, "text": "chapter 7. regularization for deep learning\n\nto nearly match the performance of standard dropout on small neural network\nproblems, but has not yet yielded a significant improvement or been applied to a\nlarge problem.\n\net al. (\n\njust as stochasticity is not necessary to achieve the regularizing\u00a0effect of\ndropout, it is also not sufficient. to demonstrate this, warde-farley\n2014\n)\ndesigned control experiments using a method called dropout boosting that they\ndesigned to use exactly the same mask noise as traditional dropout but lack\nits regularizing effect. dropout boosting trains the entire ensemble to jointly\nmaximize the log-likelihood on the training set. in the same sense that traditional\ndropout is analogous to bagging,\u00a0this approach is analogous to boosting. as\nintended, experiments with dropout boosting show almost no regularization effect\ncompared to training the entire network as a single model. this demonstrates that\nthe interpretation of dropout as bagging has value beyond the interpretation of\ndropout as robustness to noise. the regularization effect of the bagged ensemble is\nonly achieved when the stochastically sampled ensemble members are trained to\nperform well independently of each other.\n\ndropout has inspired other stochastic approaches to training exponentially\nlarge ensembles of models that share weights.\u00a0dropconnect is a special case of\ndropout where each product between a single scalar weight and a single hidden\nunit state is considered a unit that can be dropped (wan\n). stochastic\npooling is a form of randomized pooling (see sec.\n) for building ensembles\n9.3\nof convolutional networks with each convolutional network attending to different\nspatial locations of each feature map.\u00a0so far, dropout remains the most widely\nused implicit ensemble method.\n\net al.,\n\n2013\n\none of the key insights of dropout is that training a network with stochastic\nbehavior and making predictions by averaging over multiple stochastic decisions\nimplements a form of bagging with parameter sharing. earlier,\u00a0we described\ndropout as\u00a0bagging an ensemble of models formed by including or\u00a0excluding\nunits. however, there is no need for this model averaging strategy to be based on\ninclusion and exclusion. in principle, any kind of random modification is admissible.\nin practice, we must choose modification families that neural networks are able\nto learn to resist. ideally, we should also use model families that allow a fast\napproximate inference rule. we can think of any form of modification parametrized\n| x \u00b5) for all possible\nby a vector \u00b5 as training an ensemble consisting of p(y\nvalues of \u00b5. there is no requirement that \u00b5 have a finite number of values. for\nexample, \u00b5 can be real-valued. srivastava\n) showed that multiplying the\nweights by \u00b5 \u223c n (1, i) can outperform dropout based on binary masks. because\ne[\u00b5] = 1 the standard network automatically implements approximate inference\n\net al. (\n\n2014\n\n,\n\n265\n\n "}, {"Page_number": 281, "text": "chapter 7. regularization for deep learning\n\nin the ensemble, without needing any weight scaling.\n\nso far we have described dropout purely as a means of performing efficient,\napproximate bagging. however, there is another view of dropout that goes further\nthan this. dropout trains not just a bagged ensemble of models, but an ensemble\nof models that share hidden units. this means each hidden unit must be able to\nperform well regardless of which other hidden units are in the model. hidden units\nmust be prepared to be swapped and interchanged between models. hinton et al.\n(\n) were inspired by an idea from biology: sexual reproduction, which involves\n2012c\nswapping genes between two different organisms, creates evolutionary pressure for\ngenes to become not just good, but to become readily swapped between different\norganisms. such genes and such features are very robust to changes in their\nenvironment because they are not able to incorrectly adapt to unusual features\nof any one organism or model. dropout thus regularizes each hidden unit to be\nnot merely a good feature but a feature that is good in many contexts.\u00a0warde-\nfarley\n) compared dropout training to training of large ensembles and\nconcluded that dropout offers additional improvements to generalization error\nbeyond those obtained by ensembles of independent models.\n\net al. (\n\n2014\n\nit is important to understand that a large portion of the power of dropout\narises from the fact that the masking noise is applied to the hidden units. this\ncan be seen as a form of highly intelligent, adaptive destruction of the information\ncontent of the input rather than destruction of the raw values of the input. for\nexample, if the model learns a hidden unit hi that detects a face by finding the nose,\nthen dropping hi corresponds to erasing the information that there is a nose in\nthe image. the model must learn another hi , either that redundantly encodes the\npresence of a nose, or that detects the face by another feature, such as the mouth.\ntraditional noise injection techniques that add unstructured noise at the input are\nnot able to randomly erase the information about a nose from an image of a face\nunless the magnitude of the noise is so great that nearly all of the information in\nthe image is removed. destroying extracted features rather than original values\nallows the destruction process to make use of all of the knowledge about the input\ndistribution that the model has acquired so far.\n\nanother important aspect of dropout is that the noise is multiplicative. if the\nnoise were additive with fixed scale, then a rectified linear hidden unit h i with\nadded noise (cid:115) could simply learn to have hi become very large in order to make\nthe added noise (cid:115) insignificant by comparison. multiplicative noise does not allow\nsuch a pathological solution to the noise robustness problem.\n\nanother deep learning algorithm, batch normalization, reparametrizes the\nmodel in a way that introduces both additive and multiplicative noise on the\n\n266\n\n "}, {"Page_number": 282, "text": "chapter 7. regularization for deep learning\n\n+ .007 \u00d7\n\n=\n\nx\n\ny =\u201cpanda\u201d\nw/ 57.7%\nconfidence\n\nsign(\u2207 xj(\u03b8 x,\n\u201cnematode\u201d\n\n, y))\n\nw/ 8.2%\nconfidence\n\nx +\n\n, y))\n\n(cid:115) sign(\u2207xj (\u03b8 x,\n\u201cgibbon\u201d\nw/ 99.3 %\nconfidence\n\n,\n\nfigure 7.8:\u00a0a demonstration of adversarial example generation applied to googlenet\n(\nszegedy et al. 2014a\n) on imagenet. by adding an imperceptibly small vector whose\nelements are equal to the sign of the elements of the gradient of the cost function with\nrespect to the input, we can change googlenet\u2019s classification of the image. reproduced\nwith permission from\n\ngoodfellow et al. 2014b\n\n).\n\n(\n\nhidden units at training time. the primary purpose of batch normalization is to\nimprove optimization, but the noise can have a regularizing effect, and sometimes\nmakes dropout unnecessary. batch normalization is described further in sec.\n8.7.1\n.\n\n7.13 adversarial training\n\nin many cases, neural networks have begun to reach human performance when\nevaluated on an i.i.d. test set. it is natural therefore to wonder whether these\nmodels have obtained a true human-level understanding of these tasks. in order\nto probe the level of understanding a network has of the underlying task, we can\nsearch for examples that the model misclassifies.\n) found that\neven neural networks that perform at human level accuracy have a nearly 100%\nerror rate on examples that are intentionally constructed by using an optimization\nprocedure to search for an input x0 near a data point x such that the model output\nis very different at x0. in many cases, x0 can be so similar to x that a human\nobserver cannot tell the difference between the original example and the adversarial\nexample, but the network can make highly different predictions. see fig.\nfor\nan example.\n\nszegedy et al. 2014b\n\n7.8\n\n(\n\nadversarial examples have many implications, for example, in computer security,\nthat are beyond the scope of this chapter.\u00a0however, they are interesting in the\ncontext of regularization because one can reduce the error rate on the original i.i.d.\ntest set via adversarial training\u2014training on adversarially perturbed examples\n\n267\n\n "}, {"Page_number": 283, "text": "chapter 7. regularization for deep learning\n\nfrom the training set (\n\nszegedy et al. 2014b goodfellow\n\n,\n\n;\n\net al.,\n\n2014b\n\n).\n\n2014b\n\net al. (\n\ngoodfellow\n\n) showed that one of the primary causes of these\nadversarial\u00a0examples is\u00a0excessive\u00a0linearity. neural\u00a0networks\u00a0are built\u00a0out\u00a0of\nprimarily linear building blocks.\u00a0in some experiments the overall function they\nimplement proves to be highly linear as a result. these linear functions are easy\nto optimize. unfortunately, the value of a linear function can change very rapidly\nif it has numerous inputs. if we change each input by (cid:115), then a linear function\n||w 1, which can be a very large\nwith weights w can change by as much as (cid:115)||\namount if w is high-dimensional. adversarial training discourages this highly\nsensitive locally linear behavior by encouraging the network to be locally constant\nin the neighborhood of the training data. this can be seen as a way of explicitly\nintroducing a local constancy prior into supervised neural nets.\n\nadversarial training helps to illustrate the power of using a large function\nfamily in combination with aggressive regularization. purely linear models, like\nlogistic regression, are not able to resist adversarial examples because they are\nforced to be linear. neural networks are able to represent functions that can range\nfrom nearly linear to nearly locally constant and thus have the flexibility to capture\nlinear trends in the training data while still learning to resist local perturbation.\n\nadversarial examples also provide a means of accomplishing semi-supervised\nlearning. at a point x that is not associated with a label in the dataset, the\nmodel itself assigns some label \u02c6y. the model\u2019s label \u02c6y may not be the true label,\nbut if the model is high quality, then \u02c6y has a high probability of providing the\ntrue label. we can seek an adversarial example x0 that causes the classifier to\noutput a label y0 with y0\n6= \u02c6y. adversarial examples generated using not the\ntrue label but a label provided by a trained model are called virtual adversarial\nexamples (miyato\n). the classifier may then be trained to assign the\n2015\nsame label to x and x0. this encourages the classifier to learn a function that is\nrobust to small changes anywhere along the manifold where the unlabeled data\nlies. the assumption motivating this approach is that different classes usually lie\non disconnected manifolds, and a small perturbation should not be able to jump\nfrom one class manifold to another class manifold.\n\net al.,\n\n7.14 tangent\u00a0distance,\u00a0tangent prop, and\u00a0manifold\n\ntangent classifier\n\nmany machine learning algorithms aim to overcome the curse of dimensionality\nby assuming that the data lies near a low-dimensional manifold, as described in\n\n268\n\n "}, {"Page_number": 284, "text": "chapter 7. regularization for deep learning\n\nsec.\n\n5.11.3\n.\n\n,\n\n,\n\nsimard et al. 1993 1998\n\none of the early attempts to take advantage of the manifold hypothesis is the\ntangent distance algorithm (\n). it is a non-parametric\nnearest-neighbor algorithm in which the metric used is not the generic euclidean\ndistance but one that is derived from knowledge of the manifolds near which\nprobability concentrates. it is assumed that we are trying to classify examples and\nthat examples on the same manifold share the same category. since the classifier\nshould be invariant to the local factors of variation that correspond to movement\non the manifold, it would make sense to use as nearest-neighbor distance between\npoints x1 and x2 the distance between the manifolds m 1 and m2 to which they\nrespectively belong. although that may be computationally difficult (it would\nrequire solving an optimization problem, to find the nearest pair of points on m1\nand m2), a cheap alternative that makes sense locally is to approximate mi by its\ntangent plane at xi and measure the distance between the two tangents, or between\na tangent plane and a point. that can be achieved by solving a low-dimensional\nlinear system (in the dimension of the manifolds). of course, this algorithm requires\none to specify the tangent vectors.\n\nsimard et al. 1992\n\nin a related spirit, the tangent prop algorithm (\n\n)\n7.9\ntrains a neural net classifier with an extra penalty to make each output f (x) of\nthe neural net locally invariant to known factors of variation. these factors of\nvariation correspond to movement along the manifold near which examples of the\nsame class concentrate. local invariance is achieved by requiring \u2207xf (x) to be\northogonal to the known manifold tangent vectors v ( )i at x, or equivalently that\nthe directional derivative of f at x in the directions v( )i be small by adding a\nregularization penalty\n\n) (fig.\n\n:\u03c9\n\n,\n\n\u03c9( ) =f xi (cid:33)(\u2207xf( ))x > v ( )i(cid:34)2\n\n.\n\n(7.67)\n\nthis regularizer can of course by scaled by an appropriate hyperparameter, and, for\nmost neural networks, we would need to sum over many outputs rather than the lone\noutput f(x) described here for simplicity. as with the tangent distance algorithm,\nthe tangent vectors are derived a priori, usually from the formal knowledge of\nthe effect of transformations such as translation, rotation, and scaling in images.\nsimard et al. 1992\ntangent prop has been used not just for supervised learning (\n)\nthrun 1995\n).\nbut also in the context of reinforcement learning (\n\n,\n\n,\n\ntangent propagation is\u00a0closely related\u00a0to dataset\u00a0augmentation.\n\nin both\ncases, the user of the algorithm encodes his or her prior knowledge of the task\nby specifying a set of transformations that should not alter the output of the\n\n269\n\n "}, {"Page_number": 285, "text": "chapter 7. regularization for deep learning\n\nnetwork. the difference is that in the case of dataset augmentation, the network is\nexplicitly trained to correctly classify distinct inputs that were created by applying\nmore than an infinitesimal amount of these transformations. tangent propagation\ndoes not require explicitly visiting a new input point. instead, it analytically\nregularizes the model to resist perturbation in the directions corresponding to\nthe\u00a0specified\u00a0transformation. while\u00a0this analytical\u00a0approach\u00a0is intellectually\nelegant, it has two major drawbacks. first, it only regularizes the model to resist\ninfinitesimal perturbation. explicit dataset augmentation confers resistance to\nlarger perturbations. second, the infinitesimal approach poses difficulties for models\nbased on rectified linear units. these models can only shrink their derivatives\nby turning units off or shrinking their weights. they are not able to shrink their\nderivatives by saturating at a high value with large weights, as sigmoid or tanh\nunits can. dataset augmentation works well with rectified linear units because\ndifferent subsets of rectified units can activate for different transformed versions of\neach original input.\n\n;\n\n,\n\nszegedy et al. 2014b goodfellow et al. 2014b\n\ntangent propagation is also related to double backprop (drucker and lecun,\n1992) and adversarial training (\n).\ndouble backprop regularizes the jacobian to be small, while adversarial training\nfinds inputs near the original inputs and trains the model to produce the same\noutput on these as on the original inputs. tangent propagation and dataset\naugmentation using manually specified transformations both require that the\nmodel should be invariant to certain specified directions of change in the input.\ndouble backprop and adversarial training both require that the model should be\ninvariant to\ndirections of change in the input so long as the change is small. just\nas dataset augmentation is the non-infinitesimal version of tangent propagation,\nadversarial training is the non-infinitesimal version of double backprop.\n\nall\n\n,\n\n,\n\n14\n\n14.10\n\nrifai et al. 2011c\n\nthe manifold tangent classifier (\n\n), eliminates the need to\n, autoencoders can\nknow the tangent vectors a priori. as we will see in chapter\nestimate the manifold tangent vectors. the manifold tangent classifier makes use\nof this technique to avoid needing user-specified tangent vectors.\u00a0as illustrated\nin fig.\n, these estimated tangent vectors go beyond the classical invariants\nthat arise out of the geometry of images (such as translation, rotation and scaling)\nand include factors that must be learned because they are object-specific (such as\nmoving body parts). the algorithm proposed with the manifold tangent classifier\nis therefore simple: (1) use an autoencoder to learn the manifold structure by\nunsupervised learning, and (2) use these tangents to regularize a neural net classifier\nas in tangent prop (eq.\n\n7.67\n\n).\n\nthis chapter has described most of the general strategies used to regularize\n\n270\n\n "}, {"Page_number": 286, "text": "chapter 7. regularization for deep learning\n\n2\nx\n\nnormal tangent\n\nx1\n\nrifai\n\n2011c\n\net al.,\n\n) and manifold tangent classifier (\n\nsimard et al.\n,\nfigure 7.9:\u00a0illustration of the main idea of the tangent prop algorithm (\n1992\n), which both regularize the\nclassifier output function f ( x). each curve represents the manifold for a different class,\nillustrated here as a one-dimensional manifold embedded in a two-dimensional space.\non one curve, we have chosen a single point and drawn a vector that is tangent to the\nclass manifold (parallel to and touching the manifold) and a vector that is normal to the\nclass manifold (orthogonal to the manifold). in multiple dimensions there may be many\ntangent directions and many normal directions. we expect the classification function to\nchange rapidly as it moves in the direction normal to the manifold, and not to change as\nit moves along the class manifold. both tangent propagation and the manifold tangent\nclassifier regularize f (x) to not change very much as x moves along the manifold. tangent\npropagation requires the user to manually specify functions that compute the tangent\ndirections (such as specifying that small translations of images remain in the same class\nmanifold) while the manifold tangent classifier estimates the manifold tangent directions\nby training an autoencoder to fit the training data. the use of autoencoders to estimate\nmanifolds will be described in chapter\n\n.14\n\nneural networks. regularization is a central theme of machine learning and as such\nwill be revisited periodically by most of the remaining chapters. another central\ntheme of machine learning is optimization, described next.\n\n271\n\n "}, {"Page_number": 287, "text": "chapter 7. regularization for deep learning\n\nalgorithm\u00a07.1 the early stopping\u00a0meta-algorithm for\u00a0determining the\u00a0best\namount of time to train. this meta-algorithm is a general strategy that works\nwell with a variety of training algorithms and ways of quantifying error on the\nvalidation set.\n\nn\n\nbe the number of steps between evaluations.\n\nlet\nlet p be the \u201cpatience,\u201d the number of times to observe worsening validation set\nerror before giving up.\nlet \u03b8o be the initial parameters.\n\u03b8\u2190 o\n\u03b8\ni \u2190 0\nj \u2190 0\nv \u2190 \u221e\n\u03b8\u2217 \u2190 \u03b8\ni\u2217 \u2190 i\nwhile\n\ndo\nby running the training algorithm for\n\nn\n\nsteps.\n\ni\n\nj < p\nupdate\n\u03b8\nn\u2190 +\ni\nv0 \u2190 validationseterror( )\u03b8\nif v0 < v then\nj \u2190 0\n\u03b8\u2217 \u2190 \u03b8\ni\u2217 \u2190 i\nv\u2190 0\nv\nelse\nj\u2190 + 1\nj\n\nend if\n\nend while\nbest parameters are \u03b8\u2217 , best number of training steps is i\u2217\n\n272\n\n "}, {"Page_number": 288, "text": "chapter 7. regularization for deep learning\n\nalgorithm 7.2 a meta-algorithm for using early stopping to determine how long\nto train, then retraining on all the data.\n\n)\n\n)\n\n)\n\n)\n\ntrain and y(\n\ntrain and y(\n\ntrain into (x(\n\nsubtrain , x (valid))\n\ntrain be the training set.\n)\n\nlet x(\nsplit x(\nrespectively.\nrun early stopping (algorithm ) starting from random\nand y(\nreturns i\u2217, the optimal number of steps.\nset\ntrain on x(\n\nto random values again.\n\ntrain for i\u2217 steps.\n\ntrain and y(\n\n7.1\n\n\u03b8\n\n)\n\n)\n\n)\nsubtrain\nsubtrain for training data and x (valid) and y(valid) for validation data. this\n\n\u03b8 using x(\n\n)\n\nand y(\n\n(\n\nsubtrain , y (valid))\n\n)\n\nalgorithm 7.3 meta-algorithm using early stopping to determine at what objec-\ntive value we start to overfit, then continue training until that value is reached.\n\n)\nsubtrain\nsubtrain for training data and x (valid) and y(valid) for validation data. this\n\n\u03b8 using x(\n\n7.1\n\n)\n\n)\n\n)\n\n)\n\n)\n\ntrain and y(\n\ntrain and y(\n\ntrain into (x(\n\nsubtrain , x (valid))\n\ntrain be the training set.\n)\n\nlet x(\nsplit x(\nrespectively.\nrun early stopping (algorithm ) starting from random\nand y(\nupdates\n.\u03b8\nj ,\u2190 (\u03b8 x(\n(cid:115)\nwhile j ,(\u03b8 x(valid), y (valid)) > (cid:115) do\ntrain on x(\ntrain for\n\ntrain and y(\n\nsubtrain , y(\n\n)\nsubtrain )\n\nsteps.\n\nn\n\n)\n\n)\n\n)\n\nand y(\n\n(\n\nsubtrain , y (valid))\n\n)\n\nend while\n\n273\n\n "}, {"Page_number": 289, "text": "chapter 8\n\noptimization for training deep\nmodels\n\ndeep learning algorithms involve optimization in many contexts. for example,\nperforming inference in models such as pca involves solving an optimization\nproblem. we often use analytical optimization to write proofs or design algorithms.\nof all of the many optimization problems involved in deep learning, the most\ndifficult is neural network training. it is quite common to invest days to months of\ntime on hundreds of machines in order to solve even a single instance of the neural\nnetwork training problem. because this problem is so important and so expensive,\na specialized set of optimization techniques have been developed for solving it.\nthis chapter presents these optimization techniques for neural network training.\n\nif you are unfamiliar with the basic principles of gradient-based optimization,\nwe suggest reviewing chapter . that chapter includes a brief overview of numerical\noptimization in general.\n\n4\n\nthis chapter focuses on one particular case of optimization: finding the param-\neters \u03b8 of a neural network that significantly reduce a cost function j (\u03b8), which\ntypically includes a performance measure evaluated on the entire training set as\nwell as additional regularization terms.\n\nwe begin with a description of how optimization used as a training algorithm\nfor a machine learning task differs from pure optimization. next, we present several\nof the concrete challenges that make optimization of neural networks difficult. we\nthen define several practical algorithms, including both optimization algorithms\nthemselves and strategies for initializing the parameters. more advanced algorithms\nadapt their learning rates during training or leverage information contained in\n\n274\n\n "}, {"Page_number": 290, "text": "chapter 8. optimization for training deep models\n\nthe second derivatives of the cost function. finally, we conclude with a review of\nseveral optimization strategies that are formed by combining simple optimization\nalgorithms into higher-level procedures.\n\n8.1 how learning differs from pure optimization\n\noptimization algorithms used for training of deep models differ from traditional\noptimization algorithms in several ways. machine learning usually acts indirectly.\nin most machine learning scenarios, we care about some performance measure\np , that is defined with respect to the test set and may also be intractable.\u00a0we\ntherefore optimize p only indirectly. we reduce a different cost function j(\u03b8) in\nthe hope that doing so will improve p . this is in contrast to pure optimization,\nwhere minimizing j is a goal in and of itself. optimization algorithms for training\ndeep models also typically include some specialization on the specific structure of\nmachine learning objective functions.\n\ntypically, the cost function can be written as an average over the training set,\n\nsuch as\n\nj( ) = \n\n\u03b8\n\ne\n\n(\n\nx,y \u223cpdatal f\n\n) \u02c6\n\n( ( ; )x \u03b8\n\n, y ,\n)\n\n(8.1)\n\nwhere l is the per-example loss function, f (x; \u03b8) is the predicted output when\nthe input is x, \u02c6pdata is the empirical distribution. in the supervised learning case,\ny is the target output. throughout this chapter, we develop the unregularized\nsupervised case, where the arguments to l are f(x; \u03b8) and y. however, it is trivial\nto extend this development, for example, to include \u03b8 or x as arguments, or to\nexclude y as arguments, in order to develop various forms of regularization or\nunsupervised learning.\n\n8.1\n\neq.\n\ndefines an objective function with respect to the training set. we\nwould usually prefer to minimize the corresponding objective function where the\nexpectation is taken across the data generating distribution pdata rather than\njust over the finite training set:\n\nj\u2217( ) = \n\n\u03b8\n\ne\n(\n\n)x,y \u223cpdatal f\n\n( ( ; )x \u03b8\n\n, y .\n)\n\n(8.2)\n\n8.1.1 empirical risk minimization\n\nthe goal of a machine learning algorithm is to reduce the expected generalization\nerror given by eq.\n. we emphasize here that\nthe expectation is taken over the true underlying distribution pdata. if we knew\nthe true distribution pdata(x, y), risk minimization would be an optimization task\n\n. this quantity is known as the\n\nrisk\n\n8.2\n\n275\n\n "}, {"Page_number": 291, "text": "chapter 8. optimization for training deep models\n\nsolvable by an optimization algorithm. however, when we do not know pdata(x, y)\nbut only have a training set of samples, we have a machine learning problem.\n\nthe simplest way to convert a machine learning problem back into an op-\ntimization problem is to minimize the expected loss on the training set. this\nmeans replacing the true distribution p(x, y) with the empirical distribution \u02c6p(x, y)\ndefined by the training set. we now minimize the empirical risk\n\ne\n\nx,y\u223c\u02c6pdata(\n\n)x,y [ ( ( ; )\n\nl f x \u03b8 , y\n\n)] =\n\n1\nm\n\nmxi=1\n\nl f( (x( )i ; )\u03b8 , y ( )i )\n\n(8.3)\n\nwhere\n\nm\n\nis the number of training examples.\n\nthe training process based on minimizing this average training error is known\nas empirical risk minimization. in this setting, machine learning is still very similar\nto straightforward optimization. rather than optimizing the risk directly, we\noptimize the empirical risk, and hope that the risk decreases significantly as well.\na variety of theoretical results establish conditions under which the true risk can\nbe expected to decrease by various amounts.\n\nhowever, empirical risk minimization is prone to overfitting. models with\nhigh capacity can simply memorize the training set. in many cases, empirical\nrisk minimization is not really feasible. the most effective modern optimization\nalgorithms are based on gradient descent, but many useful loss functions, such\nas 0-1 loss, have no useful derivatives (the derivative is either zero or undefined\neverywhere). these two problems mean that, in the context of deep learning, we\nrarely use empirical risk minimization. instead, we must use a slightly different\napproach, in which the quantity that we actually optimize is even more different\nfrom the quantity that we truly want to optimize.\n\n8.1.2 surrogate loss functions and early stopping\n\nsometimes, the loss function we actually care about (say classification error) is not\none that can be optimized efficiently. for example, exactly minimizing expected 0-1\nloss is typically intractable (exponential in the input dimension), even for a linear\nclassifier (marcotte and savard 1992\n). in such situations, one typically optimizes\na surrogate loss function instead, which acts as a proxy but has advantages. for\nexample, the negative log-likelihood of the correct class is typically used as a\nsurrogate for the 0-1 loss. the negative log-likelihood allows the model to estimate\nthe conditional probability of the classes, given the input, and if the model can\ndo that well, then it can pick the classes that yield the least classification error in\nexpectation.\n\n,\n\n276\n\n "}, {"Page_number": 292, "text": "chapter 8. optimization for training deep models\n\nin some cases, a surrogate loss function actually results in being able to learn\nmore. for example, the test set 0-1 loss often continues to decrease for a long\ntime after the training set 0-1 loss has reached zero, when training using the\nlog-likelihood surrogate. this is because even when the expected 0-1 loss is zero,\none can improve the robustness of the classifier by further pushing the classes apart\nfrom each other, obtaining a more confident and reliable classifier, thus extracting\nmore information from the training data than would have been possible by simply\nminimizing the average 0-1 loss on the training set.\n\na very important difference between optimization in general and optimization\nas we use it for training algorithms is that training algorithms do not usually halt\nat a local minimum. instead, a machine learning algorithm usually minimizes\na surrogate loss function but halts when a convergence criterion based on early\nstopping (sec.\n) is satisfied. typically the early stopping criterion is based on\nthe true underlying loss function, such as 0-1 loss measured on a validation set,\nand is designed to cause the algorithm to halt whenever overfitting begins to occur.\ntraining often halts while the surrogate loss function still has large derivatives,\nwhich is very different from the pure optimization setting, where an optimization\nalgorithm is considered to have converged when the gradient becomes very small.\n\n7.8\n\n8.1.3 batch and minibatch algorithms\n\none aspect of machine learning algorithms that separates them from general\noptimization algorithms is that the objective function usually decomposes as a sum\nover the training examples. optimization algorithms for machine learning typically\ncompute each update to the parameters based on an expected value of the cost\nfunction estimated using only a subset of the terms of the full cost function.\n\nfor example, maximum likelihood estimation problems, when viewed in log\n\nspace, decompose into a sum over each example:\n\n\u03b8ml = arg max\n\n\u03b8\n\nmxi=1\n\nlog pmodel(x( )i , y( )i ; )\u03b8 .\n\n(8.4)\n\nmaximizing this sum is equivalent to maximizing the expectation over the\n\nempirical distribution defined by the training set:\n\nj( ) = \n\n\u03b8\n\nex,y\u223c\u02c6pdata log pmodel (\n\n; )\nx, y \u03b8 .\n\n(8.5)\n\nmost of the properties of the objective function j used by most of our opti-\nmization algorithms are also expectations over the training set. for example, the\n\n277\n\n "}, {"Page_number": 293, "text": "chapter 8. optimization for training deep models\n\nmost commonly used property is the gradient:\n\n\u2207\u03b8j( ) = \n\n\u03b8\n\nex,y\u223c\u02c6pdata\u2207\u03b8 log pmodel (\n\n; )\nx, y \u03b8 .\n\n(8.6)\n\ncomputing\u00a0this expectation\u00a0exactly\u00a0is very\u00a0expensive\u00a0because\u00a0it\u00a0requires\nevaluating the model on every example in the entire dataset. in practice, we can\ncompute these expectations by randomly sampling a small number of examples\nfrom the dataset, then taking the average over only those examples.\n\n5.46\n\n) estimated from\n\nrecall that the standard error of the mean (eq.\n\nn samples\nis given by \u03c3/\u221an, where \u03c3 is the true standard deviation of the value of the samples.\nthe denominator of \u221an shows that there are less than linear returns to using\nmore examples to estimate the gradient. compare two hypothetical estimates of\nthe gradient, one based on 100 examples and another based on 10,000 examples.\nthe latter requires 100 times more computation than the former, but reduces the\nstandard error of the mean only by a factor of 10. most optimization algorithms\nconverge much faster (in terms of total computation, not in terms of number of\nupdates) if they are allowed to rapidly compute approximate estimates of the\ngradient rather than slowly computing the exact gradient.\n\nanother consideration motivating statistical estimation of the gradient from a\nsmall number of samples is redundancy in the training set. in the worst case, all\nm samples in the training set could be identical copies of each other. a sampling-\nbased estimate of the gradient could compute the correct gradient with a single\nsample, using m times less computation than the naive approach. in practice, we\nare unlikely to truly encounter this worst-case situation, but we may find large\nnumbers of examples that all make very similar contributions to the gradient.\n\noptimization algorithms that use the entire training set are called batch or\ndeterministic gradient methods, because they process all of the training examples\nsimultaneously in a large batch. this terminology can be somewhat confusing\nbecause the word \u201cbatch\u201d is also often used to describe the minibatch used by\nminibatch stochastic gradient descent. typically the term \u201cbatch gradient descent\u201d\nimplies the use of the full training set, while the use of the term \u201cbatch\u201d to describe\na group of examples does not.\u00a0for example, it is very common to use the term\n\u201cbatch size\u201d to describe the size of a minibatch.\n\nor sometimes\n\noptimization algorithms that use only a single example at a time are sometimes\ncalled stochastic\nmethods. the term online is usually reserved\nfor the case where the examples are drawn from a stream of continually created\nexamples rather than from a fixed-size training set over which several passes are\nmade.\n\nonline\n\nmost algorithms used for deep learning fall somewhere in between, using more\n\n278\n\n "}, {"Page_number": 294, "text": "chapter 8. optimization for training deep models\n\nthan one but less than all of the training examples. these were traditionally called\nminibatch minibatch stochastic\nmethods and it is now common to simply call\nthem stochastic methods.\n\nor\n\nthe canonical example of a stochastic method is stochastic gradient descent,\n\npresented in detail in sec.\n\n.\n8.3.1\n\nminibatch sizes are generally driven by the following factors:\n\n\u2022 larger batches provide a more accurate estimate of the gradient, but with\n\nless than linear returns.\n\n\u2022 multicore architectures are usually underutilized by extremely small batches.\nthis motivates using some absolute minimum batch size, below which there\nis no reduction in the time to process a minibatch.\n\n\u2022 if all examples in the batch are to be processed in parallel (as is typically\nthe case), then the amount of memory scales with the batch size. for many\nhardware setups this is the limiting factor in batch size.\n\n\u2022 some kinds of hardware achieve better runtime with specific sizes of arrays.\nespecially when using gpus, it is common for power of 2 batch sizes to offer\nbetter runtime. typical power of 2 batch sizes range from 32 to 256, with 16\nsometimes being attempted for large models.\n\nwilson and martinez 2003\n\n\u2022 small batches can offer a regularizing effect (\n\n),\nperhaps due to the noise they add to the learning process. generalization\nerror is often best for a batch size of 1. training with such a small batch\nsize might require a small learning rate to maintain stability due to the high\nvariance in the estimate of the gradient. the total runtime can be very high\ndue to the need to make more steps, both because of the reduced learning\nrate and because it takes more steps to observe the entire training set.\n\n,\n\ndifferent kinds of algorithms use different kinds of information from the mini-\nbatch in different ways. some algorithms are more sensitive to sampling error than\nothers, either because they use information that is difficult to estimate accurately\nwith few samples, or because they use information in ways that amplify sampling\nerrors more. methods that compute updates based only on the gradient g are\nusually relatively robust and can handle smaller batch sizes like 100. second-order\nmethods, which use also the hessian matrix h and compute updates such as\nh\u22121g, typically require much larger batch sizes like 10,000. these large batch\nsizes are required to minimize fluctuations in the estimates of h\u22121g. suppose\nthat h is estimated perfectly but has a poor condition number. multiplication by\n\n279\n\n "}, {"Page_number": 295, "text": "chapter 8. optimization for training deep models\n\nh or its inverse amplifies pre-existing errors, in this case, estimation errors in g.\nvery small changes in the estimate of g can thus cause large changes in the update\nh\u22121g, even if h were estimated perfectly. of course, h will be estimated only\napproximately, so the update h\u22121g will contain even more error than we would\npredict from applying a poorly conditioned operation to the estimate of\n\n.g\n\nit is also crucial that the minibatches be selected randomly. computing an\nunbiased estimate of the expected gradient from a set of samples requires that those\nsamples be independent. we also wish for two subsequent gradient estimates to be\nindependent from each other, so two subsequent minibatches of examples should\nalso be independent from each other. many datasets are most naturally arranged\nin a way where successive examples are highly correlated. for example, we might\nhave a dataset of medical data with a long list of blood sample test results. this\nlist might be arranged so that first we have five blood samples taken at different\ntimes from the first patient, then we have three blood samples taken from the\nsecond patient, then the blood samples from the third patient, and so on. if we\nwere to draw examples in order from this list, then each of our minibatches would\nbe extremely biased, because it would represent primarily one patient out of the\nmany patients in the dataset. in cases such as these where the order of the dataset\nholds some significance, it is necessary to shuffle the examples before selecting\nminibatches. for very large datasets, for example datasets containing billions of\nexamples in a data center, it can be impractical to sample examples truly uniformly\nat random every time we want to construct a minibatch. fortunately, in practice\nit is usually sufficient to shuffle the order of the dataset once and then store it in\nshuffled fashion. this will impose a fixed set of possible minibatches of consecutive\nexamples that all models trained thereafter will use, and each individual model\nwill be forced to reuse this ordering every time it passes through the training\ndata. however, this deviation from true random selection does not seem to have a\nsignificant detrimental effect. failing to ever shuffle the examples in any way can\nseriously reduce the effectiveness of the algorithm.\n\nmany optimization problems in machine learning decompose over examples\nwell enough that we can compute entire separate updates over different examples\nin parallel. in other words, we can compute the update that minimizes j(x) for\none minibatch of examples x at the same time that we compute the update for\nseveral other minibatches. such asynchronous parallel distributed approaches are\ndiscussed further in sec.\n\n.\n12.1.3\n\nan interesting motivation for minibatch stochastic gradient descent is that it\nfollows the gradient of the true generalization error (eq.\n) so long as no\nexamples are repeated. most implementations of minibatch stochastic gradient\n\n8.2\n\n280\n\n "}, {"Page_number": 296, "text": "chapter 8. optimization for training deep models\n\ndescent shuffle the dataset once and then pass through it multiple times. on the\nfirst pass, each minibatch is used to compute an unbiased estimate of the true\ngeneralization error. on the second pass, the estimate becomes biased because it is\nformed by re-sampling values that have already been used, rather than obtaining\nnew fair samples from the data generating distribution.\n\nthe fact that stochastic gradient descent minimizes generalization error is\neasiest to see in the online learning case, where examples or minibatches are drawn\nfrom a stream of data.\u00a0in other words, instead of receiving a fixed-size training\nset, the learner is similar to a living being who sees a new example at each instant,\nwith every example (x, y) coming from the data generating distribution pdata(x, y).\nin this scenario, examples are never repeated; every experience is a fair sample\nfrom p data.\n\nthe equivalence is easiest to derive when both x and y are discrete.\u00a0in this\n\ncase, the generalization error (eq.\n\n8.2\n\n) can be written as a sum\n\nwith the exact gradient\n\nj\u2217( ) =\u03b8 xx xy\ng = \u2207\u03b8j \u2217( ) =\u03b8 xx xy\n\npdata(\n\nx, y l f x \u03b8 , y ,\n)\n\n) ( ( ; )\n\npdata(\n\n)x, y \u2207xl f\n\n( ( ; )x \u03b8\n\n, y .\n)\n\n(8.7)\n\n(8.8)\n\nwe have already seen the same fact demonstrated for the log-likelihood in eq. 8.5\nand eq.\nl besides the\nlikelihood. a similar result can be derived when x and y are continuous, under\nmild assumptions regarding pdata and .l\n\n; we observe now that this holds for other functions\n\n8.6\n\nhence,\u00a0we can obtain an unbiased estimator of the exact gradient of\u00a0the\n)m } with cor-\nfrom the data generating distribution pdata, and computing\n\ngeneralization error by sampling a minibatch of examples {x(1) , . . . x(\nresponding targets y( )i\nthe gradient of the loss with respect to the parameters for that minibatch:\n\n\u02c6g =\n\n1\n\nm\u2207 \u03b8xi\n\nl f( (x( )i ; )\u03b8 , y ( )i ).\n\n(8.9)\n\nupdating\n\n\u03b8\n\nin the direction of\n\n\u02c6g performs sgd on the generalization error.\n\nof course,\u00a0this interpretation only\u00a0applies when examples are not reused.\nnonetheless, it is usually best to make several passes through the training set,\nunless the training set is extremely large.\u00a0when multiple such epochs are used,\nonly the first epoch follows the unbiased gradient of the generalization error, but\n\n281\n\n "}, {"Page_number": 297, "text": "chapter 8. optimization for training deep models\n\nof course, the additional epochs usually provide enough benefit due to decreased\ntraining error to offset the harm they cause by increasing the gap between training\nerror and test error.\n\nwith some datasets growing rapidly in size, faster than computing power, it\nis becoming more common for machine learning applications to use each training\nexample only once or even to make an incomplete pass through the training\nset. when using an extremely large training set, overfitting is not an issue, so\nunderfitting and computational efficiency become the predominant concerns. see\nalso\n) for a discussion of the effect of computational\nbottlenecks on generalization error, as the number of training examples grows.\n\nbottou and bousquet 2008\n\n(\n\n8.2 challenges in neural network optimization\n\noptimization in general is an extremely difficult task. traditionally, machine\nlearning has avoided the difficulty of general optimization by carefully designing\nthe objective function and constraints to ensure that the optimization problem is\nconvex. when training neural networks, we must confront the general non-convex\ncase. even convex optimization is not without its complications. in this section,\nwe summarize several of the most prominent challenges involved in optimization\nfor training deep models.\n\n8.2.1\n\nill-conditioning\n\nsome challenges arise even when optimizing convex functions. of these, the most\nprominent is ill-conditioning of the hessian matrix h. this is a very general\nproblem in most numerical optimization, convex or otherwise, and is described in\nmore detail in sec.\n\n4.3.1\n.\n\nthe ill-conditioning problem is generally believed to be present in neural\nnetwork training problems. ill-conditioning can manifest by causing sgd to get\n\u201cstuck\u201d in the sense that even very small steps increase the cost function.\n\nrecall from eq.\n\n4.9\n\nthat a second-order taylor series expansion of the cost\n\nfunction predicts that a gradient descent step of\n\n\u2212(cid:115)g\n\nwill add\n\n1\n2\n\n(cid:115)2g> hg\n\ng\u2212 (cid:115) >g\n\n(8.10)\n\n2(cid:115)2g>hg\nto the cost. ill-conditioning of the gradient becomes a problem when 1\nexceeds (cid:115)g> g.\u00a0to determine whether ill-conditioning is detrimental to a neural\nnetwork training task, one can monitor the squared gradient norm g>g and the\n\n282\n\n "}, {"Page_number": 298, "text": "chapter 8. optimization for training deep models\n\n16\n\n14\n\n12\n\n10\n\n8\n\n6\n\n4\n\n2\n\nm\nr\no\nn\n \nt\nn\ne\ni\n\nd\na\nr\ng\n\n0\n\u22122\n\n\u221250\n\ne\nt\na\nr\n\nr\no\nr\nr\ne\n\nn\no\ni\nt\na\nc\nfi\n\ni\ns\ns\na\nl\nc\n\n1 0.\n\n0 9.\n\n0 8.\n\n0 7.\n\n0 6.\n\n0 5.\n\n0 4.\n\n0 3.\n\n0 2.\n\n0 1.\n\n0\n\n50 100 150 200 250\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\ntraining time (epochs)\n\ntraining time (epochs)\n\nfigure 8.1: gradient descent often does not arrive at a critical point of any kind. in\nthis example, the gradient norm increases throughout training of a convolutional network\nused for object detection. (left) a scatterplot showing how the norms of individual\ngradient evaluations are distributed over time. to improve legibility, only one gradient\nnorm is plotted per epoch. the running average of all gradient norms is plotted as a solid\ncurve. the gradient norm clearly increases over time, rather than decreasing as we would\nexpect if the training process converged to a critical point.\ndespite the increasing\ngradient, the training process is reasonably successful. the validation set classification\nerror decreases to a low level.\n\n(right)\n\ng>hg term.\nin many cases,\u00a0the gradient norm does not shrink significantly\nthroughout learning, but the g> hg term grows by more than order of magnitude.\nthe result is that learning becomes very slow despite the presence of a strong\ngradient because the learning rate must be shrunk to compensate for even stronger\ncurvature. fig.\nshows an example of the gradient increasing significantly during\nthe successful training of a neural network.\n\n8.1\n\nthough ill-conditioning is present in other settings besides neural network\ntraining, some of the techniques used to combat it in other contexts are less\napplicable to neural networks. for example, newton\u2019s method is an excellent tool\nfor minimizing convex functions with poorly conditioned hessian matrices, but in\nthe subsequent sections we will argue that newton\u2019s method requires significant\nmodification before it can be applied to neural networks.\n\n8.2.2 local minima\n\none of the most prominent features of a convex optimization problem is that it\ncan be reduced to the problem of finding a local minimum. any local minimum is\n\n283\n\n "}, {"Page_number": 299, "text": "chapter 8. optimization for training deep models\n\nguaranteed to be a global minimum. some convex functions have a flat region at\nthe bottom rather than a single global minimum point, but any point within such\na flat region is an acceptable solution. when optimizing a convex function, we\nknow that we have reached a good solution if we find a critical point of any kind.\n\nwith non-convex functions, such as neural nets, it is possible to have many\nlocal minima. indeed, nearly any deep model is essentially guaranteed to have\nan extremely large number of local minima. however, as we will see, this is not\nnecessarily a major problem.\n\nneural networks and any models with multiple equivalently parametrized latent\nvariables all have multiple local minima because of the model identifiability problem.\na model is said to be identifiable if a sufficiently large training set can rule out all\nbut one setting of the model\u2019s parameters. models with latent variables are often\nnot identifiable because we can obtain equivalent models by exchanging latent\nvariables with each other. for example, we could take a neural network and modify\nlayer 1 by swapping the incoming weight vector for unit i with the incoming weight\nvector for unit j, then doing the same for the outgoing weight vectors. if we have\nm layers with n units each, then there are n! m ways of arranging the hidden units.\nthis kind of non-identifiability is known as weight space symmetry.\n\nin addition to weight space symmetry, many kinds of neural networks have\nadditional causes of non-identifiability. for example, in any rectified linear or\nmaxout network, we can scale all of the incoming weights and biases of a unit by\n\u03b1 if we also scale all of its outgoing weights by 1\n\u03b1. this means that\u2014if the cost\nfunction does not include terms such as weight decay that depend directly on the\nweights rather than the models\u2019 outputs\u2014every local minimum of a rectified linear\nor maxout network lies on an (m n\u00d7 )-dimensional hyperbola of equivalent local\nminima.\n\nthese model identifiability issues mean that there can be an extremely large\nor even uncountably infinite amount of local minima in a neural network cost\nfunction. however, all of these local minima arising from non-identifiability are\nequivalent to each other in cost function value. as a result, these local minima are\nnot a problematic form of non-convexity.\n\nlocal minima can be problematic if they have high cost in comparison to the\nglobal minimum. one can construct small neural networks, even without hidden\nunits, that have local minima with higher cost than the global minimum (sontag\nand sussman 1989 brady\n). if local minima\nwith high cost are common, this could pose a serious problem for gradient-based\noptimization algorithms.\n\n1989 gori and tesi 1992\n\net al.,\n\n,\n\n;\n\n;\n\n,\n\nit remains an open question whether there are many local minima of high cost\n\n284\n\n "}, {"Page_number": 300, "text": "chapter 8. optimization for training deep models\n\nfor networks of practical interest and whether optimization algorithms encounter\nthem. for many years, most practitioners believed that local minima were a\ncommon problem plaguing neural network optimization. today, that does not\nappear to be the case. the problem remains an active area of research, but experts\nnow suspect that, for sufficiently large neural networks, most local minima have a\nlow cost function value, and that it is not important to find a true global minimum\nrather than to find a point in parameter space that has low but not minimal cost\n(\nsaxe et al. 2013 dauphin et al. 2014 goodfellow et al. 2015 choromanska\net al.,\n\n2014\n\n,\n).\n\n;\n\n,\n\n;\n\n,\n\n;\n\nmany practitioners attribute nearly all difficulty with neural network optimiza-\ntion to local minima. we encourage practitioners to carefully test for specific\nproblems. a test that can rule out local minima as the problem is to plot the\nnorm of the gradient over time. if the norm of the gradient does not shrink to\ninsignificant size, the problem is neither local minima nor any other kind of critical\npoint. this kind of negative test can rule out local minima. in high dimensional\nspaces, it can be very difficult to positively establish that local minima are the\nproblem. many structures other than local minima also have small gradients.\n\n8.2.3 plateaus, saddle points and other flat regions\n\nfor many high-dimensional non-convex functions, local minima (and maxima)\nare in fact rare compared to another kind of point with zero gradient: a saddle\npoint. some points around a saddle point have greater cost than the saddle point,\nwhile others have a lower cost. at a saddle point, the hessian matrix has both\npositive and negative eigenvalues. points lying along eigenvectors associated with\npositive eigenvalues have greater cost than the saddle point, while points lying\nalong negative eigenvalues have lower value. we can think of a saddle point as\nbeing a local minimum along one cross-section of the cost function and a local\nmaximum along another cross-section. see fig.\n\nfor an illustration.\n\n4.5\n\nmany classes\u00a0of random functions exhibit the following behavior:\n\nin low-\ndimensional spaces, local minima are common. in higher dimensional spaces, local\nminima are rare and saddle points are more common. for a function f : rn \u2192 r of\nthis type, the expected ratio of the number of saddle points to local minima grows\nexponentially with n. to understand the intuition behind this behavior, observe\nthat the hessian matrix at a local minimum has only positive eigenvalues. the\nhessian matrix at a saddle point has a mixture of positive and negative eigenvalues.\nimagine that the sign of each eigenvalue is generated by flipping a coin. in a single\ndimension, it is easy to obtain a local minimum by tossing a coin and getting heads\nonce. in n-dimensional space, it is exponentially unlikely that all n coin tosses will\n\n285\n\n "}, {"Page_number": 301, "text": "chapter 8. optimization for training deep models\n\nbe heads. see\n\ndauphin et al. 2014\n\n(\n\n) for a review of the relevant theoretical work.\n\nan amazing property of many random functions is that the eigenvalues of the\nhessian become more likely to be positive as we reach regions of lower cost.\u00a0in\nour coin tossing analogy, this means we are more likely to have our coin come up\nheads n times if we are at a critical point with low cost.\u00a0this means that local\nminima are much more likely to have low cost than high cost. critical points with\nhigh cost are far more likely to be saddle points. critical points with extremely\nhigh cost are more likely to be local maxima.\n\n(\n\n14\n\nbaldi and hornik 1989\n\nthis happens for many classes of random functions. does it happen for neural\nnetworks?\n) showed theoretically that shallow autoencoders\n(feedforward networks trained to copy their input to their output, described in\nchapter\n) with no nonlinearities have global minima and saddle points but no\nlocal minima with higher cost than the global minimum. they observed without\nproof that these results extend to deeper networks without nonlinearities. the\noutput of such networks is a linear function of their input, but they are useful\nto study as a model of nonlinear neural networks because their loss function is\na non-convex function of their parameters. such networks are essentially just\nmultiple matrices composed together.\n) provided exact solutions\nto the complete learning dynamics in such networks and showed that learning in\nthese models captures many of the qualitative features observed in the training of\ndeep models with nonlinear activation functions.\n) showed\nexperimentally that real neural networks also have loss functions that contain very\nmany high-cost saddle points. choromanska\n) provided additional\ntheoretical arguments, showing that another class of high-dimensional random\nfunctions related to neural networks does so as well.\n\ndauphin et al. 2014\n\nsaxe et al. 2013\n\net al. (\n\n2014\n\n(\n\n(\n\nwhat are the implications of the proliferation of saddle points for training algo-\nrithms? for first-order optimization algorithms that use only gradient information,\nthe situation is unclear. the gradient can often become very small near a saddle\npoint. on the other hand, gradient descent empirically seems to be able to escape\nsaddle points in many cases.\n) provided visualizations of\nseveral learning trajectories of state-of-the-art neural networks, with an example\ngiven in fig.\n. these visualizations show a flattening of the cost function near\na prominent saddle point where the weights are all zero, but they also show the\ngradient descent trajectory rapidly escaping this region.\ngoodfellow et al. 2015\n)\nalso argue that continuous-time gradient descent may be shown analytically to be\nrepelled from, rather than attracted to, a nearby saddle point, but the situation\nmay be different for more realistic uses of gradient descent.\n\ngoodfellow et al. 2015\n\n8.2\n\n(\n\n(\n\nfor newton\u2019s method,\u00a0it is clear that saddle points constitute a problem.\n\n286\n\n "}, {"Page_number": 302, "text": "chapter 8. optimization for training deep models\n\nj\n(\n\u03b8\n)\n\nprojection 1 of \u03b8\n\n\u03b8\n\nf\n\no\n\n2\n\nn\n\nt i o\n\nc\n\ne\n\nj\n\no\n\np r\n\n2015\n\net al. (\n\nfigure 8.2: a visualization of the cost function of a neural network. image adapted\nwith permission from goodfellow\n).\u00a0these visualizations appear similar for\nfeedforward neural networks, convolutional networks, and recurrent networks applied\nto real object recognition and natural language processing tasks. surprisingly, these\nvisualizations usually do not show many conspicuous obstacles.\u00a0prior to the success of\nstochastic gradient descent for training very large models beginning in roughly 2012,\nneural net cost function surfaces were generally believed to have much more non-convex\nstructure than is revealed by these projections.\u00a0the primary obstacle revealed by this\nprojection is a saddle point of high cost near where the parameters are initialized, but, as\nindicated by the blue path, the sgd training trajectory escapes this saddle point readily.\nmost of training time is spent traversing the relatively flat valley of the cost function,\nwhich may be due to high noise in the gradient, poor conditioning of the hessian matrix\nin this region, or simply the need to circumnavigate the tall \u201cmountain\u201d visible in the\nfigure via an indirect arcing path.\n\n287\n\n "}, {"Page_number": 303, "text": "chapter 8. optimization for training deep models\n\ngradient descent is designed to move \u201cdownhill\u201d and is not explicitly designed\nto seek a critical point. newton\u2019s method, however, is designed to solve for a\npoint where the gradient is zero. without appropriate modification, it can jump\nto a saddle point. the proliferation of saddle points in high dimensional spaces\npresumably explains why second-order methods have not succeeded in replacing\ngradient descent for neural network training.\n) introduced\na saddle-free newton method for second-order optimization and showed that it\nimproves significantly over the traditional version. second-order methods remain\ndifficult to scale to large neural networks, but this saddle-free approach holds\npromise if it could be scaled.\n\ndauphin et al. 2014\n\n(\n\nthere are other kinds of points with zero gradient besides minima and saddle\npoints. there are also maxima,\u00a0which are much like saddle points from the\nperspective of optimization\u2014many algorithms are not attracted to them,\u00a0but\nunmodified newton\u2019s method\u00a0is. maxima become exponentially rare in high\ndimensional space, just like minima do.\n\nthere may also be wide, flat regions of constant value. in these locations, the\ngradient and also the hessian are all zero. such degenerate locations pose major\nproblems for all numerical optimization algorithms. in a convex problem, a wide,\nflat region must consist entirely of global minima, but in a general optimization\nproblem, such a region could correspond to a high value of the objective function.\n\n8.2.4 cliffs and exploding gradients\n\nneural networks with many layers often have extremely steep regions resembling\ncliffs, as illustrated in fig.\n. these result from the multiplication of several large\nweights together. on the face of an extremely steep cliff structure, the gradient\nupdate step can move the parameters extremely far, usually jumping off of the\ncliff structure altogether.\n\n8.3\n\n288\n\n "}, {"Page_number": 304, "text": "chapter 8. optimization for training deep models\n\n)\nb\n;\nw\n(\nj\n\nw\n\nb\n\nfigure 8.3: the objective function for highly nonlinear deep neural networks or for\nrecurrent neural networks often contains sharp nonlinearities in parameter space resulting\nfrom the multiplication of several parameters. these nonlinearities give rise to very\nhigh derivatives in some places. when the parameters get close to such a cliff region, a\ngradient descent update can catapult the parameters very far, possibly losing most of the\noptimization work that had been done.\u00a0figure adapted with permission from pascanu\net al. (\n\n2013a\n\n).\n\n10.11.1\n\nthe cliff can be dangerous whether we approach it from above or from below,\nbut fortunately its most serious consequences can be avoided using the gradient\nclipping heuristic described in sec.\u00a0\n.\u00a0the basic idea is to recall that the\ngradient does not specify the optimal step size, but only the optimal direction\nwithin an infinitesimal region. when the traditional gradient descent algorithm\nproposes to make a very large step, the gradient clipping heuristic intervenes to\nreduce the step size to be small enough that it is less likely to go outside the region\nwhere the gradient indicates the direction of approximately steepest descent. cliff\nstructures are most common in the cost functions for recurrent neural networks,\nbecause such models involve a multiplication of many factors, with one factor\nfor each time step. long temporal sequences thus incur an extreme amount of\nmultiplication.\n\n8.2.5 long-term dependencies\n\nanother difficulty that neural network optimization algorithms must overcome arises\nwhen the computational graph becomes extremely deep.\u00a0feedforward networks\nwith many layers have such deep computational graphs. so do recurrent networks,\n,\u00a0which construct very deep computational graphs by\ndescribed in chapter\n\n10\n\n289\n\n "}, {"Page_number": 305, "text": "chapter 8. optimization for training deep models\n\nrepeatedly applying the same operation at each time step of a long temporal\nsequence. repeated application of the same parameters gives rise to especially\npronounced difficulties.\n\nfor example, suppose that a computational graph contains a path that consists\nof repeatedly multiplying by a matrix w . after t steps, this is equivalent to mul-\ntiplying by w t. suppose that w has an eigendecomposition w = v diag(\u03bb)v \u22121.\nin this simple case, it is straightforward to see that\n\nw t =(cid:23)v\n\n\u03bb v\n\ndiag( ) \u22121(cid:24)t\n\n= \n\nv diag \u03bb tv \u22121.\n\n( )\n\n(8.11)\n\n1\n\n1\n\nin magnitude or vanish if they are less than\n\nany eigenvalues \u03bbi that are not near an absolute value of will either explode if\nthey are greater than\nin magnitude.\nthe vanishing and exploding gradient problem refers to the fact that gradients\nthrough such a graph are also scaled according to diag( \u03bb)t. vanishing gradients\nmake it difficult to know which direction the parameters should move to improve\nthe cost function, while exploding gradients can make learning unstable. the cliff\nstructures described earlier that motivate gradient clipping are an example of the\nexploding gradient phenomenon.\n\n1\n\nthe repeated multiplication by w at each time step described here is very\nsimilar to the power method algorithm used to find the largest eigenvalue of a matrix\nw and the corresponding eigenvector. from this point of view it is not surprising\nthat x> wt will eventually discard all components of x that are orthogonal to the\nprincipal eigenvector of\n\n.w\n\nrecurrent networks use the same matrix w at each time step, but feedforward\nnetworks do not, so even very deep feedforward networks can largely avoid the\nvanishing and exploding gradient problem (\n\nsussillo 2014\n\n).\n\n,\n\nwe defer a further discussion of the challenges of training recurrent networks\n\nuntil sec.\n\n10.7\n\n, after recurrent networks have been described in more detail.\n\n8.2.6\n\ninexact gradients\n\nmost optimization algorithms are primarily motivated by the case where we have\nexact knowledge of the gradient or hessian matrix. in practice, we usually only\nhave a noisy or even biased estimate of these quantities. nearly every deep learning\nalgorithm relies on sampling-based estimates at least insofar as using a minibatch\nof training examples to compute the gradient.\n\nin other cases, the objective function we want to minimize is actually intractable.\nwhen the objective function is intractable, typically its gradient is intractable as\nwell. in such cases we can only approximate the gradient. these issues mostly arise\n\n290\n\n "}, {"Page_number": 306, "text": "chapter 8. optimization for training deep models\n\nwith the more advanced models in part\n. for example, contrastive divergence\ngives a technique for approximating the gradient of the intractable log-likelihood\nof a boltzmann machine.\n\niii\n\nvarious neural network optimization algorithms are designed to account for\nimperfections in the gradient estimate. one can also avoid the problem by choosing\na surrogate loss function that is easier to approximate than the true loss.\n\n8.2.7 poor correspondence between local and global structure\n\nmany of the problems we have discussed so far correspond to properties of the\nloss function at a single point\u2014it can be difficult to make a single step if j (\u03b8 ) is\npoorly conditioned at the current point \u03b8, or if \u03b8 lies on a cliff, or if \u03b8 is a saddle\npoint hiding the opportunity to make progress downhill from the gradient.\n\nit is possible to overcome all of these problems at a single point and still\nperform poorly if the direction that results in the most improvement locally does\nnot point toward distant regions of much lower cost.\n\n2015\n\net al. (\n\ngoodfellow\n\n) argue that much of the runtime of training is due to\nthe length of the trajectory needed to arrive at the solution. fig.\nshows that\nthe learning trajectory spends most of its time tracing out a wide arc around a\nmountain-shaped structure.\n\n8.2\n\nmuch of research into the difficulties of optimization has focused on whether\ntraining arrives at a global minimum, a local minimum, or a saddle point, but in\npractice neural networks do not arrive at a critical point of any kind. fig. 8.1\nshows that neural networks often do not arrive at a region of small gradient. indeed,\nsuch critical points do not even necessarily exist. for example, the loss function\n\u2212 log p(y | x; \u03b8) can lack a global minimum point and instead asymptotically\napproach some value as the model becomes more confident. for a classifier with\ndiscrete y and p(y | x) provided by a softmax, the negative log-likelihood can\nbecome arbitrarily close to zero if the model is able to correctly classify every\nexample in the training set, but it is impossible to actually reach the value of\nzero. likewise, a model of real values p( y | x) = n (y; f (\u03b8), \u03b2 \u22121) can have negative\nlog-likelihood that asymptotes to negative infinity\u2014if f(\u03b8) is able to correctly\npredict the value of all training set y targets, the learning algorithm will increase\n\u03b2 without bound. see fig.\nfor an example of a failure of local optimization to\nfind a good cost function value even in the absence of any local minima or saddle\npoints.\n\n8.4\n\nfuture research will need to develop further understanding of the factors that\ninfluence the length of the learning trajectory and better characterize the outcome\n\n291\n\n "}, {"Page_number": 307, "text": "chapter 8. optimization for training deep models\n\n)\n\u03b8\n(\nj\n\n\u03b8\n\nfigure 8.4: optimization based on local downhill moves can fail if the local surface does\nnot point toward the global solution. here we provide an example of how this can occur,\neven if there are no saddle points and no local minima. this example cost function\ncontains only asymptotes toward low values, not minima. the main cause of difficulty in\nthis case is being initialized on the wrong side of the \u201cmountain\u201d and not being able to\ntraverse it.\u00a0in higher dimensional space, learning algorithms can often circumnavigate\nsuch mountains but the trajectory associated with doing so may be long and result in\nexcessive training time, as illustrated in fig.\n\n.8.2\n\nof the process.\n\nmany existing research directions are aimed at finding good initial points for\nproblems that have difficult global structure, rather than developing algorithms\nthat use non-local moves.\n\ngradient descent and essentially all learning algorithms that are effective for\ntraining neural networks are based on making small, local moves. the previous\nsections have primarily focused on how the correct direction of these local moves\ncan be difficult to compute. we may be able to compute some properties of the\nobjective function, such as its gradient, only approximately, with bias or variance\nin our estimate of the correct direction. in these cases, local descent may or may\nnot define a reasonably short path to a valid solution, but we are not actually\nable to follow the local descent path. the objective function may have issues\nsuch as poor conditioning or discontinuous gradients, causing the region where\nthe gradient provides a good model of the objective function to be very small. in\nthese cases, local descent with steps of size (cid:115) may define a reasonably short path\nto the solution, but we are only able to compute the local descent direction with\n(cid:115)(cid:129) . in these cases, local descent may or may not define a path\nsteps of size \u03b4\nto the solution, but the path contains many steps, so following the path incurs a\n\n292\n\n "}, {"Page_number": 308, "text": "chapter 8. optimization for training deep models\n\nhigh computational cost. sometimes local information provides us no guide, when\nthe function has a wide flat region, or if we manage to land exactly on a critical\npoint (usually this latter scenario only happens to methods that solve explicitly\nfor critical points, such as newton\u2019s method). in these cases, local descent does\nnot define a path to a solution at all. in other cases, local moves can be too greedy\nand lead us along a path that moves downhill but away from any solution, as in\nfig.\n.\n8.2\ncurrently, we do not understand which of these problems are most relevant to\nmaking neural network optimization difficult, and this is an active area of research.\n\n, or along an unnecessarily long trajectory to the solution, as in fig.\n\n8.4\n\nregardless of which of these problems are most significant, all of them might be\navoided if there exists a region of space connected reasonably directly to a solution\nby a path that local descent can follow, and if we are able to initialize learning\nwithin that well-behaved region.\u00a0this last view suggests research into choosing\ngood initial points for traditional optimization algorithms to use.\n\n8.2.8 theoretical limits of optimization\n\nseveral theoretical results show that there are limits on the performance of any\noptimization algorithm we might design for neural networks (\nblum and rivest\n,\n1992 judd 1989 wolpert and macready 1997\n). typically these results have\nlittle bearing on the use of neural networks in practice.\n\n;\n\n,\n\n;\n\n,\n\nsome theoretical results apply only to the case where the units of a neural\nnetwork output\u00a0discrete values. however,\u00a0most\u00a0neural network units output\nsmoothly increasing values that make optimization via local search feasible. some\ntheoretical results show that there exist problem classes that are intractable, but\nit can be difficult to tell whether a particular problem falls into that class. other\nresults show that finding a solution for a network of a given size is intractable, but\nin practice we can find a solution easily by using a larger network for which many\nmore parameter settings correspond to an acceptable solution. moreover, in the\ncontext of neural network training, we usually do not care about finding the exact\nminimum of a function, but only in reducing its value sufficiently to obtain good\ngeneralization error.\u00a0theoretical analysis of whether an optimization algorithm\ncan accomplish this goal is extremely difficult. developing more realistic bounds\non the performance of optimization algorithms therefore remains an important\ngoal for machine learning research.\n\n293\n\n "}, {"Page_number": 309, "text": "chapter 8. optimization for training deep models\n\n8.3 basic algorithms\n\nwe have previously introduced the gradient descent (sec.\n) algorithm that\nfollows the gradient of an entire training set downhill. this may be accelerated\nconsiderably by using stochastic gradient descent to follow the gradient of randomly\nselected minibatches downhill, as discussed in sec.\n\nand sec.\n\n8.1.3\n.\n\n4.3\n\n5.9\n\n8.3.1 stochastic gradient descent\n\nstochastic gradient descent (sgd) and its variants are probably the most used\noptimization algorithms for machine learning in general and for deep learning in\nparticular. as discussed in sec.\n, it is possible to obtain an unbiased estimate\nof the gradient by taking the average gradient on a minibatch of m examples drawn\ni.i.d from the data generating distribution.\n\n8.1.3\n\nalgorithm\n\n8.1\n\nshows how to follow this estimate of the gradient downhill.\n\nalgorithm 8.1 stochastic gradient descent (sgd) update at training iteration k\n\nrequire: learning rate (cid:115)k.\nrequire: initial parameter \u03b8\n\ndo\n\nstopping criterion not met\n\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding targets y( )i .\ncompute gradient estimate: \u02c6g \u2190 + 1\napply update: \u03b8\nend while\n\nm\u2207 \u03b8pi l f( (x( )i ; )\u03b8 , y( )i )\n\n\u03b8\u2190 \u2212 (cid:115)\u02c6g\n\n)m } with\n\na crucial parameter for the sgd algorithm is the learning rate. previously, we\nhave described sgd as using a fixed learning rate (cid:115). in practice, it is necessary to\ngradually decrease the learning rate over time, so we now denote the learning rate\nat iteration\n\nask\n\n(cid:115) k.\n\nthis is because the sgd gradient estimator introduces a source of noise (the\nrandom sampling of m training examples) that does not vanish even when we arrive\nat a minimum. by comparison, the true gradient of the total cost function becomes\nsmall and then 0 when we approach and reach a minimum using batch gradient\ndescent, so batch gradient descent can use a fixed learning rate. a sufficient\ncondition to guarantee convergence of sgd is that\n\n\u221exk=1\n\n(cid:115)k = \n\n\u221e,\n\nand\n\n294\n\n(8.12)\n\n "}, {"Page_number": 310, "text": "chapter 8. optimization for training deep models\n\n\u221exk=1\n\n(cid:115)2\nk < .\u221e\n\n(8.13)\n\nin practice, it is common to decay the learning rate linearly until iteration :\u03c4\n\n)\u2212 \u03b1 (cid:115)0 + \u03b1(cid:115)\u03c4\n\u03c4. after iteration , it is common to leave\n\n(cid:115)k = (1\n\n\u03c4\n\nwith \u03b1 = k\n\n(8.14)\n\n(cid:115)\n\nconstant.\n\n1%\n\nthe value of\n\nthe learning rate may be chosen by trial and error, but it is usually best\nto choose it by monitoring learning curves that plot the objective function as a\nfunction of time. this is more of an art than a science, and most guidance on this\nsubject should be regarded with some skepticism. when using the linear schedule,\nthe parameters to choose are (cid:115)0, (cid:115)\u03c4 , and \u03c4. usually \u03c4 may be set to the number of\niterations required to make a few hundred passes through the training set. usually\n(cid:115)\u03c4 should be set to roughly\n(cid:115)0. the main question is how to set (cid:115)0.\nif it is too large, the learning curve will show violent oscillations, with the cost\nfunction often increasing significantly. gentle oscillations are fine, especially if\ntraining with a stochastic cost function such as the cost function arising from the\nuse of dropout. if the learning rate is too low, learning proceeds slowly, and if the\ninitial learning rate is too low, learning may become stuck with a high cost value.\ntypically, the optimal initial learning rate, in terms of total training time and the\nfinal cost value, is higher than the learning rate that yields the best performance\nafter the first 100 iterations or so. therefore, it is usually best to monitor the first\nseveral iterations and use a learning rate that is higher than the best-performing\nlearning rate at this time, but not so high that it causes severe instability.\n\nthe most important property of sgd and related minibatch or online gradient-\nbased optimization is that computation time per update does not grow with the\nnumber of training examples. this allows convergence even when the number\nof training examples becomes very large. for a large enough dataset, sgd may\nconverge to within some fixed tolerance of its final test set error before it has\nprocessed the entire training set.\n\nto study the convergence rate of an optimization algorithm it is common to\nmeasure the excess error j (\u03b8)\u2212 min \u03b8 j (\u03b8), which is the amount that the current\ncost function exceeds the minimum possible cost. when sgd is applied to a convex\nproblem, the excess error is o ( 1\u221ak\n) after k iterations, while in the strongly convex\ncase it is o( 1\nk ). these bounds cannot be improved unless extra conditions are\nassumed. batch gradient descent enjoys better convergence rates than stochastic\ngradient descent in theory. however, the cram\u00e9r-rao bound (\ncram\u00e9r 1946 rao\n,\n,\n1945) states that generalization error cannot decrease faster than o (1\nk ). bottou\n\n;\n\n295\n\n "}, {"Page_number": 311, "text": "chapter 8. optimization for training deep models\n\n(\n\n) argue that it therefore may not be worthwhile to pursue\nand bousquet 2008\nan optimization algorithm that converges faster than o (1\nk ) for machine learning\ntasks\u2014faster convergence presumably corresponds to overfitting. moreover, the\nasymptotic analysis obscures many advantages that stochastic gradient descent\nhas after a small number of steps. with large datasets, the ability of sgd to make\nrapid initial progress while evaluating the gradient for only very few examples\noutweighs its slow asymptotic convergence. most of the algorithms described in\nthe remainder of this chapter achieve benefits that matter in practice but are lost\nin the constant factors obscured by the o( 1\nk ) asymptotic analysis. one can also\ntrade off the benefits of both batch and stochastic gradient descent by gradually\nincreasing the minibatch size during the course of learning.\n\nfor more information on sgd, see\n\n8.3.2 momentum\n\nbottou 1998\n\n(\n\n).\n\nwhile stochastic gradient descent remains a very popular optimization strategy,\nlearning with it can sometimes be slow. the method of momentum (polyak 1964\n)\nis designed to accelerate learning, especially in the face of high curvature, small but\nconsistent gradients, or noisy gradients. the momentum algorithm accumulates\nan exponentially decaying moving average of past gradients and continues to move\nin their direction. the effect of momentum is illustrated in fig.\n\n.8.5\n\n,\n\nformally, the momentum algorithm introduces a variable v that plays the role\nof velocity\u2014it is the direction and speed at which the parameters move through\nparameter space. the velocity is set to an exponentially decaying average of\nthe negative gradient. the name\nderives from a physical analogy, in\nwhich the negative gradient is a force moving a particle through parameter space,\naccording to newton\u2019s laws of motion. momentum in physics is mass times velocity.\nin the momentum learning algorithm, we assume unit mass, so the velocity vector v\nmay also be regarded as the momentum of the particle. a hyperparameter \u03b1 \u2208 [0 ,1)\ndetermines how quickly the contributions of previous gradients exponentially decay.\nthe update rule is given by:\n\nmomentum\n\n\u03b8\u00a0 1\n\nm\n\nmxi=1\n\nl( (f x ( )i ; )\u03b8 , y( )i )! ,\n\n(8.15)\n\n(8.16)\n\nv\n\n\u03b8\n\nv\u2190 \u03b1 \u2212 \u2207(cid:115)\n\u2190 + .\nv\n\n\u03b8\n\nthe velocity v accumulates the gradient elements \u2207\u03b8(cid:23) 1\n\nthe larger \u03b1 is relative to (cid:115), the more previous gradients affect the current direction.\nthe sgd algorithm with momentum is given in algorithm .8.2\n\ni=1 l( (f x( )i ; )\u03b8 , y( )i )(cid:24).\n\nmpm\n\n296\n\n "}, {"Page_number": 312, "text": "chapter 8. optimization for training deep models\n\n20\n\n10\n\n0\n\n\u221210\n\n\u221220\n\n\u221230\n\u2212\n30\n\n\u2212\n20\n\n\u2212\n10\n\n0\n\n10\n\n20\n\nfigure 8.5: momentum aims primarily to solve two problems: poor conditioning of the\nhessian matrix and variance in the stochastic gradient. here, we illustrate how momentum\novercomes the first of these two problems. the contour lines depict a quadratic loss\nfunction with a poorly conditioned hessian matrix. the red path cutting across the\ncontours indicates the path followed by the momentum learning rule as it minimizes this\nfunction. at each step along the way, we draw an arrow indicating the step that gradient\ndescent would take at that point. we can see that a poorly conditioned quadratic objective\nlooks like a long, narrow valley or canyon with steep sides. momentum correctly traverses\nthe canyon lengthwise, while gradient steps waste time moving back and forth across the\nnarrow axis of the canyon. compare also fig.\n, which shows the behavior of gradient\ndescent without momentum.\n\n4.6\n\n297\n\n "}, {"Page_number": 313, "text": "chapter 8. optimization for training deep models\n\npreviously, the size of the step was simply the norm of the gradient multiplied\nby the learning rate. now, the size of the step depends on how large and how\naligned a sequence of gradients are. the step size is largest when many successive\ngradients point in exactly the same direction. if the momentum algorithm always\nobserves gradient g, then it will accelerate in the direction of \u2212g, until reaching a\nterminal velocity where the size of each step is\n\n(cid:115)||\n||g\n1 \u2212 \u03b1\n\n.\n\n(8.17)\n\nit is thus helpful to think of the momentum hyperparameter in terms of\nexample, \u03b1 = .9 corresponds to multiplying the maximum speed by\nthe gradient descent algorithm.\n\n10\n\n1\n\n1\u2212\u03b1. for\nrelative to\n\ncommon values of \u03b1 used in practice include .5, .9, and .99. like the learning\nrate, \u03b1 may also be adapted over time. typically it begins with a small value and\nis later raised. it is less important to adapt \u03b1 over time than to shrink (cid:115) over time.\n\nalgorithm 8.2 stochastic gradient descent (sgd) with momentum\n\nrequire: learning rate , momentum parameter\nrequire: initial parameter\n\n, initial velocity .\nv\n\n\u03b8\n\n(cid:115)\n\n.\n\u03b1\n\ndo\n\nstopping criterion not met\n\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding targets y( )i .\ncompute gradient estimate: g \u2190 1\ncompute velocity update: v\nv\napply update: \u03b8\n\nm \u2207\u03b8pi l f( (x( )i ; )\u03b8 , y( )i )\n\n\u2190 \u03b1 \u2212 (cid:115)\ng\n\nv\n\nend while\n\n\u03b8\n\n\u2190 +\n\n)m } with\n\nwe can view the momentum algorithm as simulating a particle subject to\ncontinuous-time newtonian dynamics. the physical analogy can help to build\nintuition for how the momentum and gradient descent algorithms behave.\n\nthe position of the particle at any point in time is given by \u03b8(t ). the particle\n\nexperiences net force\n\nf ( )t\n\n. this force causes the particle to accelerate:\n\nf( ) =t\n\n\u2202 2\n\u2202t2 \u03b8( )t .\n\n(8.18)\n\nrather than viewing this as a second-order differential equation of the position,\nwe can introduce the variable v(t) representing the velocity of the particle at time\nt and rewrite the newtonian dynamics as a first-order differential equation:\n\nv( ) =t\n\n\u2202\n\u2202t\n298\n\n\u03b8( )t ,\n\n(8.19)\n\n "}, {"Page_number": 314, "text": "chapter 8. optimization for training deep models\n\nf( ) =t\n\n\u2202\n\u2202t\n\nv( )t .\n\n(8.20)\n\nthe momentum algorithm then consists of solving the differential equations via\nnumerical simulation. a simple numerical method for solving differential equations\nis euler\u2019s method, which simply consists of simulating the dynamics defined by\nthe equation by taking small, finite steps in the direction of each gradient.\n\nthis explains the basic form of the momentum update, but what specifically are\nthe forces? one force is proportional to the negative gradient of the cost function:\n\u2212\u2207\u03b8 j(\u03b8). this force pushes the particle downhill along the cost function surface.\nthe gradient descent algorithm would simply take a single step based on each\ngradient, but the newtonian scenario used by the momentum algorithm instead\nuses this force to alter the velocity of the particle. we can think of the particle\nas being like a hockey puck sliding down an icy surface. whenever it descends a\nsteep part of the surface, it gathers speed and continues sliding in that direction\nuntil it begins to go uphill again.\n\none other force is necessary. if the only force is the gradient of the cost function,\nthen the particle might never come to rest. imagine a hockey puck sliding down\none side of a valley and straight up the other side, oscillating back and forth forever,\nassuming the ice is perfectly frictionless. to resolve this problem, we add one\nother force, proportional to \u2212v(t). in physics terminology, this force corresponds\nto viscous drag, as if the particle must push through a resistant medium such as\nsyrup. this causes the particle to gradually lose energy over time and eventually\nconverge to a local minimum.\n\nwhy do we use \u2212v (t) and viscous drag in particular?\u00a0part of the reason to\nuse \u2212v(t) is mathematical convenience\u2014an integer power of the velocity is easy\nto work with. however, other physical systems have other kinds of drag based\non other integer powers of the velocity. for example, a particle traveling through\nthe air experiences turbulent drag, with force proportional to the square of the\nvelocity, while a particle moving along the ground experiences dry friction, with a\nforce of constant magnitude. we can reject each of these options. turbulent drag,\nproportional to the square of the velocity, becomes very weak when the velocity is\nsmall. it is not powerful enough to force the particle to come to rest. a particle\nwith a non-zero initial velocity that experiences only the force of turbulent drag\nwill move away from its initial position forever, with the distance from the starting\npoint growing like o(log t). we must therefore use a lower power of the velocity.\nif we use a power of zero, representing dry friction, then the force is too strong.\nwhen the force due to the gradient of the cost function is small but non-zero, the\nconstant force due to friction can cause the particle to come to rest before reaching\na local minimum. viscous drag avoids both of these problems\u2014it is weak enough\n\n299\n\n "}, {"Page_number": 315, "text": "chapter 8. optimization for training deep models\n\nthat the gradient can continue to cause motion until a minimum is reached, but\nstrong enough to prevent motion if the gradient does not justify moving.\n\n8.3.3 nesterov momentum\n\n2013\n\net al. (\n\nsutskever\ninspired by nesterov\u2019s accelerated gradient method (\nupdate rules in this case are given by:\n\n) introduced a variant of the momentum algorithm that was\n). the\n\nnesterov 1983 2004\n\n,\n\n,\n\nv\u2190 \u03b1 \u2212 \u2207(cid:115) \u03b8\" 1\n\nm\n\n\u03b8\n\n\u2190 + ,\nv\n\nv\n\n\u03b8\n\nmxi=1\n\nl(cid:25)f x(\n\n( )i ; + )\n\n\u03b8 \u03b1v , y ( )i(cid:26)# ,\n\n(8.21)\n\n(8.22)\n\nwhere the parameters \u03b1 and (cid:115) play a similar role as in the standard momentum\nmethod. the difference between nesterov momentum and standard momentum is\nwhere the gradient is evaluated. with nesterov momentum the gradient is evaluated\nafter the current velocity is applied. thus one can interpret nesterov momentum\nas attempting to add a correction factor to the standard method of momentum.\nthe complete nesterov momentum algorithm is presented in algorithm .8.3\n\nin the convex batch gradient case, nesterov momentum brings the rate of\nconvergence of the excess error from o(1/k) (after k steps) to o(1/k2) as shown\nby nesterov 1983\n). unfortunately,\u00a0in the stochastic gradient case,\u00a0nesterov\nmomentum does not improve the rate of convergence.\n\n(\n\nalgorithm 8.3 stochastic gradient descent (sgd) with nesterov momentum\n\nrequire: learning rate , momentum parameter\nrequire: initial parameter\n\n, initial velocity .\nv\n\n\u03b8\n\n(cid:115)\n\n.\n\u03b1\n\ndo\n\nstopping criterion not met\n\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding labels y( )i .\napply interim update: \u02dc\u03b8\nm\u2207 \u02dc\u03b8pi l f( (x( )i ; \u02dc\u03b8 y),\ncompute gradient (at interim point): g \u2190 1\ncompute velocity update: v\napply update: \u03b8\n\n\u2190 + \u03b1\nv\nv\n\n\u2190 \u03b1 \u2212 (cid:115)\ng\n\nv\n\n\u03b8\n\n( )i )\n\nend while\n\n\u03b8\n\n\u2190 +\n\n)m } with\n\n300\n\n "}, {"Page_number": 316, "text": "chapter 8. optimization for training deep models\n\n8.4 parameter initialization strategies\n\nsome optimization algorithms are not iterative by nature and simply solve for a\nsolution point. other optimization algorithms are iterative by nature but, when\napplied to the right class of optimization problems, converge to acceptable solutions\nin an acceptable amount of time regardless of initialization. deep learning training\nalgorithms usually do not have either of these luxuries. training algorithms for deep\nlearning models are usually iterative in nature and thus require the user to specify\nsome initial point from which to begin the iterations. moreover, training deep\nmodels is a sufficiently difficult task that most algorithms are strongly affected by\nthe choice of initialization. the initial point can determine whether the algorithm\nconverges at all, with some initial points being so unstable that the algorithm\nencounters numerical difficulties and fails altogether. when learning does converge,\nthe initial point can determine how quickly learning converges and whether it\nconverges to a point with high\u00a0or low cost. also,\u00a0points of comparable cost\ncan have wildly varying generalization error, and the initial point can affect the\ngeneralization as well.\n\nmodern initialization strategies are simple and heuristic. designing improved\ninitialization strategies is a difficult task because neural network optimization is\nnot yet well understood. most initialization strategies are based on achieving some\nnice properties when the network is initialized. however, we do not have a good\nunderstanding of which of these properties are preserved under which circumstances\nafter learning begins to proceed. a further difficulty is that some initial points\nmay be beneficial from the viewpoint of optimization but detrimental from the\nviewpoint of generalization. our understanding of how the initial point affects\ngeneralization is especially primitive, offering little to no guidance for how to select\nthe initial point.\n\nperhaps the only property known with complete certainty is that the initial\nparameters need to \u201cbreak symmetry\u201d\u00a0between different units.\nif two hidden\nunits with the same activation function are connected to the same inputs, then\nthese units must have different initial parameters.\u00a0if they have the same initial\nparameters, then a deterministic learning algorithm applied to a deterministic cost\nand model will constantly update both of these units in the same way. even if the\nmodel or training algorithm is capable of using stochasticity to compute different\nupdates for different units (for example, if one trains with dropout), it is usually\nbest to initialize each unit to compute a different function from all of the other\nunits. this may help to make sure that no input patterns are lost in the null\nspace of forward propagation and no gradient patterns are lost in the null space\nof back-propagation. the goal of having each unit compute a different function\n\n301\n\n "}, {"Page_number": 317, "text": "chapter 8. optimization for training deep models\n\nmotivates random initialization of the parameters. we could explicitly search\nfor a large set of basis functions that are all mutually different from each other,\nbut this often incurs a noticeable computational cost. for example, if we have at\nmost as many outputs as inputs, we could use gram-schmidt orthogonalization\non an initial weight matrix, and be guaranteed that each unit computes a very\ndifferent function from each other unit. random initialization from a high-entropy\ndistribution over a high-dimensional space is computationally cheaper and unlikely\nto assign any units to compute the same function as each other.\n\ntypically, we set the biases for each unit to heuristically chosen constants, and\ninitialize only the weights randomly. extra parameters, for example, parameters\nencoding the conditional variance of a prediction, are usually set to heuristically\nchosen constants much like the biases are.\n\nwe almost always initialize all the weights in\u00a0the model\u00a0to values\u00a0drawn\nrandomly\u00a0from a\u00a0gaussian\u00a0or uniform\u00a0distribution. the\u00a0choice\u00a0of\u00a0gaussian\nor uniform distribution does not seem to matter very much, but has not been\nexhaustively studied. the scale of the initial distribution, however, does have a\nlarge effect on both the outcome of the optimization procedure and on the ability\nof the network to generalize.\n\nlarger initial weights will yield a stronger symmetry breaking effect, helping\nto avoid redundant units. they also help to avoid losing signal during forward or\nback-propagation through the linear component of each layer\u2014larger values in the\nmatrix result in larger outputs of matrix multiplication. initial weights that are\ntoo large may, however, result in exploding values during forward propagation or\nback-propagation.\u00a0in recurrent networks, large weights can also result in chaos\n(such extreme sensitivity to small perturbations of the input that the behavior\nof the deterministic forward propagation procedure appears random).\u00a0to some\nextent, the exploding gradient problem can be mitigated by gradient clipping\n(thresholding the values of the gradients before performing a gradient descent step).\nlarge weights may also result in extreme values that cause the activation function\nto saturate, causing complete loss of gradient through saturated units. these\ncompeting factors determine the ideal initial scale of the weights.\n\nthe perspectives of regularization and optimization can give very different\ninsights into how we should initialize a network. the optimization perspective\nsuggests that the weights should be large enough to propagate information success-\nfully, but some regularization concerns encourage making them smaller. the use\nof an optimization algorithm such as stochastic gradient descent that makes small\nincremental changes to the weights and tends to halt in areas that are nearer to\nthe initial parameters (whether due to getting stuck in a region of low gradient, or\n\n302\n\n "}, {"Page_number": 318, "text": "chapter 8. optimization for training deep models\n\n7.8\n\ndue to triggering some early stopping criterion based on overfitting) expresses a\nprior that the final parameters should be close to the initial parameters. recall\nfrom sec.\nthat gradient descent with early stopping is equivalent to weight\ndecay for some models. in the general case, gradient descent with early stopping is\nnot the same as weight decay, but does provide a loose analogy for thinking about\nthe effect of initialization. we can think of initializing the parameters \u03b8 to \u03b80 as\nbeing similar to imposing a gaussian prior p(\u03b8) with mean \u03b80. from this point\nof view, it makes sense to choose \u03b80 to be near 0. this prior says that it is more\nlikely that units do not interact with each other than that they do interact. units\ninteract only if the likelihood term of the objective function expresses a strong\npreference for them to interact. on the other hand, if we initialize \u03b80 to large\nvalues, then our prior specifies which units should interact with each other, and\nhow they should interact.\n\nsome heuristics are available for choosing the initial scale of the weights. one\nheuristic is to initialize the weights of a fully connected layer with m inputs and\nn outputs by sampling each weight from u(\u2212 1\u221am , 1\u221am ), while glorot and bengio\n(\n2010\n\nnormalized initialization\n\n) suggest using the\n\nwi,j \u223c \u2212u(\n\n6\n\n\u221am n+\n\n,\n\n6\n\n\u221am n+\n\n).\n\n(8.23)\n\nthis latter heuristic is designed to compromise between the goal of initializing\nall layers to have the same activation variance and the goal of initializing all\nlayers to have the same gradient variance. the formula is derived using the\nassumption that the network consists only of a chain of matrix multiplications,\nwith no nonlinearities. real neural networks obviously violate this assumption,\nbut many strategies designed for the linear model perform reasonably well on its\nnonlinear counterparts.\n\ngain\n\n2013\n\nsaxe\n\nfactor\n\net al. (\n\n) recommend initializing to random orthogonal matrices, with\na carefully chosen scaling or\ng that accounts for the nonlinearity applied\nat each layer. they derive specific values of the scaling factor for different types of\nnonlinear activation functions. this initialization scheme is also motivated by a\nmodel of a deep network as a sequence of matrix multiplies without nonlinearities.\nunder such a model, this initialization scheme guarantees that the total number of\ntraining iterations required to reach convergence is independent of depth.\n\nincreasing the scaling factor g pushes the network toward the regime where\nactivations increase in norm as they propagate forward through the network and\ngradients increase in norm as they propagate backward.\n) showed\nthat setting the gain factor correctly is sufficient to train networks as deep as\n1,000 layers, without needing to use orthogonal initializations.\u00a0a key insight of\n\nsussillo 2014\n\n(\n\n303\n\n "}, {"Page_number": 319, "text": "chapter 8. optimization for training deep models\n\nthis approach is that in feedforward networks, activations and gradients can grow\nor shrink on each step of forward or back-propagation, following a random walk\nbehavior. this is because feedforward networks use a different weight matrix at\neach layer. if this random walk is tuned to preserve norms, then feedforward\nnetworks can mostly avoid the vanishing and exploding gradients problem that\narises when the same weight matrix is used at each step, described in sec.\n8.2.5\n.\n\nunfortunately, these optimal criteria for initial weights often do not lead to\noptimal performance. this may be for three different reasons. first, we may\nbe using the wrong criteria\u2014it may not actually be beneficial to preserve the\nnorm of a signal throughout the entire network. second, the properties imposed\nat initialization may not persist after learning has begun to proceed. third, the\ncriteria might succeed at improving the speed of optimization but inadvertently\nincrease generalization error. in practice, we usually need to treat the scale of the\nweights as a hyperparameter whose optimal value lies somewhere roughly near but\nnot exactly equal to the theoretical predictions.\n\n(\n\nmartens 2010\n\none drawback to scaling rules that set all of the initial weights to have the same\n1\u221am, is that every individual weight becomes extremely\nstandard deviation, such as\nsmall when the layers become large.\n) introduced an alternative\ninitialization scheme called sparse initialization in which each unit is initialized to\nhave exactly k non-zero weights. the idea is to keep the total amount of input to\nthe unit independent from the number of inputs m without making the magnitude\nof individual weight elements shrink with m. sparse initialization helps to achieve\nmore diversity among the units at initialization time.\u00a0however, it also imposes\na very strong prior on the weights that are chosen to have large gaussian values.\nbecause it takes a long time for gradient descent to shrink \u201cincorrect\u201d large values,\nthis initialization scheme can cause problems for units such as maxout units that\nhave several filters that must be carefully coordinated with each other.\n\nwhen computational resources allow it, it is usually a good idea to treat the\ninitial scale of the weights for each layer as a hyperparameter, and to choose these\nscales using a hyperparameter search algorithm described in sec.\n, such\nas random search. the choice of whether to use dense or sparse initialization\ncan also be made a hyperparameter. alternately, one can manually search for\nthe best initial scales. a good rule of thumb for choosing the initial scales is to\nlook at the range or standard deviation of activations or gradients on a single\nminibatch of data. if the weights are too small, the range of activations across the\nminibatch will shrink as the activations propagate forward through the network.\nby repeatedly identifying the first layer with unacceptably small activations and\nincreasing its weights, it is possible to eventually obtain a network with reasonable\n\n11.4.2\n\n304\n\n "}, {"Page_number": 320, "text": "chapter 8. optimization for training deep models\n\ninitial activations throughout. if learning is still too slow at this point, it can be\nuseful to look at the range or standard deviation of the gradients as well as the\nactivations.\u00a0this procedure can in principle be automated and is generally less\ncomputationally costly than hyperparameter optimization based on validation set\nerror because it is based on feedback from the behavior of the initial model on a\nsingle batch of data, rather than on feedback from a trained model on the validation\nset. while long used heuristically, this protocol has recently been specified more\nformally and studied by\n\nmishkin and matas 2015\n\n).\n\n(\n\nso\u00a0far\u00a0we\u00a0have\u00a0focused\u00a0on\u00a0the initialization\u00a0of\u00a0the\u00a0weights. fortunately,\n\ninitialization of other parameters is typically easier.\n\nthe approach for setting the biases must be coordinated with the approach\nfor settings the weights. setting the biases to zero is compatible with most weight\ninitialization schemes. there are a few situations where we may set some biases to\nnon-zero values:\n\n\u2022 if a bias is for an output unit, then it is often beneficial to initialize the bias to\nobtain the right marginal statistics of the output. to do this, we assume that\nthe initial weights are small enough that the output of the unit is determined\nonly by the bias. this justifies setting the bias to the inverse of the activation\nfunction applied to the marginal statistics of the output in the training set.\nfor example, if the output is a distribution over classes and this distribution\nis a highly skewed distribution with the marginal probability of class i given\nby element ci of some vector c, then we can set the bias vector b by solving\nthe equation softmax(b) = c . this applies not only to classifiers but also to\nmodels we will encounter in part\n, such as autoencoders and boltzmann\nmachines. these models have layers whose output should resemble the input\ndata x, and it can be very helpful to initialize the biases of such layers to\nmatch the marginal distribution over\n\niii\n\n.x\n\n\u2022 sometimes we\u00a0may want to choose\u00a0the bias to avoid causing too\u00a0much\nsaturation at initialization. for example, we may set the bias of a relu\nhidden unit to 0.1 rather than 0 to avoid saturating the relu at initialization.\nthis approach is not compatible with weight initialization schemes that do\nnot expect strong input from the biases though. for example, it is not\nrecommended for use with random walk initialization (\n\nsussillo 2014\n\n).\n\n,\n\n\u2022 sometimes a unit controls whether other units are able to participate in a\nfunction. in such situations, we have a unit with output u and another unit\nh \u2208 [0,1], then we can view h as a gate that determines whether uh \u2248 1 or\nuh \u2248 0. in these situations, we want to set the bias for h so that h \u2248 1 most\n\n305\n\n "}, {"Page_number": 321, "text": "chapter 8. optimization for training deep models\n\nof the time at initialization.\u00a0otherwise u does not have a chance to learn.\nfor example,\nfor the\nforget gate of the lstm model, described in sec.\n\n) advocate setting the bias to\n\njozefowicz et al. 2015\n\n10.10\n.\n\n1\n\n(\n\nanother common type of parameter is a variance or precision parameter. for\nexample, we can perform linear regression with a conditional variance estimate\nusing the model\n\np y\n(\n\n|\n\nn\nx) = \n\ny\n(\n\n| wt x + 1\n\nb, /\u03b2\n\n)\n\n(8.24)\n\nwhere \u03b2 is a precision parameter. we can usually initialize variance or precision\nparameters to 1 safely. another approach is to assume the initial weights are close\nenough to zero that the biases may be set while ignoring the effect of the weights,\nthen set the biases to produce the correct marginal mean of the output, and set\nthe variance parameters to the marginal variance of the output in the training set.\n\niii\n\nbesides these simple constant or random methods of initializing model parame-\nters, it is possible to initialize model parameters using machine learning. a common\nstrategy discussed in part\nof this book is to initialize a supervised model with\nthe parameters learned by an unsupervised model trained on the same inputs.\none can also perform supervised training on a related task. even performing\nsupervised training on an unrelated task can sometimes yield an initialization that\noffers faster convergence than a random initialization. some of these initialization\nstrategies may yield faster convergence and better generalization because they\nencode information about the distribution in the initial parameters of the model.\nothers apparently perform well primarily because they set the parameters to have\nthe right scale or set different units to compute different functions from each other.\n\n8.5 algorithms with adaptive learning rates\n\nneural network researchers have long realized that the learning rate was reliably one\nof the hyperparameters that is the most difficult to set because it has a significant\nimpact on model performance. as we have discussed in sec.\n, the\ncost is often highly sensitive to some directions in parameter space and insensitive\nto others. the momentum algorithm can mitigate these issues somewhat, but\ndoes so at the expense of introducing another hyperparameter. in the face of this,\nit is natural to ask if there is another way. if we believe that the directions of\nsensitivity are somewhat axis-aligned, it can make sense to use a separate learning\nrate for each parameter, and automatically adapt these learning rates throughout\nthe course of learning.\n\nand sec.\n\n8.2\n\n4.3\n\n306\n\n "}, {"Page_number": 322, "text": "chapter 8. optimization for training deep models\n\n,\n\njacobs 1988\n\nthe delta-bar-delta algorithm (\n\n) is an early heuristic approach\nto adapting individual learning rates for model parameters during training. the\napproach is based on a simple idea: if the partial derivative of the loss, with respect\nto a given model parameter, remains the same sign, then the learning rate should\nincrease. if the partial derivative with respect to that parameter changes sign,\nthen the learning rate should decrease.\u00a0of course, this kind of rule can only be\napplied to full batch optimization.\n\nmore recently, a number of incremental (or mini-batch-based) methods have\nbeen introduced that adapt the learning rates of model parameters. this section\nwill briefly review a few of these algorithms.\n\n8.5.1 adagrad\n\n8.4\n\nthe adagrad algorithm, shown in algorithm , individually adapts the learning\nrates of all model parameters by scaling them inversely proportional to the square\nroot of the sum of all of their historical squared values (duchi\n). the\nparameters with the largest partial derivative of the loss have a correspondingly\nrapid decrease in their learning rate, while parameters with small partial derivatives\nhave a relatively small decrease in their learning rate. the net effect is greater\nprogress in the more gently sloped directions of parameter space.\n\net al.,\n\n2011\n\nin the context of convex optimization, the adagrad algorithm enjoys some\ndesirable theoretical properties. however, empirically it has been found that\u2014for\ntraining deep neural network models\u2014the accumulation of squared gradients from\nthe beginning of training can result in a premature and excessive decrease\nin the effective learning rate.\u00a0adagrad performs well for some but not all deep\nlearning models.\n\n8.5.2 rmsprop\n\n,\n\nthe rmsprop algorithm (hinton 2012\n) modifies adagrad to perform better in the\nnon-convex setting by changing the gradient accumulation into an exponentially\nweighted moving average. adagrad is designed to converge rapidly when applied\nto a convex function. when applied to a non-convex function to train a neural\nnetwork, the learning trajectory may pass through many different structures and\neventually arrive at a region that is a locally convex bowl. adagrad shrinks the\nlearning rate according to the entire history of the squared gradient and may\nhave made the learning rate too small before arriving at such a convex structure.\nrmsprop uses an exponentially decaying average to discard history from the\n\n307\n\n "}, {"Page_number": 323, "text": "chapter 8. optimization for training deep models\n\nalgorithm 8.4 the adagrad algorithm\nrequire: global learning rate (cid:115)\nrequire: initial parameter \u03b8\nrequire: small constant\n\n, perhaps\n\n\u03b4\n\n10\u22127, for numerical stability\n\ndo\n\nstopping criterion not met\n\ninitialize gradient accumulation variable r = 0\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding targets y( )i .\ncompute gradient: g \u2190 1\naccumulate squared gradient: r\ncompute update: \u2206\u03b8 \u2190 \u2212 (cid:115)\nelement-wise)\napply update: \u03b8\n\nm\u2207\u03b8pi l f( (x( )i ; )\u03b8 , y( )i )\n\nr\n\u03b4+\u221ar (cid:128) g.\n\n\u2190 + (cid:128)\n\ng\n\ng\n\n\u03b8\n\n\u2190 + \u2206\n\u03b8\n\nend while\n\n(division and square root applied\n\n)m } with\n\nextreme past so that it can converge rapidly after finding a convex bowl, as if it\nwere an instance of the adagrad algorithm initialized within that bowl.\n\nrmsprop is shown in its standard form in algorithm\n\nand combined with\nnesterov momentum in algorithm . compared to adagrad, the use of the\nmoving average introduces a new hyperparameter, \u03c1, that controls the length scale\nof the moving average.\n\n8.6\n\n8.5\n\nempirically, rmsprop has been shown to be an effective and practical op-\ntimization algorithm for deep neural networks. it is currently one of the go-to\noptimization methods being employed routinely by deep learning practitioners.\n\n8.5.3 adam\n\n,\n\n8.7\n\nkingma and ba 2014\n\n) is yet another adaptive learning rate optimization\nadam (\n. the name \u201cadam\u201d derives from\nalgorithm and is presented in algorithm\nin the context of the earlier algorithms, it is\nthe phrase \u201cadaptive moments.\u201d\nperhaps best seen as a variant on the combination of rmsprop and momentum\nwith a few important distinctions. first, in adam, momentum is incorporated\ndirectly as an estimate of the first order moment (with exponential weighting) of\nthe gradient. the most straightforward way to add momentum to rmsprop is to\napply momentum to the rescaled gradients. the use of momentum in combination\nwith rescaling does not have a clear theoretical motivation. second, adam includes\nbias corrections to the estimates of both the first-order moments (the momentum\nterm) and the (uncentered) second-order moments to account for their initialization\n\n308\n\n "}, {"Page_number": 324, "text": "chapter 8. optimization for training deep models\n\nalgorithm 8.5 the rmsprop algorithm\nrequire: global learning rate , decay rate .\n\u03c1\nrequire: initial parameter \u03b8\nrequire: small constant \u03b4,\u00a0usually 10\u22126,\u00a0used to stabilize division\u00a0by small\n\n(cid:115)\n\ndo\n\nstopping criterion not met\n\nnumbers.\ninitialize accumulation variables r = 0\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding targets y( )i .\ncompute gradient: g \u2190 1\naccumulate squared gradient: r\ncompute parameter update: \u2206\u03b8 = \u2212 (cid:115)\u221a \u03b4+r(cid:128) g.\napply update: \u03b8\nend while\n\nm\u2207\u03b8pi l f( (x( )i ; )\u03b8 , y( )i )\n\n\u2190 + \u2206\n\u03b8\n\n\u2190 \u03c1 + (1\n\n)\u2212 \u03c1 (cid:128)\n\ng\n1\u221a\u03b4+r\n\ng\n(\n\n\u03b8\n\nr\n\napplied element-wise)\n\n)m } with\n\n8.7\n\nat the origin (see algorithm ). rmsprop also incorporates an estimate of the\n(uncentered) second-order moment, however it lacks the correction factor. thus,\nunlike in adam, the rmsprop second-order moment estimate may have high bias\nearly in training. adam is generally regarded as being fairly robust to the choice\nof hyperparameters, though the learning rate sometimes needs to be changed from\nthe suggested default.\n\n8.5.4 choosing the right optimization algorithm\n\nin this section, we discussed a series of related algorithms that each seek to address\nthe challenge of optimizing deep models by adapting the learning rate for each\nmodel parameter. at this point, a natural question is: which algorithm should one\nchoose?\n\nunfortunately, there is currently no consensus on this point.\n\nschaul et al. 2014\n)\npresented a valuable comparison of a large number of optimization algorithms\nacross a wide range of learning tasks. while the results suggest that the family of\nalgorithms with adaptive learning rates (represented by rmsprop and adadelta)\nperformed fairly robustly, no single best algorithm has emerged.\n\n(\n\ncurrently, the most popular optimization algorithms actively in use include\nsgd, sgd with momentum, rmsprop, rmsprop with momentum, adadelta\nand adam. the choice of which algorithm to use, at this point, seems to depend\nlargely on the user\u2019s familiarity with the algorithm (for ease of hyperparameter\ntuning).\n\n309\n\n "}, {"Page_number": 325, "text": "chapter 8. optimization for training deep models\n\nalgorithm 8.6 rmsprop algorithm with nesterov momentum\nrequire: global learning rate , decay rate , momentum coefficient\nrequire: initial parameter\n\n, initial velocity .\nv\n\n\u03b8\n\n\u03c1\n\n(cid:115)\n\n.\n\u03b1\n\ndo\n\nstopping criterion not met\n\ninitialize accumulation variable r = 0\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding targets y( )i .\ncompute interim update: \u02dc\u03b8\ncompute gradient: g \u2190 1\naccumulate gradient: r\ncompute velocity update: v\napply update: \u03b8\n\n\u2190 + \u03b1\nv\nm\u2207 \u02dc\u03b8pi l f( (x( )i ; \u02dc\u03b8 y),\n)\u2212 \u03c1 (cid:128)\nr\ng\nv\u2190 \u03b1 \u2212 (cid:115)\u221ar (cid:128) g.\n\n\u2190 \u03c1 + (1\n\n( )i )\n\nv\n\n\u03b8\n\ng\n\n( 1\u221ar applied element-wise)\n\n)m } with\n\nend while\n\n\u03b8\n\n\u2190 +\n\n8.6 approximate second-order methods\n\nin this section we discuss the application of second-order methods to the training\nof deep networks. see\n) for an earlier treatment of this subject.\nfor simplicity of exposition, the only objective function we examine is the empirical\nrisk:\n\nlecun et al. 1998a\n\n(\n\nj( ) = \n\n\u03b8\n\ne\n\nx,y\u223c\u02c6pdata (\n\n)x,y [ ( ( ;\n\nl f x \u03b8 , y\n\n)\n\n)] =\n\n1\nm\n\nmxi=1\n\nl f( (x( )i ; )\u03b8 , y( )i ).\n\n(8.25)\n\nhowever the methods we discuss here extend readily to more general objective\nfunctions that, for instance, include parameter regularization terms such as those\ndiscussed in chapter\n\n.7\n\n8.6.1 newton\u2019s method\n\n4.3\n\n, we introduced second-order gradient methods. in contrast to first-\nin sec.\norder methods, second-order methods make use of second derivatives to improve\noptimization. the most widely used second-order method is newton\u2019s method. we\nnow describe newton\u2019s method in more detail, with emphasis on its application to\nneural network training.\n\nnewton\u2019s method is an optimization scheme based on using a second-order tay-\nlor series expansion to approximate j(\u03b8) near some point \u03b8 0, ignoring derivatives\n\n310\n\n "}, {"Page_number": 326, "text": "chapter 8. optimization for training deep models\n\nalgorithm 8.7 the adam algorithm\n\nrequire: step size\nrequire: exponential decay rates for moment\u00a0estimates, \u03c11 and \u03c12 in [0 , 1).\n\n(suggested default:\n\n)\n0 001.\n\n(cid:115)\n\n(suggested defaults:\n\n0 9.\n\nand\n\n0 999.\n\nrespectively)\n\nrequire: small constant \u03b4 used for numerical stabilization. (suggested default:\n\n10\u22128)\n\nrequire: initial parameters \u03b8\n\n,\n\ndo\n\ns = 0 r = 0\n\nstopping criterion not met\n\ninitialize 1st and 2nd moment variables\ninitialize time step t = 0\nwhile\nsample a minibatch of m examples from the training set {x(1), . . . , x (\ncorresponding targets y( )i .\ncompute gradient: g \u2190 1\nt\u2190 + 1\nt\nupdate biased first moment estimate: s \u2190 \u03c11s + (1 \u2212 \u03c11)g\nupdate biased second moment estimate: r \u2190 \u03c12r + (1 \u2212 \u03c12 )g\ncorrect bias in first moment: \u02c6s \u2190 s\n1\u2212\u03c1 t\ncorrect bias in second moment: \u02c6r \u2190 r\n1\u2212\u03c1 t\ncompute update: \u2206 = \u03b8 \u2212(cid:115)\n\u2190 + \u2206\napply update: \u03b8\n\u03b8\n\nm\u2207\u03b8pi l f( (x( )i ; )\u03b8 , y( )i )\n\n(operations applied element-wise)\n\n\u02c6s\u221a \u02c6r+\u03b4\n\ng(cid:128)\n\n\u03b8\n\n1\n\n2\n\nend while\n\n)m } with\n\nof higher order:\n\nj\n\n( ) \u03b8 \u2248 (\u03b80) + (\u03b8\n\nj\n\n\u03b8\u2212 0)>\u2207\u03b8j(\u03b80) +\n\n1\n2\n\n(\u03b8\n\n\u03b8\u2212 0)> h \u03b8\n\n( \u2212 0 ),\n\n\u03b8\n\n(8.26)\n\nwhere h is the hessian of j with respect to \u03b8 evaluated at \u03b80. if we then solve for\nthe critical point of this function, we obtain the newton parameter update rule:\n\n\u03b8\u2217 = \u03b80 \u2212 h\u22121\u2207 \u03b8j(\u03b8 0)\n\n(8.27)\n\nthus for a locally quadratic function (with positive definite h ), by rescaling\nthe gradient by h \u22121, newton\u2019s method jumps directly to the minimum.\u00a0if the\nobjective function is convex but not quadratic (there are higher-order terms), this\nupdate can be iterated, yielding the training algorithm associated with newton\u2019s\nmethod, given in algorithm .8.8\n\nfor surfaces that are not quadratic, as long as the hessian remains positive\ndefinite, newton\u2019s method can be applied iteratively. this implies a two-step\n\n311\n\n "}, {"Page_number": 327, "text": "chapter 8. optimization for training deep models\n\nnewton\u2019s method\n\nwith\n\nobjective\n\nj (\u03b8)\n\n=\n\nalgorithm 8.8\n1\n\ni=1 l f( (x ( )i ; )\u03b8 , y( )i ).\n\nmpm\n\nrequire: initial parameter \u03b80\nrequire: training set of\n\nm\n\nexamples\ndo\n\nstopping criterion not met\n\nwhile\nm\u2207\u03b8pi l f( (x( )i ; )\u03b8 , y( )i )\ncompute gradient: g \u2190 1\n\u03b8pi l f( (x( )i ; )\u03b8 , y ( )i )\ncompute hessian: h \u2190 1\nm \u22072\ncompute hessian inverse: h\u22121\ncompute update: \u2206 = \u03b8 \u2212h\u22121g\napply update: \u03b8\nend while\n\n=  + \u2206\n\n\u03b8\n\n\u03b8\n\niterative procedure. first, update or compute the inverse hessian (i.e. by updating\nthe quadratic approximation). second, update the parameters according to eq.\n8.27.\n\n8.2.3\n\nin sec.\n\n, we discussed how newton\u2019s method is appropriate only when\nthe hessian is positive definite. in deep learning, the surface of the objective\nfunction is typically non-convex with many features, such as saddle points, that\nare problematic for newton\u2019s method.\u00a0if the eigenvalues of the hessian are not\nall positive, for example, near a saddle point, then newton\u2019s method can actually\ncause updates to move in the wrong direction. this situation can be avoided\nby regularizing the hessian. common regularization strategies include adding a\n, along the diagonal of the hessian. the regularized update becomes\nconstant,\n\n\u03b1\n\n\u03b8\u2217 = \u03b80 \u2212 [\n\n( (h f \u03b80)) + ]\u03b1i \u22121\u2207\u03b8 f(\u03b80).\n\n(8.28)\n\n;\n\n,\n\nthis regularization strategy is used in approximations to newton\u2019s method, such\nas the levenberg\u2013marquardt algorithm (levenberg 1944 marquardt 1963\n), and\nworks fairly well as long as the negative eigenvalues of the hessian are still relatively\nclose to zero. in cases where there are more extreme directions of curvature, the\nvalue of \u03b1 would have to be sufficiently large to offset the negative eigenvalues.\nhowever, as \u03b1 increases in size, the hessian becomes dominated by the \u03b1i diagonal\nand the direction chosen by newton\u2019s method converges to the standard gradient\ndivided by \u03b1.\u00a0when strong negative curvature is present, \u03b1 may need to be so\nlarge that newton\u2019s method would make smaller steps than gradient descent with\na properly chosen learning rate.\n\n,\n\nbeyond the challenges created by certain features of the objective function,\nsuch as saddle points, the application of newton\u2019s method for training large neural\nnetworks is limited by the significant computational burden it imposes. the\n\n312\n\n "}, {"Page_number": 328, "text": "chapter 8. optimization for training deep models\n\nnumber of elements in the hessian is squared in the number of parameters, so with\nk parameters (and for even very small neural networks the number of parameters\nk\u00d7\nk can be in the millions), newton\u2019s method would require the inversion of a k\nmatrix\u2014with computational complexity of o(k 3). also, since the parameters\nwill change with every update, the inverse hessian has to be computed at every\ntraining iteration. as a consequence, only networks a with very small number\nof parameters can be practically trained via newton\u2019s method. in the remainder\nof this section,\u00a0we will discuss alternatives that attempt to gain some of the\nadvantages of newton\u2019s method while side-stepping the computational hurdles.\n\n8.6.2 conjugate gradients\n\nconjugate gradients is a method to efficiently avoid the calculation of the inverse\nhessian by iteratively descending conjugate directions. the inspiration for this\napproach follows from a careful study of the weakness of the method of steepest\ndescent (see sec.\nfor details), where line searches are applied iteratively in\nthe direction associated with the gradient. fig.\nillustrates how the method of\nsteepest descent, when applied in a quadratic bowl, progresses in a rather ineffective\nback-and-forth, zig-zag pattern. this happens because each line search direction,\nwhen given by the gradient, is guaranteed to be orthogonal to the previous line\nsearch direction.\n\n4.3\n\n8.6\n\nlet the previous search direction be dt\u22121. at the minimum, where the line\nsearch terminates, the directional derivative is zero in direction dt\u22121: \u2207\u03b8j (\u03b8 ) \u00b7\ndt\u22121 = 0. since the gradient at this point defines the current search direction,\ndt = \u2207\u03b8j (\u03b8) will have no contribution in the direction dt\u22121. thus dt is orthogonal\nfor\nto dt\u22121. this\u00a0relationship between dt\u22121 and dt\nmultiple iterations of steepest descent. as demonstrated in the figure, the choice of\northogonal directions of descent do not preserve the minimum along the previous\nsearch directions. this gives rise to the zig-zag pattern of progress, where by\ndescending to the minimum in the current gradient direction, we must re-minimize\nthe objective in the previous gradient direction. thus, by following the gradient at\nthe end of each line search we are, in a sense, undoing progress we have already\nmade in the direction of the previous line search. the method of conjugate gradients\nseeks to address this problem.\n\nis illustrated\u00a0in fig.\n\n8.6\n\nin the method of conjugate gradients, we seek to find a search direction that is\nconjugate to the previous line search direction, i.e. it will not undo progress made\nin that direction.\u00a0at training iteration t, the next search direction dt takes the\nform:\n\ndt = \u2207 \u03b8j\n\n( ) +\u03b8\n\n\u03b2\n\ntdt\u22121\n\n313\n\n(8.29)\n\n "}, {"Page_number": 329, "text": "chapter 8. optimization for training deep models\n\n20\n\n10\n\n0\n\n\u221210\n\n\u221220\n\n\u221230\n\n\u221230 \u221220 \u221210\n\n0\n\n10\n\n20\n\nfigure 8.6: the method of steepest descent applied to a quadratic cost surface. the\nmethod of steepest descent involves jumping to the point of lowest cost along the line\ndefined by the gradient at the initial point on each step. this resolves some of the problems\nseen with using a fixed learning rate in fig.\n, but even with the optimal step size the\nalgorithm still makes back-and-forth progress toward the optimum. by definition, at\nthe minimum of the objective along a given direction, the gradient at the final point is\northogonal to that direction.\n\n4.6\n\nwere \u03b2t is a coefficient whose magnitude controls how much of the direction, d t\u22121,\nwe should add back to the current search direction.\n\ntwo directions, dt and dt\u22121, are defined as conjugate if d>t h d( )j t\u22121 = 0.\n\nd>t hd t\u22121 = 0\n\n(8.30)\n\nthe straightforward way to impose conjugacy would involve calculation of the\neigenvectors of h to choose \u03b2t, which would not satisfy our goal of developing\na method that is more computationally viable than newton\u2019s method for large\nproblems.\u00a0can we calculate the conjugate directions without resorting to these\ncalculations? fortunately the answer to that is yes.\n\ntwo popular methods for computing the \u03b2t are:\n\n1.\u00a0fletcher-reeves:\n\n2.\u00a0polak-ribi\u00e8re:\n\n\u03b2t = \u2207\u03b8j(\u03b8t)>\u2207\u03b8j(\u03b8t)\n\u2207\u03b8 j(\u03b8t\u22121 )>\u2207\u03b8j(\u03b8t\u22121)\n\n\u03b2t =\n\n(\u2207\u03b8j(\u03b8t) \u2212 \u2207\u03b8j(\u03b8t\u22121))> \u2207\u03b8j(\u03b8t)\n\n\u2207\u03b8 j(\u03b8t\u22121 )>\u2207\u03b8j(\u03b8t\u22121)\n\n314\n\n(8.31)\n\n(8.32)\n\n "}, {"Page_number": 330, "text": "chapter 8. optimization for training deep models\n\nfor a quadratic surface, the conjugate directions ensure that the gradient along\nthe previous direction does not increase in magnitude. we therefore stay at the\nminimum along the previous directions. as a consequence, in a k-dimensional\nparameter space, conjugate gradients only requires k line searches to achieve the\nminimum. the conjugate gradient algorithm is given in algorithm .8.9\n\nalgorithm 8.9 conjugate gradient method\nrequire: initial parameters \u03b80\nrequire: training set of\n\nexamples\n\nm\n\ninitialize \u03c10 = 0\ninitialize g0 = 0\ninitialize t = 1\nwhile\n\nstopping criterion not met\n\ndo\n\n(polak-ribi\u00e8re)\n\nm\u2207\u03b8pi l f( (x ( )i ; )\u03b8 , y( )i )\n\ninitialize the gradient gt = 0\ncompute gradient: gt \u2190 1\ncompute \u03b2t = (gt \u2212gt\u22121) >gt\ng>t\u22121g t\u22121\n(nonlinear conjugate gradient: optionally reset \u03b2t to zero, for example if t is\na multiple of some constant\n)\nk = 5\ncompute search direction: \u03c1t = \u2212gt + \u03b2t\u03c1t\u22121\nperform line search to find: (cid:115)\u2217 = argmin(cid:115)\n(on a truly quadratic cost function, analytically solve for (cid:115)\u2217 rather than\nexplicitly searching for it)\napply update: \u03b8t+1 = \u03b8t + (cid:115)\u2217\u03c1t\nt\u2190 + 1\nt\nend while\n\ni=1 l f( (x ( )i ; \u03b8t + (cid:115)\u03c1t ), y( )i )\n\nmpm\n\n, such as\n\nk\n\n1\n\nnonlinear conjugate gradients: so far we have discussed the method of\nconjugate gradients as it is applied to quadratic objective functions.\u00a0of course,\nour primary interest in this chapter is to explore optimization methods for training\nneural networks and other related deep learning models where the corresponding\nobjective function is far from quadratic. perhaps surprisingly, the method of\nconjugate gradients is still applicable in this setting, though with some modification.\nwithout any assurance that the objective is quadratic, the conjugate directions are\nno longer assured to remain at the minimum of the objective for previous directions.\nas a result, the nonlinear conjugate gradients algorithm includes occasional resets\nwhere the method of conjugate gradients is restarted with line search along the\nunaltered gradient.\n\npractitioners report reasonable results in applications of the nonlinear conjugate\n\n315\n\n "}, {"Page_number": 331, "text": "chapter 8. optimization for training deep models\n\ngradients algorithm to training neural networks, though it is often beneficial to\ninitialize the optimization with a few iterations of stochastic gradient descent before\ncommencing nonlinear conjugate gradients. also, while the (nonlinear) conjugate\ngradients algorithm has traditionally been cast as a batch method, minibatch\nversions have been used successfully for the training of neural networks (\nle et al.\n,\n2011).\u00a0adaptations of conjugate gradients specifically for neural networks have\nbeen proposed earlier, such as the scaled conjugate gradients algorithm (\nmoller\n,\n1993).\n\n8.6.3 bfgs\n\nthe broyden\u2013fletcher\u2013goldfarb\u2013shanno (bfgs) algorithm attempts to bring some\nof the advantages of newton\u2019s method without the computational burden. in that\nrespect, bfgs is similar to cg. however, bfgs takes a more direct approach to\nthe approximation of newton\u2019s update. recall that newton\u2019s update is given by\n\n\u03b8\u2217 = \u03b80 \u2212 h \u22121\u2207\u03b8j(\u03b80),\n\n(8.33)\n\nwhere h is the hessian of j with respect to \u03b8 evaluated at \u03b8 0. the primary\ncomputational difficulty in applying newton\u2019s update is the calculation of the\ninverse hessian h\u22121. the approach adopted by quasi-newton methods (of which\nthe bfgs algorithm is the most prominent) is to approximate the inverse with\na matrix m t that is iteratively refined by low rank updates to become a better\napproximation of h\u22121.\n\nfrom newton\u2019s update, in eq.\n\n, we can see that the parameters at learning\nsteps t are related via the secant condition (also known as the quasi-newton\ncondition):\n\n8.33\n\n\u03b8t+1 \u2212 \u03b8 t = \u2212h\u22121 (\u2207\u03b8j(\u03b8t+1) \u2212 \u2207\u03b8j(\u03b8t))\n\n(8.34)\n\n8.34\n\neq.\nholds precisely in the quadratic case, or approximately otherwise. the\napproximation to the hessian inverse used in the bfgs procedure is constructed\nso as to satisfy this condition, with m in place of h\u22121. specifically, m is updated\naccording to:\n\nmt = mt\u22121 +(cid:27)1 +\n\n\u03c6>mt\u22121\u03c6\n\n\u2206>\u03c6 (cid:28) \u03c6> \u03c6\n\n\u2206>\u03c6 \u2212(cid:27) \u2206\u03c6>mt\u22121 + mt\u22121\u03c6\u2206>\n\n\u2206>\u03c6\n\n(cid:28) , (8.35)\n\nwhere gt = \u2207\u03b8j (\u03b8t), \u03c6 = gt \u2212 gt\u22121 and \u2206 = \u03b8t \u2212 \u03b8t\u22121. eq.\nshows that the\nbfgs procedure iteratively refines the approximation of the inverse of the hessian\nwith rank updates of rank one. this mean that if \u03b8 \u2208 rn, then the computational\n\n8.35\n\n316\n\n "}, {"Page_number": 332, "text": "chapter 8. optimization for training deep models\n\ncomplexity of the update is o(n2). the derivation of the bfgs approximation is\ngiven in many textbooks on optimization, including\n\nluenberger 1984\n\n).\n\n(\n\nonce the inverse hessian approximation mt is updated, the direction of descent\n\u03c1t is determined by \u03c1t = mtgt. a line search is performed in this direction to\ndetermine the size of the step, (cid:115)\u2217 , taken in this direction. the final update to the\nparameters is given by:\n\n\u03b8t+1 = \u03b8t + (cid:115)\u2217\u03c1t .\n\n(8.36)\n\nthe complete bfgs algorithm is presented in algorithm\n\n8.10\n.\n\nalgorithm 8.10 bfgs method\nrequire: initial parameters \u03b80\n\ninitialize inverse hessian m0 = i\nwhile\nstopping criterion not met\n\ndo\ncompute gradient: gt = \u2207\u03b8j(\u03b8t)\ncompute \u03c6 g=  t \u2212 gt\u22121, \u2206 = \u03b8t \u2212 \u03b8t\u22121\n\napprox h\u22121 : mt = mt\u22121 +(cid:25)1 + \u03c6 >mt\u22121\u03c6\n\ncompute search direction: \u03c1t = mtgt\nperform line search to find: (cid:115)\u2217 = argmin(cid:115) j(\u03b8t + (cid:115)\u03c1t)\napply update: \u03b8t+1 = \u03b8t + (cid:115)\u2217\u03c1t\n\n\u2206> \u03c6 (cid:26) \u03c6>\u03c6\n\n\u2206>\u03c6 \u2212(cid:25) \u2206\u03c6>mt\u22121 +mt\u22121 \u03c6\u2206>\n\n\u2206>\u03c6\n\n(cid:26)\n\nend while*\n\nlike the method of conjugate gradients, the bfgs algorithm iterates a series of\nline searches with the direction incorporating second-order information. however\nunlike conjugate gradients, the success of the approach is not heavily dependent\non the line search finding a point very close to the true minimum along the line.\nthus, relative to conjugate gradients, bfgs has the advantage that it can spend\nless time refining each line search. on the other hand, the bfgs algorithm must\nstore the inverse hessian matrix, m, that requires o(n2) memory, making bfgs\nimpractical for most modern deep learning models that typically have millions of\nparameters.\n\nlimited\u00a0memory\u00a0bfgs\u00a0(or\u00a0l-bfgs) the\u00a0memory costs\u00a0of the\u00a0bfgs\nalgorithm can be significantly decreased by avoiding storing the complete inverse\nhessian approximation m. alternatively, by replacing the mt\u22121 in eq.\nwith\nan identity matrix, the bfgs search direction update formula becomes:\n\n8.35\n\n\u03c1t = \u2212gt +\n\n+b\u2206 a ,\u03c6\n\n(8.37)\n\n317\n\n "}, {"Page_number": 333, "text": "chapter 8. optimization for training deep models\n\nwhere the scalars\n\na\n\nand are given by:\n\nb\n\na = \u2212(cid:27)1 +\n\nb =\n\n\u2206>gt\n\u2206> \u03c6\n\n\u03c6> \u03c6\n\n\u2206>\u03c6(cid:28) \u2206>gt\n\n\u2206>\u03c6\n\n+\n\n\u03c6> gt\n\u2206>\u03c6\n\n(8.38)\n\n(8.39)\n\n8.37\n\nwith \u03c6 and \u2206 as defined above.\u00a0if used with exact line searches, the directions\ndefined by eq.\nare mutually conjugate. however,\u00a0unlike the method of\nconjugate gradients, this procedure remains well behaved when the minimum of\nthe line search is reached only approximately. this strategy can be generalized to\ninclude more information about the hessian by storing previous values of \u03c6 and\n\u2206.\n\n8.7 optimization strategies and meta-algorithms\n\nmany optimization techniques are not exactly algorithms,\u00a0but rather general\ntemplates that can be specialized to yield algorithms, or subroutines that can be\nincorporated into many different algorithms.\n\n8.7.1 batch normalization\n\nioffe and szegedy 2015\n\nbatch normalization (\n) is one of the most exciting recent\ninnovations in optimizing deep neural networks and it is actually not an optimization\nalgorithm at all. instead, it is a method of adaptive reparametrization, motivated\nby the difficulty of training very deep models.\n\n,\n\nvery deep models involve the composition of several functions or layers. the\ngradient tells how to update each parameter, under the assumption that the other\nlayers do not change.\nin practice, we update all of the layers simultaneously.\nwhen we make the update, unexpected results can happen because many functions\ncomposed together are changed simultaneously, using updates that were computed\nunder the assumption that the other functions remain constant. as a simple\nexample, suppose we have a deep neural network that has only one unit per layer\nand does not use an activation function at each hidden layer: \u02c6y = xw1w2 w3 . . . wl.\nhere, wi provides the weight used by layer i. the output of layer i is h i = h i\u22121wi.\nthe output \u02c6y is a linear function of the input x, but a nonlinear function of the\nweights wi . suppose our cost function has put a gradient of\n\u02c6y, so we wish to\ndecrease \u02c6y slightly. the back-propagation algorithm can then compute a gradient\n\u2190 \u2212 (cid:115) . the\ng = \u2207w\u02c6y. consider what happens when we make an update w w\n\non1\n\ng\n\n318\n\n "}, {"Page_number": 334, "text": "chapter 8. optimization for training deep models\n\nfirst-order taylor series approximation of \u02c6y predicts that the value of \u02c6y will decrease\nby (cid:115)g>g. if we wanted to decrease \u02c6y by .1, this first-order information available in\nthe gradient suggests we could set the learning rate (cid:115) to .1\ng>g . however, the actual\nupdate will include second-order and third-order effects, on up to effects of order l.\nthe new value of \u02c6y is given by\n\nx w( 1 \u2212 (cid:115)g1 )(w2 \u2212 (cid:115)g 2)\n\n(\n\n. . . wl \u2212 (cid:115)gl ).\n\n(8.40)\n\n3\n\nthrough\n\nan example of one second-order term arising from this update is (cid:115)2 g1 g2ql\nthis term might be negligible if ql\n\ni=3 wi.\ni=3 wi is small, or might be exponentially large\nif the weights on layers\nl are greater than . this makes it very hard\nto choose an appropriate learning rate, because the effects of an update to the\nparameters for one layer depends so strongly on all of the other layers. second-order\noptimization algorithms address this issue by computing an update that takes these\nsecond-order interactions into account, but we can see that in very deep networks,\neven higher-order interactions can be significant. even second-order optimization\nalgorithms are expensive and usually require numerous approximations that prevent\nthem from truly accounting for all significant second-order interactions. building\nan n-th order optimization algorithm for n > 2 thus seems hopeless. what can we\ndo instead?\n\n1\n\nbatch normalization provides an elegant way of reparametrizing almost any deep\nnetwork. the reparametrization significantly reduces the problem of coordinating\nupdates across many layers. batch normalization can be applied to any input\nor hidden layer in a network. let h be a minibatch of activations of the layer\nto normalize, arranged as a design matrix, with the activations for each example\nappearing in a row of the matrix. to normalize\n\n, we replace it with\n\nh\n\nh0 =\n\nh \u00b5\u2212\n\n\u03c3\n\n,\n\n(8.41)\n\nwhere \u00b5 is a vector containing the mean of each unit and \u03c3 is a vector containing\nthe standard deviation of each unit. the arithmetic here is based on broadcasting\nthe vector \u00b5 and the vector \u03c3 to be applied to every row of the matrix h . within\neach row, the arithmetic is element-wise, so hi,j is normalized by subtracting \u00b5j\nand dividing by \u03c3j . the rest of the network then operates on h0 in exactly the\nsame way that the original network operated on\n\n.h\n\nat training time,\n\n\u00b5 =\n\nhi,:\n\n1\n\nmxi\n\n319\n\n(8.42)\n\n "}, {"Page_number": 335, "text": "chapter 8. optimization for training deep models\n\nand\n\n\u03c3 =s\u03b4 +\n\n1\n\nmxi\n\nh \u00b5\u2212 2\n)\n(\ni ,\n\n(8.43)\n\nwhere \u03b4 is a small positive value such as 10\u22128 imposed to avoid encountering the\nundefined gradient of \u221az at z = 0. crucially, we back-propagate through\nthese operations for computing the mean and the standard deviation, and for\napplying them to normalize h . this means that the gradient will never propose\nan operation\u00a0that acts simply to increase the standard\u00a0deviation or mean of\nhi; the normalization operations remove the effect of such an action and zero\nout its component in the gradient. this was a major innovation of the batch\nnormalization approach.\u00a0previous approaches had involved adding penalties to\nthe cost function to encourage units to have normalized activation statistics or\ninvolved intervening to renormalize unit statistics after each gradient descent step.\nthe former approach usually resulted in imperfect normalization and the latter\nusually resulted in significant wasted time as the learning algorithm repeatedly\nproposed changing the mean and variance and the normalization step repeatedly\nundid this change. batch normalization reparametrizes the model to make some\nunits always be standardized by definition, deftly sidestepping both problems.\n\nat test time, \u00b5 and \u03c3 may be replaced by running averages that were collected\nduring training time. this allows the model to be evaluated on a single example,\nwithout needing to use definitions of \u00b5 and \u03c3 that depend on an entire minibatch.\n\nrevisiting the \u02c6y = xw1 w2 . . . wl example, we see that we can mostly resolve the\ndifficulties in learning this model by normalizing hl\u22121. suppose that x is drawn\nfrom a unit gaussian. then hl\u22121 will also come from a gaussian, because the\ntransformation from x to hl is linear. however, hl\u22121 will no longer have zero mean\nand unit variance. after applying batch normalization, we obtain the normalized\n\u02c6hl\u22121 that restores the zero mean and unit variance properties. for almost any\nupdate to the lower layers, \u02c6hl\u22121 will remain a unit gaussian. the output \u02c6y may\nthen be learned as a simple linear function \u02c6y = wl\u02c6h l\u22121. learning in this model is\nnow very simple because the parameters at the lower layers simply do not have an\neffect in most cases; their output is always renormalized to a unit gaussian.\u00a0in\nsome corner cases, the lower layers can have an effect. changing one of the lower\nlayer weights to\ncan make the output become degenerate, and changing the sign\nof one of the lower weights can flip the relationship between \u02c6hl\u22121 and y.\u00a0these\nsituations are very rare. without normalization, nearly every update would have\nan extreme effect on the statistics of hl\u22121. batch normalization has thus made\nthis model significantly easier to learn. in this example, the ease of learning of\ncourse came at the cost of making the lower layers useless. in our linear example,\n\n0\n\n320\n\n "}, {"Page_number": 336, "text": "chapter 8. optimization for training deep models\n\nthe lower layers no longer have any harmful effect, but they also no longer have\nany beneficial effect. this is because we have normalized out the first and second\norder statistics, which is all that a linear network can influence. in a deep neural\nnetwork with nonlinear activation functions, the lower layers can perform nonlinear\ntransformations of the data, so they remain useful. batch normalization acts to\nstandardize only the mean and variance of each unit in order to stabilize learning,\nbut allows the relationships between units and the nonlinear statistics of a single\nunit to change.\n\nbecause the final layer of the network is able to learn a linear transformation,\nwe may actually wish to remove all linear relationships between units within a\nlayer. indeed, this is the approach taken by\n), who provided\nthe inspiration for batch normalization. unfortunately,\u00a0eliminating all linear\ninteractions is much more expensive than standardizing the mean and standard\ndeviation of each individual unit, and so far batch normalization remains the most\npractical approach.\n\ndesjardins et al. 2015\n\n(\n\nnormalizing the mean and standard deviation of a unit can reduce the expressive\npower of the\u00a0neural network containing\u00a0that unit.\nin\u00a0order to maintain\u00a0the\nexpressive power of the network, it is common to replace the batch of hidden unit\nactivations h with \u03b3h0 + \u03b2 rather than simply the normalized h0. the variables\n\u03b3 and \u03b2 are learned parameters that allow the new variable to have any mean\nand standard deviation. at first glance, this may seem useless\u2014why did we set\nthe mean to 0, and then introduce a parameter that allows it to be set back to\nany arbitrary value \u03b2? the answer is that the new parametrization can represent\nthe same family of functions of the input as the old parametrization, but the new\nparametrization has different learning dynamics. in the old parametrization, the\nmean of h was determined by a complicated interaction between the parameters\nin the layers below h. in the new parametrization, the mean of \u03b3h0 + \u03b2 is\ndetermined solely by \u03b2 . the new parametrization is much easier to learn with\ngradient descent.\n\nmost neural network layers take the form of \u03c6(xw + b) where \u03c6 is some\nfixed nonlinear activation function such as the rectified linear transformation. it\nis natural to wonder whether we should apply batch normalization to the input\nx , or to the transformed value xw + b.\n) recommend\nthe latter. more specifically, xw + b should be replaced by a normalized version\nof xw . the bias term should be omitted because it becomes redundant with\nthe \u03b2 parameter applied by the batch normalization reparametrization. the input\nto a layer is usually the output of a nonlinear activation function such as the\nrectified linear function in a previous layer.\u00a0the statistics of the input are thus\n\nioffe and szegedy 2015\n\n(\n\n321\n\n "}, {"Page_number": 337, "text": "chapter 8. optimization for training deep models\n\nmore non-gaussian and less amenable to standardization by linear operations.\n\nin convolutional networks, described in chapter\n\n, it is important to apply the\n9\nsame normalizing \u00b5 and \u03c3 at every spatial location within a feature map, so that\nthe statistics of the feature map remain the same regardless of spatial location.\n\n8.7.2 coordinate descent\n\nin some cases, it may be possible to solve an optimization problem quickly by\nbreaking it into separate pieces. if we minimize f (x) with respect to a single variable\nxi, then minimize it with respect to another variable xj and so on, repeatedly\ncycling through all variables, we are guaranteed to arrive at a (local) minimum.\nthis practice is known as coordinate descent, because we optimize one coordinate\nat a time. more generally, block coordinate descent refers to minimizing with\nrespect to a subset of the variables simultaneously. the term \u201ccoordinate descent\u201d\nis often used to refer to block coordinate descent as well as the strictly individual\ncoordinate descent.\n\ncoordinate descent makes the most sense when the different variables in the\noptimization problem can be clearly separated into groups that play relatively\nisolated roles, or when optimization with respect to one group of variables is\nsignificantly more efficient than optimization with respect to all of the variables.\nfor example, consider the cost function\n\nj\n\n,\n\n(h w ) =xi,j\n\n|hi,j| +xi,j(cid:25)x w\u2212 >h(cid:26)2\n\ni,j\n\n.\n\n(8.44)\n\nthis function describes a learning problem called sparse coding, where the goal is\nto find a weight matrix w that can linearly decode a matrix of activation values\nh to reconstruct the training set x. most applications of sparse coding also\ninvolve weight decay or a constraint on the norms of the columns of w , in order\nto prevent the pathological solution with extremely small\n\nand large\n\nw\n\nh\n\n.\n\nthe function j is not convex. however,\u00a0we can divide the inputs to the\ntraining algorithm into two sets: the dictionary parameters w and the code\nrepresentations h . minimizing the objective function with respect to either one of\nthese sets of variables is a convex problem. block coordinate descent thus gives\nus an optimization strategy that allows us to use efficient convex optimization\nalgorithms, by alternating between optimizing w with h fixed, then optimizing\nh\n\nwwith\n\nfixed.\n\ncoordinate descent is not a very good strategy when the value of one variable\nstrongly influences the optimal value of another variable, as in the function f(x) =\n\n322\n\n "}, {"Page_number": 338, "text": "chapter 8. optimization for training deep models\n\n1 + x2\n\n(x1 \u2212 x 2)2 + \u03b1(cid:23)x2\n\n2(cid:24) where \u03b1 is a positive constant. the first term encourages\n\nthe two variables to have similar value, while the second term encourages them\nto be near zero. the solution is to set both to zero. newton\u2019s method can solve\nthe problem in a single step because it is a positive definite quadratic problem.\nhowever, for small \u03b1, coordinate descent will make very slow progress because the\nfirst term does not allow a single variable to be changed to a value that differs\nsignificantly from the current value of the other variable.\n\n8.7.3 polyak averaging\n\n,\n\npolyak averaging (polyak and juditsky 1992\n) consists of averaging together several\npoints\u00a0in the\u00a0trajectory\u00a0through parameter\u00a0space visited\u00a0by\u00a0an optimization\nalgorithm.\u00a0if t iterations of gradient descent visit points \u03b8(1), . . . , \u03b8 ( )t , then the\noutput of the polyak averaging algorithm is \u02c6\u03b8( )t = 1\nclasses, such as gradient descent applied to convex problems, this approach has\nstrong convergence guarantees. when applied to neural networks, its justification\nis more heuristic, but it performs well in practice. the basic idea is that the\noptimization algorithm may leap back and forth across a valley several times\nwithout ever visiting a point near the bottom of the valley. the average of all of\nthe locations on either side should be close to the bottom of the valley though.\n\ntpi \u03b8( )i .\u00a0on some problem\n\nin non-convex problems, the path taken by the optimization trajectory can be\nvery complicated and visit many different regions. including points in parameter\nspace from the distant past that may be separated from the current point by large\nbarriers in the cost function does not seem like a useful behavior. as a result,\nwhen applying polyak averaging to non-convex problems, it is typical to use an\nexponentially decaying running average:\n\n\u02c6\u03b8( )t = \u03b1 \u02c6\u03b8 (\n\n1)\n\nt\u2212 + (1\n\n)\u2212 \u03b1 \u03b8( )t .\n\n(8.45)\n\nthe running average approach is used in numerous applications. see szegedy\n\net al. (\n\n2015\n\n) for a recent example.\n\n8.7.4 supervised pretraining\n\nsometimes, directly training a model to solve a specific task can be too ambitious\nif the model is complex and hard to optimize or if the task is very difficult. it is\nsometimes more effective to train a simpler model to solve the task, then make the\nmodel more complex. it can also be more effective to train the model to solve a\nsimpler task, then move on to confront the final task. these strategies that involve\n\n323\n\n "}, {"Page_number": 339, "text": "chapter 8. optimization for training deep models\n\ntraining simple models on simple tasks before confronting the challenge of training\nthe desired model to perform the desired task are collectively known as pretraining.\n\ngreedy algorithms break a problem into many components, then solve for the\noptimal version of each component in isolation. unfortunately, combining the\nindividually optimal components is not guaranteed to yield an optimal complete\nsolution. however, greedy algorithms can be computationally much cheaper than\nalgorithms that solve for the best joint solution, and the quality of a greedy solution\nis often acceptable if not optimal. greedy algorithms may also be followed by a\nfine-tuning stage in which a joint optimization algorithm searches for an optimal\nsolution to the full problem. initializing the joint optimization algorithm with a\ngreedy solution can greatly speed it up and improve the quality of the solution it\nfinds.\n\npretraining, and especially greedy pretraining, algorithms are ubiquitous in\ndeep learning. in this section, we describe specifically those pretraining algorithms\nthat break supervised learning problems into other simpler supervised learning\nproblems. this approach is known as greedy supervised pretraining.\n\n,\n\n8.7\n\nin the original (\n\nbengio et al. 2007\n\n) version of greedy supervised pretraining,\neach stage consists of a supervised learning training task involving only a subset of\nthe layers in the final neural network. an example of greedy supervised pretraining\nis illustrated in fig.\n, in which each added hidden layer is pretrained as part of\na shallow supervised mlp, taking as input the output of the previously trained\nhidden layer. instead of pretraining one layer at a time, simonyan and zisserman\n(\n) pretrain a deep convolutional network (eleven weight layers) and then use\n2015\nthe first four and last three layers from this network to initialize even deeper\nnetworks (with up to nineteen layers of weights). the middle layers of the new,\nvery deep network are initialized randomly. the new network is then jointly trained.\nanother option, explored by yu\nof the previously\ntrained mlps, as well as the raw input, as inputs for each added stage.\n\n) is to use the\n\noutputs\n\net al. (\n\n2010\n\nwhy\u00a0would\u00a0greedy\u00a0supervised\u00a0pretraining\u00a0help? the\u00a0hypothesis\u00a0initially\n) is that it helps to provide better guidance to the\ndiscussed by\nintermediate levels of a deep hierarchy. in general, pretraining may help both in\nterms of optimization and in terms of generalization.\n\nbengio et al. 2007\n\n(\n\nan approach related to supervised pretraining extends the idea to the context\nof transfer learning: yosinski\n) pretrain a deep convolutional net with 8\nlayers of weights on a set of tasks (a subset of the 1000 imagenet object categories)\nand then initialize a same-size network with the first k layers of the first net. all\nthe layers of the second network (with the upper layers initialized randomly) are\n\net al. (\n\n2014\n\n324\n\n "}, {"Page_number": 340, "text": "chapter 8. optimization for training deep models\n\nu(1)u(1)\n\nw(1)w(1)\n\nyy\n\nh(1)h(1)\n\nxx\n\n(a)\n\nh(2)h(2)\n\nw(2)w(2)\n\nu(2)u(2)\n\nh(1)h(1)\n\nw(1)w(1)\n\nu(1)u(1)\n\nyy\n\nyy\n\nxx\n\n(c)\n\nh(1)h(1)\n\nw(1)w(1)\n\nu(1)u(1)\n\nyy\n\nxx\n\n(b)\n\ny\n\nh(2)h(2)\n\nh(1)h(1)\n\nu(2)u(2)\n\nw(2)w(2)\n\nw(1)w(1)\n\nu(1)u(1)\n\nyy\n\nxx\n\n(d)\n\n(c)\n\nbengio et al. 2007\n).\nfigure 8.7: illustration of one form of greedy supervised pretraining (\n(a) we start by training a sufficiently shallow architecture.\nanother drawing of the\nsame architecture. we keep only the input-to-hidden layer of the original network and\ndiscard the hidden-to-output layer. we send the output of the first hidden layer as input\nto another supervised single hidden layer mlp that is trained with the same objective\nas the first network was, thus adding a second hidden layer. this can be repeated for\nas many layers as desired.\nanother drawing of the result, viewed as a feedforward\nnetwork. to further improve the optimization, we can jointly fine-tune all the layers,\neither only at the end or at each stage of this process.\n\n(d)\n\n(b)\n\n,\n\n325\n\n "}, {"Page_number": 341, "text": "chapter 8. optimization for training deep models\n\nthen jointly trained to perform a different set of tasks (another subset of the 1000\nimagenet object categories), with fewer training examples than for the first set of\ntasks. other approaches to transfer learning with neural networks are discussed in\nsec.\n\n15.2\n.\n\n,\n\n(\n\nstudent\n\nfitnets romero et al. 2015\n\nanother related line of work is the\n\n) approach. this\napproach begins by training a network that has low enough depth and great enough\nwidth (number of units per layer) to be easy to train. this network then becomes\na teacher for a second network, designated the\n.\u00a0the student network is\nmuch deeper and thinner (eleven to nineteen layers) and would be difficult to train\nwith sgd under normal circumstances. the training of the student network is\nmade easier by training the student network not only to predict the output for\nthe original task, but also to predict the value of the middle layer of the teacher\nnetwork. this extra task provides a set of hints about how the hidden layers\nshould be used and can simplify the optimization problem. additional parameters\nare introduced to regress the middle layer of the 5-layer teacher network from\nthe middle layer of the deeper student network.\u00a0however, instead of predicting\nthe final classification target, the objective is to predict the middle hidden layer\nof the teacher network. the lower layers of the student networks thus have two\nobjectives: to help the outputs of the student network accomplish their task, as\nwell as to predict the intermediate layer of the teacher network. although a thin\nand deep network appears to be more difficult to train than a wide and shallow\nnetwork, the thin and deep network may generalize better and certainly has lower\ncomputational cost if it is thin enough to have far fewer parameters. without\nthe hints on the hidden layer, the student network performs very poorly in the\nexperiments, both on the training and test set. hints on middle layers may thus\nbe one of the tools to help train neural networks that otherwise seem difficult to\ntrain, but other optimization techniques or changes in the architecture may also\nsolve the problem.\n\n8.7.5 designing models to aid optimization\n\nto improve optimization, the best strategy is not always to improve the optimization\nalgorithm. instead, many improvements in the optimization of deep models have\ncome from designing the models to be easier to optimize.\n\nin principle, we could use activation functions that increase and decrease in\njagged non-monotonic patterns. however, this would make optimization extremely\ndifficult.\u00a0in practice, it is more important to choose a model family that\nis easy to optimize than to use a powerful optimization algorithm. most\nof the advances in neural network learning over the past 30 years have been\n\n326\n\n "}, {"Page_number": 342, "text": "chapter 8. optimization for training deep models\n\nobtained by changing the model family rather than changing the optimization\nprocedure. stochastic gradient descent with momentum, which was used to train\nneural networks in the 1980s, remains in use in modern state of the art neural\nnetwork applications.\n\nspecifically, modern neural networks reflect a design choice to use linear trans-\nformations between layers and activation functions that are differentiable almost\neverywhere and have significant slope in large portions of their domain.\u00a0in par-\nticular, model innovations like the lstm, rectified linear units and maxout units\nhave all moved toward using more linear functions than previous models like deep\nnetworks based on sigmoidal units. these models have nice properties that make\noptimization easier. the gradient flows through many layers provided that the\njacobian of the linear transformation has reasonable singular values.\u00a0moreover,\nlinear functions consistently increase in a single direction, so even if the model\u2019s\noutput is very far from correct, it is clear simply from computing the gradient\nwhich direction its output should move to reduce the loss function. in other words,\nmodern neural nets have been designed so that their local gradient information\ncorresponds reasonably well to moving toward a distant solution.\n\n2015\n\net al.,\n\nother model design strategies can help to make optimization easier. for\nexample, linear paths or skip connections between layers reduce the length of\nthe shortest path from the lower layer\u2019s parameters to the output,\u00a0and\u00a0thus\nmitigate the vanishing gradient problem (srivastava\n). a related idea\nto skip connections is adding extra copies of the output that are attached to the\nintermediate hidden layers of the network, as in googlenet (\nszegedy et al. 2014a\n)\nand deeply-supervised nets (\n). these \u201cauxiliary heads\u201d are trained\nto perform the same task as the primary output at the top of the network in order\nto ensure that the lower layers receive a large gradient. when training is complete\nthe auxiliary heads may be discarded.\u00a0this is an alternative to the pretraining\nstrategies, which were introduced in the previous section. in this way, one can\ntrain jointly all the layers in a single phase but change the architecture, so that\nintermediate layers (especially the lower ones) can get some hints about what they\nshould do, via a shorter path. these hints provide an error signal to lower layers.\n\nlee et al. 2014\n\n,\n\n,\n\n8.7.6 continuation methods and curriculum learning\n\n8.2.7\n\nas argued in sec.\n, many of the challenges in optimization arise from the global\nstructure of the cost function and cannot be resolved merely by making better\nestimates of local update directions. the predominant strategy for overcoming this\nproblem is to attempt to initialize the parameters in a region that is connected\nto the solution by a short path through parameter space that local descent can\n\n327\n\n "}, {"Page_number": 343, "text": "chapter 8. optimization for training deep models\n\ndiscover.\n\ncontinuation methods are a family of strategies that can make optimization\neasier by choosing initial points to ensure that local optimization spends most of\nits time in well-behaved regions of space. the idea behind continuation methods is\nto construct a series of objective functions over the same parameters. in order to\nminimize a cost function j (\u03b8 ), we will construct new cost functions {j (0), . . . , j ( )n }.\nthese cost functions are designed to be increasingly difficult, with j(0) being fairly\neasy to minimize, and j ( )n , the most difficult, being j(\u03b8), the true cost function\nmotivating the entire process. when we say that j ( )i\n, we\nmean that it is well behaved over more of \u03b8 space. a random initialization is more\nlikely to land in the region where local descent can minimize the cost function\nsuccessfully because this region is larger. the series of cost functions are designed\nso that a solution to one is a good initial point of the next. we thus begin by\nsolving an easy problem then refine the solution to solve incrementally harder\nproblems until we arrive at a solution to the true underlying problem.\n\nis easier than j( +1)\n\ni\n\n(\n\ntraditional continuation methods (predating the use of continuation methods\nfor neural network training) are usually based on smoothing the objective function.\nsee wu 1997\n) for an example of such a method and a review of some related\nmethods. continuation methods are also closely related to simulated annealing,\nwhich adds noise to the parameters (kirkpatrick\n). continuation\nmethods have been extremely successful in recent years. see mobahi and fisher\n(\n2015\n\n) for an overview of recent literature, especially for ai applications.\n\net al.,\u00a0\n\n1983\n\ncontinuation methods traditionally were mostly designed with the goal of\novercoming the challenge of local minima. specifically, they were designed to\nreach a global minimum despite the presence of many local minima. to do so,\nthese continuation methods would construct easier cost functions by \u201cblurring\u201d the\noriginal cost function. this blurring operation can be done by approximating\n\nj ( )i ( ) = \n\n\u03b8\n\ne\n\n\u03b80\u223cn (\u03b8 0;\u03b8,\u03c3( )2i\n\n)j(\u03b80)\n\n(8.46)\n\nvia sampling. the intuition for this approach is that some non-convex functions\nbecome approximately convex when blurred. in many cases, this blurring preserves\nenough information about the location of a global minimum that we can find the\nglobal minimum by solving progressively less blurred versions of the problem. this\napproach can break down in three different ways. first, it might successfully define\na series of cost functions where the first is convex and the optimum tracks from\none function to the next arriving at the global minimum, but it might require so\nmany incremental cost functions that the cost of the entire procedure remains high.\nnp-hard optimization problems remain np-hard, even when continuation methods\n\n328\n\n "}, {"Page_number": 344, "text": "chapter 8. optimization for training deep models\n\nare applicable. the other two ways that continuation methods fail both correspond\nto the method not being applicable. first, the function might not become convex,\nno matter how much it is blurred. consider for example the function j(\u03b8) = \u2212\u03b8>\u03b8.\nsecond, the function may become convex as a result of blurring, but the minimum\nof this blurred function may track to a local rather than a global minimum of the\noriginal cost function.\n\nthough continuation methods were mostly originally designed to deal with the\nproblem of local minima, local minima are no longer believed to be the primary\nproblem for neural network optimization. fortunately, continuation methods can\nstill help. the easier objective functions introduced by the continuation method can\neliminate flat regions, decrease variance in gradient estimates, improve conditioning\nof the hessian matrix, or do anything else that will either make local updates\neasier to compute or improve the correspondence between local update directions\nand progress toward a global solution.\n\n;\n\n,\n\n;\n\n,\n\n,\n\n;\n\n,\n\n;\n\n,\n\n(\n\n).\n\n2009\n\net al. (\n\nbengio\n\nskinner 1958 peterson 2004 krueger and dayan 2009\nsolomonoff 1989 elman 1993 sanger 1994\n\n) observed that an approach called curriculum learning or\nshaping can be interpreted as a continuation method. curriculum learning is based\non the idea of planning a learning process to begin by learning simple concepts\nand progress to learning more complex concepts that depend on these simpler\nconcepts. this basic strategy was previously known to accelerate progress in animal\n,\n) and machine\ntraining (\nlearning (\nbengio et al. 2009\n)\njustified this strategy as a continuation method, where earlier j( )i are made easier by\nincreasing the influence of simpler examples (either by assigning their contributions\nto the cost function larger coefficients, or by sampling them more frequently), and\nexperimentally demonstrated that better results could be obtained by following a\ncurriculum on a large-scale neural language modeling task. curriculum learning\nhas been successful on a wide range of natural language (spitkovsky\n2010\n;\n) and computer\ncollobert\nvision (\nkumar et al. 2010 lee and grauman 2011 supancic and ramanan 2013\n)\ntasks. curriculum learning was also verified as being consistent with the way in\nwhich humans teach (\n): teachers start by showing easier and\nmore prototypical examples and then help the learner refine the decision surface\nwith the less obvious cases. curriculum-based strategies are more effective for\nteaching humans than strategies based on uniform sampling of examples, and can\nalso increase the effectiveness of other teaching strategies (\nbasu and christensen\n,\n2013).\n\n2011b tu and honavar 2011\n\nkhan et al. 2011\n\n2011a mikolov\n\net al.,\n\net al.,\n\net al.,\n\n,\n\n;\n\n;\n\n,\n\n,\n\n;\n\n,\n\n;\n\n,\n\nanother important contribution to research on curriculum learning arose in the\ncontext of training recurrent neural networks to capture long-term dependencies:\n\n329\n\n "}, {"Page_number": 345, "text": "chapter 8. optimization for training deep models\n\n(\n\nzaremba and sutskever 2014\n) found that much better results were obtained with\na stochastic curriculum, in which a random mix of easy and difficult examples is\nalways presented to the learner, but where the average proportion of the more\ndifficult examples (here, those with longer-term dependencies) is gradually increased.\nwith a deterministic curriculum, no improvement over the baseline (ordinary\ntraining from the full training set) was observed.\n\nwe have now described the basic family of neural network models and how to\nregularize and optimize them. in the chapters ahead, we turn to specializations of\nthe neural network family, that allow neural networks to scale to very large sizes and\nprocess input data that has special structure. the optimization methods discussed\nin this chapter are often directly applicable to these specialized architectures with\nlittle or no modification.\n\n330\n\n "}, {"Page_number": 346, "text": "chapter 9\n\nconvolutional networks\n\n,\n\ncnns\n\n(\nlecun 1989\n\n), also known as\n\nconvolutional neural networks\nconvolutional networks\nor\n, are a specialized kind of neural network for processing data that has\na known, grid-like topology. examples include time-series data, which can be\nthought of as a 1d grid taking samples at regular time intervals, and image data,\nwhich can be thought of as a 2d grid of pixels. convolutional networks have been\ntremendously successful in practical applications. the name \u201cconvolutional neural\nnetwork\u201d\u00a0indicates that the network employs a mathematical operation called\nconvolution. convolution is a specialized kind of linear operation. convolutional\nnetworks are simply neural networks that use convolution in place of\ngeneral matrix multiplication in at least one of their layers.\n\nin this chapter,\u00a0we will first\u00a0describe what convolution is. next,\u00a0we will\nexplain the motivation behind using convolution in a neural network. we will\nthen describe an operation called pooling, which almost all convolutional networks\nemploy. usually, the operation used in a convolutional neural network does not\ncorrespond precisely to the definition of convolution as used in other fields such\nas engineering or pure mathematics. we will describe several variants on the\nconvolution function that are widely used in practice for neural networks. we\nwill also\u00a0show how convolution\u00a0may\u00a0be applied\u00a0to many kinds of data,\u00a0with\ndifferent numbers of dimensions.\u00a0we then discuss means of making convolution\nmore efficient. convolutional networks stand out as an example of neuroscientific\nprinciples influencing deep learning. we will discuss these neuroscientific principles,\nthen conclude with comments about the role convolutional networks have played\nin the history of deep learning. one topic this chapter does not address is how to\nchoose the architecture of your convolutional network. the goal of this chapter is\nto describe the kinds of tools that convolutional networks provide, while chapter 11\n\n331\n\n "}, {"Page_number": 347, "text": "chapter 9. convolutional networks\n\ndescribes general guidelines for choosing which tools to use in which circumstances.\nresearch into convolutional network architectures proceeds so rapidly that a new\nbest architecture for a given benchmark is announced every few weeks to months,\nrendering it impractical to describe the best architecture in print. however, the\nbest architectures have consistently been composed of the building blocks described\nhere.\n\n9.1 the convolution operation\n\nin its most general form, convolution is an operation on two functions of a real-\nvalued argument. to motivate the definition of convolution, we start with examples\nof two functions we might use.\n\nsuppose we are tracking the location of a spaceship with a laser sensor. our\nlaser sensor provides a single output x(t), the position of the spaceship at time\nt. both x and t are real-valued, i.e., we can get a different reading from the laser\nsensor at any instant in time.\n\nnow suppose that our laser sensor is somewhat noisy. to obtain a less noisy\nestimate of the spaceship\u2019s position, we would like to average together several\nmeasurements. of course, more recent measurements are more relevant, so we will\nwant this to be a weighted average that gives more weight to recent measurements.\nwe can do this with a weighting function w(a), where a is the age of a measurement.\nif we apply such a weighted average operation at every moment, we obtain a new\nfunction\n\nproviding a smoothed estimate of the position of the spaceship:\n\ns\n\ns t( ) =z x a w t\n\n( ) ( \u2212 )\n\na da\n\n(9.1)\n\nthis operation is called convolution. the convolution operation is typically\n\ndenoted with an asterisk:\n\n( ) = ( \u2217 )( )\ns t\nx w t\n\n(9.2)\n\nin our example, w needs to be a valid probability density function, or the\noutput is not a weighted average. also, w needs to be\nfor all negative arguments,\nor it will look into the future, which is presumably beyond our capabilities. these\nlimitations are particular to our example though. in general, convolution is defined\nfor any functions for which the above integral is defined, and may be used for other\npurposes besides taking weighted averages.\n\n0\n\nin convolutional network terminology, the first argument (in this example, the\nand the second\n\nfunction x) to the convolution is often referred to as the\n\ninput\n\n332\n\n "}, {"Page_number": 348, "text": "chapter 9. convolutional networks\n\nargument (in this example, the function w) as the\nreferred to as the feature map.\n\nkernel\n\n. the output is sometimes\n\nin our example, the idea of a laser sensor that can provide measurements\nat every instant in time is not realistic. usually, when we work with data on a\ncomputer, time will be discretized, and our sensor will provide data at regular\nintervals. in our example, it might be more realistic to assume that our laser\nprovides a measurement once per second. the time index t can then take on only\ninteger values. if we now assume that x and w are defined only on integer t, we\ncan define the discrete convolution:\n\n( ) = ( \u2217 )( ) =\ns t\n\nx w t\n\n\u221exa=\u2212\u221e\n\n( ) ( \u2212 )\nx a w t\na\n\n(9.3)\n\nin machine learning applications, the input is usually a multidimensional array\nof data and the kernel is usually a multidimensional array of parameters that are\nadapted by the learning algorithm. we will refer to these multidimensional arrays\nas tensors. because each element of the input and kernel must be explicitly stored\nseparately, we usually assume that these functions are zero everywhere but the\nfinite set of points for which we store the values. this means that in practice we\ncan implement the infinite summation as a summation over a finite number of\narray elements.\n\nfinally, we often use convolutions over more than one axis at a time. for\nexample, if we use a two-dimensional image i as our input, we probably also want\nto use a two-dimensional kernel\n\n:k\n\ns i, j\n\n(\n\n) = ( \u2217\n\ni k i, j\n\n)(\n\n) =xm xn\n\ni m, n k i m, j\n\n(\n\n)\n\n( \u2212\n\n\u2212 )\nn .\n\n(9.4)\n\nconvolution is commutative, meaning we can equivalently write:\n\ns i, j\n\n(\n\n) = (\n\nk i i, j\n\n\u2217 )(\n\n) =xm xn\n\n( \u2212\ni i m, j\n\nn k m, n .\n)\n\n(\n\n\u2212 )\n\n(9.5)\n\nusually the latter formula is more straightforward to implement in a machine\nlearning library, because there is less variation in the range of valid values of m\nand .n\n\nthe commutative property of convolution arises because we have flipped the\nkernel relative to the input, in the sense that as m increases, the index into the\ninput increases, but the index into the kernel decreases. the only reason to flip\nthe kernel is to obtain the commutative property. while the commutative property\n\n333\n\n "}, {"Page_number": 349, "text": "chapter 9. convolutional networks\n\nis useful for writing proofs, it is not usually an important property of a neural\nnetwork implementation. instead, many neural network libraries implement a\nrelated function called the cross-correlation, which is the same as convolution but\nwithout flipping the kernel:\n\ns i, j\n\n(\n\n) = ( \u2217\n\ni k i, j\n\n)(\n\n) =xm xn\n\ni i m, j\n\n( +\n\n+ )\n\nn k m, n .\n)\n\n(\n\n(9.6)\n\nmany machine learning libraries implement cross-correlation but call it convolution.\nin this text we will follow this convention of calling both operations convolution,\nand specify whether we mean to flip the kernel or not in contexts where kernel\nflipping is relevant. in the context of machine learning, the learning algorithm will\nlearn the appropriate values of the kernel in the appropriate place, so an algorithm\nbased on convolution with kernel flipping will learn a kernel that is flipped relative\nto the kernel learned by an algorithm without the flipping. it is also rare for\nconvolution to be used alone in machine learning; instead convolution is used\nsimultaneously with other functions, and the combination of these functions does\nnot commute regardless of whether the convolution operation flips its kernel or\nnot.\n\nsee fig.\n\n9.1\n\nfor an example of convolution (without kernel flipping) applied to\n\na 2-d tensor.\n\ndiscrete convolution can be viewed as multiplication by a matrix. however, the\nmatrix has several entries constrained to be equal to other entries. for example,\nfor univariate discrete convolution, each row of the matrix is constrained to be\nequal to the row above shifted by one element. this is known as a toeplitz matrix.\nin two dimensions, a doubly block circulant matrix corresponds to convolution.\nin addition to these constraints that several elements be equal to each other,\nconvolution usually corresponds to a very sparse matrix (a matrix whose entries are\nmostly equal to zero). this is because the kernel is usually much smaller than the\ninput image. any neural network algorithm that works with matrix multiplication\nand does not depend on specific properties of the matrix structure should work\nwith convolution, without requiring any further changes to the neural network.\ntypical convolutional neural networks do make use of further specializations in\norder to deal with large inputs efficiently, but these are not strictly necessary from\na theoretical perspective.\n\n334\n\n "}, {"Page_number": 350, "text": "chapter 9. convolutional networks\n\ninput\n\na\n\ne\n\ni\n\nb\n\nf\n\nj\n\nc\n\ng\n\nk\n\nd\n\nh\n\nl\n\noutput\n\nkernel\n\nw\n\ny\n\nx\n\nz\n\naw + bx +\naw + bx +\ney + f z\ney + f z\n\nbw + cx +\nbw + cx +\nf y + gz\nf y + gz\n\ncw + dx +\ncw + dx +\ngy + hz\ngy + hz\n\new + f x +\new + f x +\niy + jz\niy + jz\n\nf w + gx +\nf w + gx +\njy + kz\njy + kz\n\ngw + hx +\ngw + hx +\nky + lz\nky + lz\n\nfigure 9.1: an example of 2-d convolution without kernel-flipping. in this case we restrict\nthe output to only positions where the kernel lies entirely within the image, called \u201cvalid\u201d\nconvolution in some contexts. we draw boxes with arrows to indicate how the upper-left\nelement of the output tensor is formed by applying the kernel to the corresponding\nupper-left region of the input tensor.\n\n335\n\n "}, {"Page_number": 351, "text": "chapter 9. convolutional networks\n\n9.2 motivation\n\nconvolution leverages three important ideas that can help improve a machine\nlearning system: sparse interactions parameter sharing\nequivariant representa-\ntions. moreover, convolution provides a means for working with inputs of variable\nsize. we now describe each of these ideas in turn.\n\nand\n\n,\n\nor\n\nsparse weights\n\ntraditional neural network layers use matrix multiplication by a matrix of\nparameters with a separate parameter describing the interaction between each\ninput unit and each output unit. this means every output unit interacts with every\ninput unit. convolutional networks, however, typically have sparse interactions\n(also referred to as sparse connectivity\n). this is accomplished by\nmaking the kernel smaller than the input. for example, when processing an image,\nthe input image might have thousands or millions of pixels, but we can detect small,\nmeaningful features such as edges with kernels that occupy only tens or hundreds of\npixels. this means that we need to store fewer parameters, which both reduces the\nmemory requirements of the model and improves its statistical efficiency. it also\nmeans that computing the output requires fewer operations. these improvements\nin efficiency are usually quite large. if there are m inputs and n outputs, then\nmatrix multiplication requires m n\u00d7 parameters and the algorithms used in practice\nhave o(m n\u00d7 ) runtime (per example). if we limit the number of connections\neach output may have to k, then the sparsely connected approach requires only\nn\u00d7 parameters and o(k\nn\u00d7 ) runtime. for many practical applications, it is\nk\npossible to obtain good performance on the machine learning task while keeping\nk several orders of magnitude smaller than m.\u00a0for graphical demonstrations of\nsparse connectivity, see fig.\n. in a deep convolutional network,\nunits in the deeper layers may indirectly interact with a larger portion of the input,\nas shown in fig.\n. this allows the network to efficiently describe complicated\ninteractions between many variables by constructing such interactions from simple\nbuilding blocks that each describe only sparse interactions.\n\nand fig.\n\n9.3\n\n9.2\n\n9.4\n\nparameter\u00a0sharing refers to using\u00a0the same parameter for\u00a0more than one\nfunction in a model. in a traditional neural net, each element of the weight matrix\nis used exactly once when computing the output of a layer. it is multiplied by one\nelement of the input and then never revisited. as a synonym for parameter sharing,\none can say that a network has tied weights, because the value of the weight applied\nto one input is tied to the value of a weight applied elsewhere. in a convolutional\nneural net, each member of the kernel is used at every position of the input (except\nperhaps some of the boundary pixels, depending on the design decisions regarding\nthe boundary). the parameter sharing used by the convolution operation means\nthat rather than learning a separate set of parameters for every location, we learn\n\n336\n\n "}, {"Page_number": 352, "text": "chapter 9. convolutional networks\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\nfigure 9.2: sparse connectivity, viewed from below: we highlight one input unit, x3, and\nalso highlight the output units in s that are affected by this unit. (top) when s is formed\nby convolution with a kernel of width , only three outputs are affected by\nx. (bottom)\nwhen s is formed by matrix multiplication, connectivity is no longer sparse, so all of the\noutputs are affected by x3.\n\n3\n\n337\n\n "}, {"Page_number": 353, "text": "chapter 9. convolutional networks\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\nfigure 9.3: sparse connectivity, viewed from above: we highlight one output unit, s3, and\nalso highlight the input units in x that affect this unit. these units are known as the\nreceptive field of s3. (top) when s is formed by convolution with a kernel of width , only\nthree inputs affect s3.\ns is formed by matrix multiplication, connectivity\nis no longer sparse, so all of the inputs affect s3 .\n\n(bottom)\n\nwhen\n\n3\n\ng1g1\n\nh1h1\n\ng2g2\n\nh2h2\n\ng3g3\n\nh3h3\n\ng4g4\n\nh4h4\n\ng5g5\n\nh5h5\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\nfigure 9.4: the receptive field of the units in the deeper layers of a convolutional network\nis larger than the receptive field of the units in the shallow layers. this effect increases if\nthe network includes architectural features like strided convolution (fig.\n) or pooling\n(sec.\ndirect connections in a convolutional net are very\nsparse, units in the deeper layers can be indirectly connected to all or most of the input\nimage.\n\n). this means that even though\n\n9.12\n\n9.3\n\n338\n\n "}, {"Page_number": 354, "text": "chapter 9. convolutional networks\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\nx 1x 1\n\ns1s1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\nfigure 9.5: parameter sharing: black arrows indicate the connections that use a particular\nparameter in two different models. (top) the black arrows indicate uses of the central\nelement of a 3-element kernel in a convolutional model. due to parameter sharing, this\nsingle parameter is used at all input locations.\nthe single black arrow indicates\nthe use of the central element of the weight matrix in a fully connected model. this model\nhas no parameter sharing so the parameter is used only once.\n\n(bottom)\n\nonly one set. this does not affect the runtime of forward propagation\u2014it is still\nn\u00d7 )\u2014but it does further reduce the storage requirements of the model to\no(k\nk parameters.\u00a0recall that k is usually several orders of magnitude less than m.\nsince m and n are usually roughly the same size, k is practically insignificant\ncompared to m n\u00d7 . convolution is thus dramatically more efficient than dense\nmatrix multiplication in terms of the memory requirements and statistical efficiency.\nfor a graphical depiction of how parameter sharing works, see fig.\n\n.9.5\n\nas an example of both of these first two principles in action, fig.\n\nshows how\nsparse connectivity and parameter sharing can dramatically improve the efficiency\nof a linear function for detecting edges in an image.\n\n9.6\n\nin the case of convolution, the particular form of parameter sharing causes the\nlayer to have a property called equivariance to translation. to say a function is\nequivariant means that if the input changes, the output changes in the same way.\nspecifically, a function f(x) is equivariant to a function g if f (g(x)) = g(f (x)).\nin the case of convolution, if we let g be any function that translates the input,\ni.e., shifts it, then the convolution function is equivariant to g. for example, let i\nbe a function giving image brightness at integer coordinates. let g be a function\nmapping one image function to another image function, such that i0 = g(i ) is\n\n339\n\n "}, {"Page_number": 355, "text": "chapter 9. convolutional networks\n\nthe image function with i0(x, y) = i(x \u2212 1, y). this shifts every pixel of i one\nunit to the right.\u00a0if we apply this transformation to i , then apply convolution,\nthe result will be the same as if we applied convolution to i0, then applied the\ntransformation g to the output. when processing time series data, this means\nthat convolution produces a sort of timeline that shows when different features\nappear in the input. if we move an event later in time in the input, the exact\nsame representation of it will appear in the output, just later in time. similarly\nwith images, convolution creates a 2-d map of where certain features appear in\nthe input. if we move the object in the input, its representation will move the\nsame amount in the output. this is useful for when we know that some function\nof a small number of neighboring pixels is useful when applied to multiple input\nlocations. for example, when processing images, it is useful to detect edges in\nthe first layer of a convolutional network. the same edges appear more or less\neverywhere in the image, so it is practical to share parameters across the entire\nimage. in some cases, we may not wish to share parameters across the entire\nimage. for example, if we are processing images that are cropped to be centered\non an individual\u2019s face, we probably want to extract different features at different\nlocations\u2014the part of the network processing the top of the face needs to look for\neyebrows, while the part of the network processing the bottom of the face needs to\nlook for a chin.\n\nconvolution is not naturally equivariant to some other transformations, such\nas changes in the scale or rotation of an image. other mechanisms are necessary\nfor handling these kinds of transformations.\n\nfinally, some kinds of data cannot be processed by neural networks defined by\nmatrix multiplication with a fixed-shape matrix. convolution enables processing\nof some of these kinds of data. we discuss this further in sec.\n\n.9.7\n\n9.3 pooling\n\na typical layer of a convolutional network consists of three stages (see fig.\n). in\nthe first stage, the layer performs several convolutions in parallel to produce a set\nof linear activations. in the second stage, each linear activation is run through a\nnonlinear activation function, such as the rectified linear activation function. this\nstage is sometimes called the detector stage. in the third stage, we use a pooling\nfunction to modify the output of the layer further.\n\n9.7\n\na pooling function replaces the output of the net at a certain location with\na summary statistic of the nearby outputs. for example, the max pooling (zhou\n) operation reports the maximum output within a rectangular\nand chellappa 1988\n\n,\n\n340\n\n "}, {"Page_number": 356, "text": "chapter 9. convolutional networks\n\nfigure 9.6:\u00a0efficiency of edge detection.\u00a0the image on the right was formed by taking\neach pixel in the original image and subtracting the value of its neighboring pixel on the\nleft.\u00a0this shows the strength of all of the vertically oriented edges in the input image,\nwhich can be a useful operation for object detection. both images are 280 pixels tall.\nthe input image is 320 pixels wide while the output image is 319 pixels wide. this\ntransformation can be described by a convolution kernel containing two elements, and\nrequires 319 \u00d7 280 \u00d7 3 = 267, 960 floating point operations (two multiplications and\none addition per output pixel) to compute using convolution. to describe the same\ntransformation with a matrix multiplication would take 320 \u00d7 280\u00d7 319\u00d7 280, or over\neight billion, entries in the matrix, making convolution four billion times more efficient for\nrepresenting this transformation.\u00a0the straightforward matrix multiplication algorithm\nperforms over sixteen billion floating point operations, making convolution roughly 60,000\ntimes more efficient computationally. of course, most of the entries of the matrix would be\nzero. if we stored only the nonzero entries of the matrix, then both matrix multiplication\nand convolution would require the same number of floating point operations to compute.\nthe matrix would still need to contain 2 \u00d7 319 \u00d7 280 = 178, 640 entries. convolution\nis an extremely efficient way of describing transformations that apply the same linear\ntransformation of a small, local region across the entire input. (photo credit: paula\ngoodfellow)\n\n341\n\n "}, {"Page_number": 357, "text": "chapter 9. convolutional networks\n\ncomplex\u00a0layer\u00a0terminology\n\nsimple\u00a0layer\u00a0terminology\n\nnext\u00a0layer\n\nnext\u00a0layer\n\nconvolutional\u00a0layer\n\npooling\u00a0stage\n\npooling\u00a0layer\n\ndetector\u00a0stage:\n\nnonlinearity\n\ne.g.,\u00a0rectified\u00a0linear\n\nconvolution\u00a0stage:\n\na ne\u00a0transform\n\nffi\n\ndetector\u00a0layer:\u00a0nonlinearity\n\ne.g.,\u00a0rectified\u00a0linear\n\nconvolution\u00a0layer:\na ne\u00a0transform\u00a0\n\nffi\n\ninput\u00a0to\u00a0layer\n\ninput\u00a0to\u00a0layers\n\nfigure 9.7: the components of a typical convolutional neural network layer. there are two\ncommonly used sets of terminology for describing these layers. (left) in this terminology,\nthe convolutional net is viewed as a small number of relatively complex layers, with each\nlayer having many \u201cstages.\u201d in this terminology, there is a one-to-one mapping between\nkernel tensors and network layers. in this book we generally use this terminology. (right)\nin this terminology, the convolutional net is viewed as a larger number of simple layers;\nevery step of processing is regarded as a layer in its own right. this means that not every\n\u201clayer\u201d has parameters.\n\n342\n\n "}, {"Page_number": 358, "text": "chapter 9. convolutional networks\n\nneighborhood. other popular pooling functions include the average of a rectangular\nneighborhood, the l 2 norm of a rectangular neighborhood, or a weighted average\nbased on the distance from the central pixel.\n\n9.8\n\nfor an example of how this works.\n\nin all cases, pooling helps to make the representation become approximately\ninvariant to small translations of the input. invariance to translation means that if\nwe translate the input by a small amount, the values of most of the pooled outputs\ndo not change. see fig.\ninvariance to\nlocal translation can be a very useful property if we care more about\nwhether some feature is present than exactly where it is. for example,\nwhen determining whether an image contains a face, we need not know the location\nof the eyes with pixel-perfect accuracy, we just need to know that there is an eye on\nthe left side of the face and an eye on the right side of the face. in other contexts,\nit is more important to preserve the location of a feature. for example, if we want\nto find a corner defined by two edges meeting at a specific orientation, we need to\npreserve the location of the edges well enough to test whether they meet.\n\nthe use of pooling can be viewed as adding an infinitely strong prior that\nthe function the layer learns must be invariant to small translations. when this\nassumption is correct, it can greatly improve the statistical efficiency of the network.\n\npooling over spatial regions produces invariance to translation, but if we pool\nover the outputs of separately parametrized convolutions, the features can learn\nwhich transformations to become invariant to (see fig.\n\n9.9\n\n).\n\n9.10\n\nbecause pooling summarizes the responses over a whole neighborhood, it is\npossible to use fewer pooling units than detector units, by reporting summary\nstatistics for pooling regions spaced k pixels apart rather than 1 pixel apart. see\nfig.\nfor an example. this improves the computational efficiency of the network\nbecause the next layer has roughly k times fewer inputs to process. when the\nnumber of parameters in the next layer is a function of its input size (such as\nwhen the next layer is fully connected and based on matrix multiplication) this\nreduction in the input size can also result in improved statistical efficiency and\nreduced memory requirements for storing the parameters.\n\nfor many tasks, pooling is essential for handling inputs of varying size.\u00a0for\nexample, if we want to classify images of variable size, the input to the classification\nlayer must have a fixed size. this is usually accomplished by varying the size of an\noffset between pooling regions so that the classification layer always receives the\nsame number of summary statistics regardless of the input size. for example, the\nfinal pooling layer of the network may be defined to output four sets of summary\nstatistics, one for each quadrant of an image, regardless of the image size.\n\nsome theoretical work gives guidance as to which kinds of pooling one should\n\n343\n\n "}, {"Page_number": 359, "text": "chapter 9. convolutional networks\n\n...\n\n...\n\n...\n\n...\n\npooling\u00a0stage\n\n1.\n\n1.\n\n1.\n\n0.2\n\n0.1\n\n1.\n\n0.2\n\n0.1\n\ndetector\u00a0stage\n\npooling\u00a0stage\n\n0.3\n\n1.\n\n1.\n\n1.\n\n0.3\n\n0.1\n\n1.\n\n0.2\n\ndetector\u00a0stage\n\n...\n\n...\n\n...\n\n...\n\nfigure 9.8: max pooling introduces invariance. (top) a view of the middle of the output\nof a convolutional layer. the bottom row shows outputs of the nonlinearity. the top\nrow shows the outputs of max pooling, with a stride of one pixel between pooling regions\nand a pooling region width of three pixels.\na view of the same network, after\nthe input has been shifted to the right by one pixel. every value in the bottom row has\nchanged, but only half of the values in the top row have changed, because the max pooling\nunits are only sensitive to the maximum value in the neighborhood, not its exact location.\n\n(bottom)\n\n344\n\n "}, {"Page_number": 360, "text": "chapter 9. convolutional networks\n\nlarge\u00a0response\nin\u00a0pooling\u00a0unit\n\nlarge\n\nresponse\n\nin\u00a0detector\n\nunit\u00a01\n\nlarge\u00a0response\nin\u00a0pooling\u00a0unit\n\nlarge\n\nresponse\n\nin\u00a0detector\n\nunit\u00a03\n\nfigure 9.9: example of learned invariances: a pooling unit that pools over multiple features\nthat are learned with separate parameters can learn to be invariant to transformations of\nthe input. here we show how a set of three learned filters and a max pooling unit can learn\nto become invariant to rotation. all three filters are intended to detect a hand-written 5.\neach filter attempts to match a slightly different orientation of the 5. when a 5 appears in\nthe input, the corresponding filter will match it and cause a large activation in a detector\nunit. the max pooling unit then has a large activation regardless of which pooling unit\nwas activated. we show here how the network processes two different inputs, resulting\nin two different detector units being activated. the effect on the pooling unit is roughly\nthe same either way. this principle is leveraged by maxout networks (\ngoodfellow et al.\n,\n2013a) and other convolutional networks. max pooling over spatial positions is naturally\ninvariant to translation; this multi-channel approach is only necessary for learning other\ntransformations.\n\n1.\n\n0.2\n\n0.1\n\n0.1\n\n1.\n\n0.2\n\n0.1\n\n0.0\n\n0.1\n\nfigure 9.10: pooling with downsampling. here we use max-pooling with a pool width of\nthree and a stride between pools of two. this reduces the representation size by a factor\nof two, which reduces the computational and statistical burden on the next layer. note\nthat the rightmost pooling region has a smaller size, but must be included if we do not\nwant to ignore some of the detector units.\n\n345\n\n "}, {"Page_number": 361, "text": "chapter 9. convolutional networks\n\nboureau et al. 2010\n\nuse in various situations (\n). it is also possible to dynamically\npool features together, for example, by running a clustering algorithm on the\nlocations of interesting features (\n). this approach yields a\ndifferent set of pooling regions for each image. another approach is to learn a\nsingle pooling structure that is then applied to all images (\n\nboureau et al. 2011\n\njia et al. 2012\n\n).\n\n,\n\n,\n\n,\n\npooling can complicate some kinds of neural network architectures that use\ntop-down information, such as boltzmann machines and autoencoders. these\nissues will be discussed further when we present these types of networks in part\niii. pooling in convolutional boltzmann machines is presented in sec.\n. the\ninverse-like operations on pooling units needed in some differentiable networks will\nbe covered in sec.\n\n20.10.6\n.\n\n20.6\n\nsome examples of complete convolutional network architectures for classification\n\nusing convolution and pooling are shown in fig.\n\n9.11\n.\n\n9.4 convolution and\u00a0pooling\u00a0as an infinitely\u00a0strong\n\nprior\n\nrecall the concept of a prior probability distribution from sec.\n. this is a\nprobability distribution over the parameters of a model that encodes our beliefs\nabout what models are reasonable, before we have seen any data.\n\n5.2\n\npriors can be considered weak or strong depending on how concentrated the\nprobability density in the prior is. a weak prior is a prior distribution with high\nentropy, such as a gaussian distribution with high variance. such a prior allows\nthe data to move the parameters more or less freely. a strong prior has very low\nentropy, such as a gaussian distribution with low variance. such a prior plays a\nmore active role in determining where the parameters end up.\n\nan infinitely strong prior places zero probability on some parameters and says\nthat these parameter values are completely forbidden, regardless of how much\nsupport the data gives to those values.\n\nwe can imagine a convolutional net as being similar to a fully connected net,\nbut with an infinitely strong prior over its weights. this infinitely strong prior\nsays that the weights for one hidden unit must be identical to the weights of its\nneighbor, but shifted in space. the prior also says that the weights must be zero,\nexcept for in the small, spatially contiguous receptive field assigned to that hidden\nunit. overall, we can think of the use of convolution as introducing an infinitely\nstrong prior probability distribution over the parameters of a layer. this prior\nsays that the function the layer should learn contains only local interactions and is\n\n346\n\n "}, {"Page_number": 362, "text": "chapter 9. convolutional networks\n\noutput\u00a0of\u00a0softmax:\u00a0\n\noutput\u00a0of\u00a0softmax:\u00a0\n\noutput\u00a0of\u00a0softmax:\u00a0\n\n1,000\u00a0class\u00a0\nprobabilities\n\n1,000\u00a0class\u00a0\nprobabilities\n\n1,000\u00a0class\u00a0\nprobabilities\n\noutput\u00a0of\u00a0matrix\u00a0\nmultiply:\u00a01,000\u00a0units\n\noutput\u00a0of\u00a0matrix\u00a0\nmultiply:\u00a01,000\u00a0units\n\noutput\u00a0of\u00a0average\u00a0\npooling:\u00a01x1x1,000\n\noutput\u00a0of\u00a0reshape\u00a0to\u00a0\n\noutput\u00a0of\u00a0reshape\u00a0to\u00a0\n\nvector:\n\n16,384\u00a0units\n\nvector:\n\n576\u00a0units\n\noutput\u00a0of\u00a0\nconvolution:\n16x16x1,000\n\noutput\u00a0of\u00a0pooling\u00a0\n\nwith\u00a0stride\u00a04:\u00a0\n\n16x16x64\n\noutput\u00a0of\u00a0pooling\u00a0to\u00a0\n\n3x3\u00a0grid:\u00a03x3x64\n\noutput\u00a0of\u00a0pooling\u00a0\n\nwith\u00a0stride\u00a04:\u00a0\n\n16x16x64\n\noutput\u00a0of\u00a0\n\noutput\u00a0of\u00a0\n\noutput\u00a0of\u00a0\n\nconvolution+relu:\u00a0\n\nconvolution+relu:\u00a0\n\nconvolution+relu:\u00a0\n\n64x64x64\n\n64x64x64\n\n64x64x64\n\noutput\u00a0of\u00a0pooling\u00a0\n\noutput\u00a0of\u00a0pooling\u00a0\n\noutput\u00a0of\u00a0pooling\u00a0\n\nwith\u00a0stride\u00a04:\u00a0\n\n64x64x64\n\nwith\u00a0stride\u00a04:\u00a0\n\n64x64x64\n\nwith\u00a0stride\u00a04:\u00a0\n\n64x64x64\n\noutput\u00a0of\u00a0\n\noutput\u00a0of\u00a0\n\noutput\u00a0of\u00a0\n\nconvolution+\u00a0relu:\u00a0\n\nconvolution+\u00a0relu:\u00a0\n\nconvolution+\u00a0relu:\u00a0\n\n256x256x64\n\n256x256x64\n\n256x256x64\n\ninput\u00a0image:\u00a0\n\n256x256x3\n\ninput\u00a0image:\u00a0\n\n256x256x3\n\ninput\u00a0image:\u00a0\n\n256x256x3\n\nfigure 9.11: examples of architectures for classification with convolutional networks. the\nspecific strides and depths used in this figure are not advisable for real use; they are\ndesigned to be very shallow in order to fit onto the page.\u00a0real convolutional networks\nalso often involve significant amounts of branching, unlike the chain structures used\nhere for simplicity. (left) a convolutional network that processes a fixed image size.\nafter alternating between convolution and pooling for a few layers, the tensor for the\nconvolutional feature map is reshaped to flatten out the spatial dimensions. the rest\nof the network is an ordinary feedforward network classifier, as described in chapter\n.6\n(center) a convolutional network that processes a variable-sized image, but still maintains\na fully connected section. this network uses a pooling operation with variably-sized pools\nbut a fixed number of pools, in order to provide a fixed-size vector of 576 units to the\nfully connected portion of the network.\na convolutional network that does not\nhave any fully connected weight layer. instead, the last convolutional layer outputs one\nfeature map per class. the model presumably learns a map of how likely each class is to\noccur at each spatial location. averaging a feature map down to a single value provides\nthe argument to the softmax classifier at the top.\n\n(right)\n\n347\n\n "}, {"Page_number": 363, "text": "chapter 9. convolutional networks\n\nequivariant to translation. likewise, the use of pooling is an infinitely strong prior\nthat each unit should be invariant to small translations.\n\nof course, implementing a convolutional net as a fully connected net with an\ninfinitely strong prior would be extremely computationally wasteful. but thinking\nof a convolutional net as a fully connected net with an infinitely strong prior can\ngive us some insights into how convolutional nets work.\n\none key insight is that convolution and pooling can cause underfitting. like\nany prior, convolution and pooling are only useful when the assumptions made\nby the prior are reasonably accurate. if a task relies on preserving precise spatial\ninformation, then using pooling on all features can increase the training error.\nsome convolutional network architectures (\n) are designed to\nuse pooling on some channels but not on other channels, in order to get both\nhighly invariant features and features that will not underfit when the translation\ninvariance prior is incorrect. when a task involves incorporating information from\nvery distant locations in the input, then the prior imposed by convolution may be\ninappropriate.\n\nszegedy et al. 2014a\n\n,\n\nanother key insight from this view is that we should only compare convolu-\ntional models to other convolutional models in benchmarks of statistical learning\nperformance. models that do not use convolution would be able to learn even if\nwe permuted all of the pixels in the image.\u00a0for many image datasets, there are\nseparate benchmarks for models that are permutation invariant and must discover\nthe concept of topology via learning, and models that have the knowledge of spatial\nrelationships hard-coded into them by their designer.\n\n9.5 variants of the basic convolution function\n\nwhen discussing convolution in the context of neural networks, we usually do\nnot refer exactly to the standard discrete convolution operation as it is usually\nunderstood in the mathematical literature. the functions used in practice differ\nslightly. here we describe these differences in detail, and highlight some useful\nproperties of the functions used in neural networks.\n\nfirst, when we refer to convolution in the context of neural networks, we usually\nactually mean an operation that consists of many applications of convolution in\nparallel. this is because convolution with a single kernel can only extract one kind\nof feature, albeit at many spatial locations. usually we want each layer of our\nnetwork to extract many kinds of features, at many locations.\n\nadditionally, the input is usually not just a grid of real values. rather, it is a\n\n348\n\n "}, {"Page_number": 364, "text": "chapter 9. convolutional networks\n\ngrid of vector-valued observations.\u00a0for example, a color image has a red, green\nand blue intensity at each pixel. in a multilayer convolutional network, the input\nto the second layer is the output of the first layer, which usually has the output\nof many different convolutions at each position. when working with images, we\nusually think of the input and output of the convolution as being 3-d tensors, with\none index into the different channels and two indices into the spatial coordinates\nof each channel. software implementations usually work in batch mode, so they\nwill actually use 4-d tensors, with the fourth axis indexing different examples in\nthe batch, but we will omit the batch axis in our description here for simplicity.\n\nbecause convolutional networks usually use multi-channel convolution, the\nlinear operations they are based on are not guaranteed to be commutative, even if\nkernel-flipping is used. these multi-channel operations are only commutative if\neach operation has the same number of output channels as input channels.\n\nassume we have a 4-d kernel tensor k with element ki,j,k,l giving the connection\nstrength between a unit in channel i of the output and a unit in channel j of the\ninput, with an offset of k rows and l columns between the output unit and the\ninput unit. assume our input consists of observed data v with element vi,j,k giving\nthe value of the input unit within channel i at row j and column k. assume our\noutput consists of z with the same format as v. if z is produced by convolving k\nacross without flipping\n\n, then\n\nk\n\nv\n\nzi,j,k =xl,m,n\n\nvl,j m ,k n\n\n+ \u22121 + \u22121ki,l,m,n\n\n(9.7)\n\nwhere the summation over l , m and n is over all values for which the tensor indexing\noperations inside the summation is valid. in linear algebra notation, we index into\n\u2212 1 in the above formula.\narrays using a\nprogramming languages such as c and python index starting from , rendering\nthe above expression even simpler.\n\nfor the first entry. this necessitates the\n\n0\n\n1\n\nwe may want to skip over some positions of the kernel in order to reduce the\ncomputational cost (at the expense of not extracting our features as finely). we\ncan think of this as downsampling the output of the full convolution function. if\nwe want to sample only every s pixels in each direction in the output, then we can\ndefined a downsampled convolution function such that\n\nc\n\nzi,j,k =  (\n\nc k v,\n\n)\n\n, s i,j,k = xl,m,n(cid:21)v\n\n( \u2212 \u00d71) + ( \u2212 \u00d71) +\nl, j\ns n\n\ns m, k\n\nki,l,m,n(cid:22) .\n\n(9.8)\n\nwe refer to s as the\nto define a separate stride for each direction of motion. see fig.\nillustration.\n\nof this downsampled convolution. it is also possible\nfor an\n\nstride\n\n9.12\n\n349\n\n "}, {"Page_number": 365, "text": "chapter 9. convolutional networks\n\ns1s1\n\ns2s2\n\ns3s3\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\ns1s1\n\ns2s2\n\ns3s3\n\nz1z1\n\nz2z2\n\nz3z3\n\nz4z4\n\nz5z5\n\nstrided\n\nconvolution\n\ndownsampling\n\nconvolution\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\nfigure 9.12:\u00a0convolution with a stride.\u00a0in this example, we use a stride of two.\u00a0(top)\nconvolution with a stride length of two implemented in a single operation. (bottom)\nconvolution with a stride greater than one pixel is mathematically equivalent to convolution\nwith unit stride followed by downsampling. obviously, the two-step approach involving\ndownsampling is computationally wasteful, because it computes many values that are\nthen discarded.\n\n350\n\n "}, {"Page_number": 366, "text": "chapter 9. convolutional networks\n\none essential feature of any convolutional network implementation is the ability\nto implicitly zero-pad the input v in order to make it wider. without this feature,\nthe width of the representation shrinks by one pixel less than the kernel width\nat each layer. zero padding the input allows us to control the kernel width and\nthe size of the output independently. without zero padding, we are forced to\nchoose between shrinking the spatial extent of the network rapidly and using small\nkernels\u2014both scenarios that significantly limit the expressive power of the network.\nsee fig.\n\nfor an example.\n\n9.13\n\nvalid\n\nthree special cases of the zero-padding setting are worth mentioning. one is\nthe extreme case in which no zero-padding is used whatsoever, and the convolution\nkernel is only allowed to visit positions where the entire kernel is contained entirely\nwithin the image.\u00a0in matlab terminology, this is called\nconvolution.\u00a0in\nthis case, all pixels in the output are a function of the same number of pixels in\nthe input, so the behavior of an output pixel is somewhat more regular. however,\nthe size of the output shrinks at each layer. if the input image has width m and\nthe kernel has width k , the output will be of width m k\u2212 + 1.\u00a0the rate of this\nshrinkage can be dramatic if the kernels used are large. since the shrinkage is\ngreater than 0, it limits the number of convolutional layers that can be included\nin the network. as layers are added, the spatial dimension of the network will\neventually drop to 1 \u00d7 1, at which point additional layers cannot meaningfully\nbe considered convolutional. another special case of the zero-padding setting is\nwhen just enough zero-padding is added to keep the size of the output equal to the\nsize of the input. matlab calls this\nconvolution. in this case, the network\ncan contain as many convolutional layers as the available hardware can support,\nsince the operation of convolution does not modify the architectural possibilities\navailable to the next layer.\u00a0however, the input pixels near the border influence\nfewer output pixels than the input pixels near the center. this can make the\nborder pixels somewhat underrepresented in the model. this motivates the other\nextreme case, which matlab refers to as full convolution, in which enough zeroes\nare added for every pixel to be visited k times in each direction, resulting in an\noutput image of width\n. in this case, the output pixels near the border\nare a function of fewer pixels than the output pixels near the center. this can\nmake it difficult to learn a single kernel that performs well at all positions in\nthe convolutional feature map. usually the optimal amount of zero padding (in\nterms of test set classification accuracy) lies somewhere between \u201cvalid\u201d and \u201csame\u201d\nconvolution.\n\nm k+ \u2212 1\n\nsame\n\nin some cases, we do not actually want to use convolution, but rather locally\nconnected layers (\n). in this case, the adjacency matrix in the\ngraph of our mlp is the same, but every connection has its own weight, specified\n\nlecun 1986 1989\n\n,\n\n,\n\n351\n\n "}, {"Page_number": 367, "text": "chapter 9. convolutional networks\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\nfigure 9.13: the effect of zero padding on network size: consider a convolutional network\nwith a kernel of width six at every layer. in this example, we do not use any pooling, so\nonly the convolution operation itself shrinks the network size. (top) in this convolutional\nnetwork, we do not use any implicit zero padding. this causes the representation to\nshrink by five pixels at each layer. starting from an input of sixteen pixels, we are only\nable to have three convolutional layers, and the last layer does not ever move the kernel,\nso arguably only two of the layers are truly convolutional. the rate of shrinking can\nbe mitigated by using smaller kernels, but smaller kernels are less expressive and some\nshrinking is inevitable in this kind of architecture.\nby adding five implicit zeroes\nto each layer, we prevent the representation from shrinking with depth. this allows us to\nmake an arbitrarily deep convolutional network.\n\n(bottom)\n\n352\n\n "}, {"Page_number": 368, "text": "chapter 9. convolutional networks\n\nby a 6-d tensor w.\u00a0the indices into w are respectively: i, the output channel,\nj, the output row, k, the output column, l, the input channel, m, the row offset\nwithin the input, and n, the column offset within the input. the linear part of a\nlocally connected layer is then given by\n\nzi,j,k = xl,m,n\n\n[vl,j m ,k n\n\n+ \u22121 + \u22121wi,j,k,l,m,n] .\n\n(9.9)\n\nthis is sometimes also called unshared convolution, because it is a similar operation\nto discrete convolution with a small kernel, but without sharing parameters across\nlocations. fig.\ncompares local connections, convolution, and full connections.\n\n9.14\n\nlocally connected layers are useful when we know that each feature should be\na function of a small part of space, but there is no reason to think that the same\nfeature should occur across all of space. for example, if we want to tell if an image\nis a picture of a face, we only need to look for the mouth in the bottom half of the\nimage.\n\nit can also be useful to make versions of convolution or locally connected layers\nin which the connectivity is further restricted, for example to constrain that each\noutput channel i be a function of only a subset of the input channels l. a common\nway to do this is to make the first m output channels connect to only the first\nn input channels, the second m output channels connect to only the second n\ninput channels, and so on.\u00a0see fig.\nfor an example.\u00a0modeling interactions\nbetween few channels allows the network to have fewer parameters in order to\nreduce memory consumption and increase statistical efficiency, and also reduces\nthe amount of computation needed to perform forward and back-propagation. it\naccomplishes these goals without reducing the number of hidden units.\n\n9.15\n\n,\n\n;\n\n,\n\nevery\n\ntiled convolution (\n\ngregor and lecun 2010a le et al. 2010\n\n) offers a compromise\nbetween a convolutional layer and a locally connected layer. rather than learning\na separate set of weights at\nspatial location, we learn a set of kernels that\nwe rotate through\u00a0as we move through space. this means that immediately\nneighboring locations will have different filters, like in a locally connected layer, but\nthe memory requirements for storing the parameters will increase only by a factor\nof the size of this set of kernels, rather than the size of the entire output feature\nmap. see fig.\nfor a comparison of locally connected layers, tiled convolution,\nand standard convolution.\n\n9.16\n\nto define tiled convolution algebraically, let k be a 6-d tensor, where two of\nthe dimensions correspond to different locations in the output map. rather than\nhaving a separate index for each location in the output map, output locations cycle\nthrough a set of t different choices of kernel stack in each direction. if t is equal to\n\n353\n\n "}, {"Page_number": 369, "text": "chapter 9. convolutional networks\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\nc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0d\n\ne\u00a0\u00a0\u00a0\u00a0\u00a0f\n\ng\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0h\n\n\u00a0i\u00a0\u00a0\n\nx 1x 1\n\ns1s1\n\nx 2x 2\n\ns2s2\n\nx 3x 3\n\ns3s3\n\nx 4x 4\n\ns4s4\n\nx 5x 5\n\ns5s5\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\ns1s1\n\nx 1x 1\n\ns2s2\n\nx 2x 2\n\ns3s3\n\nx 3x 3\n\ns4s4\n\nx 4x 4\n\ns5s5\n\nx 5x 5\n\nfigure 9.14: comparison of local connections, convolution, and full connections.\n(top) a locally connected layer with a patch size of two pixels. each edge is labeled with\na unique letter to show that each edge is associated with its own weight parameter.\n(center) a convolutional layer with a kernel width of two pixels. this model has exactly\nthe same connectivity as the locally connected layer. the difference lies not in which units\ninteract with each other, but in how the parameters are shared. the locally connected layer\nhas no parameter sharing. the convolutional layer uses the same two weights repeatedly\nacross the entire input, as indicated by the repetition of the letters labeling each edge.\n(bottom) a fully connected layer resembles a locally connected layer in the sense that\neach edge has its own parameter (there are too many to label explicitly with letters in this\ndiagram). however, it does not have the restricted connectivity of the locally connected\nlayer.\n\n354\n\n "}, {"Page_number": 370, "text": "chapter 9. convolutional networks\n\noutput\u00a0tensor\n\ninput\u00a0tensor\n\ns\ne\nt\na\nn\ni\nd\nr\no\no\nc\n\u00a0\nl\ne\nn\nn\na\nh\nc\n\nspatial\u00a0coordinates\n\nfigure 9.15:\u00a0a convolutional network with the first two output channels connected to\nonly the first two input channels, and the second two output channels connected to only\nthe second two input channels.\n\n355\n\n "}, {"Page_number": 371, "text": "chapter 9. convolutional networks\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\nc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0d\n\ne\u00a0\u00a0\u00a0\u00a0\u00a0f\n\ng\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0h\n\n\u00a0i\u00a0\u00a0\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\ns1s1\n\ns2s2\n\ns3s3\n\ns4s4\n\ns5s5\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\nc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0d\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\nc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0d\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nx 1x 1\n\ns1s1\n\nx 2x 2\n\ns2s2\n\nx 3x 3\n\ns3s3\n\nx 4x 4\n\ns4s4\n\nx 5x 5\n\ns5s5\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b\n\na\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nx 1x 1\n\nx 2x 2\n\nx 3x 3\n\nx 4x 4\n\nx 5x 5\n\nfigure 9.16: a comparison of locally connected layers, tiled convolution, and standard\nconvolution. all three have the same sets of connections between units, when the same\nsize of kernel is used. this diagram illustrates the use of a kernel that is two pixels wide.\nthe differences between the methods lies in how they share parameters. (top) a locally\nconnected layer has no sharing at all. we indicate that each connection has its own weight\nby labeling each connection with a unique letter.\ntiled convolution has a set of\nt different kernels. here we illustrate the case of t = 2.\u00a0one of these kernels has edges\nlabeled \u201ca\u201d and \u201cb,\u201d while the other has edges labeled \u201cc\u201d and \u201cd.\u201d each time we move one\npixel to the right in the output, we move on to using a different kernel. this means that,\nlike the locally connected layer, neighboring units in the output have different parameters.\nunlike the locally connected layer, after we have gone through all t available kernels,\nwe cycle back to the first kernel. if two output units are separated by a multiple of t\nsteps, then they share parameters.\ntraditional convolution is equivalent to tiled\nconvolution with t = 1. there is only one kernel and it is applied everywhere, as indicated\nin the diagram by using the kernel with weights labeled \u201ca\u201d and \u201cb\u201d everywhere.\n\n(bottom)\n\n(center)\n\n356\n\n "}, {"Page_number": 372, "text": "chapter 9. convolutional networks\n\nthe output width, this is the same as a locally connected layer.\n\nzi,j,k = xl,m,n\n\nvl,j m ,k n\n\n+ \u22121 + \u22121k\n\ni,l,m,n,j\n\n% +1 % +1 ,\n\n,k t\n\nt\n\n(9.10)\n\n%\n\nis\u00a0the modulo operation,\u00a0with\n\nwhere\nit is\nstraightforward to generalize this equation to use a different tiling range for each\ndimension.\n\nt%t = 0 (,\u00a0 t + 1)%t = 1,\u00a0etc.\n\nboth locally connected layers and tiled convolutional layers have an interesting\ninteraction with max-pooling: the detector units of these layers are driven by\ndifferent filters. if these filters learn to detect different transformed versions of\nthe same underlying features, then the max-pooled units become invariant to the\n9.9\nlearned transformation (see fig.\n). convolutional layers are hard-coded to be\ninvariant specifically to translation.\n\nother operations besides convolution are usually necessary to implement a\nconvolutional network. to perform learning, one must be able to compute the\ngradient with respect to the kernel, given the gradient with respect to the outputs.\nin some simple cases,\u00a0this operation can be performed using the convolution\noperation, but many cases of interest, including the case of stride greater than 1,\ndo not have this property.\n\nrecall that convolution is a linear operation and can thus be described as a\nmatrix multiplication (if we first reshape the input tensor into a flat vector). the\nmatrix involved is a function of the convolution kernel. the matrix is sparse and\neach element of the kernel is copied to several elements of the matrix. this view\nhelps us to derive some of the other operations needed to implement a convolutional\nnetwork.\n\nmultiplication by the transpose of the matrix defined by convolution is one\nsuch operation. this is the operation needed to back-propagate error derivatives\nthrough a convolutional layer, so it is needed to train convolutional networks\nthat have more than one hidden layer. this same operation is also needed if we\nwish to reconstruct the visible units from the hidden units (\n).\nreconstructing the visible units is an operation commonly used in the models\ndescribed in part\nof this book, such as autoencoders, rbms, and sparse coding.\ntranspose convolution is necessary to construct convolutional versions of those\nmodels. like the kernel gradient operation, this input gradient operation can be\nimplemented using a convolution in some cases, but in the general case requires\na third operation to be implemented. care must be taken to coordinate this\ntranspose operation with the forward propagation. the size of the output that the\ntranspose operation should return depends on the zero padding policy and stride of\n\nsimard et al. 1992\n\niii\n\n,\n\n357\n\n "}, {"Page_number": 373, "text": "chapter 9. convolutional networks\n\nthe forward propagation operation, as well as the size of the forward propagation\u2019s\noutput map. in some cases, multiple sizes of input to forward propagation can\nresult in the same size of output map, so the transpose operation must be explicitly\ntold what the size of the original input was.\n\nthese three operations\u2014convolution, backprop from output to weights, and\nbackprop from output to inputs\u2014are sufficient to compute all of the gradients\nneeded to train any depth of feedforward convolutional network, as well as to train\nconvolutional networks with reconstruction functions based on the transpose of\nconvolution.\u00a0see\n) for a full derivation of the equations in the\nfully general multi-dimensional, multi-example case. to give a sense of how these\nequations work, we present the two dimensional, single example version here.\n\ngoodfellow 2010\n\n(\n\n, s) as in eq.\n\nsuppose we want to train a convolutional network that incorporates strided\nconvolution of kernel stack k applied to multi-channel image v with stride s as\ndefined by c(k v,\n. suppose we want to minimize some loss function\n). during forward propagation, we will need to use c itself to output z ,\nj (v k,\nwhich is then propagated through the rest of the network and used to compute\nthe cost function j. during back-propagation, we will receive a tensor g such that\ngi,j,k = \u2202\n\n.\n(v k)\n\n9.8\n\nj\n\n,\n\n\u2202zi,j,k\n\nto train the network, we need to compute the derivatives with respect to the\n\nweights in the kernel. to do so, we can use a function\n\ng\n\n(g v )i,j,k,l =\n\n, s\n\n,\n\n\u2202\n\n\u2202ki,j,k,l\n\nj\n\n,(v k) =xm,n\n\ngi,m,nv\n\nj, m\n\n( \u2212 \u00d71) + ( \u2212 \u00d71) + .\n\ns k, n\n\ns l\n\n(9.11)\n\nif this layer is not the bottom layer of the network, we will need to compute\nthe gradient with respect to v in order to back-propagate the error farther down.\nto do so, we can use a function\n\nh ,\n\n(k g )i,j,k =\n\n, s\n\nj\n\n,(v k)\n\n(9.12)\n\n\u2202\n\n\u2202vi,j,k\n\n= xl,m\n\ns.t.\n\nl\u2212 \u00d7s m j\n(\n1) + =\n\nxn,p\n\ns.t.\n\nn\u2212 \u00d7s p k\n(\n1) + =\n\nkq,i,m,pg q,l,n.\n\n(9.13)\n\nxq\n\nautoencoder networks, described in chapter\n\n, are feedforward networks\ntrained to copy their input to their output. a simple example is the pca algorithm,\nthat copies its input x to an approximate reconstruction r using the function\nw> w x.\nit\u00a0is common for more\u00a0general autoencoders to use multiplication\nby the transpose of the weight matrix just as pca does.\u00a0to make such models\n\n14\n\n358\n\n "}, {"Page_number": 374, "text": "chapter 9. convolutional networks\n\nconvolutional, we can use the function h to perform the transpose of the convolution\noperation. suppose we have hidden units h in the same format as z and we define\na reconstruction\n\nr\n\n=  (h ,\n\nk h\n\n, s .)\n\n(9.14)\n\nin order to train the autoencoder, we will receive the gradient with respect\nto r as a tensor e. to train the decoder, we need to obtain the gradient with\n, s). to train the encoder, we need to obtain\nrespect to k. this is given by g (h e,\nthe gradient with respect to h. this is given by c (k e,\n, s). it is also possible to\ndifferentiate through g using c and h, but these operations are not needed for the\nback-propagation algorithm on any standard network architectures.\n\ngenerally, we do not use only a linear operation in order to transform from\nthe inputs to the outputs in a convolutional layer.\u00a0we generally also add some\nbias term to each output before applying the nonlinearity. this raises the question\nof how to share parameters among the biases.\u00a0for locally connected layers it is\nnatural to give each unit its own bias, and for tiled convolution, it is natural to\nshare the biases with the same tiling pattern as the kernels. for convolutional\nlayers, it is typical to have one bias per channel of the output and share it across\nall locations within each convolution map. however, if the input is of known, fixed\nsize, it is also possible to learn a separate bias at each location of the output map.\nseparating the biases may slightly reduce the statistical efficiency of the model, but\nalso allows the model to correct for differences in the image statistics at different\nlocations. for example, when using implicit zero padding, detector units at the\nedge of the image receive less total input and may need larger biases.\n\n9.6 structured outputs\n\nconvolutional networks can be used to output a high-dimensional, structured\nobject, rather than just predicting a class label for a classification task or a real\nvalue for a regression task. typically this object is just a tensor, emitted by a\nstandard convolutional layer. for example, the model might emit a tensor s, where\nsi,j,k is the probability that pixel (j, k) of the input to the network belongs to class\ni. this allows the model to label every pixel in an image and draw precise masks\nthat follow the outlines of individual objects.\n\none issue that often comes up is that the output plane can be smaller than the\ninput plane, as shown in fig.\n. in the kinds of architectures typically used for\nclassification of a single object in an image, the greatest reduction in the spatial\ndimensions of the network comes from using pooling layers with large stride. in\n\n9.13\n\n359\n\n "}, {"Page_number": 375, "text": "chapter 9. convolutional networks\n\n(1)\n\n(1)\u02c6y\n\u02c6y\n\n(2)\n\n(2)\u02c6y\n\u02c6y\n\n(3)\n\n(3)\u02c6y\n\u02c6y\n\nv\n\nw\n\nv\n\nw\n\nv\n\nh(1)h(1)\n\nh(2)h(2)\n\nh(3)h(3)\n\nu\n\nu\n\nu\n\nxx\n\nx\n\nfigure 9.17: an example of a recurrent convolutional network for pixel labeling. the\ninput is an image tensor\n, with axes corresponding to image rows, image columns, and\nchannels (red, green, blue). the goal is to output a tensor of labels \u02c6y , with a probability\ndistribution over labels for each pixel. this tensor has axes corresponding to image rows,\nimage columns, and the different classes. rather than outputting \u02c6y in a single shot, the\nrecurrent network iteratively refines its estimate \u02c6y by using a previous estimate of \u02c6y\nas input for creating a new estimate.\u00a0the same parameters are used for each updated\nestimate, and the estimate can be refined as many times as we wish. the tensor of\nconvolution kernels u is used on each step to compute the hidden representation given the\ninput image. the kernel tensor v is used to produce an estimate of the labels given the\nhidden values. on all but the first step, the kernels w are convolved over \u02c6y to provide\ninput to the hidden layer. on the first time step, this term is replaced by zero. because\nthe same parameters are used on each step, this is an example of a recurrent network, as\ndescribed in chapter\n\n.10\n\norder to produce an output map of similar size as the input, one can avoid pooling\naltogether (\n). another strategy is to simply emit a lower-resolution\ngrid of labels (\n). finally, in principle, one could\nuse a pooling operator with unit stride.\n\npinheiro and collobert 2014 2015\n\njain et al. 2007\n\n,\n\n,\n\n,\n\none strategy for pixel-wise labeling of images is to produce an initial guess\nof the image labels, then refine this initial guess using the interactions between\nneighboring pixels. repeating this refinement step several times corresponds to\nusing the same convolutions at each stage, sharing weights between the last layers\nof the deep net (\n). this makes the sequence of computations\nperformed by the successive convolutional layers with weights shared across layers\n,\na particular kind of recurrent network (\n). fig.\n9.17 shows the architecture of such a recurrent convolutional network.\n\npinheiro and collobert 2014 2015\n\njain et al. 2007\n\n,\n\n,\n\nonce a prediction for each pixel is made, various methods can be used to\nfurther process these predictions in order to obtain a segmentation of the image\n).\ninto regions (\n\nbriggman et al. 2009 turaga\n\n2010 farabet\n\net al.,\n\net al.,\n\n2013\n\n,\n\n;\n\n;\n\n360\n\n "}, {"Page_number": 376, "text": "chapter 9. convolutional networks\n\nthe general idea is to assume that large groups of contiguous pixels tend to be\nassociated with the same label. graphical models can describe the probabilistic\nrelationships between neighboring pixels. alternatively, the convolutional network\ncan be trained to maximize an approximation of the graphical model training\nobjective (\n\nning et al. 2005 thompson et al. 2014\n\n).\n\n,\n\n;\n\n,\n\n9.7 data types\n\nthe data used with a convolutional network usually consists of several channels,\neach channel being the observation of a different quantity at some point in space\nor time. see table\nfor examples of data types with different dimensionalities\nand number of channels.\n\n9.1\n\nfor an example of convolutional networks applied to video, see chen et al.\n\n(\n2010\n\n).\n\nso far we have discussed only the case where every example in the train and test\ndata has the same spatial dimensions. one advantage to convolutional networks\nis that they can also process inputs with varying spatial extents. these kinds of\ninput simply cannot be represented by traditional, matrix multiplication-based\nneural networks. this provides a compelling reason to use convolutional networks\neven when computational cost and overfitting are not significant issues.\n\nfor example, consider a collection of images, where each image has a different\nwidth and height. it is unclear how to model such inputs with a weight matrix of\nfixed size. convolution is straightforward to apply; the kernel is simply applied a\ndifferent number of times depending on the size of the input, and the output of the\nconvolution operation scales accordingly. convolution may be viewed as matrix\nmultiplication; the same convolution kernel induces a different size of doubly block\ncirculant matrix for each size of input.\u00a0sometimes the output of the network is\nallowed to have variable size as well as the input, for example if we want to assign\na class label to each pixel of the input. in this case, no further design work is\nnecessary. in other cases, the network must produce some fixed-size output, for\nexample if we want to assign a single class label to the entire image. in this case\nwe must make some additional design steps, like inserting a pooling layer whose\npooling regions scale in size proportional to the size of the input, in order to\nmaintain a fixed number of pooled outputs. some examples of this kind of strategy\nare shown in fig.\n\n.\n9.11\n\nnote that the use of convolution for processing variable sized inputs only makes\nsense for inputs that have variable size because they contain varying amounts\n\n361\n\n "}, {"Page_number": 377, "text": "chapter 9. convolutional networks\n\nmulti-channel\nskeleton animation data: anima-\ntions of 3-d computer-rendered\ncharacters are generated by alter-\ning the pose of a \u201cskeleton\u201d over\ntime. at each point in time, the\npose of the character is described\nby a specification of the angles of\neach of the joints in the charac-\nter\u2019s skeleton. each channel in\nthe data we feed to the convolu-\ntional model represents the angle\nabout one axis of one joint.\ncolor image data: one channel\ncontains the red pixels, one the\ngreen\u00a0pixels,\u00a0and\u00a0one\u00a0the blue\npixels. the\u00a0convolution kernel\nmoves over both the horizontal\nand vertical axes of the image,\nconferring translation equivari-\nance in both directions.\n\nsingle channel\n\n1-d audio\u00a0waveform: the\u00a0axis\u00a0we\nconvolve over corresponds\nto\ntime. we discretize\u00a0time and\nmeasure the amplitude of the\nwaveform once per time step.\n\n2-d audio data that has been prepro-\ncessed with a fourier transform:\nwe can transform the audio wave-\nform into a 2d tensor with dif-\nferent rows corresponding to dif-\nferent frequencies and different\ncolumns corresponding to differ-\nent points in time. using convolu-\ntion in the time makes the model\nequivariant to shifts in time. us-\ning convolution across the fre-\nquency axis makes the model\nequivariant to frequency, so that\nthe same melody played in a dif-\nferent octave produces the same\nrepresentation but at a different\nheight in the network\u2019s output.\n\n3-d volumetric data: a common\nsource of this kind of data is med-\nical imaging technology, such as\nct scans.\n\ncolor video data: one axis corre-\nsponds to time, one to the height\nof the video frame, and one to\nthe width of the video frame.\n\ntable 9.1: examples of different formats of data that can be used with convolutional\nnetworks.\n\n362\n\n "}, {"Page_number": 378, "text": "chapter 9. convolutional networks\n\nof observation of the same kind of thing\u2014different lengths of recordings over\ntime, different widths of observations over space, etc. convolution does not make\nsense if the input has variable size because it can optionally include different\nkinds of observations. for example, if we are processing college applications, and\nour features consist of both grades and standardized test scores, but not every\napplicant took the standardized test, then it does not make sense to convolve the\nsame weights over both the features corresponding to the grades and the features\ncorresponding to the test scores.\n\n9.8 efficient convolution algorithms\n\nmodern convolutional network applications often involve networks containing more\nthan one million units. powerful implementations exploiting parallel computation\nresources, as discussed in sec.\n, are essential. however, in many cases it is also\npossible to speed up convolution by selecting an appropriate convolution algorithm.\n\n12.1\n\nconvolution is equivalent to converting both the input and the kernel to the\nfrequency domain using a fourier transform, performing point-wise multiplication\nof the two signals,\u00a0and converting back to the time domain using an inverse\nfourier transform. for some problem sizes, this can be faster than the naive\nimplementation of discrete convolution.\n\nwhen a d-dimensional kernel can be expressed\u00a0as the outer product of d\nvectors, one vector per dimension, the kernel is called separable. when the kernel\nis separable, naive convolution is inefficient. it is equivalent to compose d one-\ndimensional convolutions with each of these vectors. the composed approach\nis significantly faster than performing one d-dimensional convolution with their\nouter product. the kernel also takes fewer parameters to represent as vectors.\nif the kernel is w elements wide in each dimension, then naive multidimensional\nconvolution requires o (w d) runtime and parameter storage space, while separable\nconvolution requires o(w d\u00d7 ) runtime and parameter storage space. of course,\nnot every convolution can be represented in this way.\n\ndevising faster ways of performing convolution or approximate convolution\nwithout harming the accuracy of the model is an active area of research. even tech-\nniques that improve the efficiency of only forward propagation are useful because\nin the commercial setting, it is typical to devote more resources to deployment of\na network than to its training.\n\n363\n\n "}, {"Page_number": 379, "text": "chapter 9. convolutional networks\n\n9.9 random or unsupervised features\n\ntypically, the most expensive part of convolutional network training is learning the\nfeatures. the output layer is usually relatively inexpensive due to the small number\nof features provided as input to this layer after passing through several layers of\npooling. when performing supervised training with gradient descent, every gradient\nstep requires a complete run of forward propagation and backward propagation\nthrough the entire network. one way to reduce the cost of convolutional network\ntraining is to use features that are not trained in a supervised fashion.\n\ncoates et al. 2011\n\nthere are three basic strategies for obtaining convolution kernels without\nsupervised training. one is to simply initialize them randomly. another is to\ndesign them by hand, for example by setting each kernel to detect edges at a\ncertain orientation or scale. finally, one can learn the kernels with an unsupervised\ncriterion. for example,\nk -means clustering to small\nimage patches, then use each learned centroid as a convolution kernel.\u00a0part iii\ndescribes many more unsupervised learning approaches. learning the features\nwith an unsupervised criterion allows them to be determined separately from the\nclassifier layer at the top of the architecture. one can then extract the features for\nthe entire training set just once, essentially constructing a new training set for the\nlast layer. learning the last layer is then typically a convex optimization problem,\nassuming the last layer is something like logistic regression or an svm.\n\n) apply\n\n(\n\n;\n\n;\n\net al.\n,\n\net al.\n,\n\n2011 pinto\n\n2011 cox and pinto 2011\n\nrandom filters often work surprisingly well in convolutional networks (jarrett\net al.\n,\net al.\n2009 saxe\n(\n) showed that layers consisting of convolution following by pooling naturally\n2011\nbecome frequency selective and translation invariant when assigned random weights.\nthey argue that this provides an inexpensive way to choose the architecture of\na convolutional network: first evaluate the performance of several convolutional\nnetwork architectures by training only the last layer, then take the best of these\narchitectures and train the entire architecture using a more expensive approach.\n\nsaxe\n\n).\n\n;\n\n,\n\nan intermediate approach is to learn the features, but using methods that do\nnot require full forward and back-propagation at every gradient step. as with\nmultilayer perceptrons, we use greedy layer-wise pretraining, to train the first layer\nin isolation, then extract all features from the first layer only once, then train the\nsecond layer in isolation given those features, and so on. chapter\nhas described\nhow to perform supervised greedy layer-wise pretraining, and part\nextends this\nto greedy layer-wise pretraining using an unsupervised criterion at each layer. the\ncanonical example of greedy layer-wise pretraining of a convolutional model is the\n). convolutional networks offer\nconvolutional deep belief network (\n\nlee et al. 2009\n\n8\niii\n\n,\n\n364\n\n "}, {"Page_number": 380, "text": "chapter 9. convolutional networks\n\n(\n\n) do with\n\ncoates et al. 2011\n\nus the opportunity to take the pretraining strategy one step further than is possible\nwith multilayer perceptrons. instead of training an entire convolutional layer at a\ntime, we can train a model of a small patch, as\nk-means.\nwe can then use the parameters from this patch-based model to define the kernels\nof a convolutional layer. this means that it is possible to use unsupervised learning\nto train a convolutional network without ever using convolution during the\ntraining process. using this approach, we can train very large models and incur a\nranzato et al. 2007b jarrett et al.\nhigh computational cost only at inference time (\n,\n2009 kavukcuoglu\net al.,\n). this approach was popular\n2013\nfrom roughly 2007\u20132013, when labeled datasets were small and computational\npower was more limited. today, most convolutional networks are trained in a\npurely supervised fashion, using full forward and back-propagation through the\nentire network on each training iteration.\n\n2010 coates\n\net al.,\n\n;\n\n;\n\n,\n\n;\n\nas with other approaches to unsupervised pretraining, it remains difficult to\ntease apart the cause of some of the benefits seen with this approach. unsupervised\npretraining may offer some regularization relative to supervised training, or it may\nsimply allow us to train much larger architectures due to the reduced computational\ncost of the learning rule.\n\n9.10 the neuroscientific basis for convolutional net-\n\nworks\n\nconvolutional networks are perhaps the\u00a0greatest success\u00a0story of biologically\ninspired artificial intelligence. though convolutional networks have been guided\nby many other fields, some of the key design principles of neural networks were\ndrawn from neuroscience.\n\n,\n\n,\n\n,\n\nthe history of convolutional networks begins with neuroscientific experiments\nlong before the relevant computational models were developed. neurophysiologists\ndavid hubel and torsten wiesel collaborated for several years to determine many\nof the most basic facts about how the mammalian vision system works (hubel and\nwiesel 1959 1962 1968\n). their accomplishments were eventually recognized with\na nobel prize. their findings that have had the greatest influence on contemporary\ndeep learning models were based on recording the activity of individual neurons in\ncats. they observed how neurons in the cat\u2019s brain responded to images projected\nin precise locations on a screen in front of the cat. their great discovery was\nthat neurons in the early visual system responded most strongly to very specific\npatterns of light, such as precisely oriented bars, but responded hardly at all to\nother patterns.\n\n365\n\n "}, {"Page_number": 381, "text": "chapter 9. convolutional networks\n\ntheir work helped to characterize many aspects of brain function that are\nbeyond the scope of this book. from the point of view of deep learning, we can\nfocus on a simplified, cartoon view of brain function.\n\nv1\n\nin this simplified view, we focus on a part of the brain called\n\n, also known as\nthe primary visual cortex. v1 is the first area of the brain that begins to perform\nsignificantly advanced processing of visual input. in this cartoon view, images are\nformed by light arriving in the eye and stimulating the retina, the light-sensitive\ntissue in the back of the eye. the neurons in the retina perform some simple\npreprocessing of the image but do not substantially alter the way it is represented.\nthe image then passes through the optic nerve and a brain region called the lateral\ngeniculate nucleus. the main role, as far as we are concerned here, of both of these\nanatomical regions is primarily just to carry the signal from the eye to v1, which\nis located at the back of the head.\n\na convolutional network layer is designed to capture three properties of v1:\n\n1. v1 is arranged in a spatial map. it actually has a two-dimensional structure\nmirroring\u00a0the structure\u00a0of\u00a0the image\u00a0in the\u00a0retina. for\u00a0example,\u00a0light\narriving at the lower half of the retina affects only the corresponding half of\nv1. convolutional networks capture this property by having their features\ndefined in terms of two dimensional maps.\n\n2. v1 contains many simple cells. a simple cell\u2019s activity can to some extent be\ncharacterized by a linear function of the image in a small, spatially localized\nreceptive field. the detector units of a convolutional network are designed\nto emulate these properties of simple cells.\n\n3. v1 also contains many complex cells. these cells respond to features that\nare similar to those detected by simple cells, but complex cells are invariant\nto small shifts in the position of the feature. this inspires the pooling units\nof convolutional networks. complex cells are also invariant to some changes\nin lighting that cannot be captured simply by pooling over spatial locations.\nthese invariances have inspired some of the cross-channel pooling strategies\n).\nin convolutional networks, such as maxout units (\n\ngoodfellow et al. 2013a\n\n,\n\nthough we know the most about v1, it is generally believed that the same\nbasic principles apply to other areas of the visual system. in our cartoon view of\nthe visual system, the basic strategy of detection followed by pooling is repeatedly\napplied as we move deeper into the brain. as we pass through multiple anatomical\nlayers of the brain, we eventually find cells that respond to some specific concept\nand are invariant to many transformations of the input. these cells have been\n\n366\n\n "}, {"Page_number": 382, "text": "chapter 9. convolutional networks\n\nnicknamed \u201cgrandmother cells\u201d\u2014the idea is that a person could have a neuron that\nactivates when seeing an image of their grandmother, regardless of whether she\nappears in the left or right side of the image, whether the image is a close-up of\nher face or zoomed out shot of her entire body, whether she is brightly lit, or in\nshadow, etc.\n\nthese grandmother cells have been shown to actually exist in the human brain,\nin a region called the medial temporal lobe (\n). researchers\ntested whether individual neurons would respond to photos of famous individuals.\nthey found what has come to be called the \u201challe berry neuron\u201d: an individual\nneuron that is activated by the concept of halle berry. this neuron fires when a\nperson sees a photo of halle berry, a drawing of halle berry, or even text containing\nthe words \u201challe berry.\u201d\u00a0of course, this has nothing to do with halle berry herself;\nother neurons responded to the presence of bill clinton, jennifer aniston, etc.\n\nquiroga et al. 2005\n\n,\n\nthese medial temporal lobe neurons are somewhat more general than modern\nconvolutional networks, which would not automatically generalize to identifying\na person or object when reading its name. the closest analog to a convolutional\nnetwork\u2019s last layer of features is a brain area called the inferotemporal cortex\n(it). when viewing an object, information flows from the retina, through the\nlgn, to v1, then onward to v2, then v4, then it. this happens within the first\n100ms of glimpsing an object.\u00a0if a person is allowed to continue looking at the\nobject for more time, then information will begin to flow backwards as the brain\nuses top-down feedback to update the activations in the lower level brain areas.\nhowever, if we interrupt the person\u2019s gaze, and observe only the firing rates that\nresult from the first 100ms of mostly feedforward activation, then it proves to be\nvery similar to a convolutional network. convolutional networks can predict it\nfiring rates, and also perform very similarly to (time limited) humans on object\nrecognition tasks (\n\ndicarlo 2013\n\n).\n\n,\n\nthat being said, there are many differences between convolutional networks\nand the mammalian vision system. some of these differences are well known\nto computational neuroscientists, but outside the scope of this book. some of\nthese differences are not yet known, because many basic questions about how the\nmammalian vision system works remain unanswered. as a brief list:\n\n\u2022 the human eye is mostly very low resolution, except for a tiny patch called the\nfovea. the fovea only observes an area about the size of a thumbnail held at\narms length. though we feel as if we can see an entire scene in high resolution,\nthis is an illusion created by the subconscious part of our brain, as it stitches\ntogether several glimpses of small areas. most convolutional networks actually\nreceive large full resolution photographs as input. the human brain makes\n\n367\n\n "}, {"Page_number": 383, "text": "chapter 9. convolutional networks\n\nseveral eye movements called saccades to glimpse the most visually salient or\ntask-relevant parts of a scene. incorporating similar attention mechanisms\ninto deep learning models is an active research direction. in the context of\ndeep learning, attention mechanisms have been most successful for natural\nlanguage processing, as described in sec.\n. several visual models\nwith foveation mechanisms have been developed but so far have not become\nthe dominant approach (larochelle and hinton 2010 denil\n\n12.4.5.1\n\net al.,\n\n2012\n\n).\n\n,\n\n;\n\n\u2022 the human visual system is integrated with many other senses, such as\nhearing, and factors like our moods and thoughts. convolutional networks\nso far are purely visual.\n\n\u2022 the human visual system does much more than just recognize objects. it is\nable to understand entire scenes including many objects and relationships\nbetween objects, and processes rich 3-d geometric information needed for\nour bodies to interface with the world. convolutional networks have been\napplied to some of these problems but these applications are in their infancy.\n\n\u2022 even simple brain areas like v1 are heavily impacted by feedback from higher\nlevels. feedback has been explored extensively in neural network models but\nhas not yet been shown to offer a compelling improvement.\n\n\u2022 while feedforward it firing rates capture much of the same information as\nconvolutional network features, it is not clear how similar the intermediate\ncomputations are. the brain probably uses very different activation and\npooling functions.\u00a0an individual neuron\u2019s activation probably is not well-\ncharacterized by a single linear filter response. a recent model of v1 involves\nmultiple quadratic filters for each neuron (\n). indeed our\ncartoon picture of \u201csimple cells\u201d\u00a0and \u201ccomplex cells\u201d\u00a0might create a non-\nexistent distinction; simple cells and complex cells might both be the same\nkind of cell but with their \u201cparameters\u201d enabling a continuum of behaviors\nranging from what we call \u201csimple\u201d to what we call \u201ccomplex.\u201d\n\nrust et al. 2005\n\n,\n\nit is\u00a0also worth mentioning that\u00a0neuroscience has\u00a0told us relatively little\nabout how to train convolutional networks. model structures with parameter\nsharing across multiple spatial locations date back to early connectionist models\nof vision (\n),\u00a0but these models did not use the modern\nback-propagation algorithm and gradient descent. for example, the neocognitron\n(fukushima 1980\n) incorporated most of the model architecture design elements of\nthe modern convolutional network but relied on a layer-wise unsupervised clustering\nalgorithm.\n\nmarr and poggio 1976\n\n,\n\n,\n\n368\n\n "}, {"Page_number": 384, "text": "chapter 9. convolutional networks\n\n(\n\nlang and hinton 1988\n\n) introduced the use of back-propagation to train time-\ndelay neural networks (tdnns). to use contemporary terminology, tdnns are\none-dimensional convolutional networks applied to time series. back-propagation\napplied to these models was not inspired by any neuroscientific observation and\nis considered by some to be biologically implausible. following the success of\nback-propagation-based training of tdnns, (\n) developed the\nmodern convolutional network by applying the same training algorithm to 2-d\nconvolution applied to images.\n\nlecun et al. 1989\n\n,\n\nso far we have described how simple cells are roughly linear and selective for\ncertain features, complex cells are more nonlinear and become invariant to some\ntransformations of these simple cell features, and stacks of layers that alternate\nbetween selectivity and invariance can yield grandmother cells for very specific\nphenomena. we have not yet described precisely what these individual cells detect.\nin a deep, nonlinear network, it can be difficult to understand the function of\nindividual cells. simple cells in the first layer are easier to analyze, because their\nresponses are driven by a linear function. in an artificial neural network, we can\njust display an image of the convolution kernel to see what the corresponding\nchannel of a convolutional layer responds to. in a biological neural network, we\ndo not have access to the weights themselves. instead, we put an electrode in the\nneuron itself, display several samples of white noise images in front of the animal\u2019s\nretina, and record how each of these samples causes the neuron to activate. we\ncan then fit a linear model to these responses in order to obtain an approximation\nof the neuron\u2019s weights. this approach is known as reverse correlation (ringach\nand shapley 2004\n\n).\n\n,\n\nreverse correlation shows us that most v1 cells have weights that are described\nby gabor functions. the gabor function describes the weight at a 2-d point in the\nimage. we can think of an image as being a function of 2-d coordinates, i (x, y).\nlikewise, we can think of a simple cell as sampling the image at a set of locations,\ndefined by a set of x coordinates x and a set of y coordinates, y, and applying\nweights that are also a function of the location, w(x, y). from this point of view,\nthe response of a simple cell to an image is given by\n\ns i( ) =xx\u2208xxy\u2208y\n\nw x, y i x, y .\n)\n\n) (\n\n(\n\nspecifically,\n\nw x, y\n\n(\n\n)\n\ntakes the form of a gabor function:\n\nw x, y \u03b1, \u03b2\n\n(\n\n;\n\nwhere\n\nx, \u03b2y, f, \u03c6, x0, y 0, \u03c4\n\n\u03b1\n\n) =  exp(cid:19)\u2212\u03b2xx02 \u2212 \u03b2y y02(cid:20) cos(f x0 + )\u03c6 ,\n\nx0 = (x\n\nx\u2212 0 ) cos( ) + (\ny\n\n\u03c4\n\ny\u2212 0) sin( )\u03c4\n\n369\n\n(9.15)\n\n(9.16)\n\n(9.17)\n\n "}, {"Page_number": 385, "text": "chapter 9. convolutional networks\n\nand\n\ny0 =  (\u2212 x\n\nx\u2212 0) sin( ) + (\ny\n\n\u03c4\n\ny\u2212 0) cos( )\u03c4 .\n\n(9.18)\n\nhere, \u03b1, \u03b2x, \u03b2y, f, \u03c6, x0, y0, and \u03c4 are parameters that control the properties\nshows some examples of gabor functions with\n\n9.18\n\nof the gabor function.\u00a0fig.\ndifferent settings of these parameters.\n\nthe parameters x0, y0, and \u03c4 define a coordinate system.\u00a0we translate and\nrotate x and y to form x0 and y0. specifically, the simple cell will respond to image\nfeatures centered at the point (x 0, y 0), and it will respond to changes in brightness\nas we move along a line rotated\n\nradians from the horizontal.\n\n\u03c4\n\nviewed as a function of x0 and y0 , the function w then responds to changes in\nbrightness as we move along the x0 axis.\u00a0it has two important factors: one is a\ngaussian function and the other is a cosine function.\n\nthe gaussian factor \u03b1 exp(cid:19)\u2212\u03b2 xx02 \u2212 \u03b2y y02(cid:20) can be seen as a gating term that\n\nensures the simple cell will only respond to values near where x 0 and y0 are both\nzero, in other words, near the center of the cell\u2019s receptive field. the scaling factor\n\u03b1 adjusts the total magnitude of the simple cell\u2019s response, while \u03b2x and \u03b2 y control\nhow quickly its receptive field falls off.\n\nthe cosine factor cos(f x0 + \u03c6) controls how the simple cell responds to changing\nbrightness along the x0 axis. the parameter f controls the frequency of the cosine\nand\n\ncontrols its phase offset.\n\n\u03c6\n\naltogether, this cartoon view of simple cells means that a simple cell responds\nto a specific spatial frequency of brightness in a specific direction at a specific\nlocation. simple cells are most excited when the wave of brightness in the image\nhas the same phase as the weights. this occurs when the image is bright where the\nweights are positive and dark where the weights are negative. simple cells are most\ninhibited when the wave of brightness is fully out of phase with the weights\u2014when\nthe image is dark where the weights are positive and bright where the weights are\nnegative.\n\nthe cartoon view of a complex cell is that it computes the l 2 norm of the\n\n2-d vector containing two simple cells\u2019 responses: c( i) =ps0( )i 2 + s1 ( )i 2.\u00a0an\n\nimportant special case occurs when s 1 has all of the same parameters as s0 except\nfor \u03c6, and \u03c6 is set such that s1 is one quarter cycle out of phase with s0 . in\nthis case, s0 and s1 form a quadrature pair. a complex cell defined in this way\nresponds when the gaussian reweighted image i(x, y) exp(\u2212\u03b2x x02 \u2212 \u03b2y y02 ) contains\na high amplitude sinusoidal wave with frequency f in direction \u03c4 near (x0 , y0),\nregardless of the phase offset of this wave. in other words, the complex cell\nis invariant to small translations of the image in direction \u03c4, or to negating the\n\n370\n\n "}, {"Page_number": 386, "text": "chapter 9. convolutional networks\n\nfigure 9.18: gabor functions with a variety of parameter settings. white indicates\nlarge positive weight, black indicates large negative weight, and the background gray\ncorresponds to zero weight. (left) gabor functions with different values of the parameters\nthat control the coordinate system: x 0, y0, and \u03c4 .\u00a0each gabor function in this grid is\nassigned a value of x0 and y 0 proportional to its position in its grid, and \u03c4 is chosen so\nthat each gabor filter is sensitive to the direction radiating out from the center of the grid.\nfor the other two plots, x0, y 0, and \u03c4 are fixed to zero.\ngabor functions with\ndifferent gaussian scale parameters \u03b2x and \u03b2y. gabor functions are arranged in increasing\nwidth (decreasing \u03b2x) as we move left to right through the grid, and increasing height\n(decreasing \u03b2y ) as we move top to bottom. for the other two plots, the \u03b2 values are fixed\nto 1.5\u00d7 the image width.\nf\nand \u03c6. as we move top to bottom, f increases, and as we move left to right, \u03c6 increases.\nfor the other two plots,\n\u03c6\n\ngabor functions with different sinusoid parameters\n\nthe image width.\n\nis fixed to 0 and\n\n(center)\n\n(right)\n\nf\n\nis fixed to 5\n\u00d7\n\nimage (replacing black with white and vice versa).\n\n(\n\nolshausen and field 1996\n\nsome of the most striking correspondences between neuroscience and machine\nlearning come from visually comparing the features learned by machine learning\nmodels with those employed by v1.\n) showed that\na simple unsupervised learning algorithm,\u00a0sparse coding,\u00a0learns features with\nreceptive fields similar to those of simple cells. since then, we have found that\nan extremely wide variety of statistical learning algorithms learn features with\ngabor-like functions when applied to natural images. this includes most deep\nlearning algorithms, which learn these features in their first layer. fig.\nshows\nsome examples. because so many different learning algorithms learn edge detectors,\nit is difficult to conclude that any specific learning algorithm is the \u201cright\u201d model\nof the brain just based on the features that it learns (though it can certainly be a\nbad sign if an algorithm does\nlearn some sort of edge detector when applied to\nnatural images). these features are an important part of the statistical structure\nof natural images and can be recovered by many different approaches to statistical\nmodeling. see hyv\u00e4rinen\n) for a review of the field of natural image\nstatistics.\n\net al. (\n\n2009\n\n9.19\n\nnot\n\n371\n\n "}, {"Page_number": 387, "text": "chapter 9. convolutional networks\n\nfigure 9.19: many machine learning algorithms learn features that detect edges or specific\ncolors of edges when applied to natural images. these feature detectors are reminiscent of\nthe gabor functions known to be present in primary visual cortex. (left) weights learned\nby an unsupervised learning algorithm (spike and slab sparse coding) applied to small\nimage patches.\nconvolution kernels learned by the first layer of a fully supervised\nconvolutional maxout network. neighboring pairs of filters drive the same maxout unit.\n\n(right)\n\n9.11 convolutional networks and the history of deep\n\nlearning\n\nconvolutional networks have played an important role in the history of deep\nlearning. they are a key example of a successful application of insights obtained\nby studying the brain to machine learning applications. they were also some of\nthe first deep models to perform well, long before arbitrary deep models were\nconsidered viable. convolutional networks were also some of the first neural\nnetworks to solve important commercial applications and remain at the forefront\nof commercial applications of deep learning today. for example, in the 1990s, the\nneural network research group at at&t developed a convolutional network for\nreading checks (\n). by the end of the 1990s, this system deployed\nby nec was reading over 10% of all the checks in the us. later, several ocr\nand handwriting recognition systems based on convolutional nets were deployed\nby microsoft (\nfor more details on such\napplications and more modern applications of convolutional networks. see lecun\net al. (\n\n) for a more in-depth history of convolutional networks up to 2010.\n\nsimard et al. 2003\n\nlecun et al. 1998b\n\n). see chapter\n\n2010\n\n12\n\n,\n\n,\n\nconvolutional networks were also used to win many contests. the current\nintensity of commercial interest in deep learning began when krizhevsky et al.\n(\n) won the imagenet object recognition challenge, but convolutional networks\n2012\n\n372\n\n "}, {"Page_number": 388, "text": "chapter 9. convolutional networks\n\nhad been used to win other machine learning and computer vision contests with\nless impact for years earlier.\n\nconvolutional nets were some of the first working deep networks trained with\nback-propagation. it is not entirely clear why convolutional networks succeeded\nwhen general back-propagation networks were considered to have failed. it may\nsimply be that convolutional networks were more computationally efficient than\nfully connected networks, so it was easier to run multiple experiments with them\nand tune their implementation and hyperparameters. larger networks also seem\nto be easier to train. with modern hardware, large fully connected networks\nappear to perform reasonably on many tasks, even when using datasets that were\navailable and activation functions that were popular during the times when fully\nconnected networks were believed not to work well. it may be that the primary\nbarriers to the success of neural networks were psychological (practitioners did\nnot expect neural networks to work, so they did not make a serious effort to use\nneural networks). whatever the case, it is fortunate that convolutional networks\nperformed well decades ago. in many ways, they carried the torch for the rest of\ndeep learning and paved the way to the acceptance of neural networks in general.\n\nconvolutional networks provide a way to specialize neural networks to work\nwith data that has a clear grid-structured topology and to scale such models to\nvery large size. this approach has been the most successful on a two-dimensional,\nimage topology. to process one-dimensional, sequential data, we turn next to\nanother powerful specialization of the neural networks framework: recurrent neural\nnetworks.\n\n373\n\n "}, {"Page_number": 389, "text": "chapter 10\n\nsequence modeling: recurrent\nand recursive nets\n\n,\n\nor\n\nrnns\n\n(\nrumelhart et al. 1986a\n\nrecurrent neural networks\n) are a family of\nneural networks for processing sequential data. much as a convolutional network\nis a neural network that is specialized for processing a grid of values x such as\nan image, a recurrent neural network is a neural network that is specialized for\nprocessing a sequence of values x(1), . . . , x( )\u03c4 . just as convolutional networks\ncan readily scale to images with large width and height, and some convolutional\nnetworks can process images of variable size, recurrent networks can scale to much\nlonger sequences than would be practical for networks without sequence-based\nspecialization. most recurrent networks can also process sequences of variable\nlength.\n\nto go from multi-layer networks to recurrent networks, we need to take advan-\ntage of one of the early ideas found in machine learning and statistical models of\nthe 1980s: sharing parameters across different parts of a model. parameter sharing\nmakes it possible to extend and apply the model to examples of different forms\n(different lengths, here) and generalize across them. if we had separate parameters\nfor each value of the time index, we could not generalize to sequence lengths not\nseen during training, nor share statistical strength across different sequence lengths\nand across different positions in time. such sharing is particularly important when\na specific piece of information can occur at multiple positions within the sequence.\nfor example, consider the two sentences \u201ci went to nepal in 2009\u201d and \u201cin 2009,\ni went to nepal.\u201d if we ask a machine learning model to read each sentence and\nextract the year in which the narrator went to nepal, we would like it to recognize\nthe year 2009 as the relevant piece of information, whether it appears in the sixth\n\n374\n\n "}, {"Page_number": 390, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nword or the second word of the sentence. suppose that we trained a feedforward\nnetwork that processes sentences of fixed length. a traditional fully connected\nfeedforward network would have separate parameters for each input feature, so it\nwould need to learn all of the rules of the language separately at each position in\nthe sentence. by comparison, a recurrent neural network shares the same weights\nacross several time steps.\n\n;\n\n;\n\n,\n\n1990\n\net al.,\n\net al.,\n\n1989 lang\n\na related idea is the use of convolution across a 1-d temporal sequence. this\nconvolutional approach is the basis for time-delay neural networks (lang and\nhinton 1988 waibel\n). the convolution operation\nallows a network to share parameters across time, but is shallow. the output\nof convolution is a sequence where each member of the output is a function of\na small number of neighboring members of the input. the idea of parameter\nsharing manifests in the application of the same convolution kernel at each time\nstep. recurrent networks share parameters in a different way. each member of the\noutput is a function of the previous members of the output. each member of the\noutput is produced using the same update rule applied to the previous outputs.\nthis recurrent formulation results in the sharing of parameters through a very\ndeep computational graph.\n\nfor the simplicity of exposition, we refer to rnns as operating on a sequence\nthat contains vectors x( )t with the time step index t ranging from to1\n\u03c4 . in\npractice, recurrent networks usually operate on minibatches of such sequences,\nwith a different sequence length \u03c4 for each member of the minibatch. we have\nomitted the minibatch indices to simplify notation. moreover, the time step index\nneed not literally refer to the passage of time in the real world, but only to the\nposition in the sequence. rnns may also be applied in two dimensions across\nspatial data such as images, and even when applied to data involving time, the\nnetwork may have connections that go backwards in time, provided that the entire\nsequence is observed before it is provided to the network.\n\nthis chapter extends the idea of a computational graph to include cycles. these\ncycles represent the influence of the present value of a variable on its own value\nat a future time step. such computational graphs allow us to define recurrent\nneural networks. we then describe many different ways to construct, train, and\nuse recurrent neural networks.\n\nfor more information on recurrent neural networks than is available in this\n\nchapter, we refer the reader to the textbook of graves 2012\n\n(\n\n).\n\n375\n\n "}, {"Page_number": 391, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n10.1 unfolding computational graphs\n\na computational graph is a way to formalize the structure of a set of computations,\nsuch as those involved in mapping inputs and parameters to outputs and loss.\nplease refer to sec.\n6.5.1\nfor a general introduction. in this section we explain\nthe idea of\na recursive or recurrent computation into a computational\nunfolding\ngraph that has a repetitive structure, typically corresponding to a chain of events.\nunfolding this graph results in the sharing of parameters across a deep network\nstructure.\n\nfor example, consider the classical form of a dynamical system:\n\ns( )t =  (f s(\n\n1)\n\nt\u2212 ; )\u03b8 ,\n\n(10.1)\n\nwhere s ( )t\n\nis called the state of the system.\n\neq.\u00a0\n\n10.1\n\nis recurrent because the definition of\n\ns at time t refers back to the\n\nsame definition at time\n\nt \u2212 1\n.\n\nfor a finite number of time steps \u03c4, the graph can be unfolded by applying the\n\u03c4 = 3 time steps, we\n\ndefinition \u03c4 \u2212 1 times. for example, if we unfold eq.\nobtain\n\n10.1\n\nfor\n\ns(3) = (f s(2); )\u03b8\n\n= ( (\n\nf f s(1); ); )\n\u03b8 \u03b8\n\n(10.2)\n\n(10.3)\n\nunfolding the equation by repeatedly applying the definition in this way has\nyielded an expression that does not involve recurrence. such an expression can\nnow be represented by a traditional directed acyclic computational graph.\u00a0the\nunfolded computational graph of eq.\n10.1\n.\n\nis illustrated in fig.\n\nand eq.\n\n10.1\n\n10.3\n\n)...\ns(\n)...s(\n\nff\n\ns(t(cid:129)1)\ns(t(cid:129)1)\n\nff\n\ns( )ts( )t\n\nff\n\nt\n\ns( +1)\nts( +1)\n\nff\n\n)...\ns(\n)...s(\n\nfigure 10.1: the classical dynamical system described by eq.\n, illustrated as an\nunfolded computational graph.\u00a0each node represents the state at some time t and the\nfunction f maps the state at t to the state at t + 1. the same parameters (the same value\nof\n\n) are used for all time steps.\n\nused to parametrize\n\n10.1\n\n\u03b8\n\nf\n\nas another example, let us consider a dynamical system driven by an external\n\nsignal x( )t ,\n\ns( )t =  (f s(\n\n1)\n\nt\u2212 , x( )t ; )\u03b8 ,\n\n(10.4)\n\n376\n\n "}, {"Page_number": 392, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nwhere we see that the state now contains information about the whole past sequence.\n\nrecurrent neural networks can be built in many different ways. much as\nalmost any function can be considered a feedforward neural network, essentially\nany function involving recurrence can be considered a recurrent neural network.\n\nmany recurrent neural networks use eq.\n\nor a similar equation to define\nthe values of their hidden units. to indicate that the state is the hidden units of\nthe network, we now rewrite eq.\nto represent the state:\n\nusing the variable\n\n10.5\n\n10.4\n\nh\n\nh( )t =  (f h(\n\n1)\n\nt\u2212 , x( )t ; )\u03b8 ,\n\n(10.5)\n\nillustrated in fig.\noutput layers that read information out of the state\n\n10.2\n\n, typical rnns will add extra architectural features such as\n\nh\n\nto make predictions.\n\n2)\n\n1)\n\nt\u2212 , x(\n\nwhen the recurrent network is trained to perform a task that requires predicting\nthe future from the past, the network typically learns to use h( )t as a kind of lossy\nsummary of the task-relevant aspects of the past sequence of inputs up to t. this\nsummary is in general necessarily lossy, since it maps an arbitrary length sequence\nt\u2212 , . . . , x(2), x(1)) to a fixed length vector h( )t . depending on the\n(x( )t , x(\ntraining criterion, this summary might selectively keep some aspects of the past\nsequence with more precision than other aspects.\u00a0for example, if the rnn is used\nin statistical language modeling, typically to predict the next word given previous\nwords, it may not be necessary to store all of the information in the input sequence\nup to time t, but rather only enough information to predict the rest of the sentence.\nthe most demanding situation is when we ask h( )t to be rich enough to allow\none to approximately recover the input sequence, as in autoencoder frameworks\n(chapter\n\n).14\n\nhh\n\nxx\n\nff\n\nunfold\n\n)...\nh(\n)...h(\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nh( )th( )t\n\nt\n\nh( +1)\nth( +1)\n\n)...\nh(\n)...h(\n\nff\n\nff\n\nff\n\nf\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\nx( )tx( )t\n\nt\n\nx( +1)\ntx( +1)\n\nfigure 10.2: a recurrent network with no outputs. this recurrent network just processes\ninformation from the input x by incorporating it into the state h that is passed forward\nthrough time. (left) circuit diagram. the black square indicates a delay of 1 time step.\n(right) the same network seen as an unfolded computational graph, where each node is\nnow associated with one particular time instance.\n\n10.5\n\neq.\u00a0\n\ncan be drawn in two different ways.\u00a0one way to draw the rnn is\nwith a diagram containing one node for every component that might exist in a\n\n377\n\n "}, {"Page_number": 393, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nphysical implementation of the model, such as a biological neural network. in\nthis view, the network defines a circuit that operates in real time, with physical\nparts whose current state can influence their future state, as in the left of fig.\n.\n10.2\nthroughout this chapter, we use a black square in a circuit diagram to indicate\nthat an interaction takes place with a delay of 1 time step, from the state at time\nt to the state at time t + 1.\u00a0the other way to draw the rnn is as an unfolded\ncomputational graph, in which each component is represented by many different\nvariables, with one variable per time step, representing the state of the component\nat that point in time. each variable for each time step is drawn as a separate node\nof the computational graph, as in the right of fig.\nis\nthe operation that maps a circuit as in the left side of the figure to a computational\ngraph with repeated pieces as in the right side. the unfolded graph now has a size\nthat depends on the sequence length.\n\n. what we call\n\nunfolding\n\n10.2\n\nwe can represent the unfolded recurrence after\n\nt\n\nsteps with a function\n\nh( )t =g( )t (x( )t , x(\n\n1)\n\nt\u2212 , x(\n\n2)\n\nt\u2212 , . . . , x(2), x (1))\n\n= (f h(\n\n1)\n\nt\u2212 , x( )t ; )\u03b8\n\ng( )t :\n\n(10.6)\n\n(10.7)\n\nt\u2212 , . . . , x(2), x(1))\nthe function g( )t takes the whole past sequence ( x( )t , x(\nas input and produces the current state, but the unfolded recurrent structure\nallows us to factorize g ( )t\ninto repeated application of a function f. the unfolding\nprocess thus introduces two major advantages:\n\nt\u2212 , x (\n\n1)\n\n2)\n\n1. regardless of the sequence length, the learned model always has the same\ninput size, because it is specified in terms of transition from one state to\nanother state, rather than specified in terms of a variable-length history of\nstates.\n\n2. it is possible to use the same transition function f with the same parameters\n\nat every time step.\n\nthese two factors make it possible to learn a single model f that operates on\nall time steps and all sequence lengths, rather than needing to learn a separate\nmodel g( )t\nfor all possible time steps. learning a single, shared model allows\ngeneralization to sequence lengths that did not appear in the training set, and\nallows the model to be estimated with far fewer training examples than would be\nrequired without parameter sharing.\n\nboth the recurrent graph and the unrolled graph have their uses. the recurrent\ngraph is succinct. the unfolded graph provides an explicit description of which\ncomputations to perform. the unfolded graph also helps to illustrate the idea of\n\n378\n\n "}, {"Page_number": 394, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ninformation flow forward in time (computing outputs and losses) and backward\nin time (computing gradients) by explicitly showing the path along which this\ninformation flows.\n\n10.2 recurrent neural networks\n\narmed with the graph unrolling and parameter sharing ideas of sec.\ndesign a wide variety of recurrent neural networks.\n\n10.1\n\n, we can\n\nyy\n\nll\n\noo\n\nvv\n\nhh\n\nuu\n\nxx\n\ny (t(cid:129)1)\ny (t(cid:129)1)\n\ny ( )ty ( )t\n\nt\n\ny ( +1)\nty ( +1)\n\nl(t(cid:129)1)\nl(t(cid:129)1)\n\nl( )tl( )t\n\nt\n\nl( +1)\ntl( +1)\n\no(t(cid:129)1)\no(t(cid:129)1)\n\no( )to( )t\n\nt\n\no( +1)\nto( +1)\n\nunfold\n\nww\n\nvv\n\nww\n\nww\n\nvv\n\nww\n\nvv\n\nww\n\n)...\nh(\n)...h(\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nh( )th( )t\n\nt\n\nh( +1)\nth( +1)\n\n)...\nh(\n)...h(\n\nuu\n\nuu\n\nuu\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\nx( )tx( )t\n\nt\n\nx( +1)\ntx( +1)\n\nfigure 10.3: the computational graph to compute the training loss of a recurrent network\nthat maps an input sequence of x values to a corresponding sequence of output o values.\na loss l measures how far each o is from the corresponding training target y. when using\nsoftmax outputs, we assume o is the unnormalized log probabilities. the loss l internally\ncomputes \u02c6y = softmax(o) and compares this to the target y. the rnn has input to hidden\nconnections parametrized by a weight matrix u, hidden-to-hidden recurrent connections\nparametrized by a weight matrix w , and hidden-to-output connections parametrized by\na weight matrix v . eq.\n(left) the rnn\nand its loss drawn with recurrent connections. (right) the same seen as an time-unfolded\ncomputational graph, where each node is now associated with one particular time instance.\n\ndefines forward propagation in this model.\n\n10.8\n\nsome examples of important design patterns for recurrent neural networks\n\ninclude the following:\n\n\u2022 recurrent networks that produce an output at each time step and have\n\n379\n\n "}, {"Page_number": 395, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nrecurrent connections between hidden units, illustrated in fig.\n\n.\n10.3\n\n\u2022 recurrent networks that produce an output at each time step and have\nrecurrent connections only from the output at one time step to the hidden\nunits at the next time step, illustrated in fig. 10.4\n\n\u2022 recurrent networks with recurrent connections between hidden units, that\nread an entire sequence and then produce a single output, illustrated in fig.\n10.5.\n\n10.3\n\nfig.\nmost of the chapter.\n\nis a reasonably representative example that we return to throughout\n\n,\n\n;\n\n,\n\n10.8\n\n10.3\n\nand eq.\n\nthe recurrent neural network of fig.\n\nis universal in the\nsense that any function computable by a turing machine can be computed by such\na recurrent network of a finite size. the output can be read from the rnn after\na number of time steps that is asymptotically linear in the number of time steps\nused by the turing machine and asymptotically linear in the length of the input\nsiegelmann and sontag 1991 siegelmann 1995 siegelmann and sontag 1995\n(\n;\nhyotyniemi 1996\n). the functions computable by a turing machine are discrete,\nso these results regard exact implementation of the function, not approximations.\nthe rnn, when used as a turing machine, takes a binary sequence as input and its\noutputs must be discretized to provide a binary output. it is possible to compute all\nfunctions in this setting using a single specific rnn of finite size (siegelmann and\nsontag 1995\n) use 886 units). the \u201cinput\u201d of the turing machine is a specification\nof the function to be computed, so the same network that simulates this turing\nmachine is sufficient for all problems. the theoretical rnn used for the proof\ncan simulate an unbounded stack by representing its activations and weights with\nrational numbers of unbounded precision.\n\n(\n\n,\n\n;\n\n,\n\n10.3\n\nwe now develop the forward propagation equations for the rnn depicted in\nfig.\n. the figure does not specify the choice of activation function for the\nhidden units.\u00a0here we assume the hyperbolic tangent activation function. also,\nthe figure does not specify exactly what form the output and loss function take.\nhere we assume that the output is discrete, as if the rnn is used to predict words\nor characters. a natural way to represent discrete variables is to regard the output\no as giving the unnormalized log probabilities of each possible value of the discrete\nvariable. we can then apply the softmax operation as a post-processing step to\nobtain a vector \u02c6y of normalized probabilities over the output. forward propagation\nbegins with a specification of the initial state h(0) . then, for each time step from\nt\n\n= 1 to =  , we apply the following update equations:\nt\u2212 + u x( )t\n\n+b w h(\n\na( )t =\n\n(10.8)\n\n1)\n\n\u03c4\n\nt\n\n380\n\n "}, {"Page_number": 396, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nyy\n\nll\n\nv\n\noo\n\nhh\n\nu\n\nxx\n\ny(t(cid:129)1)\ny(t(cid:129)1)\n\ny ( )ty ( )t\n\nt\n\ny( +1)\nty( +1)\n\nl(t(cid:129)1)\nl(t(cid:129)1)\n\nl( )tl( )t\n\nt\n\nl( +1)\ntl( +1)\n\n)...\no(\n)...o(\n\no(t(cid:129)1)\no(t(cid:129)1)\n\no( )to( )t\n\nt\n\no( +1)\nto( +1)\n\nw\n\nw\n\nw\n\nw\n\nv\n\nv\n\nv\n\nw\n\nunfold\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nh( )th( )t\n\nt\n\nh( +1)\nth( +1)\n\n)...\nh(\n)...h(\n\nu\n\nu\n\nu\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\nx( )tx( )t\n\nt\n\nx( +1)\ntx( +1)\n\n10.3\n\nfigure 10.4: an rnn whose only recurrence is the feedback connection from the output\nto the hidden layer. at each time step t , the input is xt, the hidden layer activations are\nh( )t , the outputs are o( )t , the targets are y( )t and the loss is l( )t . (left) circuit diagram.\n(right) unfolded computational graph. such an rnn is less powerful (can express a\nsmaller set of functions) than those in the family represented by fig.\n. the rnn\nin fig.\ncan choose to put any information it wants about the past into its hidden\nrepresentation h and transmit h to the future. the rnn in this figure is trained to\nput a specific output value into o, and o is the only information it is allowed to send\nto the future. there are no direct connections from h going forward. the previous h\nis connected to the present only indirectly, via the predictions it was used to produce.\nunless o is very high-dimensional and rich, it will usually lack important information\nfrom the past. this makes the rnn in this figure less powerful, but it may be easier to\ntrain because each time step can be trained in isolation from the others, allowing greater\nparallelization during training, as described in sec.\n\n10.2.1\n.\n\n10.3\n\n381\n\n "}, {"Page_number": 397, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nh( )t = tanh(a( )t )\no( )t =\n+c v h( )t\n\u02c6y( )t = softmax(o( )t )\n\n(10.9)\n(10.10)\n\n(10.11)\n\nwhere the parameters are the bias vectors b and c along with the weight matrices\nu , v and w , respectively for input-to-hidden, hidden-to-output and hidden-to-\nhidden connections. this is an example of a recurrent network that maps an\ninput sequence to an output sequence of the same length. the total loss for a\nvalues would then be just\ngiven sequence of\nthe sum of the losses over all the time steps. for example, if l( )t\nis the negative\nlog-likelihood of y( )t given x(1), . . . , x( )t , then\n\nvalues paired with a sequence of\n\nx\n\ny\n\n(10.12)\n\n(10.13)\n\n(10.14)\n\nl( )t\n\nl(cid:22){x(1), . . . , x( )\u03c4 } {, y(1), . . . , y ( )\u03c4 }(cid:23)\n=xt\n| {x(1), . . . , x ( )t }(cid:23),\n= \u2212xt\n| {x(1), . . . , x( )t }(cid:20) is given by reading the entry for y ( )t\n\nlog pmodel(cid:22)y( )t\n\nwhere pmodel(cid:19)y ( )t\n\n10.3\n\nfrom the\nmodel\u2019s output vector \u02c6y ( )t . computing the gradient of this loss function with\nrespect to the parameters is an expensive operation. the gradient computation\ninvolves performing a forward propagation pass moving left to right through our\nillustration of the unrolled graph in fig.\n, followed by a backward propagation\npass moving right to left through the graph. the runtime is o(\u03c4) and cannot be\nreduced by parallelization because the forward propagation graph is inherently\nsequential; each time step may only be computed after the previous one. states\ncomputed in the forward pass must be stored until they are reused during the\nbackward pass, so the memory cost is also o(\u03c4 ). the back-propagation algorithm\napplied to the unrolled graph with o(\u03c4) cost is called back-propagation through\ntime\n. the network with recurrence\nbetween hidden units is thus very powerful but also expensive to train. is there an\nalternative?\n\nand is discussed further sec.\n\nbptt\n\n10.2.2\n\nor\n\n10.2.1 teacher forcing and networks with output recurrence\n\nthe network with recurrent connections only from the output at one time step to\nthe hidden units at the next time step (shown in fig.\n) is strictly less powerful\nbecause it lacks hidden-to-hidden recurrent connections. for example, it cannot\nsimulate a universal turing machine. because this network lacks hidden-to-hidden\n\n10.4\n\n382\n\n "}, {"Page_number": 398, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nrecurrence, it requires that the output units capture all of the information about\nthe past that the network will use to predict the future. because the output units\nare explicitly trained to match the training set targets, they are unlikely to capture\nthe necessary information about the past history of the input, unless the user\nknows how to describe the full state of the system and provides it as part of the\ntraining set targets. the advantage of eliminating hidden-to-hidden recurrence\nis that, for any loss function based on comparing the prediction at time t to the\ntraining target at time t, all the time steps are decoupled. training can thus be\nparallelized, with the gradient for each step t computed in isolation. there is no\nneed to compute the output for the previous time step first, because the training\nset provides the ideal value of that output.\n\nl( )\u2327l( )\u2327\n\ny( )\u2327y( )\u2327\n\no( )\u2327o( )\u2327\n\nv\n\n. . .\n. . .\n\nw\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nw\n\nh( )th( )t\n\nw\n\n. . .\n. . .\n\nw\n\nh( )\u2327h( )\u2327\n\nu\n\nu\n\nu\n\nu\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\nx( )tx( )t\n\n)...\nx(\n)...x(\n\nx( )\u2327x( )\u2327\n\nfigure 10.5: time-unfolded recurrent neural network with a single output at the end\nof the sequence. such a network can be used to summarize a sequence and produce a\nfixed-size representation used as input for further processing.\u00a0there might be a target\nright at the end (as depicted here) or the gradient on the output o( )t can be obtained by\nback-propagating from further downstream modules.\n\nmodels that have recurrent connections from their outputs leading back into\nthe model may be trained with teacher forcing. teacher forcing is a procedure\nthat emerges from the maximum likelihood criterion, in which during training the\nmodel receives the ground truth output y( )t as input at time t + 1.\u00a0we can see\nthis by examining a sequence with two time steps. the conditional maximum\nlikelihood criterion is\n\nlog p(cid:22)y (1), y(2) | x(1), x (2)(cid:23)\n\n383\n\n(10.15)\n\n "}, {"Page_number": 399, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ny(t(cid:129)1)\ny(t(cid:129)1)\n\ny( )ty( )t\n\nl(t(cid:129)1)\nl(t(cid:129)1)\n\nl( )tl( )t\n\nw\n\no(t(cid:129)1)\no(t(cid:129)1)\n\nv\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nu\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\no( )to( )t\n\nv\n\nh( )th( )t\n\nu\n\nx( )tx( )t\n\no(t(cid:129)1)\no(t(cid:129)1)\n\nw\n\nv\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\no( )to( )t\n\nv\n\nh( )th( )t\n\nu\n\nu\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\nx( )tx( )t\n\ntrain\u00a0time\n\ntest\u00a0time\n\nfigure 10.6:\nillustration of teacher forcing. teacher forcing is a training technique that is\napplicable to rnns that have connections from their output to their hidden states at the\nnext time step. (left)\ny( )t drawn from the train\nat train time, we feed the\nset as input to h( +1)\n. (right) when the model is deployed, the true output is generally\nnot known. in this case, we approximate the correct output y( )t with the model\u2019s output\no ( )t , and feed the output back into the model.\n\ncorrect output\n\nt\n\n384\n\n "}, {"Page_number": 400, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n= log p(cid:22)y (2) | y (1), x(1), x (2)(cid:23) + log p(cid:22)y (1) | x (1), x(2)(cid:23)\n\n(10.16)\n\nin this example, we see that at time t = 2, the model is trained to maximize the\nconditional probability of y(2) given both the x sequence so far and the previous y\nvalue from the training set. maximum likelihood thus specifies that during training,\nrather than feeding the model\u2019s own output back into itself, these connections\nshould be fed with the target values specifying what the correct output should be.\nthis is illustrated in fig.\n\n.\n10.6\n\nwe originally motivated teacher forcing as allowing us to avoid back-propagation\nthrough time in models that lack hidden-to-hidden connections. teacher forcing\nmay still be applied to models that have hidden-to-hidden connections so long as\nthey have connections from the output at one time step to values computed in the\nnext time step. however, as soon as the hidden units become a function of earlier\ntime steps, the bptt algorithm is necessary. some models may thus be trained\nwith both teacher forcing and bptt.\n\nthe disadvantage of strict teacher forcing arises if the network is going to be\nlater used in an open-loop mode, with the network outputs (or samples from the\noutput distribution) fed back as input. in this case, the kind of inputs that the\nnetwork sees during training could be quite different from the kind of inputs that\nit will see at test time.\u00a0one way to mitigate this problem is to train with both\nteacher-forced inputs and with free-running inputs, for example by predicting the\ncorrect target a number of steps in the future through the unfolded recurrent\noutput-to-input paths. in this way, the network can learn to take into account\ninput conditions (such as those it generates itself in the free-running mode) not\nseen during training and how to map the state back towards one that will make\nthe network generate proper outputs after a few steps. another approach (bengio\net al.,\n) to mitigate the gap between the inputs seen at train time and the\ninputs seen at test time randomly chooses to use generated values or actual data\nvalues as input. this approach exploits a curriculum learning strategy to gradually\nuse more of the generated values as input.\n\n2015b\n\n10.2.2 computing the gradient in a recurrent neural network\n\ncomputing the gradient through a recurrent neural network is straightforward.\none simply applies the generalized back-propagation algorithm of sec.\nto the\nunrolled computational graph. no specialized algorithms are necessary. the use\nof back-propagation on the unrolled graph is called the back-propagation through\ntime (bptt) algorithm. gradients obtained by back-propagation may then be\nused with any general-purpose gradient-based techniques to train an rnn.\n\n6.5.6\n\n385\n\n "}, {"Page_number": 401, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n10.8\n\nand eq.\n\nto gain some intuition for how the bptt algorithm behaves, we provide an\nexample of how to compute gradients by bptt for the rnn equations above\n(eq.\n). the nodes of our computational graph include the\nparameters u, v , w , b and c as well as the sequence of nodes indexed by t for\nx( )t , h( )t , o ( )t and l( )t . for each node n we need to compute the gradient \u2207 n l\nrecursively, based on the gradient computed at nodes that follow it in the graph.\nwe start the recursion with the nodes immediately preceding the final loss\n\n10.12\n\n\u2202l\n\u2202l( )t\n\n= 1.\n\n(10.17)\n\nin this derivation we assume that the outputs o( )t are used as the argument to the\nsoftmax function to obtain the vector \u02c6y of probabilities over the output. we also\nassume that the loss is the negative log-likelihood of the true target y( )t given the\ninput so far. the gradient \u2207 o( )t l on the outputs at time step t, for all i, t , is as\nfollows:\n\n(\u2207o( )t l)i =\n\n\u2202l\n\u2202o( )t\ni\n\n=\n\n\u2202l\n\u2202l( )t\n\n\u2202l( )t\n\u2202o( )t\ni\n\n= \u02c6y ( )t\n\ni \u2212 1 i,y ( )t .\n\n(10.18)\n\nwe work our way backwards, starting from the end of the sequence. at the final\ntime step ,\u03c4 h ( )\u03c4 only has o( )\u03c4 as a descendent, so its gradient is simple:\n\n\u2207h ( )\u03c4 l = (\u2207o( )\u03c4 l)\n\n\u2202o( )\u03c4\n\u2202h( )\u03c4\n\n= (\u2207o ( )\u03c4 l\n\n) v\n\n.\n\n(10.19)\n\nwe can then iterate backwards in time to back-propagate gradients through time,\nfrom t = \u03c4 \u2212 1 down to t = 1, noting that h( )t (for t < \u03c4) has as descendents both\no( )t and h( +1)\n\n. its gradient is thus given by\n\nt\n\nt\n\n\u2202h( +1)\n\u2202h( )t\n\n+ (\u2207o( )t l)\n\n\u2202o( )t\n\u2202h( )t\n\nt l)\n\n\u2207h ( )t l = (\u2207h( +1)\n= (\u2207h( +1)\n\nt l) diag(cid:24)1 \u2212(cid:22)h( +1)\n(cid:20) 2(cid:23) indicates the diagonal matrix containing the elements\n\n(cid:23)2(cid:25)w + (\u2207 o( )t l) v\n\n(10.21)\n\n)2 . this is the jacobian of the hyperbolic tangent associated with the\n\nt\n\n(10.20)\n\nwhere diag(cid:22)1 \u2212(cid:19)h( +1)\n1 \u2212 (h( +1)\n\nt\n\nt\n\ni\n\nhidden unit\n\ni\n\nat time\n\n.\nt + 1\n\nonce the gradients on the internal nodes of the computational graph are ob-\ntained, we can obtain the gradients on the parameter nodes, which have descendents\nat all the time steps:\n\n\u2207cl = xt\n\n(\u2207o( )t l)\n\n\u2202o( )t\n\u2202c\n\n=xt\n\n\u2207o( )t l\n\n386\n\n "}, {"Page_number": 402, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n\u2207bl = xt\n\u2207 v l = xt\n\u2207wl = xt\n\u2207 ul = xt\n\n(\u2207h ( )t l)\n\n(\u2207o( )t l)\n\n(\u2207h ( )t l)\n\n(\u2207h ( )t l)\n\n\u2202h ( )t\n\u2202b\n\n\u2202o( )t\n\u2202v\n\n\u2202h ( )t\n\u2202w\n\n\u2202h ( )t\n\u2202u\n\n=xt\n=xt\n=xt\n=xt\n\n(\u2207h( )t l) diag(cid:24)1 \u2212(cid:22)h( )t (cid:23)2(cid:25)\n(\u2207o( )t l) h( )t >\n(\u2207h( )t l) diag(cid:24)1 \u2212(cid:22)h( )t (cid:23)2(cid:25) h(\n(\u2207h( )t l) diag(cid:24)1 \u2212(cid:22)h( )t (cid:23)2(cid:25) x( )t >\n\nt\u2212 >\n1)\n\nwe do not need to compute the gradient with respect to x( )t\nfor training because\nit does not have any parameters as ancestors in the computational graph defining\nthe loss.\n\n\u2202w or \u2202h( )t\n\nwe are abusing notation somewhat in the above equations. we correctly use\n\u2207h ( )t l to indicate the full influence of h( )t through all paths from h( )t to l. this\nis in contrast to our usage of \u2202h( )t\n\u2202b , which we use here in an unconventional\nmanner. by \u2202h ( )t\n\u2202w we refer to the effect of w on h( )t only via the use of w at time\nstep t. this is not standard calculus notation, because the standard definition of\nthe jacobian would actually include the complete influence of w on h( )t via its\nt\u2212 . what we refer to here is\nuse in all of the preceding time steps to produce h(\n, that computes the contribution of a single\nin fact the bprop method of sec.\nedge in the computational graph to the gradient.\n\n6.5.6\n\n1)\n\n10.2.3 recurrent networks as directed graphical models\n\nin the example recurrent network we have developed so far, the losses l( )t were\ncross-entropies between training targets y( )t and outputs o( )t . as with a feedforward\nnetwork, it is in principle possible to use almost any loss with a recurrent network.\nthe loss should be chosen based on the task. as with a feedforward network, we\nusually wish to interpret the output of the rnn as a probability distribution, and\nwe usually use the cross-entropy associated with that distribution to define the loss.\nmean squared error is the cross-entropy loss associated with an output distribution\nthat is a unit gaussian, for example, just as with a feedforward network.\n\nwhen we use a predictive log-likelihood training objective, such as eq.\n\n, we\ntrain the rnn to estimate the conditional distribution of the next sequence element\ny( )t given the past inputs. this may mean that we maximize the log-likelihood\n\n10.12\n\nlog (p y( )t\n\n| x(1) , . . . , x( )t ),\n\n387\n\n(10.22)\n\n "}, {"Page_number": 403, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nor, if the model includes connections from the output at one time step to the next\ntime step,\n\nlog (p y( )t\n\n| x(1) , . . . , x( )t , y(1) , . . . , y(\n\n1)\n\nt\u2212 ).\n\n(10.23)\n\ndecomposing the joint probability over the sequence of y values as a series of\none-step probabilistic predictions is one way to capture the full joint distribution\nacross the whole sequence. when we do not feed past y values as inputs that\ncondition the next step prediction, the directed graphical model contains no edges\nfrom any y( )i\nin the past to the current y( )t . in this case, the outputs y are\nconditionally independent given the sequence of x values. when we do feed the\nactual y values (not their prediction, but the actual observed or generated values)\nback into the network, the directed graphical model contains edges from all y( )i\nvalues in the past to the current y ( )t value.\n\ny (1)y (1)\n\ny(2)y(2)\n\ny (3)y (3)\n\ny (4)y (4)\n\ny(5)y(5)\n\n)...\ny (\n)...y (\n\nfigure 10.7: fully connected graphical model for a sequence y(1), y (2), . . . , y ( )t , . . . : every\npast observation y( )i may influence the conditional distribution of some y ( )t (for t > i),\ngiven the previous values. parametrizing the graphical model directly according to this\ngraph (as in eq.\n) might be very inefficient, with an ever growing number of inputs\nand parameters for each element of the sequence. rnns obtain the same full connectivity\nbut efficient parametrization, as illustrated in fig.\n\n10.6\n\n10.8\n\n.\n\nas a simple example, let us consider the case where the rnn models only a\nsequence of scalar random variables y = {y(1), . . . , y ( )\u03c4 }, with no additional inputs\nx. the input at time step t is simply the output at time step t\u2212 1. the rnn then\ndefines a directed graphical model over the y variables. we parametrize the joint\ndistribution of these observations using the chain rule (eq.\n) for conditional\nprobabilities:\n\n3.6\n\np\n\n( ) = \ny\n\np\n\n(y(1) , . . . , y( )\u03c4 ) =\n\n\u03c4yt=1\n\np (y( )t | y (\n\n1)\n\nt\u2212 , y (\n\n2)\n\nt\u2212 , . . . , y(1))\n\n(10.24)\n\n388\n\n "}, {"Page_number": 404, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nwhere the right-hand side of the bar is empty for t = 1, of course. hence the\nnegative log-likelihood of a set of values {y(1) , . . . , y ( )\u03c4 } according to such a model\nis\n(10.25)\n\nl( )t\n\nwhere\n\nl( )t = \n\u2212\n\nlog (\n\np y( )t = y ( )t\n\n1)\n\nt\u2212 , y(\n\n2)\n\nt\u2212 , . . . , y (1)).\n\n(10.26)\n\nl =xt\n| y(\n\nh(1)h(1)\n\nh(2)h(2)\n\nh(3)h(3)\n\nh(4)h(4)\n\nh(5)h(5)\n\n)...\nh(\n)...h(\n\ny(1)y(1)\n\ny(2)y(2)\n\ny (3)y (3)\n\ny (4)y (4)\n\ny(5)y(5)\n\n)...\ny (\n)...y (\n\nfigure 10.8: introducing the state variable in the graphical model of the rnn, even\nthough it is a deterministic function of its inputs, helps to see how we can obtain a very\nh ( )t and\nefficient parametrization, based on eq.\ny( )t ) involves the same structure (the same number of inputs for each node) and can share\nthe same parameters with the other stages.\n\n.\u00a0every stage in the sequence (for\n\n10.5\n\n)\n\n1)\n\nt k\u2212 , . . . , y(\n\nthe edges in a graphical model indicate which variables depend directly on other\nvariables. many graphical models aim to achieve statistical and computational\nefficiency by omitting edges that do not correspond to strong interactions. for\nexample, it is common to make the markov assumption that the graphical model\nt\u2212 } to y( )t , rather than containing\nshould only contain edges from {y(\nedges from the entire past history. however, in some cases, we believe that all past\ninputs should have an influence on the next element of the sequence. rnns are\nuseful when we believe that the distribution over y( )t may depend on a value of y( )i\nt\u2212 .\n1)\nfrom the distant past in a way that is not captured by the effect of y( )i on y(\none way to interpret an rnn as a graphical model is to view the rnn as\ndefining a graphical model whose structure is the complete graph, able to represent\ndirect dependencies between any pair of y values. the graphical model over the\ny values with the complete graph structure is shown in fig.\n. the complete\ngraph interpretation of the rnn is based on ignoring the hidden units h( )t by\nmarginalizing them out of the model.\n\n10.7\n\nit is more interesting to consider the graphical model structure of rnns that\nresults from regarding the hidden units h( )t as random variables.1 including the\n\n1the conditional distribution over these variables given their parents is deterministic. this is\n\n389\n\n "}, {"Page_number": 405, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nhidden units in the graphical model reveals that the rnn provides a very efficient\nparametrization of the joint distribution over the observations. suppose that we\nrepresented an arbitrary joint distribution over discrete values with a tabular\nrepresentation\u2014an array containing a separate entry for each possible assignment\nof values, with the value of that entry giving the probability of that assignment\noccurring.\u00a0if y can take on k different values, the tabular representation would\nhave o(k\u03c4 ) parameters.\u00a0by comparison, due to parameter sharing, the number\nof parameters in the rnn is o(1) as a function of sequence length. the number\nof parameters in the rnn may be adjusted to control model capacity but is not\nforced to scale with sequence length. eq.\nshows that the rnn parametrizes\nlong-term relationships between variables efficiently, using recurrent applications\nof the same function f and same parameters \u03b8 at each time step. fig. 10.8\nillustrates the graphical model interpretation. incorporating the h( )t nodes in\nthe graphical model decouples the past and the future, acting as an intermediate\nquantity between them. a variable y ( )i\nin the distant past may influence a variable\ny( )t via its effect on h. the structure of this graph shows that the model can be\nefficiently parametrized by using the same conditional probability distributions at\neach time step, and that when the variables are all observed, the probability of the\njoint assignment of all variables can be evaluated efficiently.\n\n10.5\n\neven with the efficient parametrization of the graphical model, some operations\nremain computationally challenging. for example, it is difficult to predict missing\nvalues in the middle of the sequence.\n\nthe price recurrent networks pay for their reduced number of parameters is\n\nthat\n\noptimizing\n\nthe parameters may be difficult.\n\nthe parameter sharing used in recurrent networks relies on the assumption\nthat the same parameters can be used for different time steps. equivalently, the\nassumption is that the conditional probability distribution over the variables at\ntime t + 1 given the variables at time t is\n, meaning that the relationship\nbetween the previous time step and the next time step does not depend on t. in\nprinciple, it would be possible to use t as an extra input at each time step and let\nthe learner discover any time-dependence while sharing as much as it can between\ndifferent time steps. this would already be much better than using a different\nconditional probability distribution for each t, but the network would then have to\nextrapolate when faced with new values of\n\nstationary\n\n.t\n\nto complete our view of an rnn as a graphical model, we must describe how\nto draw samples from the model. the main operation that we need to perform is\n\nperfectly legitimate, though it is somewhat rare to design a graphical model with such deterministic\nhidden units.\n\n390\n\n "}, {"Page_number": 406, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nsimply to sample from the conditional distribution at each time step.\u00a0however,\nthere is one additional complication. the rnn must have some mechanism for\ndetermining the length of the sequence. this can be achieved in various ways.\n\nin the case when the output is a symbol taken from a vocabulary, one can\nadd a special symbol corresponding to the end of a sequence (schmidhuber 2012\n).\nwhen that symbol is generated, the sampling process stops. in the training set,\nwe insert this symbol as an extra member of the sequence, immediately after x( )\u03c4\nin each training example.\n\n,\n\nanother option is to introduce an extra bernoulli output to the model that\nrepresents the decision to either continue generation or halt generation at each\ntime step. this approach is more general than the approach of adding an extra\nsymbol to the vocabulary, because it may be applied to any rnn, rather than\nonly rnns that output a sequence of symbols. for example, it may be applied to\nan rnn that emits a sequence of real numbers. the new output unit is usually a\nsigmoid unit trained with the cross-entropy loss. in this approach the sigmoid is\ntrained to maximize the log-probability of the correct prediction as to whether the\nsequence ends or continues at each time step.\n\nanother way to determine the sequence length \u03c4 is to add an extra output to\nthe model that predicts the integer \u03c4 itself. the model can sample a value of \u03c4\nand then sample \u03c4 steps worth of data. this approach requires adding an extra\ninput to the recurrent update at each time step so that the recurrent update is\naware of whether it is near the end of the generated sequence. this extra input\nt\u2212 , the number of remaining\ncan either consist of the value of \u03c4 or can consist of \u03c4\ntime steps. without this extra input, the rnn might generate sequences that\nend abruptly, such as a sentence that ends before it is complete. this approach is\nbased on the decomposition\n\np (x(1), . . . , x ( )\u03c4 ) =  ( ) (\n\np \u03c4 p x(1), . . . , x( )\u03c4\n\n| \u03c4 .)\n\n(10.27)\n\nthe strategy of predicting \u03c4 directly is used for example by goodfellow et al.\n(\n2014d\n\n).\n\n10.2.4 modeling sequences conditioned on context with rnns\n\nin the previous section we described how an rnn could correspond to a directed\ngraphical model over a sequence of random variables y( )t with no inputs x. of\ncourse, our development of rnns as in eq.\nincluded a sequence of inputs\nx(1), x(2), . . . , x( )\u03c4 . in general, rnns allow the extension of the graphical model\nview to represent not only a joint distribution over the y variables but also a\n\n10.8\n\n391\n\n "}, {"Page_number": 407, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n6.2.1.1\n\n, any model representing a variable\n\nconditional distribution over y given x. as discussed in the context of feedforward\np (y; \u03b8) can be reinter-\nnetworks in sec.\npreted as a model representing a conditional distribution p(y \u03c9|\n) with \u03c9 = \u03b8. we\ncan extend such a model to represent a distribution p (y x|\n) by using the same\np( y \u03c9|\n) as before, but making \u03c9 a function of x.\u00a0in the case of an rnn, this\ncan be achieved in different ways. we review here the most common and obvious\nchoices.\n\npreviously, we have discussed rnns that take a sequence of vectors x ( )t\n\nfor\nt = 1, . . . , \u03c4 as input.\u00a0another option is to take only a single vector x as input.\nwhen x is a fixed-size vector, we can simply make it an extra input of the rnn\nthat generates the y sequence. some common ways of providing an extra input to\nan rnn are:\n\n1.\u00a0as an extra input at each time step, or\n\n2.\u00a0as the initial state h(0) , or\n\n3.\u00a0both.\n\n10.9\n\nthe first and most common approach is illustrated in fig.\n. the interaction\nbetween the input x and each hidden unit vector h( )t\nis parametrized by a newly\nintroduced weight matrix r that was absent from the model of only the sequence\nof y values.\u00a0the same product x> r is added as additional input to the hidden\nunits at every time step. we can think of the choice of x as determining the value\nof x>r that is effectively a new bias parameter used for each of the hidden units.\nthe weights remain independent of the input. we can think of this model as taking\nthe parameters \u03b8 of the non-conditional model and turning them into \u03c9, where\nthe bias parameters within\n\nare now a function of the input.\n\n\u03c9\n\nrather than receiving only a single vector x as input, the rnn may receive a\ncorresponds to a\n| x(1) , . . . , x( )\u03c4 ) that makes a conditional\n\nsequence of vectors x( )t as input. the rnn described in eq.\nconditional distribution p (y(1), . . . , y( )\u03c4\nindependence assumption that this distribution factorizes as\n\n10.8\n\nyt\n\np (y( )t\n\n| x(1), . . . , x( )t ).\n\n(10.28)\n\nto remove the conditional independence assumption, we can add connections from\nthe output at time t to the hidden unit at time t + 1, as shown in fig.\n. the\nmodel can then represent arbitrary probability distributions over the y sequence.\nthis kind of model representing a distribution over a sequence given another\nsequence still has one restriction, which is that the length of both sequences must\nbe the same. we describe how to remove this restriction in sec.\n\n10.10\n\n10.4\n.\n\n392\n\n "}, {"Page_number": 408, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ny (t(cid:129)1)\ny (t(cid:129)1)\n\ny( )ty( )t\n\nt\n\ny( +1)\nty( +1)\n\n)...\ny (\n)...y (\n\nu\n\nl(t(cid:129)1)\nl(t(cid:129)1)\n\nu\n\nl( )tl( )t\n\nu\n\nt\n\nl( +1)\ntl( +1)\n\no(t(cid:129)1)\no(t(cid:129)1)\n\no( )to( )t\n\nt\n\no( +1)\nto( +1)\n\nv\n\nv\n\nv\n\nw\n\n)...\ns(\n)...s(\n\nw\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nw\n\nh( )th( )t\n\nw\n\nt\n\nh( +1)\nth( +1)\n\n)...\nh(\n)...h(\n\nr\n\nr\n\nr r\n\nr\n\nxx\n\nfigure 10.9: an rnn that maps a fixed-length vector x into a distribution over sequences\ny. this rnn is appropriate for tasks such as image captioning, where a single image is\nused as input to a model that then produces a sequence of words describing the image.\neach element y( )t of the observed output sequence serves both as input (for the current\ntime step) and, during training, as target (for the previous time step).\n\n393\n\n "}, {"Page_number": 409, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ny(t(cid:129)1)\ny(t(cid:129)1)\n\ny( )ty( )t\n\nt\n\ny ( +1)\nty ( +1)\n\nl(t(cid:129)1)\nl(t(cid:129)1)\n\nl( )tl( )t\n\nt\n\nl( +1)\ntl( +1)\n\nr\n\nr\n\nr\n\no(t(cid:129)1)\no(t(cid:129)1)\n\nv\n\nw\n\nw\n\n)...\nh(\n)...h(\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nu\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\no( )to( )t\n\nv\n\nh( )th( )t\n\nu\n\nx( )tx( )t\n\nt\n\no( +1)\nto( +1)\n\nv\n\nw\n\nw\n\nt\n\nh( +1)\nth( +1)\n\nu\n\nt\n\nx( +1)\ntx( +1)\n\n)...\nh(\n)...h(\n\n10.3\n\nfigure 10.10: a conditional recurrent neural network mapping a variable-length sequence\nof x values into a distribution over sequences of y values of the same length. compared\nto fig.\n, this rnn contains connections from the previous output to the current state.\nthese connections allow this rnn to model an arbitrary distribution over sequences of y\ngiven sequences of\nis only able to represent\ndistributions in which the y values are conditionally independent from each other given\nthe\n\nof the same length. the rnn of fig.\n\nvalues.\n\n10.3\n\nx\n\nx\n\n394\n\n "}, {"Page_number": 410, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ny(t(cid:129)1)\ny(t(cid:129)1)\n\ny( )ty( )t\n\nt\n\ny( +1)\nty( +1)\n\nl(t(cid:129)1)\nl(t(cid:129)1)\n\nl( )tl( )t\n\nt\n\nl( +1)\ntl( +1)\n\no(t(cid:129)1)\no(t(cid:129)1)\n\no( )to( )t\n\nt\n\no( +1)\nto( +1)\n\ng (t(cid:129)1)\ng (t(cid:129)1)\n\ng ( )tg ( )t\n\nt\n\ng ( +1)\ntg ( +1)\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nh( )th( )t\n\nt\n\nh( +1)\nth( +1)\n\nx(t(cid:129)1)\nx(t(cid:129)1)\n\nx( )tx( )t\n\nt\n\nx( +1)\ntx( +1)\n\nfigure 10.11: computation of a typical bidirectional recurrent neural network, meant\nto learn to map input sequences x to target sequences y, with loss l( )t at each step t.\nthe h recurrence propagates information forward in time (towards the right) while the\ng recurrence propagates information backward in time (towards the left). thus at each\npoint t, the output units o( )t can benefit from a relevant summary of the past in its h( )t\ninput and from a relevant summary of the future in its g ( )t\n\ninput.\n\n395\n\n "}, {"Page_number": 411, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n10.3 bidirectional rnns\n\nall of the recurrent networks we have considered up to now have a \u201ccausal\u201d struc-\nture, meaning that the state at time t only captures information from the past,\nt\u2212 , and the present input x( )t . some of the models we have discussed\nx(1), . . . , x(\nalso allow information from past y values to affect the current state when the y\nvalues are available.\n\n1)\n\nhowever, in many applications we want to output a prediction of y ( )t which\nmay depend on the whole input sequence. for example, in speech recognition,\nthe correct interpretation of the current sound as a phoneme may depend on the\nnext few phonemes because of co-articulation and potentially may even depend on\nthe next few words because of the linguistic dependencies between nearby words: if\nthere are two interpretations of the current word that are both acoustically plausible,\nwe may have to look far into the future (and the past) to disambiguate them.\nthis is also true of handwriting recognition and many other sequence-to-sequence\nlearning tasks, described in the next section.\n\nbidirectional recurrent neural networks (or bidirectional rnns) were invented\n). they have been extremely suc-\n) in applications where that need arises, such as handwriting\n), speech recogni-\net al.,\nbaldi\n\nto address that need (schuster and paliwal 1997\ncessful (graves 2012\nrecognition (graves\ntion (graves and schmidhuber 2005 graves\net al.,\n\n2008 graves and schmidhuber 2009\n\n) and bioinformatics (\n\net al.,\n\n1999\n\n2013\n\n).\n\n,\n\n;\n\n,\n\n,\n\n,\n\n;\n\nas the name suggests, bidirectional rnns combine an rnn that moves forward\nthrough time beginning from the start of the sequence with another rnn that\nmoves backward through time beginning from the end of the sequence. fig. 10.11\nillustrates the typical bidirectional rnn, with h( )t standing for the state of the\nsub-rnn that moves forward through time and g( )t standing for the state of the\nsub-rnn that moves backward through time. this allows the output units o( )t to\ncompute a representation that depends on both the past and the future but\nis most sensitive to the input values around time t, without having to specify a\nfixed-size window around t (as one would have to do with a feedforward network,\na convolutional network, or a regular rnn with a fixed-size look-ahead buffer).\n\nthis idea can be naturally extended to 2-dimensional input, such as images,\nby having four rnns, each one going in one of the four directions: up, down,\nleft, right. at each point (i, j) of a 2-d grid, an output oi,j could then compute a\nrepresentation that would capture mostly local information but could also depend\non\u00a0long-range inputs, if\u00a0the rnn\u00a0is\u00a0able to\u00a0learn to\u00a0carry that information.\ncompared to a convolutional network, rnns applied to images are typically more\n\n396\n\n "}, {"Page_number": 412, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nvisin et al. 2015 kalchbrenner\n\nexpensive but allow for long-range lateral interactions between features in the\nsame feature map (\n). indeed, the\nforward propagation equations for such rnns may be written in a form that shows\nthey use a convolution that computes the bottom-up input to each layer, prior\nto the recurrent propagation across the feature map that incorporates the lateral\ninteractions.\n\net al.,\n\n2015\n\n,\n\n;\n\n10.4 encoder-decoder sequence-to-sequence architec-\n\ntures\n\n10.5\n\nwe have seen in fig.\nvector. we have seen in fig.\nsequence. we have seen in fig.\nrnn can map an input sequence to an output sequence of the same length.\n\nhow an rnn can map an input sequence to a fixed-size\nhow an rnn can map a fixed-size vector to a\nhow an\n\nand fig.\n\n, fig.\n\n, fig.\n\n10.10\n\n10.11\n\n10.9\n\n10.4\n\n10.3\n\nhere we discuss how an rnn can be trained to map an input sequence to an\noutput sequence which is not necessarily of the same length.\u00a0this comes up in\nmany applications, such as speech recognition, machine translation or question\nanswering, where the input and output sequences in the training set are generally\nnot of the same length (although their lengths might be related).\n\nwe often call the input to the rnn the \u201ccontext.\u201d we want to produce a\nrepresentation of this context, c . the context c might be a vector or sequence of\nvectors that summarize the input sequence x x= ( (1), . . . , x(nx )).\n\n(\n\n2014\n\net al. (\n\ncho et al. 2014a\n\nthe simplest rnn architecture for mapping a variable-length sequence to an-\n) and shortly\nother variable-length sequence was first proposed by\nafter by sutskever\n), who independently developed that architecture and\nwere the first to obtain state-of-the-art translation using this approach. the former\nsystem is based on scoring proposals generated by another machine translation\nsystem, while the latter uses a standalone recurrent network to generate the trans-\nlations. these authors respectively called this architecture, illustrated in fig.\n10.12\n,\nthe encoder-decoder or sequence-to-sequence architecture. the idea is very simple:\n(1) an encoder\nrnn processes the input sequence. the encoder\nemits the context c, usually as a simple function of its final hidden state. (2) a\ndecoder\nrnn is conditioned on that fixed-length vector (just like\ny = (y(1), . . . , y (ny )). the innovation\nin fig.\nof this kind of architecture over those presented in earlier sections of this chapter is\nthat the lengths nx and ny can vary from each other, while previous architectures\nconstrained nx = ny = \u03c4 . in a sequence-to-sequence architecture, the two rnns\n\nwriter\n) to generate the output sequence\n\nor\n10.9\n\noutput\n\nreader\n\ninput\n\nor\n\nor\n\nor\n\n397\n\n "}, {"Page_number": 413, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nencoder\n\n\u2026\n\nx(1)x(1)\n\nx(2)x(2)\n\n)...\nx(\n)...x(\n\nx(n x)\nx(n x)\n\ncc\n\ndecoder\n\n\u2026\n\ny (1)y (1)\n\ny (2)y (2)\n\n)...\ny (\n)...y (\n\ny(n y)\ny(n y)\n\nfigure 10.12: example of an encoder-decoder or sequence-to-sequence rnn architecture,\nfor learning to generate an output sequence (y(1) , . . . , y(n y)) given an input sequence\n(x(1) , x(2) , . . . , x(nx)). it is composed of an encoder rnn that reads the input sequence\nand a decoder rnn that generates the output sequence (or computes the probability of a\ngiven output sequence). the final hidden state of the encoder rnn is used to compute a\ngenerally fixed-size context variable c which represents a semantic summary of the input\nsequence and is given as input to the decoder rnn.\n\n398\n\n "}, {"Page_number": 414, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nare trained jointly to maximize the average of log p (y (1) , . . . , y(n y) | x(1), . . . , x(nx))\nover all the pairs of x and y sequences in the training set. the last state hnx of\nthe encoder rnn is typically used as a representation c of the input sequence\nthat is provided as input to the decoder rnn.\n\nif the context c is a vector, then the decoder rnn is simply a vector-to-\nsequence rnn as described in sec.\n. as we have seen, there are at least two\nways for a vector-to-sequence rnn to receive input. the input can be provided as\nthe initial state of the rnn, or the input can be connected to the hidden units at\neach time step. these two ways can also be combined.\n\n10.2.4\n\nthere is no constraint that the encoder must have the same size of hidden layer\n\nas the decoder.\n\none clear limitation of this architecture is when the context c output by the\nencoder rnn has a dimension that is too small to properly summarize a long\nsequence. this phenomenon was observed by\n) in the context\nof machine translation. they proposed to make c a variable-length sequence rather\nthan a fixed-size vector.\u00a0additionally, they introduced an attention mechanism\nthat learns to associate elements of the sequence c to elements of the output\nsequence. see sec.\n\nbahdanau et al. 2015\n\nfor more details.\n\n12.4.5.1\n\n(\n\n10.5 deep recurrent networks\n\nthe computation in most rnns can be decomposed into three blocks of parameters\nand associated transformations:\n\n1.\u00a0from the input to the hidden state,\n\n2.\u00a0from the previous hidden state to the next hidden state, and\n\n3.\u00a0from the hidden state to the output.\n\n10.3\n\nwith the rnn architecture of fig.\n, each of these three blocks is associated\nwith a single weight matrix. in other words, when the network is unfolded, each\nof these corresponds to a shallow transformation.\u00a0by a shallow transformation,\nwe mean a transformation that would be represented by a single layer within\na deep mlp. typically this is a transformation represented by a learned affine\ntransformation followed by a fixed nonlinearity.\n\nwould it be advantageous to introduce depth in each of these operations?\nexperimental evidence (graves\n) strongly suggests\nso. the experimental evidence is in agreement with the idea that we need enough\n\n2013 pascanu\n\n2014a\n\net al.,\n\net al.,\n\n;\n\n399\n\n "}, {"Page_number": 415, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ny\n\nh\n\nx\n\n(b)\n\ny\n\nh\n\nx\n\n(c)\n\ny\n\nz\n\nh\n\nx\n\n(a)\n\n2014a\n\n(a)\n(b)\n\nfigure 10.13: a recurrent neural network can be made deep in many ways (pascanu\n).\net al.,\nthe hidden recurrent state can be broken down into groups organized\nhierarchically.\ndeeper computation (e.g., an mlp) can be introduced in the input-to-\nhidden, hidden-to-hidden and hidden-to-output parts.\u00a0this may lengthen the shortest\npath linking different time steps.\nthe path-lengthening effect can be mitigated by\nintroducing skip connections.\n\n(c)\n\n400\n\n "}, {"Page_number": 416, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ndepth in order to perform the required mappings. see also schmidhuber 1992\nel hihi and bengio 1996\n\n) for earlier work on deep rnns.\n\njaeger 2007a\n\n), or\n\n(\n\n(\n\n(\n\n),\n\n2013\n\n2014a\n\net al. (\n\ngraves\n\net al. (\n\n10.13\n10.13\n\n) were the first to show a significant benefit of decomposing\n(left). we can think\nthe state of an rnn into multiple layers as in fig.\nof the lower layers in the hierarchy depicted in fig.\u00a0\na as playing a role in\ntransforming the raw input into a representation that is more appropriate, at\nthe higher levels of the hidden state. pascanu\n) go a step further\nand propose to have a separate mlp (possibly deep) for each of the three blocks\nenumerated above, as illustrated in fig.\nb. considerations of representational\ncapacity suggest to allocate enough capacity in each of these three steps, but doing\nso by adding depth may hurt learning by making optimization difficult. in general,\nit is easier to optimize shallower architectures, and adding the extra depth of\nfig.\nt to a variable\nin time step t + 1 become longer.\u00a0for example, if an mlp with a single hidden\nlayer is used for the state-to-state transition, we have doubled the length of the\nshortest path between variables in any two different time steps, compared with the\nordinary rnn of fig.\n), this\ncan be mitigated by introducing skip connections in the hidden-to-hidden path, as\nillustrated in fig.\n\nb makes the shortest path from a variable in time step\n\n.\u00a0however, as argued by\n\npascanu\n\net al. (\n\n2014a\n\n10.13\n\n10.13\n\n10.13\n\n10.3\n\nc.\n\n10.6 recursive neural networks\n\nrecursive neural networks2 represent yet another generalization of recurrent net-\nworks, with a different kind of computational graph, which is structured as a deep\ntree, rather than the chain-like structure of rnns. the typical computational\ngraph for a recursive network is illustrated in fig.\n. recursive neural networks\nwere introduced by pollack 1990\n) and their potential use for learning to reason\nwas described by by\n). recursive networks have been successfully\napplied to processing data structures as input to neural nets (frasconi et al.,\n1997 1998\n) as well\nas in computer vision (\n\n), in natural language processing (\n).\n\nsocher et al. 2011b\n\n2011a c 2013a\n\nbottou 2011\n\nsocher\n\net al.,\n\n10.14\n\n, ,\n\n(\n\n(\n\n,\n\n,\n\none clear advantage of recursive nets over recurrent nets is that for a sequence\nof the same length \u03c4 , the depth (measured as the number of compositions of\nnonlinear operations) can be drastically reduced from \u03c4 to o( log \u03c4), which might\nhelp deal with long-term dependencies. an open question is how to best structure\nthe tree. one option is to have a tree structure which does not depend on the data,\n\n2we suggest to not abbreviate \u201crecursive neural network\u201d as \u201crnn\u201d to avoid confusion with\n\n\u201crecurrent neural network.\u201d\n\n401\n\n "}, {"Page_number": 417, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nll\n\noo\n\nyy\n\nu\n\nw\n\nu w\n\nu w\n\nv\n\nv\n\nv\n\nv\n\nx(1)x(1)\n\nx(2)x(2)\n\nx(3)x(3)\n\nx(4)x(4)\n\nfigure 10.14: a recursive network has a computational graph that generalizes that of the\nrecurrent network from a chain to a tree. a variable-size sequence x (1) , x(2), . . . , x( )t can\nbe mapped to a fixed-size representation (the output o), with a fixed set of parameters\n(the weight matrices u, v , w ). the figure illustrates a supervised learning case in which\nsome target\n\nis provided which is associated with the whole sequence.\n\ny\n\n402\n\n "}, {"Page_number": 418, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nsuch as a balanced binary tree. in some application domains, external methods\ncan suggest the appropriate tree structure. for example, when processing natural\nlanguage sentences, the tree structure for the recursive network can be fixed to\nthe structure of the parse tree of the sentence provided by a natural language\nparser (\n).\u00a0ideally, one would like the learner itself to\ndiscover and infer the tree structure that is appropriate for any given input, as\nsuggested by\n\nsocher et al. 2011a 2013a\n\nbottou 2011\n\n).\n\n(\n\n,\n\n,\n\n1997\n\n1998\n\n) and\n\net al. (\n\nfrasconi\n\nmany variants of the recursive net idea are possible. for example, frasconi\net al. (\n) associate the data with a tree structure,\nand associate the inputs and targets with individual\u00a0nodes of the\u00a0tree. the\ncomputation performed by each node does not have to be the traditional artificial\nneuron computation (affine transformation of all inputs followed by a monotone\nnonlinearity). for example,\n) propose using tensor operations\nand bilinear forms, which have previously been found useful to model relationships\nbetween concepts (weston\n) when the concepts are\nrepresented by continuous vectors (embeddings).\n\nsocher et al. 2013a\n\n2010 bordes\n\net al.,\n\net al.,\n\n2012\n\n(\n\n;\n\n10.7 the challenge of long-term dependencies\n\n8.2.5\n\nthe mathematical challenge of learning long-term dependencies in recurrent net-\nworks was introduced in sec.\n. the basic problem is that gradients propagated\nover many stages tend to either vanish (most of the time) or explode (rarely, but\nwith much damage to the optimization). even if we assume that the parameters are\nsuch that the recurrent network is stable (can store memories, with gradients not\nexploding), the difficulty with long-term dependencies arises from the exponentially\nsmaller weights given to long-term interactions (involving the multiplication of\nmany jacobians) compared to short-term ones. many other sources provide a\ndeeper treatment (\net al.,\n2013a) . in this section, we describe the problem in more detail. the remaining\nsections describe approaches to overcoming the problem.\n\nhochreiter 1991 doya 1993 bengio\n\n1994 pascanu\n\net al.,\n\n,\n\n;\n\n,\n\n;\n\n;\n\nrecurrent networks involve the composition of the same function multiple\ntimes, once per time step. these compositions can result in extremely nonlinear\nbehavior, as illustrated in fig.\n\n10.15\n.\n\nin particular, the function composition employed by recurrent neural networks\nsomewhat resembles matrix multiplication. we can think of the recurrence relation\n\nh( )t = w >h(\n\nt\u2212\n\n1)\n\n(10.29)\n\nas a very simple recurrent neural network lacking a nonlinear activation function,\n\n403\n\n "}, {"Page_number": 419, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nt\nu\np\nt\nu\no\n\u00a0\nf\no\n\u00a0\nn\no\ni\nt\nc\ne\nj\no\nr\np\n\n4\n3\n2\n1\n0\n\u22121\n\u22122\n\u22123\n\u22124\n\n\u221260\n\nrepeated\u00a0function\u00a0composition\n\n0\n1\n2\n3\n4\n5\n\n\u221240\n\n\u221220\n\n0\n\ninput\u00a0coordinate\n\n20\n\n40\n\n60\n\nfigure 10.15: when composing many nonlinear functions (like the linear-tanh layer shown\nhere), the result is highly nonlinear, typically with most of the values associated with a tiny\nderivative, some values with a large derivative, and many alternations between increasing\nand decreasing. in this plot, we plot a linear projection of a 100-dimensional hidden state\ndown to a single dimension, plotted on the y-axis.\u00a0the x -axis is the coordinate of the\ninitial state along a random direction in the 100-dimensional space. we can thus view this\nplot as a linear cross-section of a high-dimensional function. the plots show the function\nafter each time step, or equivalently, after each number of times the transition function\nhas been composed.\n\n404\n\n "}, {"Page_number": 420, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nand lacking inputs x. as described in sec.\ndescribes the power method. it may be simplified to\n\n8.2.5\n\n, this recurrence relation essentially\n\nh( )t =(cid:19)w t(cid:20)> h (0),\n\nadmits an eigendecomposition\n\nand if\n\nw\n\nw q q\n\n=  \u03bb > ,\n\nthe recurrence may be simplified further to\n\nh( )t = q>\u03bbt qh(0).\n\n(10.30)\n\n(10.31)\n\n(10.32)\n\nthe eigenvalues are raised to the power of t causing eigenvalues with magnitude\nless than one to decay to zero and eigenvalues with magnitude greater than one to\nexplode. any component of h(0) that is not aligned with the largest eigenvector\nwill eventually be discarded.\n\nthis problem is particular to recurrent networks. in the scalar case, imagine\nmultiplying a weight w by itself many times. the product wt will either vanish or\nexplode depending on the magnitude of w. however, if we make a non-recurrent\nnetwork that has a different weight w ( )t at each time step, the situation is different.\nt is given byqt w( )t . suppose\nif the initial state is given by , then the state at time\nthat the w( )t values are generated randomly, independently from one another, with\nzero mean and variance v . the variance of the product is o (v n). to obtain some\ndesired variance v\u2217 we may choose the individual weights with variance v = n\u221av\u2217.\nvery deep feedforward networks with carefully chosen scaling can thus avoid the\nvanishing and exploding gradient problem, as argued by\n\nsussillo 2014\n\n).\n\n1\n\n(\n\n,\n\n,\n\n;\n\n,\n\nthe vanishing and exploding gradient problem for rnns was independently\nhochreiter 1991 bengio et al. 1993 1994\n).\ndiscovered by separate researchers (\none may hope that the problem can be avoided simply by staying in a region of\nparameter space where the gradients do not vanish or explode. unfortunately, in\norder to store memories in a way that is robust to small perturbations, the rnn\nmust enter a region of parameter space where gradients vanish (\nbengio et al. 1993\n,\n1994). specifically, whenever the model is able to represent long term dependencies,\nthe gradient of a long term interaction has exponentially smaller magnitude than\nthe gradient of a short term interaction.\u00a0it does not mean that it is impossible\nto learn, but that it might take a very long time to learn long-term dependencies,\nbecause the signal about these dependencies will tend to be hidden by the smallest\nfluctuations arising from short-term dependencies.\u00a0in practice, the experiments\n) show that as we increase the span of the dependencies that\nin\n\nbengio et al. 1994\n\n(\n\n,\n\n405\n\n "}, {"Page_number": 421, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nneed to be captured, gradient-based optimization becomes increasingly difficult,\nwith the probability of successful training of a traditional rnn via sgd rapidly\nreaching 0 for sequences of only length 10 or 20.\n\n(\n\n),\n\net al. (\n2013a\n\nfor a deeper treatment of the dynamical systems view of recurrent networks,\n1994\n), with a\nsee doya 1993 bengio\nreview in pascanu\n). the remaining sections of this chapter discuss\nvarious approaches that have been proposed to reduce the difficulty of learning\nlong-term dependencies (in some cases allowing an rnn to learn dependencies\nacross hundreds of steps), but the problem of learning long-term dependencies\nremains one of the main challenges in deep learning.\n\nsiegelmann and sontag 1995\n\net al. (\n\n) and\n\n(\n\n10.8 echo state networks\n\n;\n\n,\n\n;\n\n,\n\n,\n\n1)\n\nt\u2212 to h( )t and the input weights mapping\nthe recurrent weights mapping from h(\nfrom x( )t to h( )t are some of the most difficult parameters to learn in a recurrent\nnetwork. one proposed (\njaeger 2003 maass et al. 2002 jaeger and haas 2004\n;\njaeger 2007b\n) approach to avoiding this difficulty is to set the recurrent weights\nsuch that the recurrent hidden units do a good job of capturing the history of\npast inputs, and only learn the output weights. this is the idea that was\njaeger and haas 2004\nindependently proposed for echo state networks or esns (\n;\njaeger 2007b\n2002\n). the latter is similar,\nexcept that it uses spiking neurons (with binary outputs) instead of the continuous-\nvalued hidden units used for esns. both esns and liquid state machines are\ntermed reservoir computing (luko\u0161evi\u010dius and jaeger 2009\n) to denote the fact\nthat the hidden units form of reservoir of temporal features which may capture\ndifferent aspects of the history of inputs.\n\nliquid state machines\n\nmaass\n(\n\net al.,\n\n) and\n\n,\n\n,\n\n,\n\n,\n\none way to think about these reservoir computing recurrent networks is that\nthey are similar to kernel machines: they map an arbitrary length sequence (the\nhistory of inputs up to time t) into a fixed-length vector (the recurrent state h( )t ),\non which a linear predictor (typically a linear regression) can be applied to solve\nthe problem of interest. the training criterion may then be easily designed to be\nconvex as a function of the output weights. for example, if the output consists\nof linear regression from the hidden units to the output targets, and the training\ncriterion is mean squared error, then it is convex and may be solved reliably with\nsimple learning algorithms (\n\njaeger 2003\n\n).\n\n,\n\nthe important question is therefore:\u00a0how do we set the input and recurrent\nweights so that a rich set of histories can be represented in the recurrent neural\nnetwork state?\u00a0the answer proposed in the reservoir computing literature is to\n\n406\n\n "}, {"Page_number": 422, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nview the recurrent net as a dynamical system, and set the input and recurrent\nweights such that the dynamical system is near the edge of stability.\n\nthe original idea was to make the eigenvalues of the jacobian of the state-to-\nstate transition function be close to . as explained in sec.\n, an important\ncharacteristic of a recurrent network is the eigenvalue spectrum of the jacobians\nj ( )t = \u2202s( )t\n. of particular importance is the spectral radius of j( )t , defined to be\nt\u2212\nthe maximum of the absolute values of its eigenvalues.\n\n8.2.5\n\n\u2202s(\n\n1\n\n1)\n\nto understand the effect of the spectral radius, consider the simple case of\nback-propagation with a jacobian matrix j that does not change with t . this\ncase happens, for example, when the network is purely linear. suppose that j has\nan eigenvector v with corresponding eigenvalue \u03bb. consider what happens as we\npropagate a gradient vector backwards through time. if we begin with a gradient\nvector g, then after one step of back-propagation, we will have j g, and after n\nsteps we will have jn g. now consider what happens if we instead back-propagate\na perturbed version of g. if we begin with g + \u03b4v, then after one step, we will\nhave j (g + \u03b4v). after n steps, we will have j n(g + \u03b4v). from this we can see\nthat back-propagation starting from g and back-propagation starting from g + \u03b4v\ndiverge by \u03b4j nv after n steps of back-propagation. if v is chosen to be a unit\neigenvector of j with eigenvalue \u03bb, then multiplication by the jacobian simply\nscales the difference at each step. the two executions of back-propagation are\nseparated by a distance of \u03b4 \u03bb|\n|\u03bb ,\nthis perturbation achieves the widest possible separation of an initial perturbation\nof size .\u03b4\nwhen |\n\n|n. when v corresponds to the largest value of |\n\n|n grows exponentially large. when |\n\n|\u03bb > 1, the deviation size \u03b4 \u03bb|\n\n|\u03bb < 1,\n\nthe deviation size becomes exponentially small.\n\nof course, this example assumed that the jacobian was the same at every\ntime step, corresponding to a recurrent network with no nonlinearity. when a\nnonlinearity is present, the derivative of the nonlinearity will approach zero on\nmany time steps, and help to prevent the explosion resulting from a large spectral\nradius.\u00a0indeed, the most recent work on echo state networks advocates using a\nspectral radius much larger than unity (\n\nyildiz et al. 2012 jaeger 2012\n\n).\n\n,\n\n;\n\n,\n\neverything we have said about back-propagation via repeated matrix multipli-\ncation applies equally to forward propagation in a network with no nonlinearity,\nwhere the state h( +1)\n\n= h( )t >w .\n\nt\n\nwhen a linear map w > always shrinks h as measured by the l 2 norm, then\nwe say that the map is contractive. when the spectral radius is less than one, the\nmapping from h( )t to h( +1)\nis contractive, so a small change becomes smaller after\neach time step. this necessarily makes the network forget information about the\n\nt\n\n407\n\n "}, {"Page_number": 423, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\npast when we use a finite level of precision (such as 32 bit integers) to store the\nstate vector.\n\nt\n\nthe jacobian matrix tells us how a small change of h( )t propagates one step\nforward, or equivalently, how the gradient on h( +1)\npropagates one step backward,\nduring back-propagation. note that neither w nor j need to be symmetric (al-\nthough they are square and real), so they can have complex-valued eigenvalues and\neigenvectors, with imaginary components corresponding to potentially oscillatory\nbehavior (if the same jacobian was applied iteratively). even though h( )t or a\nsmall variation of h ( )t of interest in back-propagation are real-valued, they can\nbe expressed in such a complex-valued basis. what matters is what happens to\nthe magnitude (complex absolute value) of these possibly complex-valued basis\ncoefficients,\u00a0when we multiply the matrix by the vector. an eigenvalue with\nmagnitude greater than one corresponds to magnification (exponential growth, if\napplied iteratively) or shrinking (exponential decay, if applied iteratively).\n\nwith a nonlinear map,\u00a0the jacobian is free to change at each step. the\ndynamics therefore become more complicated.\u00a0however, it remains true that a\nsmall initial variation can turn into a large variation after several steps. one\ndifference between the purely linear case and the nonlinear case is that the use of\na squashing nonlinearity such as tanh can cause the recurrent dynamics to become\nbounded. note that\u00a0it is possible for\u00a0back-propagation\u00a0to retain\u00a0unbounded\ndynamics even when forward propagation has bounded dynamics, for example,\nwhen a sequence of tanh units are all in the middle of their linear regime and are\nconnected by weight matrices with spectral radius greater than . however, it is\nrare for all of the\nunits to simultaneously lie at their linear activation point.\n\ntanh\n\n1\n\nthe strategy of echo state networks is simply to fix the weights to have some\nspectral radius such as\n, where information is carried forward through time but\ndoes not explode due to the stabilizing effect of saturating nonlinearities like tanh.\n\n3\n\nmore recently, it has been shown that the techniques used to set the weights\nin esns could be used to initialize the weights in a fully trainable recurrent net-\nwork (with the hidden-to-hidden recurrent weights trained using back-propagation\nthrough time), helping to learn long-term dependencies (sutskever 2012 sutskever\net al.,\n). in this setting, an initial spectral radius of 1.2 performs well, combined\nwith the sparse initialization scheme described in sec.\n\n2013\n\n.8.4\n\n,\n\n;\n\n408\n\n "}, {"Page_number": 424, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n10.9 leaky units and other strategies for multiple\n\ntime scales\n\none way to deal with long-term dependencies is to design a model that operates\nat multiple time scales, so that some parts of the model operate at fine-grained\ntime scales and can handle small details, while other parts operate at coarse time\nscales and transfer information from the distant past to the present more efficiently.\nvarious strategies for building both fine and coarse time scales are possible. these\ninclude the addition of skip connections across time, \u201cleaky units\u201d that integrate\nsignals with different time constants, and the removal of some of the connections\nused to model fine-grained time scales.\n\n10.9.1 adding skip connections through time\n\none way to obtain coarse time scales is to add direct connections from variables in\nthe distant past to variables in the present. the idea of using such skip connections\n) and follows from the idea of incorporating delays in\ndates back to\nfeedforward neural networks (\n). in an ordinary recurrent\nnetwork, a recurrent connection goes from a unit at time t to a unit at time t+ 1.\nit is possible to construct recurrent networks with longer delays (\n\nlang and hinton 1988\n\nlin et al. 1996\n\nbengio 1991\n\n).\n\n(\n\n,\n\n,\n\n8.2.5\n\nas we have seen in sec.\n\n, gradients may vanish or explode exponentially\nwith respect to the number of time steps.\n) introduced\nrecurrent connections with a time-delay of d to mitigate this problem. gradients\nnow diminish exponentially as a function of \u03c4\nd rather than \u03c4. since there are both\ndelayed and single step connections, gradients may still explode exponentially in \u03c4.\nthis allows the learning algorithm to capture longer dependencies although not all\nlong-term dependencies may be represented well in this way.\n\nlin et al. 1996\n\n(\n\n10.9.2 leaky units and a spectrum of different time scales\n\nanother way to obtain paths on which the product of derivatives is close to one is to\nhave units with linear self-connections and a weight near one on these connections.\nwhen we accumulate a running average \u00b5( )t of some value v ( )t by applying the\nt\u2212 + (1 \u2212 \u03b1)v( )t\nupdate \u00b5( )t \u2190 \u03b1\u00b5(\nthe \u03b1 parameter is an example of a linear self-\nt\u2212 to \u00b5( )t . when \u03b1 is near one, the running average remembers\nconnection from \u00b5(\ninformation about the past for a long time, and when \u03b1 is near zero, information\nabout the past is rapidly discarded. hidden units with linear self-connections can\nbehave similarly to such running averages. such hidden units are called leaky units.\n\n1)\n\n1)\n\n409\n\n "}, {"Page_number": 425, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nskip connections through d time steps are a way of ensuring that a unit can\nalways learn to be influenced by a value from d time steps earlier. the use of a\nlinear self-connection with a weight near one is a different way of ensuring that the\nunit can access values from the past. the linear self-connection approach allows\nthis effect to be adapted more smoothly and flexibly by adjusting the real-valued\n\u03b1 rather than by adjusting the integer-valued skip length.\n\nthese ideas were proposed by\n\n).\nel hihi and bengio 1996\nleaky units were also found to be useful in the context of echo state networks\n(\njaeger et al. 2007\n\nmozer 1992\n\n) and by\n\n).\n\n(\n\n(\n\n,\n\nthere are two basic strategies for setting the time constants used by leaky\nunits.\u00a0one strategy is to manually fix them to values that remain constant, for\nexample by sampling their values from some distribution once at initialization time.\nanother strategy is to make the time constants free parameters and learn them.\nhaving such leaky units at different time scales appears to help with long-term\ndependencies (\n\nmozer 1992 pascanu\n\n2013a\n\net al.,\n\n).\n\n,\n\n;\n\n10.9.3 removing connections\n\nanother approach to handle long-term dependencies is the idea of organizing\nthe state of the rnn at multiple time-scales (\n), with\ninformation flowing more easily through long distances at the slower time scales.\n\nel hihi and bengio 1996\n\n,\n\nthis idea differs from the skip connections through time discussed earlier\nbecause it involves actively removing length-one connections and replacing them\nwith longer connections. units modified in such a way are forced to operate on a\nlong time scale. skip connections through time add edges. units receiving such\nnew connections may learn to operate on a long time scale but may also choose to\nfocus on their other short-term connections.\n\nthere are different ways in which a group of recurrent units can be forced to\noperate at different time scales. one option is to make the recurrent units leaky,\nbut to have different groups of units associated with different fixed time scales.\nthis was the proposal in\npascanu\net al. (\n). another option is to have explicit and discrete updates taking place\nat different times, with a different frequency for different groups of units. this is\nthe approach of\n). it worked\nwell on a number of benchmark datasets.\n\n) and has been successfully used in\n\nel hihi and bengio 1996\n\nmozer 1992\n\nkoutnik\n\net al. (\n\n2013a\n\n) and\n\n2014\n\n(\n\n(\n\n410\n\n "}, {"Page_number": 426, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n10.10 the long short-term memory and other gated\n\nrnns\n\nas of this writing, the most effective sequence models used in practical applications\nare called gated rnns\nand networks\nbased on the gated recurrent unit.\n\nlong short-term memory\n\n. these include the\n\nlike leaky units, gated rnns are based on the idea of creating paths through\ntime that have derivatives that neither\u00a0vanish nor explode. leaky\u00a0units did\nthis with connection weights that were either manually chosen constants or were\nparameters. gated rnns generalize this to connection weights that may change\nat each time step.\n\nleaky units allow the network to accumulate information (such as evidence\nfor a particular feature or category) over a long duration. however, once that\ninformation has been used, it might be useful for the neural network to forget the\nold state. for example, if a sequence is made of sub-sequences and we want a leaky\nunit to accumulate evidence inside each sub-subsequence, we need a mechanism to\nforget the old state by setting it to zero. instead of manually deciding when to\nclear the state, we want the neural network to learn to decide when to do it. this\nis what gated rnns do.\n\n10.10.1 lstm\n\n,\n\n,\n\nthe clever idea of introducing self-loops to produce paths where the gradient can\nflow for long durations is a core contribution of the initial long short-term memory\n(lstm) model (hochreiter and schmidhuber 1997\n). a crucial addition has been\nto make the weight on this self-loop conditioned on the context, rather than fixed\n(\ngers et al. 2000\n). by making the weight of this self-loop gated (controlled by\nanother hidden unit), the time scale of integration can be changed dynamically. in\nthis case, we mean that even for an lstm with fixed parameters, the time scale of\nintegration can change based on the input sequence, because the time constants\nare output by the model itself. the lstm has been found extremely successful\nin many applications, such as unconstrained handwriting recognition (graves\n),\net al.,\n2013 graves and jaitly 2014\nhandwriting generation (graves 2013\n),\n2014\nimage captioning (\n) and\nparsing (vinyals\n\n), machine translation (sutskever\n\nkiros et al. 2014b vinyals\n\n), speech recognition (\n\n,\net al.,\n2015\n\n2014b xu\n\n,\n,\n).\n\ngraves\n\net al.,\n\net al.,\n\net al.,\n\net al.,\n\n2014a\n\n2009\n\n;\n\n;\n\n;\n\nthe lstm block diagram is illustrated in fig.\n\n. the corresponding\nforward propagation equations are given below, in the case of a shallow recurrent\n\n10.16\n\n411\n\n "}, {"Page_number": 427, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\noutput\n\nself-loop\n\n\u00d7\n\nstate\n\n\u00d7\n\n+\n\n\u00d7\n\ninput\n\ninput\u00a0gate\n\nforget\u00a0gate\n\noutput\u00a0gate\n\nfigure 10.16: block diagram of the lstm recurrent network \u201ccell.\u201d cells are connected\nrecurrently to each other, replacing the usual hidden units of ordinary recurrent networks.\nan input feature is computed with a regular artificial neuron unit. its value can be\naccumulated into the state if the sigmoidal input gate allows it. the state unit has a\nlinear self-loop whose weight is controlled by the forget gate. the output of the cell can\nbe shut off by the output gate. all the gating units have a sigmoid nonlinearity, while the\ninput unit can have any squashing nonlinearity. the state unit can also be used as an\nextra input to the gating units. the black square indicates a delay of 1 time unit.\n\n412\n\n "}, {"Page_number": 428, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n;\n\net al.,\n\n2014a\n\n2013 pascanu\n\nnetwork architecture. deeper architectures have also been successfully used (graves\net al.,\n). instead of a unit that simply applies an element-\nwise nonlinearity to the affine transformation of inputs and recurrent units, lstm\nrecurrent networks have \u201clstm cells\u201d that have an internal recurrence (a self-loop),\nin addition to the outer recurrence of the rnn. each cell has the same inputs\nand outputs as an ordinary recurrent network, but has more parameters and a\nsystem of gating units that controls the flow of information. the most important\ncomponent is the state unit s( )t\nthat has a linear self-loop similar to the leaky\ni\nunits described in the previous section. however, here, the self-loop weight (or the\nassociated time constant) is controlled by a forget gate unit f ( )t\n(for time step t\nand cell ), that sets this weight to a value between 0 and 1 via a sigmoid unit:\n\ni\n\ni\n\nf ( )t\n\ni = \u03c3(cid:26)(cid:27)bf\n\ni +xj\n\ni,j x( )t\nu f\n\nj +xj\n\nw f\n\ni,j h(\n\n1)\n\nt\u2212j (cid:28)(cid:29),\n\nis the current input vector and h( )t\n\nwhere x( )t\nis the current hidden layer vector,\ncontaining the outputs of all the lstm cells, and bf ,u f , w f are respectively\nbiases, input weights and recurrent weights for the forget gates. the lstm cell\ninternal state is thus updated as follows, but with a conditional self-loop weight\nf ( )t\ni\n\n:\n\ns( )t\ni = f ( )t\n\ni s(\n\nt\u2212i\n\n1)\n\n+ g ( )t\n\ni \u03c3(cid:26)(cid:27)bi +xj\n\nui,j x( )t\n\nj +xj\n\nwi,jh(\n\n1)\n\nt\u2212j (cid:28)(cid:29) ,\n\nwhere b, u and w respectively denote the biases, input weights and recurrent\nweights into the lstm cell. the\nis computed similarly\nto the forget gate (with a sigmoid unit to obtain a gating value between 0 and 1),\nbut with its own parameters:\n\nexternal input gate\n\ng( )t\ni\n\nunit\n\n(10.33)\n\n(10.34)\n\n(10.35)\n\ni,jx( )t\nu g\n\nj +xj\n\nwg\n\ni,j h(\n\n1)\n\nt\u2212j (cid:28)(cid:29) .\n\nthe output h( )t\ni\nwhich also uses a sigmoid unit for gating:\n\nof the lstm cell can also be shut off, via the output gate q ( )t\n,\n\ni\n\ng( )t\n\ni = \u03c3(cid:26)(cid:27)bg\ni +xj\ni = tanh(cid:22) s( )t\ni (cid:23) q ( )t\ni = \u03c3(cid:26)(cid:27)bo\ni +xj\n\ni\n\nq ( )t\n\nh ( )t\n\n(10.36)\n\n(10.37)\n\nw o\n\ni,jh(\n\n1)\n\nt\u2212j (cid:28)(cid:29)\n\ni,jx( )t\nuo\n\nj +xj\n\n413\n\n "}, {"Page_number": 429, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nwhich has parameters bo, u o, w o for its biases, input weights and recurrent\nweights, respectively. among the variants, one can choose to use the cell state s( )t\ni\nas an extra input (with its weight) into the three gates of the i-th unit, as shown\nin fig.\n\n. this would require three additional parameters.\n\n10.16\n\nlstm networks have been shown to learn long-term dependencies more easily\nthan the simple recurrent architectures, first on artificial data sets designed for\ntesting the ability to learn long-term dependencies (\nbengio et al. 1994 hochreiter\n), then on challenging sequence\nand schmidhuber 1997 hochreiter\nprocessing tasks where state-of-the-art performance was obtained (graves 2012\n;\ngraves\n). variants and alternatives to the lstm\nhave been studied and used and are discussed next.\n\n2013 sutskever\n\net al.,\n\net al.,\n\net al.,\n\n2000\n\n2014\n\n,\n\n;\n\n,\n\n,\n\n;\n\n;\n\n10.10.2 other gated rnns\n\nwhich pieces of\u00a0the lstm architecture are actually necessary? what other\nsuccessful architectures could be designed that allow the network to dynamically\ncontrol the time scale and forgetting behavior of different units?\n\net al.,\n\nsome answers to these questions are given with the recent work on gated rnns,\ncho et al. 2014b\n;\nwhose units are also known as gated recurrent units or grus (\nchung\n). the main\net al.,\ndifference with the lstm is that a single gating unit simultaneously controls the\nforgetting factor and the decision to update the state unit. the update equations\nare the following:\n\n2014 2015a jozefowicz\n\n2015 chrupala\n\net al.,\n\n2015\n\n,\n\n,\n\n;\n\n;\n\nh( )t\ni = u(\n\nt\u2212i\n\n1)\n\n1)\n\nt\u2212i\nh(\n\nui,jx(\n\nt\u2212j\n\n1)\n\nwi,jr(\n\nt\u2212j\n\n1)\n\nh(\n\n(10.38)\nwhere u stands for \u201cupdate\u201d gate and r for \u201creset\u201d gate. their value is defined as\nusual:\n\n1)\n\nt\u2212i\n+ (1 \u2212 u(\n\n)\u03c3(cid:26)(cid:27)bi +xj\ni = \u03c3(cid:26)(cid:27)bu\ni +xj\ni = \u03c3(cid:26)(cid:27)b r\ni +xj\n\ni,j x( )t\nu r\n\ni,jx ( )t\nuu\n\nu( )t\n\nr( )t\n\nj +xj\nj +xj\n\n+xj\nj (cid:28)(cid:29)\nj (cid:28)(cid:29).\n\nw u\n\ni,j h( )t\n\nw r\n\ni,jh( )t\n\n1)\n\nt\u2212j (cid:28)(cid:29) ,\n\n(10.39)\n\n(10.40)\n\nthe reset and updates gates can individually \u201cignore\u201d parts of the state vector.\nthe update gates act like conditional leaky integrators that can linearly gate any\n\n414\n\nand\n\n "}, {"Page_number": 430, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ndimension, thus choosing to copy it (at one extreme of the sigmoid) or completely\nignore it (at the other extreme) by replacing it by the new \u201ctarget state\u201d value\n(towards which the leaky integrator wants to converge). the reset gates control\nwhich parts of the state get used to compute the next target state, introducing an\nadditional nonlinear effect in the relationship between past state and future state.\n\nmany more variants around this theme can be designed. for example the\nreset gate (or forget gate) output could be shared across multiple hidden units.\nalternately, the product of a global gate (covering a whole group of units, such as\nan entire layer) and a local gate (per unit) could be used to combine global control\nand local control. however, several investigations over architectural variations\nof the lstm and gru found no variant that would clearly beat both of these\n2015 greff\nacross a wide range of tasks (\net al. (\njozefowicz\n) found that a crucial ingredient is the forget gate, while\n) found that adding a bias of 1 to the lstm forget gate, a practice\net al. (\nadvocated by\ngers et al. 2000\n), makes the lstm as strong as the best of the\nexplored architectural variants.\n\ngreff et al. 2015 jozefowicz\n\n2015\n2015\n\net al.,\n\n).\n\n(\n\n,\n\n;\n\n10.11 optimization for long-term dependencies\n\n8.2.5\n\nsec.\nproblems that occur when optimizing rnns over many time steps.\n\nhave described the vanishing and exploding gradient\n\nand sec.\n\n10.7\n\n(\n\nan interesting idea proposed by martens and sutskever 2011\n\n) is that second\nderivatives may vanish at the same time that first derivatives vanish. second-order\noptimization algorithms may roughly be understood as dividing the first derivative\nby the second derivative (in higher dimension, multiplying the gradient by the\ninverse hessian). if the second derivative shrinks at a similar rate to the first\nderivative, then the ratio of first and second derivatives may remain relatively\nconstant. unfortunately, second-order methods have many drawbacks, including\nhigh computational cost, the need for a large minibatch, and a tendency to be\nattracted to saddle points. martens and sutskever 2011\n) found promising results\nusing second-order methods. later, sutskever\net al. (\n2013\n) found that simpler\nmethods such as nesterov momentum with careful initialization could achieve\nsimilar results. see sutskever 2012\n) for more detail. both of these approaches\nhave largely been replaced by simply using sgd (even without momentum) applied\nto lstms. this is part of a continuing theme in machine learning that it is often\nmuch easier to design a model that is easy to optimize than it is to design a more\npowerful optimization algorithm.\n\n(\n\n(\n\n415\n\n "}, {"Page_number": 431, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n10.11.1 clipping gradients\n\n8.2.4\n\nas discussed in sec.\n, strongly nonlinear functions such as those computed by\na recurrent net over many time steps tend to have derivatives that can be either\nvery large or very small in magnitude. this is illustrated in fig.\n10.17\n,\nin which we see that the objective function (as a function of the parameters) has a\n\u201clandscape\u201d in which one finds \u201ccliffs\u201d: wide and rather flat regions separated by\ntiny regions where the objective function changes quickly, forming a kind of cliff.\n\nand fig.\n\n8.3\n\nthe difficulty that arises is that when the parameter gradient is very large, a\ngradient descent parameter update could throw the parameters very far, into a\nregion where the objective function is larger, undoing much of the work that had\nbeen done to reach the current solution. the gradient tells us the direction that\ncorresponds to the steepest descent within an infinitesimal region surrounding the\ncurrent parameters. outside of this infinitesimal region, the cost function may\nbegin to curve back upwards. the update must be chosen to be small enough to\navoid traversing too much upward curvature. we typically use learning rates that\ndecay slowly enough that consecutive steps have approximately the same learning\nrate. a step size that is appropriate for a relatively linear part of the landscape is\noften inappropriate and causes uphill motion if we enter a more curved part of the\nlandscape on the next step.\n\n416\n\n "}, {"Page_number": 432, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nwithout\u00a0clipping\n\nwith\u00a0clipping\n\n)\nb\n;\nw\n(\nj\n\n)\nb\n;\nw\n(\nj\n\nw\n\nw\n\nb\n\nb\n\nfigure 10.17: example of the effect of gradient clipping in a recurrent network with\ntwo parameters w and b. gradient clipping can make gradient descent perform more\nreasonably in the vicinity of extremely steep cliffs. these steep cliffs commonly occur\nin recurrent networks near where a recurrent network behaves approximately linearly.\nthe cliff is exponentially steep in the number of time steps because the weight matrix\nis multiplied by itself once for each time step. (left) gradient descent without gradient\nclipping overshoots the bottom of this small ravine, then receives a very large gradient\nfrom the cliff face. the large gradient catastrophically propels the parameters outside the\naxes of the plot. (right) gradient descent with gradient clipping has a more moderate\nreaction to the cliff. while it does ascend the cliff face, the step size is restricted so that\nit cannot be propelled away from steep region near the solution. figure adapted with\npermission from pascanu\n\net al. (\n\n2013a\n\n).\n\na simple type of solution has been in use by practitioners for many years:\nclipping the gradient. there are different instances of this idea (mikolov 2012\n;\n). one option is to clip the parameter gradient from a\npascanu\n) just before the parameter update. another\nminibatch\n(\nis to clip the norm ||\n) just before the\nparameter update:\n\net al.,\nelement-wise mikolov 2012\n\n||g of the gradient g (pascanu\n\net al.,\n\n2013a\n\n2013a\n\n,\n\n,\n\nif || ||g > v\ng \u2190\n\ngv\n||\n||g\n\n(10.41)\n\n(10.42)\n\nwhere v is the norm threshold and g is used to update parameters. because the\ngradient of all the parameters (including different groups of parameters, such as\nweights and biases) is renormalized jointly with a single scaling factor, the latter\nmethod has the advantage that it guarantees that each step is still in the gradient\ndirection, but experiments suggest that both forms work similarly. although\n\n417\n\n "}, {"Page_number": 433, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nthe parameter update has the same direction as the true gradient, with gradient\nnorm clipping, the parameter update vector norm is now bounded. this bounded\ngradient avoids performing a detrimental step when the gradient explodes. in\nfact, even simply taking a random step when the gradient magnitude is above\na threshold tends to work almost as well. if the explosion is so severe that the\ngradient is numerically inf or nan (considered infinite or not-a-number), then\na random step of size v can be taken and will typically move away from the\nnumerically unstable configuration. clipping the gradient norm per-minibatch will\nnot change the direction of the gradient for an individual minibatch. however,\ntaking the average of the norm-clipped gradient from many minibatches is not\nequivalent to clipping the norm of the true gradient (the gradient formed from\nusing all examples). examples that have large gradient norm, as well as examples\nthat appear in the same minibatch as such examples, will have their contribution\nto the final direction diminished. this stands in contrast to traditional minibatch\ngradient descent, where the true gradient direction is equal to the average over all\nminibatch gradients. put another way, traditional stochastic gradient descent uses\nan unbiased estimate of the gradient, while gradient descent with norm clipping\nintroduces a heuristic bias that we know empirically to be useful. with element-\nwise clipping, the direction of the update is not aligned with the true gradient\nor the minibatch gradient, but it is still a descent direction. it has also been\nproposed (graves 2013\n) to clip the back-propagated gradient (with respect to\nhidden units) but no comparison has been published between these variants; we\nconjecture that all these methods behave similarly.\n\n,\n\n10.11.2 regularizing to encourage information flow\n\ngradient clipping helps to deal with exploding gradients, but it does not help with\nvanishing gradients. to address vanishing gradients and better capture long-term\ndependencies, we discussed the idea of creating paths in the computational graph of\nthe unfolded recurrent architecture along which the product of gradients associated\nwith arcs is near 1. one approach to achieve this is with lstms and other self-loops\nand gating mechanisms, described above in sec.\n. another idea is to regularize\nor constrain the parameters so as to encourage \u201cinformation flow.\u201d\u00a0in particular,\nwe would like the gradient vector \u2207h( )t l being back-propagated to maintain its\nmagnitude, even if the loss function only penalizes the output at the end of the\nsequence. formally, we want\n\n10.10\n\n(\u2207h( )t l)\n\n\u2202h( )t\nt\u2212\n1)\n\u2202h(\n\n418\n\n(10.43)\n\n "}, {"Page_number": 434, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\n(10.44)\n\nto be as large as\n\nwith this objective, pascanu\n\n\u2207h( )t l.\n2013a\n\net al. (\n\n) propose the following regularizer:\n\n\u03c9 =xt\n\n(cid:26)(cid:27)\n\n(cid:21)(cid:21)(cid:21)| \u2207( h( )t l) \u2202h( )t\nt\u2212 (cid:21)(cid:21)(cid:21) |\n\n||\u2207h( )t l||\n\n\u2202h(\n\n1)\n\n\u2212 1(cid:28)(cid:29)\n\n2\n\n.\n\n(10.45)\n\n2013a\n\ncomputing the gradient of this regularizer may appear difficult, but pascanu\net al. (\n) propose an approximation in which we consider the back-propagated\nvectors \u2207 h( )t l as if they were constants (for the purpose of this regularizer, so\nthat there is no need to back-propagate through them). the experiments with\nthis regularizer suggest that, if combined with the norm clipping heuristic (which\nhandles gradient explosion), the regularizer can considerably increase the span of\nthe dependencies that an rnn can learn. because it keeps the rnn dynamics\non the edge of explosive gradients, the gradient clipping is particularly important.\nwithout gradient clipping, gradient explosion prevents learning from succeeding.\n\na key weakness of this approach is that it is not as effective as the lstm for\n\ntasks where data is abundant, such as language modeling.\n\n10.12 explicit memory\n\nintelligence requires knowledge and acquiring knowledge can be done via learning,\nwhich has motivated the development of large-scale deep architectures. however,\nthere are different kinds of knowledge. some knowledge can be implicit, sub-\nconscious, and difficult to verbalize\u2014such as how to walk, or how a dog looks\ndifferent from a cat. other knowledge can be explicit, declarative, and relatively\nstraightforward to put into words\u2014every day commonsense knowledge, like \u201ca cat\nis a kind of animal,\u201d or very specific facts that you need to know to accomplish\nyour current goals, like \u201cthe meeting with the sales team is at 3:00 pm in room\n141.\u201d\n\nneural networks excel at storing implicit knowledge. however, they struggle\nto memorize facts. stochastic gradient descent requires many presentations of\nthe same input before it can be stored in a neural network parameters, and even\nthen, that input will not be stored especially precisely. graves\n2014b\n)\nhypothesized that this is because neural networks lack the equivalent of the working\nmemory system that allows human beings to explicitly hold and manipulate pieces\nof information that are relevant to achieving some goal. such explicit memory\n\net al. (\n\n419\n\n "}, {"Page_number": 435, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nmemory\u00a0cells\n\nwriting\n\nmechanism\n\nreading\n\nmechanism\n\ntask\u00a0network,\n\ncontrolling\u00a0the\u00a0memory\n\nfigure 10.18: a schematic of an example of a network with an explicit memory, capturing\nsome of the key design elements of the neural turing machine.\nin this diagram we\ndistinguish the \u201crepresentation\u201d part of the model (the \u201ctask network,\u201d here a recurrent\nnet in the bottom) from the \u201cmemory\u201d part of the model (the set of cells), which can\nstore facts. the task network learns to \u201ccontrol\u201d the memory, deciding where to read from\nand where to write to within the memory (through the reading and writing mechanisms,\nindicated by bold arrows pointing at the reading and writing addresses).\n\n420\n\n "}, {"Page_number": 436, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\ncomponents would allow our systems not only to rapidly and \u201cintentionally\u201d store\nand retrieve specific facts but also to sequentially reason with them.\u00a0the need\nfor neural networks that can process information in a sequence of steps, changing\nthe way the input is fed into the network at each step, has long been recognized\nas important for the ability to reason rather than to make automatic, intuitive\nresponses to the input (\n\nhinton 1990\n\n).\n\n,\n\n2014\n\net al. (\n\net al. (\n\n) introduced\n\nmemory networks\n\nto resolve this difficulty, weston\n\nthat\ninclude a set of memory cells that can be accessed via an addressing mechanism.\nmemory networks originally required a supervision signal instructing them how\nto use their memory cells. graves\n) introduced the neural turing\nmachine, which is able to learn to read from and write arbitrary content to memory\ncells without explicit supervision about which actions to undertake, and allowed\nend-to-end training without this supervision signal, via the use of a content-based\nsoft attention mechanism (see\n). this\nsoft addressing mechanism has become standard with other related architectures\nemulating algorithmic mechanisms in a way that still allows gradient-based opti-\nmization (\n2015\n;\nvinyals\n\nsukhbaatar et al. 2015 joulin and mikolov 2015 kumar\n\n;\n2015a grefenstette\n\nbahdanau et al. 2015\n\n) and sec.\n\n12.4.5.1\n\n2014b\n\net al.,\n\net al.,\n\net al.,\n\n2015\n\n).\n\n(\n\n,\n\n,\n\n;\n\n;\n\neach memory cell can be thought of as an extension of the memory cells in\nlstms and grus. the difference is that the network outputs an internal state\nthat chooses which cell to read from or write to, just as memory accesses in a\ndigital computer read from or write to a specific address.\n\nit is difficult to optimize functions that produce exact, integer addresses. to\nalleviate this problem, ntms actually read to or write from many memory cells\nsimultaneously. to read, they take a weighted average of many cells. to write, they\nmodify multiple cells by different amounts. the coefficients for these operations\nare chosen to be focused on a small number of cells, for example, by producing\nthem via a softmax function. using these weights with non-zero derivatives allows\nthe functions controlling access to the memory to be optimized using gradient\ndescent. the gradient on these coefficients indicates whether each of them should\nbe increased or decreased, but the gradient will typically be large only for those\nmemory addresses receiving a large coefficient.\n\nthese memory cells are typically augmented to contain a vector, rather than\nthe single scalar stored by an lstm or gru memory cell. there are two reasons\nto increase the size of the memory cell. one reason is that we have increased the\ncost of accessing a memory cell. we pay the computational cost of producing a\ncoefficient for many cells, but we expect these coefficients to cluster around a small\nnumber of cells. by reading a vector value, rather than a scalar value, we can\n\n421\n\n "}, {"Page_number": 437, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\noffset some of this cost. another reason to use vector-valued memory cells is that\nthey allow for content-based addressing, where the weight used to read to or write\nfrom a cell is a function of that cell. vector-valued cells allow us to retrieve a\ncomplete vector-valued memory if we are able to produce a pattern that matches\nsome but not all of its elements. this is analogous to the way that people can\nrecall the lyrics of a song based on a few words. we can think of a content-based\nread instruction as saying, \u201cretrieve the lyrics of the song that has the chorus \u2018we\nall live in a yellow submarine.\u2019 \u201d content-based addressing is more useful when we\nmake the objects to be retrieved large\u2014if every letter of the song was stored in a\nseparate memory cell, we would not be able to find them this way. by comparison,\nlocation-based addressing is not allowed to refer to the content of the memory. we\ncan think of a location-based read instruction as saying \u201cretrieve the lyrics of\nthe song in slot 347.\u201d location-based addressing can often be a perfectly sensible\nmechanism even when the memory cells are small.\n\nif the content of a memory cell is copied (not forgotten) at most time steps, then\nthe information it contains can be propagated forward in time and the gradients\npropagated backward in time without either vanishing or exploding.\n\n10.18\n\nthe explicit memory approach is illustrated in fig.\n\n, where we see that\na \u201ctask neural network\u201d is coupled with a memory. although that task neural\nnetwork could be feedforward or recurrent, the overall system is a recurrent network.\nthe task network can choose to read from or write to specific memory addresses.\nexplicit memory seems to allow models to learn tasks that ordinary rnns or lstm\nrnns cannot learn. one reason for this advantage may be because information and\ngradients can be propagated (forward in time or backwards in time, respectively)\nfor very long durations.\n\nas an alternative to back-propagation through weighted averages of memory\ncells, we can interpret the memory addressing coefficients as probabilities and\nstochastically read just one cell (zaremba and sutskever 2015\n). optimizing models\nthat make discrete decisions requires specialized optimization algorithms, described\nin sec.\n. so far, training these stochastic architectures that make discrete\ndecisions remains harder than training deterministic algorithms that make soft\ndecisions.\n\n20.9.1\n\n,\n\nwhether it is soft (allowing back-propagation) or stochastic and hard, the mech-\nanism for choosing an address is in its form identical to the attention mechanism\nwhich had been previously introduced in the context of machine translation (bah-\ndanau\n. the idea of attention mechanisms\nfor neural networks was introduced even earlier, in the context of handwriting\n), with an attention mechanism that was constrained to\ngeneration (graves 2013\n\n) and discussed in sec.\n\n12.4.5.1\n\net al.,\n\n2015\n\n,\n\n422\n\n "}, {"Page_number": 438, "text": "chapter 10. sequence modeling: recurrent and recursive nets\n\nmove only forward in time through the sequence. in the case of machine translation\nand memory networks, at each step, the focus of attention can move to a completely\ndifferent place, compared to the previous step.\n\nrecurrent neural networks provide a way to extend deep learning to sequential\ndata. they are the last major tool in our deep learning toolbox. our discussion now\nmoves to how to choose and use these tools and how to apply them to real-world\ntasks.\n\n423\n\n "}, {"Page_number": 439, "text": "chapter 11\n\npractical methodology\n\nsuccessfully applying deep learning techniques requires more than just a good\nknowledge of what algorithms exist and the principles that explain how they\nwork. a good machine learning practitioner also needs to know how to choose an\nalgorithm for a particular application and how to monitor and respond to feedback\nobtained from experiments in order to improve a machine learning system. during\nday to day development of machine learning systems, practitioners need to decide\nwhether to gather more data, increase or decrease model capacity, add or remove\nregularizing features, improve the optimization of a model, improve approximate\ninference in a model, or debug the software implementation of the model. all of\nthese operations are at the very least time-consuming to try out, so it is important\nto be able to determine the right course of action rather than blindly guessing.\n\nmost of this book is about different machine learning models, training algo-\nrithms, and objective functions. this may give the impression that the most\nimportant ingredient to being a machine learning expert is knowing a wide variety\nof machine learning techniques and being good at different kinds of math. in prac-\ntice, one can usually do much better with a correct application of a commonplace\nalgorithm than by sloppily applying an obscure algorithm. correct application of\nan algorithm depends on mastering some fairly simple methodology. many of the\nrecommendations in this chapter are adapted from\n\nng 2015\n\n).\n\n(\n\nwe recommend the following practical design process:\n\n\u2022 determine your goals\u2014what error metric to use, and your target value for\nthis error metric. these goals and error metrics should be driven by the\nproblem that the application is intended to solve.\n\n\u2022 establish a working end-to-end pipeline as soon as possible, including the\n\n424\n\n "}, {"Page_number": 440, "text": "chapter 11. practical methodology\n\nestimation of the appropriate performance metrics.\n\n\u2022 instrument the system well to determine bottlenecks in performance. diag-\nnose which components are performing worse than expected and whether it\nis due to overfitting, underfitting, or a defect in the data or software.\n\n\u2022 repeatedly make incremental changes such as gathering new data, adjusting\nhyperparameters, or changing algorithms, based on specific findings from\nyour instrumentation.\n\n,\n\ngoodfellow et al. 2014d\n\nas a running example, we will use street view address number transcription\nsystem (\n). the purpose of this application is to add\nbuildings to google maps. street view cars photograph the buildings and record\nthe gps coordinates associated with each photograph. a convolutional network\nrecognizes the address number in each photograph, allowing the google maps\ndatabase to add that address in the correct location. the story of how this\ncommercial application was developed gives an example of how to follow the design\nmethodology we advocate.\n\nwe now describe each of the steps in this process.\n\n11.1 performance metrics\n\ndetermining your goals, in terms of which error metric to use, is a necessary first\nstep because your error metric will guide all of your future actions.\u00a0you should\nalso have an idea of what level of performance you desire.\n\nkeep in mind that for most applications, it is impossible to achieve absolute\nzero error. the bayes error defines the minimum error rate that you can hope to\nachieve, even if you have infinite training data and can recover the true probability\ndistribution. this\u00a0is because\u00a0your\u00a0input features\u00a0may not\u00a0contain\u00a0complete\ninformation about the output variable, or because the system might be intrinsically\nstochastic. you will also be limited by having a finite amount of training data.\n\nthe amount of training data can be limited for a variety of reasons. when your\ngoal is to build the best possible real-world product or service, you can typically\ncollect more data but must determine the value of reducing error further and weigh\nthis against the cost of collecting more data. data collection can require time,\nmoney, or human suffering (for example, if your data collection process involves\nperforming invasive medical tests). when your goal is to answer a scientific question\nabout which algorithm performs better on a fixed benchmark, the benchmark\n\n425\n\n "}, {"Page_number": 441, "text": "chapter 11. practical methodology\n\nspecification usually determines the training set and you are not allowed to collect\nmore data.\n\nhow can one determine a reasonable level of performance to expect? typically,\nin the academic setting, we have some estimate of the error rate that is attainable\nbased on previously published benchmark results. in the real-word setting, we\nhave some idea of the error rate that is necessary for an application to be safe,\ncost-effective, or appealing to consumers. once you have determined your realistic\ndesired error rate, your design decisions will be guided by reaching this error rate.\n\nanother important consideration besides the target value of the performance\nmetric is the choice of which metric to use. several different performance metrics\nmay be used to measure the effectiveness of a complete application that includes\nmachine learning components. these performance metrics are usually different\nfrom the cost function used to train the model.\u00a0as described in sec.\u00a0\n, it is\ncommon to measure the accuracy, or equivalently, the error rate, of a system.\n\n5.1.2\n\nhowever, many applications require more advanced metrics.\n\nsometimes it is much more costly to make one kind of a mistake than another.\nfor example, an e-mail spam detection system can make two kinds of mistakes:\nincorrectly classifying a legitimate message as spam, and incorrectly allowing a\nspam message to appear in the inbox. it is much worse to block a legitimate\nmessage than to allow a questionable message to pass through. rather than\nmeasuring the error rate of a spam classifier, we may wish to measure some form\nof total cost, where the cost of blocking legitimate messages is higher than the cost\nof allowing spam messages.\n\nsometimes we wish to train a binary classifier that is intended to detect some\nrare event. for example, we might design a medical test for a rare disease. suppose\nthat only one in every million people has this disease. we can easily achieve\n99.9999% accuracy on the detection task, by simply hard-coding the classifier\nto always report that the disease is absent. clearly, accuracy is a poor way to\ncharacterize the performance of such a system. one way to solve this problem is to\ninstead measure precision\n. precision is the fraction of detections reported\nby the model that were correct, while recall is the fraction of true events that\nwere detected. a detector that says no one has the disease would achieve perfect\nprecision, but zero recall. a detector that says everyone has the disease would\nachieve perfect recall, but precision equal to the percentage of people who have\nthe disease (0.0001% in our example of a disease that only one people in a million\nhave).\u00a0when using precision and recall, it is common to plot a\n, with\nprecision on the y-axis and recall on the x-axis. the classifier generates a score\nthat is higher if the event to be detected occurred.\u00a0for example, a feedforward\n\npr curve\n\nrecall\n\nand\n\n426\n\n "}, {"Page_number": 442, "text": "chapter 11. practical methodology\n\nnetwork designed to detect a disease outputs \u02c6y = p (y = 1 | x), estimating the\nprobability that a person whose medical results are described by features x has\nthe disease. we choose to report a detection whenever this score exceeds some\nthreshold.\u00a0by varying the threshold, we can trade precision for recall.\u00a0in many\ncases, we wish to summarize the performance of the classifier with a single number\nrather than a curve. to do so, we can convert precision p and recall r into an\nf-score given by\n\nf =\n\n.\n\n(11.1)\n\n2pr\np\nr+\n\nanother option is to report the total area lying beneath the pr curve.\n\nin some applications, it is possible for the machine learning system to refuse to\nmake a decision. this is useful when the machine learning algorithm can estimate\nhow confident it should be about a decision, especially if a wrong decision can\nbe harmful and if a human operator is able to occasionally take over. the street\nview transcription system provides an example of this situation. the task is to\ntranscribe the address number from a photograph in order to associate the location\nwhere the photo was taken with the correct address in a map. because the value\nof the map degrades considerably if the map is inaccurate, it is important to add\nan address only if the transcription is correct. if the machine learning system\nthinks that it is less likely than a human being to obtain the correct transcription,\nthen the best course of action is to allow a human to transcribe the photo instead.\nof course, the machine learning system is only useful if it is able to dramatically\nreduce the amount of photos that the human operators must process. a natural\nperformance metric to use in this situation is coverage. coverage is the fraction of\nexamples for which the machine learning system is able to produce a response. it\nis possible to trade coverage for accuracy. one can always obtain 100% accuracy\nby refusing to process any example, but this reduces the coverage to 0%. for the\nstreet view task, the goal for the project was to reach human-level transcription\naccuracy while maintaining 95% coverage. human-level performance on this task\nis 98% accuracy.\n\nmany other metrics are possible. we can for example, measure click-through\nrates, collect user satisfaction surveys, and so on.\u00a0many specialized application\nareas have application-specific criteria as well.\n\nwhat is important is to determine which performance metric to improve ahead\nof time, then concentrate on improving this metric. without clearly defined goals,\nit can be difficult to tell whether changes to a machine learning system make\nprogress or not.\n\n427\n\n "}, {"Page_number": 443, "text": "chapter 11. practical methodology\n\n11.2 default baseline models\n\nafter choosing performance metrics and goals,\u00a0the next step in any practical\napplication is to establish a reasonable end-to-end system as soon as possible. in\nthis section, we provide recommendations for which algorithms to use as the first\nbaseline approach in various situations. keep in mind that deep learning research\nprogresses quickly, so better default algorithms are likely to become available soon\nafter this writing.\n\ndepending on the complexity of your problem, you may even want to begin\nwithout using deep learning. if your problem has a chance of being solved by\njust choosing a few linear weights correctly, you may want to begin with a simple\nstatistical model like logistic regression.\n\nif you know that your problem falls into an \u201cai-complete\u201d category like object\nrecognition, speech recognition, machine translation, and so on, then you are likely\nto do well by beginning with an appropriate deep learning model.\n\nfirst, choose the general category of model based on the structure of your\ndata. if you want to perform supervised learning with fixed-size vectors as input,\nuse a feedforward network with fully connected layers. if the input has known\ntopological structure (for example, if the input is an image), use a convolutional\nnetwork. in these cases, you should begin by using some kind of piecewise linear\nunit (relus or their generalizations like leaky relus, prelus and maxout). if\nyour input or output is a sequence, use a gated recurrent net (lstm or gru).\n\na reasonable choice of optimization algorithm is sgd with momentum with a\ndecaying learning rate (popular decay schemes that perform better or worse on\ndifferent problems include decaying linearly until reaching a fixed minimum learning\nrate, decaying exponentially, or decreasing the learning rate by a factor of 2-10\neach time validation error plateaus). another very reasonable alternative is adam.\nbatch normalization can have a dramatic effect on optimization performance,\nespecially for convolutional networks and networks with sigmoidal nonlinearities.\nwhile it is reasonable to omit batch normalization from the very first baseline, it\nshould be introduced quickly if optimization appears to be problematic.\n\nunless your training set contains tens of millions of examples or more, you\nshould include some mild forms of regularization from the start. early stopping\nshould be used almost universally. dropout is an excellent regularizer that is easy\nto implement and compatible with many models and training algorithms. batch\nnormalization also sometimes reduces generalization error and allows dropout to\nbe omitted, due to the noise in the estimate of the statistics used to normalize\neach variable.\n\n428\n\n "}, {"Page_number": 444, "text": "chapter 11. practical methodology\n\nif your task is similar to another task that has been studied extensively, you\nwill probably do well by first copying the model and algorithm that is already\nknown to perform best on the previously studied task. you may even want to copy\na trained model from that task. for example, it is common to use the features\nfrom a convolutional network trained on imagenet to solve other computer vision\ntasks (\n\ngirshick et al. 2015\n\n).\n\n,\n\niii\n\na common question is whether to begin by using unsupervised learning, de-\nscribed further in part\n. this is somewhat domain specific. some domains, such\nas natural language processing, are known to benefit tremendously from unsuper-\nvised learning techniques such as learning unsupervised word embeddings. in other\ndomains, such as computer vision, current unsupervised learning techniques do\nnot bring a benefit, except in the semi-supervised setting, when the number of\nlabeled examples is very small (\n). if your\napplication is in a context where unsupervised learning is known to be important,\nthen include it in your first end-to-end baseline. otherwise, only use unsupervised\nlearning in your first attempt if the task you want to solve is unsupervised. you\ncan always try adding unsupervised learning later if you observe that your initial\nbaseline overfits.\n\nkingma et al. 2014 rasmus\n\net al.,\n\n2015\n\n,\n\n;\n\n11.3 determining whether to gather more data\n\nafter the first end-to-end system is established, it is time to measure the perfor-\nmance of the algorithm and determine how to improve it. many machine learning\nnovices are tempted to make improvements by trying out many different algorithms.\nhowever, it is often much better to gather more data than to improve the learning\nalgorithm.\n\nhow does one decide whether to gather more data? first, determine whether\nthe performance on the training set is acceptable. if performance on the training\nset is poor, the learning algorithm is not using the training data that is already\navailable, so there is no reason to gather more data. instead, try increasing the\nsize of the model by adding more layers or adding more hidden units to each layer.\nalso, try improving the learning algorithm, for example by tuning the learning rate\nhyperparameter. if large models and carefully tuned optimization algorithms do\nnot work well, then the problem might be the quality of the training data. the\ndata may be too noisy or may not include the right inputs needed to predict the\ndesired outputs. this suggests starting over, collecting cleaner data or collecting a\nricher set of features.\n\nif the performance on the training set is acceptable, then measure the per-\n\n429\n\n "}, {"Page_number": 445, "text": "chapter 11. practical methodology\n\nformance on a test set.\nif the performance on the test set is also acceptable,\nthen there is nothing left to be done. if test set performance is much worse than\ntraining set performance, then gathering more data is one of the most effective\nsolutions.\u00a0the key considerations are the cost and feasibility of gathering more\ndata, the cost and feasibility of reducing the test error by other means, and the\namount of data that is expected to be necessary to improve test set performance\nsignificantly.\u00a0at large internet companies with millions or billions of users, it is\nfeasible to gather large datasets, and the expense of doing so can be considerably\nless than the other alternatives, so the answer is almost always to gather more\ntraining data. for example, the development of large labeled datasets was one of\nthe most important factors in solving object recognition. in other contexts, such as\nmedical applications, it may be costly or infeasible to gather more data. a simple\nalternative to gathering more data is to reduce the size of the model or improve\nregularization, by adjusting hyperparameters such as weight decay coefficients,\nor by adding regularization strategies such as dropout. if you find that the gap\nbetween train and test performance is still unacceptable even after tuning the\nregularization hyperparameters, then gathering more data is advisable.\n\nwhen deciding whether to gather more data, it is also necessary to decide\nhow much to gather. it is helpful to plot curves showing the relationship between\ntraining set size and generalization error, like in fig.\n. by extrapolating such\ncurves, one can predict how much additional training data would be needed to\nachieve a certain level of performance. usually, adding a small fraction of the total\nnumber of examples will not have a noticeable impact on generalization error. it is\ntherefore recommended to experiment with training set sizes on a logarithmic scale,\nfor example doubling the number of examples between consecutive experiments.\n\n5.4\n\nif gathering much more data is not feasible, the only other way to improve\ngeneralization error is to improve the learning algorithm itself. this becomes the\ndomain of research and not the domain of advice for applied practitioners.\n\n11.4 selecting hyperparameters\n\nmost deep learning algorithms come with many hyperparameters that control many\naspects of the algorithm\u2019s behavior. some of these hyperparameters affect the time\nand memory cost of running the algorithm. some of these hyperparameters affect\nthe quality of the model recovered by the training process and its ability to infer\ncorrect results when deployed on new inputs.\n\nthere are two basic approaches to choosing these hyperparameters: choosing\nthem manually and choosing them automatically. choosing the hyperparameters\n\n430\n\n "}, {"Page_number": 446, "text": "chapter 11. practical methodology\n\nmanually requires understanding what the hyperparameters do and how machine\nlearning models achieve good generalization. automatic hyperparameter selection\nalgorithms greatly reduce the need to understand these ideas, but they are often\nmuch more computationally costly.\n\n11.4.1 manual hyperparameter tuning\n\nto set hyperparameters manually, one must understand the relationship between\nhyperparameters, training error, generalization error and computational resources\n(memory and runtime). this means establishing a solid foundation on the fun-\ndamental ideas concerning the effective capacity of a learning algorithm from\nchapter\n\n.5\n\nthe goal of manual hyperparameter search is usually to find the lowest general-\nization error subject to some runtime and memory budget. we do not discuss how\nto determine the runtime and memory impact of various hyperparameters here\nbecause this is highly platform-dependent.\n\nthe primary goal of manual hyperparameter search is to adjust the effective\ncapacity of the model to match the complexity of the task. effective capacity\nis constrained by three factors:\u00a0the representational capacity of the model, the\nability of the learning algorithm to successfully minimize the cost function used to\ntrain the model, and the degree to which the cost function and training procedure\nregularize the model. a model with more layers and more hidden units per layer has\nhigher representational capacity\u2014it is capable of representing more complicated\nfunctions. it can not necessarily actually learn all of these functions though, if\nthe training algorithm cannot discover that certain functions do a good job of\nminimizing the training cost, or if regularization terms such as weight decay forbid\nsome of these functions.\n\nthe generalization error typically follows a u-shaped curve when plotted as\na function of one of the hyperparameters, as in fig.\n. at one extreme, the\nhyperparameter value corresponds to low capacity, and generalization error is high\nbecause training error is high. this is the underfitting regime. at the other extreme,\nthe hyperparameter value corresponds to high capacity, and the generalization\nerror is high because the gap between training and test error is high. somewhere\nin the middle lies the optimal model capacity, which achieves the lowest possible\ngeneralization error, by adding a medium generalization gap to a medium amount\nof training error.\n\n5.3\n\nfor some hyperparameters, overfitting occurs when the value of the hyper-\nparameter is large.\u00a0the number of hidden units in a layer is one such example,\n\n431\n\n "}, {"Page_number": 447, "text": "chapter 11. practical methodology\n\nbecause increasing the number of hidden units increases the capacity of the model.\nfor some hyperparameters, overfitting occurs when the value of the hyperparame-\nter is small. for example, the smallest allowable weight decay coefficient of zero\ncorresponds to the greatest effective capacity of the learning algorithm.\n\nnot every hyperparameter will be able to explore the entire u-shaped curve.\nmany hyperparameters are discrete, such as the number of units in a layer or the\nnumber of linear pieces in a maxout unit, so it is only possible to visit a few points\nalong the curve. some hyperparameters are binary. usually these hyperparameters\nare switches that\u00a0specify\u00a0whether or not to\u00a0use some optional component of\nthe learning algorithm, such as a preprocessing step that normalizes the input\nfeatures by subtracting their mean and dividing by their standard deviation. these\nhyperparameters can only explore two points on the curve. other hyperparameters\nhave some minimum or maximum value that prevents them from exploring some\npart of the curve. for example, the minimum weight decay coefficient is zero. this\nmeans that if the model is underfitting when weight decay is zero, we can not enter\nthe overfitting region by modifying the weight decay coefficient. in other words,\nsome hyperparameters can only subtract capacity.\n\nif you\nthe learning rate is perhaps the most important hyperparameter.\nit\u00a0con-\nhave\u00a0time to\u00a0tune only\u00a0one hyperparameter, tune\u00a0the learning\u00a0rate.\ntrols the effective capacity of the model in a more complicated way than other\nhyperparameters\u2014the effective capacity of the model is highest when the learning\nrate is correct for the optimization problem, not when the learning rate is espe-\ncially large or especially small. the learning rate has a u-shaped curve for training\nerror, illustrated in fig.\n. when the learning rate is too large, gradient descent\ncan inadvertently increase rather than decrease the training error. in the idealized\nquadratic case, this occurs if the learning rate is at least twice as large as its\noptimal value (\n). when the learning rate is too small, training\nis not only slower, but may become permanently stuck with a high training error.\nthis effect is poorly understood (it would not happen for a convex loss function).\n\nlecun et al. 1998a\n\n11.1\n\n,\n\ntuning the parameters other than the learning rate requires monitoring both\ntraining and test error to diagnose whether your model is overfitting or underfitting,\nthen adjusting its capacity appropriately.\n\nif your error on the training set is higher than your target error rate, you have\nno choice but to increase capacity. if you are not using regularization and you are\nconfident that your optimization algorithm is performing correctly, then you must\nadd more layers to your network or add more hidden units. unfortunately, this\nincreases the computational costs associated with the model.\n\nif your error on the test set is higher than than your target error rate, you can\n\n432\n\n "}, {"Page_number": 448, "text": "chapter 11. practical methodology\n\n8\n\n7\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\nr\no\nr\nr\ne\n\ng\nn\n\ni\n\nn\ni\na\nr\nt\n\n0\n10\u22122\n\n10\u22121\n\n100\n\nlearning rate (logarithmic scale)\n\nfigure 11.1: typical relationship between the learning rate and the training error. notice\nthe sharp rise in error when the learning is above an optimal value. this is for a fixed\ntraining time, as a smaller learning rate may sometimes only slow down training by a\nfactor proportional to the learning rate reduction.\u00a0generalization error can follow this\ncurve or be complicated by regularization effects arising out of having a too large or\ntoo small learning rates, since poor optimization can, to some degree, reduce or prevent\noverfitting, and even points with equivalent training error can have different generalization\nerror.\n\nnow take two kinds of actions. the test error is the sum of the training error and\nthe gap between training and test error. the optimal test error is found by trading\noff these quantities. neural networks typically perform best when the training\nerror is very low (and thus, when capacity is high) and the test error is primarily\ndriven by the gap between train and test error.\u00a0your goal is to reduce this gap\nwithout increasing training error faster than the gap decreases. to reduce the gap,\nchange regularization hyperparameters to reduce effective model capacity, such as\nby adding dropout or weight decay. usually the best performance comes from a\nlarge model that is regularized well, for example by using dropout.\n\nmost hyperparameters can be set by reasoning about whether they increase or\n\ndecrease model capacity. some examples are included in table\n\n.\n11.1\n\nwhile manually tuning hyperparameters, do not lose sight of your end goal:\ngood performance on the test set. adding regularization is only one way to achieve\nthis goal. as long as you have low training error, you can always reduce general-\nization error by collecting more training data. the brute force way to practically\nguarantee success is to continually increase model capacity and training set size\nuntil the task is solved. this approach does of course increase the computational\ncost of training and inference, so it is only feasible given appropriate resources. in\n\n433\n\n "}, {"Page_number": 449, "text": "chapter 11. practical methodology\n\nreason\n\ncaveats\n\nhyperparameter\n\nnumber of hid-\nden units\n\nincreases\ncapacity\nwhen. . .\nincreased\n\nlearning rate\n\ntuned op-\ntimally\n\nconvolution ker-\nnel width\n\nincreased\n\nincreasing the number of\nhidden units increases the\nrepresentational capacity\nof the model.\n\nan improper learning rate,\nwhether\u00a0too high\u00a0or too\nlow, results in a model\nwith low effective capacity\ndue to optimization failure\nincreasing the kernel width\nincreases the number of pa-\nrameters in the model\n\nincreasing\nthe number\nof hidden units\u00a0increases\nboth the time and memory\ncost of essentially every op-\neration on the model.\n\na wider kernel results in\na narrower output dimen-\nsion, reducing model ca-\npacity unless you use im-\nplicit zero padding to re-\nduce this effect. wider\nkernels require more mem-\nory for parameter storage\nand increase runtime, but\na narrower output reduces\nmemory cost.\nincreased time and mem-\nory cost of most opera-\ntions.\n\nimplicit\npadding\n\nzero\n\nweight decay co-\nefficient\n\ndropout rate\n\nincreased adding implicit zeros be-\nfore convolution keeps the\nrepresentation size large\n\ndecreased decreasing the weight de-\ncay coefficient frees the\nmodel parameters to be-\ncome larger\n\ndecreased dropping units less often\ngives the units more oppor-\ntunities to \u201cconspire\u201d with\neach other to fit the train-\ning set\n\ntable 11.1: the effect of various hyperparameters on model capacity.\n\n434\n\n "}, {"Page_number": 450, "text": "chapter 11. practical methodology\n\nprinciple, this approach could fail due to optimization difficulties, but for many\nproblems optimization does not seem to be a significant barrier, provided that the\nmodel is chosen appropriately.\n\n11.4.2 automatic hyperparameter optimization algorithms\n\nthe ideal learning algorithm just takes a dataset and outputs a function, without\nrequiring hand-tuning of hyperparameters. the popularity of several learning\nalgorithms such as logistic regression and svms stems in part from their ability to\nperform well with only one or two tuned hyperparameters. neural networks can\nsometimes perform well with only a small number of tuned hyperparameters, but\noften benefit significantly from tuning of forty or more hyperparameters. manual\nhyperparameter tuning can work very well when the user has a good starting point,\nsuch as one determined by others having worked on the same type of application\nand architecture, or when the user has months or years of experience in exploring\nhyperparameter values for neural networks applied to similar tasks. however,\nfor many applications, these starting points are not available.\nin these cases,\nautomated algorithms can find useful values of the hyperparameters.\n\nif we think about the way in which the user of a learning algorithm searches\nfor good values of the hyperparameters, we realize that an optimization is taking\nplace: we are trying to find a value of the hyperparameters that optimizes an\nobjective function, such as validation error, sometimes under constraints (such as a\nbudget for training time, memory or recognition time). it is therefore possible, in\nprinciple, to develop hyperparameter optimization algorithms that wrap a learning\nalgorithm and choose its hyperparameters, thus hiding the hyperparameters of the\nlearning algorithm from the user. unfortunately, hyperparameter optimization\nalgorithms often have their own hyperparameters, such as the range of values that\nshould be explored for each of the learning algorithm\u2019s hyperparameters. however,\nthese secondary hyperparameters are usually easier to choose, in the sense that\nacceptable performance may be achieved on a wide range of tasks using the same\nsecondary hyperparameters for all tasks.\n\n11.4.3 grid search\n\nwhen there are three or fewer hyperparameters, the common practice is to perform\ngrid search. for each hyperparameter, the user selects a small finite set of values to\nexplore. the grid search algorithm then trains a model for every joint specification\nof hyperparameter values in the cartesian product of the set of values for each\nindividual hyperparameter. the experiment that yields the best validation set\n\n435\n\n "}, {"Page_number": 451, "text": "chapter 11. practical methodology\n\ngrid\n\nrandom\n\n(right)\n\nfigure 11.2: comparison of grid search and random search. for illustration purposes we\ndisplay two hyperparameters but we are typically interested in having many more. (left)\nto perform grid search, we provide a set of values for each hyperparameter. the search\nalgorithm runs training for every joint hyperparameter setting in the cross product of these\nsets.\nto perform random search, we provide a probability distribution over joint\nhyperparameter configurations. usually most of these hyperparameters are independent\nfrom each other. common choices for the distribution over a single hyperparameter include\nuniform and log-uniform (to sample from a log-uniform distribution, take the exp of a\nsample from a uniform distribution). the search algorithm then randomly samples joint\nhyperparameter configurations and runs training with each of them. both grid search\nand random search evaluate the validation set error and return the best configuration.\nthe figure illustrates the typical case where only some hyperparameters have a significant\ninfluence on the result. in this illustration, only the hyperparameter on the horizontal axis\nhas a significant effect. grid search wastes an amount of computation that is exponential\nin the number of non-influential hyperparameters, while random search tests a unique\nvalue of every influential hyperparameter on nearly every trial.\n\n436\n\n "}, {"Page_number": 452, "text": "chapter 11. practical methodology\n\nerror is then chosen as having found the best hyperparameters. see the left of\nfig.\n\nfor an illustration of a grid of hyperparameter values.\n\n11.2\n\nhow should the lists of values to search over be chosen? in the case of numerical\n(ordered) hyperparameters, the smallest and largest element of each list is chosen\nconservatively, based on prior experience with similar experiments, to make sure\nthat the optimal value is very likely to be in the selected range. typically, a\ngrid search involves picking values approximately on a logarithmic scale, e.g., a\nlearning rate taken within the set {.1, .01,10\u22123, 10\u22124, 10\u22125}, or a number of hidden\nunits taken with the set\n\n,\n\n,\n\n,\n\n}\n{\n.\n50 100 200 500 1000 2000\n\n,\n\n,\n\ngrid search usually performs best when it is performed repeatedly. for example,\nsuppose that we ran a grid search over a hyperparameter \u03b1 using values of {\u22121, 0, 1}.\nif the best value found is\n\u03b1\nlies and we should shift the grid and run another search with \u03b1 in, for example,\n{1, 2, 3}. if we find that the best value of \u03b1 is\n, then we may wish to refine our\nestimate by zooming in and running a grid search over\n\n, then we underestimated the range in which the best\n1\n\n0\n\n{\u2212\n\n}\n.\n. ,\n, .\n1 0 1\n\nthe obvious problem with grid search is that its computational cost grows\nexponentially with the number of hyperparameters. if there are m hyperparameters,\neach taking at most n values, then the number of training and evaluation trials\nrequired grows as o (nm). the trials may be run in parallel and exploit loose\nparallelism (with almost no need for communication between different machines\ncarrying out the search) unfortunately, due to the exponential cost of grid search,\neven parallelization may not provide a satisfactory size of search.\n\n11.4.4 random search\n\nfortunately, there is an alternative to grid search that is as simple to program, more\nconvenient to use, and converges much faster to good values of the hyperparameters:\nrandom search (\n\nbergstra and bengio 2012\n\n).\n\n,\n\na random search proceeds as follows. first we define a marginal distribution\nfor each hyperparameter, e.g., a bernoulli or multinoulli for binary or discrete\nhyperparameters, or a uniform distribution on a log-scale for positive real-valued\nhyperparameters. for example,\n\nlog learning rate\n\n_\n\n_\n\nlearning rate\n\n_\n\nu( 1, 5)\n\n\u223c \u2212 \u2212\n= 10log learning rate\n\n_\n\n_\n\n.\n\n(11.2)\n\n(11.3)\n\nwhere u(a, b ) indicates a sample of the uniform distribution in the interval (a, b).\nsimilarly the log number of hidden units\nmay be sampled from u(log (50),\nlog(2000)).\n\n_ _\n\n_\n\n_\n\n437\n\n "}, {"Page_number": 453, "text": "chapter 11. practical methodology\n\nunlike in the case of a grid search, one should not discretize or bin the\nvalues of the hyperparameters. this allows one to explore a larger set of values, and\ndoes not incur additional computational cost. in fact, as illustrated in fig.\n, a\nrandom search can be exponentially more efficient than a grid search, when there\nare several hyperparameters that do not strongly affect the performance measure.\nthis is studied at length in\n), who found that random\nsearch reduces the validation set error much faster than grid search, in terms of\nthe number of trials run by each method.\n\nbergstra and bengio 2012\n\n11.2\n\n(\n\nas with grid search, one may often want to run repeated versions of random\n\nsearch, to refine the search based on the results of the first run.\n\nthe main reason why random search finds good solutions faster than grid search\nis that the there are no wasted experimental runs, unlike in the case of grid search,\nwhen two values of a hyperparameter (given values of the other hyperparameters)\nwould give the same result. in the case of grid search, the other hyperparameters\nwould have the same values for these two runs, whereas with random search, they\nwould usually have different values. hence if the change between these two values\ndoes not marginally make much difference in terms of validation set error, grid\nsearch will unnecessarily repeat two equivalent experiments while random search\nwill still give two independent explorations of the other hyperparameters.\n\n11.4.5 model-based hyperparameter optimization\n\nthe search for good hyperparameters can be cast as an optimization problem.\nthe decision variables are the hyperparameters. the cost to be optimized is the\nvalidation set error that results from training using these hyperparameters. in\nsimplified settings where it is feasible to compute the gradient of some differentiable\nerror measure on the validation set with respect to the hyperparameters, we can\nsimply follow this gradient (\nbengio et al. 1999 bengio 2000 maclaurin et al.\n,\n2015). unfortunately, in most practical settings, this gradient is unavailable, either\ndue to its high computation and memory cost, or due to hyperparameters having\nintrinsically non-differentiable interactions with the validation set error, as in the\ncase of discrete-valued hyperparameters.\n\n,\n\n;\n\n,\n\n;\n\nto compensate for this lack of a gradient, we can build a model of the validation\nset error, then propose new hyperparameter guesses by performing optimization\nwithin this model. most model-based algorithms for hyperparameter search use a\nbayesian regression model to estimate both the expected value of the validation set\nerror for each hyperparameter and the uncertainty around this expectation. opti-\nmization thus involves a tradeoff between exploration (proposing hyperparameters\n\n438\n\n "}, {"Page_number": 454, "text": "chapter 11. practical methodology\n\nfor which there is high uncertainty, which may lead to a large improvement but may\nalso perform poorly) and exploitation (proposing hyperparameters which the model\nis confident will perform as well as any hyperparameters it has seen so far\u2014usually\nhyperparameters that are very similar to ones it has seen before). contemporary\napproaches to hyperparameter optimization include spearmint (\n),\ntpe (\n\nbergstra et al. 2011\n\nhutter et al. 2011\n\nsnoek et al. 2012\n\n) and smac (\n\n).\n\n,\n\n,\n\n,\n\ncurrently, we cannot unambiguously recommend bayesian hyperparameter\noptimization as an established tool for achieving better deep learning results or\nfor obtaining those results with less effort. bayesian hyperparameter optimization\nsometimes performs comparably to human experts, sometimes better, but fails\ncatastrophically on other problems. it may be worth trying to see if it works on\na particular problem but is not yet sufficiently mature or reliable. that being\nsaid, hyperparameter optimization is an important field of research that, while\noften driven primarily by the needs of deep learning, holds the potential to benefit\nnot only the entire field of machine learning but the discipline of engineering in\ngeneral.\n\none drawback common to most hyperparameter optimization algorithms with\nmore sophistication than random search is that they require for a training ex-\nperiment to run to completion before they are able to extract any information\nfrom the experiment. this is much less efficient, in the sense of how much infor-\nmation can be gleaned early in an experiment, than manual search by a human\npractitioner, since one can usually tell early on if some set of hyperparameters is\ncompletely pathological.\n) have introduced an early version\nof an algorithm that maintains a set of multiple experiments. at various time\npoints, the hyperparameter optimization algorithm can choose to begin a new\nexperiment, to \u201cfreeze\u201d a running experiment that is not promising, or to \u201cthaw\u201d\nand resume an experiment that was earlier frozen but now appears promising given\nmore information.\n\nswersky et al. 2014\n\n(\n\n11.5 debugging strategies\n\nwhen a machine learning system performs poorly, it is usually difficult to tell\nwhether the poor performance is intrinsic to the algorithm itself or whether there\nis a bug in the implementation of the algorithm.\u00a0machine learning systems are\ndifficult to debug for a variety of reasons.\n\nin most cases, we do not know a priori what the intended behavior of the\nalgorithm is. in fact, the entire point of using machine learning is that it will\ndiscover useful behavior that we were not able to specify ourselves. if we train a\n\n439\n\n "}, {"Page_number": 455, "text": "chapter 11. practical methodology\n\nneural network on a\nclassification task and it achieves 5% test error, we have\nno straightforward way of knowing if this is the expected behavior or sub-optimal\nbehavior.\n\nnew\n\na further difficulty is that most machine learning models have multiple parts\nthat are each adaptive. if one part is broken, the other parts can adapt and still\nachieve roughly acceptable performance. for example, suppose that we are training\na neural net with several layers parametrized by weights w and biases b. suppose\nfurther that we have manually implemented the gradient descent rule for each\nparameter separately, and we made an error in the update for the biases:\n\nb\n\nb\u2190 \u2212 \u03b1\n\n(11.4)\n\nwhere \u03b1 is the learning rate. this erroneous update does not use the gradient at\nall. it causes the biases to constantly become negative throughout learning, which\nis clearly not a correct implementation of any reasonable learning algorithm. the\nbug may not be apparent just from examining the output of the model though.\ndepending on the distribution of the input, the weights may be able to adapt to\ncompensate for the negative biases.\n\nmost debugging strategies for neural nets are designed to get around one or\nboth of these two difficulties. either we design a case that is so simple that the\ncorrect behavior actually can be predicted, or we design a test that exercises one\npart of the neural net implementation in isolation.\n\nsome important debugging tests include:\n\nvisualize the model in action : when training a model to detect objects in\nimages, view some images with the detections proposed by the model displayed\nsuperimposed on the image. when training a generative model of speech, listen to\nsome of the speech samples it produces. this may seem obvious, but it is easy to\nfall into the practice of only looking at quantitative performance measurements\nlike accuracy or log-likelihood. directly observing the machine learning model\nperforming its task will help you to determine whether the quantitative performance\nnumbers it achieves seem reasonable. evaluation bugs can be some of the most\ndevastating bugs because they can mislead you into believing your system is\nperforming well when it is not.\n\nvisualize the worst mistakes :\u00a0most models are able to output some sort of\nconfidence measure for the task they perform. for example, classifiers based on a\nsoftmax output layer assign a probability to each class. the probability assigned\nto the most likely class thus gives an estimate of the confidence the model has in\nits classification decision. typically, maximum likelihood training results in these\nvalues being overestimates rather than accurate probabilities of correct prediction,\n\n440\n\n "}, {"Page_number": 456, "text": "chapter 11. practical methodology\n\nbut they are somewhat useful in the sense that examples that are actually less\nlikely to be correctly labeled receive smaller probabilities under the model. by\nviewing the training set examples that are the hardest to model correctly, one can\noften discover problems with the way the data has been preprocessed or labeled.\nfor example, the street view transcription system originally had a problem where\nthe address number detection system would crop the image too tightly and omit\nsome of the digits. the transcription network then assigned very low probability\nto the correct answer on these images. sorting the images to identify the most\nconfident mistakes showed that there was a systematic problem with the cropping.\nmodifying the detection system to crop much wider images resulted in much better\nperformance of the overall system, even though the transcription network needed\nto be able to process greater variation in the position and scale of the address\nnumbers.\n\nreasoning about software using train and test error: it is often difficult to\ndetermine whether the underlying software is correctly implemented. some clues\ncan be obtained from the train and test error. if training error is low but test error\nis high, then it is likely that that the training procedure works correctly, and the\nmodel is overfitting for fundamental algorithmic reasons. an alternative possibility\nis that the test error is measured incorrectly due to a problem with saving the\nmodel after training then reloading it for test set evaluation, or if the test data\nwas prepared differently from the training data. if both train and test error are\nhigh, then it is difficult to determine whether there is a software defect or whether\nthe model is underfitting due to fundamental algorithmic reasons. this scenario\nrequires further tests, described next.\n\nfit a tiny dataset: if you have high error on the training set, determine whether\nit is due to genuine underfitting or due to a software defect. usually even small\nmodels can be guaranteed to be able fit a sufficiently small dataset. for example,\na classification dataset with only one example can be fit just by setting the biases\nof the output layer correctly. usually if you cannot train a classifier to correctly\nlabel a single example, an autoencoder to successfully reproduce a single example\nwith high fidelity, or a generative model to consistently emit samples resembling a\nsingle example, there is a software defect preventing successful optimization on the\ntraining set. this test can be extended to a small dataset with few examples.\n\ncompare back-propagated derivatives to numerical derivatives: if you are using\na software framework that requires you to implement your own gradient com-\nputations, or if you are adding a new operation to a differentiation library and\nmust define its bprop method, then a common source of error is implementing this\ngradient expression incorrectly. one way to verify that these derivatives are correct\n\n441\n\n "}, {"Page_number": 457, "text": "chapter 11. practical methodology\n\nis to compare the derivatives computed by your implementation of automatic\ndifferentiation to the derivatives computed by a finite differences. because\n\nx\n\nf0( ) = lim\n(cid:115)\u21920\n\n( + ) \u2212 ( )\nf x\nf x\n\n(cid:115)\n(cid:115)\n\n,\n\n(11.5)\n\nwe can approximate the derivative by using a small, finite :(cid:115)\n\nf0( ) x \u2248\n\n( + ) \u2212 ( )\nf x\nf x\n\n(cid:115)\n\n(cid:115)\n\n.\n\n(11.6)\n\nwe can improve the accuracy of the approximation by using the centered difference:\n\nf0( ) x \u2248\n\nf x( + 1\n) \u2212 ( \u2212 1\n2(cid:115))\n2(cid:115)\n(cid:115)\n\nf x\n\n.\n\n(11.7)\n\nthe perturbation size (cid:115) must chosen to be large enough to ensure that the pertur-\nbation is not rounded down too much by finite-precision numerical computations.\n\nusually, we will want to test the gradient or jacobian of a vector-valued function\ng : rm \u2192 rn. unfortunately, finite differencing only allows us to take a single\nderivative at a time. we can either run finite differencing mn times to evaluate all\nof the partial derivatives of g, or we can apply the test to a new function that uses\nrandom projections at both the input and output of g. for example, we can apply\nour test of the implementation of the derivatives to f(x) where f (x) = ut g(vx),\nwhere u and v are randomly chosen vectors. computing f0(x) correctly requires\nbeing able to back-propagate through g correctly, yet is efficient to do with finite\ndifferences because f has only a single input and a single output. it is usually\na good idea to repeat this test for more than one value of u and v to reduce\nthe chance that the test overlooks mistakes that are orthogonal to the random\nprojection.\n\nif one has access to numerical computation on complex numbers, then there is\na very efficient way to numerically estimate the gradient by using complex numbers\nas input to the function (squire and trapp 1998\n). the method is based on the\nobservation that\n\n,\n\nf x\n\n( + ) =  ( ) +\n\nf x\n\ni(cid:115)\n\ni(cid:115)f\n\n0( ) + (\n\no (cid:115)2)\n\nx\n\nreal( ( + )) =  ( ) + (\n\nf x\n\nf x\n\ni(cid:115)\n\no (cid:115)2)\n,\n\nimag(\n\n) = f0( ) + (\n\no (cid:115)2),\n\nx\n\n(11.8)\n\n(11.9)\n\nf x\n\n( + )\ni(cid:115)\n\n(cid:115)\n\nwhere i = \u221a\u22121. unlike in the real-valued case above, there is no cancellation effect\ndue to taking the difference between the value of f at different points. this allows\nthe use of tiny values of (cid:115) like (cid:115) = 10\u2212150, which make the o((cid:115)2) error insignificant\nfor all practical purposes.\n\n442\n\n "}, {"Page_number": 458, "text": "chapter 11. practical methodology\n\nmonitor histograms of activations and gradient: it is often useful to visualize\nstatistics of neural network activations and gradients, collected over a large amount\nof training iterations (maybe one epoch). the pre-activation value of hidden units\ncan tell us if the units saturate, or how often they do. for example, for rectifiers,\nhow often are they off? are there units that are always off? for tanh units,\nthe average of the absolute value of the pre-activations tells us how saturated\nthe unit is. in a deep network where the propagated gradients quickly grow or\nquickly vanish, optimization may be hampered. finally, it is useful to compare the\nmagnitude of parameter gradients to the magnitude of the parameters themselves.\nas suggested by\n), we would like the magnitude of parameter updates\nover a minibatch to represent something like 1% of the magnitude of the parameter,\nnot 50% or 0.001% (which would make the parameters move too slowly). it may\nbe that some groups of parameters are moving at a good pace while others are\nstalled. when the data is sparse (like in natural language), some parameters may\nbe very rarely updated, and this should be kept in mind when monitoring their\nevolution.\n\nbottou 2015\n\n(\n\niii\n\nfinally, many deep learning algorithms provide some sort of guarantee about\nthe results produced at each step. for example, in part\n, we will see some\napproximate inference algorithms that work by using algebraic solutions to op-\ntimization problems. typically these can be debugged by testing each of their\nguarantees. some guarantees that some optimization algorithms offer include that\nthe objective function will never increase after one step of the algorithm, that\nthe gradient with respect to some subset of variables will be zero after each step\nof the algorithm, and that the gradient with respect to all variables will be zero\nat convergence. usually due to rounding error, these conditions will not hold\nexactly in a digital computer, so the debugging test should include some tolerance\nparameter.\n\n11.6 example: multi-digit number recognition\n\nto provide an end-to-end description of how to apply our design methodology\nin practice, we present a brief account of the street view transcription system,\nfrom the point of view of designing the deep learning components. obviously,\nmany other components of the complete system, such as the street view cars, the\ndatabase infrastructure, and so on, were of paramount importance.\n\nfrom the point of view of the machine learning task, the process began with\ndata collection.\u00a0the cars collected the raw data and human operators provided\nlabels. the transcription task was preceded by a significant amount of dataset\n\n443\n\n "}, {"Page_number": 459, "text": "chapter 11. practical methodology\n\ncuration, including using other machine learning techniques to detect the house\nnumbers prior to transcribing them.\n\nthe transcription project began with a choice of performance metrics and\ndesired values for these metrics.\u00a0an important general principle is to tailor the\nchoice of metric to the business goals for the project. because maps are only useful\nif they have high accuracy, it was important to set a high accuracy requirement\nfor this project.\u00a0specifically, the goal was to obtain human-level, 98% accuracy.\nthis level of accuracy may not always be feasible to obtain. in order to reach\nthis level of accuracy, the street view transcription system sacrifices coverage.\ncoverage thus became the main performance metric optimized during the project,\nwith accuracy held at 98%. as the convolutional network improved, it became\npossible to reduce the confidence threshold below which the network refuses to\ntranscribe the input, eventually exceeding the goal of 95% coverage.\n\nafter choosing quantitative goals, the next step in our recommended methodol-\nogy is to rapidly establish a sensible baseline system. for vision tasks, this means a\nconvolutional network with rectified linear units. the transcription project began\nwith such a model. at the time, it was not common for a convolutional network\nto output a sequence of predictions. in order to begin with the simplest possible\nbaseline, the first implementation of the output layer of the model consisted of n\ndifferent softmax units to predict a sequence of n characters. these softmax units\nwere trained exactly the same as if the task were classification, with each softmax\nunit trained independently.\n\nour recommended methodology is to iteratively refine the baseline and test\nwhether each change makes an improvement. the first change to the street view\ntranscription system was motivated by a theoretical understanding of the coverage\nmetric and the structure of the data. specifically, the network refuses to classify\nan input x whenever the probability of the output sequence p(y x|\n) < t for\nsome threshold t. initially, the definition of p(y x|\n) was ad-hoc, based on simply\nmultiplying all of the softmax outputs together. this motivated the development\nof a specialized output layer and cost function that actually computed a principled\nlog-likelihood. this approach allowed the example rejection mechanism to function\nmuch more effectively.\n\nat this point, coverage was still below 90%, yet there were no obvious theoretical\nproblems with the approach. our methodology therefore suggests to instrument\nthe train and test set performance in order to determine whether the problem\nis underfitting or overfitting. in this case, train and test set error were nearly\nidentical. indeed, the main reason this project proceeded so smoothly was the\navailability of a dataset with tens of millions of labeled examples. because train\n\n444\n\n "}, {"Page_number": 460, "text": "chapter 11. practical methodology\n\nand test set error were so similar, this suggested that the problem was either due\nto underfitting or due to a problem with the training data. one of the debugging\nstrategies we recommend is to visualize the model\u2019s worst errors. in this case, that\nmeant visualizing the incorrect training set transcriptions that the model gave the\nhighest confidence. these proved to mostly consist of examples where the input\nimage had been cropped too tightly, with some of the digits of the address being\nremoved by the cropping operation. for example, a photo of an address \u201c1849\u201d\nmight be cropped too tightly, with only the \u201c849\u201d remaining visible. this problem\ncould have been resolved by spending weeks improving the accuracy of the address\nnumber detection system responsible for determining the cropping regions. instead,\nthe team took a much more practical decision, to simply expand the width of the\ncrop region to be systematically wider than the address number detection system\npredicted. this single change added ten percentage points to the transcription\nsystem\u2019s coverage.\n\nfinally, the last few percentage points of performance came from adjusting\nhyperparameters. this mostly consisted of making the model larger while main-\ntaining some restrictions on its computational cost. because train and test error\nremained roughly equal, it was always clear that any performance deficits were due\nto underfitting, as well as due to a few remaining problems with the dataset itself.\n\noverall, the transcription project was a great success, and allowed hundreds of\nmillions of addresses to be transcribed both faster and at lower cost than would\nhave been possible via human effort.\n\nwe hope that the design principles described in this chapter will lead to many\n\nother similar successes.\n\n445\n\n "}, {"Page_number": 461, "text": "chapter 12\n\napplications\n\nin this chapter, we describe how to use deep learning to solve applications in com-\nputer vision, speech recognition, natural language processing, and other application\nareas of commercial interest. we begin by discussing the large scale neural network\nimplementations required for most serious ai applications. next, we review several\nspecific application areas that deep learning has been used to solve.\u00a0while one\ngoal of deep learning is to design algorithms that are capable of solving a broad\nvariety of tasks, so far some degree of specialization is needed. for example, vision\ntasks require processing a large number of input features (pixels) per example.\nlanguage tasks require modeling a large number of possible values (words in the\nvocabulary) per input feature.\n\n12.1 large scale deep learning\n\ndeep learning is based on the philosophy of connectionism: while an individual\nbiological neuron or an individual feature in a machine learning model is not\nintelligent, a large population of these neurons or features acting together can\nexhibit intelligent behavior. it truly is important to emphasize the fact that the\nnumber of neurons must be large. one of the key factors responsible for the\nimprovement in neural network\u2019s accuracy and the improvement of the complexity\nof tasks they can solve between the 1980s and today is the dramatic increase in\nthe size of the networks we use. as we saw in sec.\n, network sizes have grown\nexponentially for the past three decades, yet artificial neural networks are only as\nlarge as the nervous systems of insects.\n\n1.2.3\n\nbecause the size of neural networks is of paramount importance, deep learning\n\n446\n\n "}, {"Page_number": 462, "text": "chapter 12. applications\n\nrequires high performance hardware and software infrastructure.\n\n12.1.1 fast cpu implementations\n\ntraditionally, neural networks were trained using the cpu of a single machine.\ntoday, this approach is generally considered insufficient. we now mostly use gpu\ncomputing or the cpus of many machines networked together. before moving to\nthese expensive setups, researchers worked hard to demonstrate that cpus could\nnot manage the high computational workload required by neural networks.\n\na description of how to implement efficient numerical cpu code is beyond\nthe scope of this book, but we emphasize here that careful implementation for\nspecific cpu families can yield large improvements. for example, in 2011, the best\ncpus available could run neural network workloads faster when using fixed-point\narithmetic rather than floating-point arithmetic. by creating a carefully tuned\n) obtained a 3\u00d7 speedup over\nfixed-point implementation, vanhoucke\na strong floating-point system. each new model of cpu has different performance\ncharacteristics, so sometimes floating-point implementations can be faster too.\nthe important principle is that careful specialization of numerical computation\nroutines can yield a large payoff. other strategies, besides choosing whether to use\nfixed or floating point, include optimizing data structures to avoid cache misses\nand using vector instructions. many machine learning researchers neglect these\nimplementation details, but when the performance of an implementation restricts\nthe size of the model, the accuracy of the model suffers.\n\net al. (\n\n2011\n\n12.1.2 gpu implementations\n\nmost modern neural network implementations are based on graphics processing\nunits. graphics processing units (gpus) are specialized hardware components\nthat were originally developed for graphics applications. the consumer market for\nvideo gaming systems spurred development of graphics processing hardware. the\nperformance characteristics needed for good video gaming systems turn out to be\nbeneficial for neural networks as well.\n\nvideo game rendering requires performing many operations in parallel quickly.\nmodels of\u00a0characters\u00a0and environments\u00a0are specified\u00a0in terms of\u00a0lists of\u00a03-d\ncoordinates of vertices. graphics cards must perform matrix multiplication and\ndivision on many vertices in parallel to convert these 3-d coordinates into 2-d\non-screen coordinates. the graphics card must then perform many computations\nat each pixel in parallel to determine the color of each pixel.\u00a0in both cases, the\n\n447\n\n "}, {"Page_number": 463, "text": "chapter 12. applications\n\ncomputations are fairly simple and do not involve much branching compared to\nthe computational workload that a cpu usually encounters. for example, each\nvertex in the same rigid object will be multiplied by the same matrix; there is no\nneed to evaluate an if statement per-vertex to determine which matrix to multiply\nby. the computations are also entirely independent of each other, and thus may\nbe parallelized easily. the computations also involve processing massive buffers of\nmemory, containing bitmaps describing the texture (color pattern) of each object\nto be rendered. together, this results in graphics cards having been designed to\nhave a high degree of parallelism and high memory bandwidth, at the cost of\nhaving a lower clock speed and less branching capability relative to traditional\ncpus.\n\nneural network algorithms require the same performance characteristics as the\nreal-time graphics algorithms described above. neural networks usually involve\nlarge and numerous buffers of parameters, activation values, and gradient values,\neach of which must be completely updated during every step of training. these\nbuffers are large enough to fall outside the cache of a traditional desktop computer\nso the memory bandwidth of the system often becomes the rate limiting factor.\ngpus offer a compelling advantage over cpus due to their high memory bandwidth.\nneural network training algorithms typically do not involve much branching or\nsophisticated control, so they are appropriate for gpu hardware. since neural\nnetworks can be divided into multiple individual \u201cneurons\u201d that can be processed\nindependently from the other neurons in the same layer, neural networks easily\nbenefit from the parallelism of gpu computing.\n\ngpu hardware was originally so specialized that it could only be used for\ngraphics tasks. over time, gpu hardware became more flexible, allowing custom\nsubroutines to be used to transform the coordinates of vertices or assign colors\nto pixels. in principle, there was no requirement that these pixel values actually\nbe based on a rendering task. these gpus could be used for scientific computing\nby writing the output of a computation to a buffer of pixel values. steinkrau\net al. (\n) implemented a two-layer fully connected neural network on a gpu\nand reported a 3x speedup over their cpu-based baseline. shortly thereafter,\nchellapilla\n) demonstrated that the same technique could be used to\naccelerate supervised convolutional networks.\n\net al. (\n\n2005\n\n2006\n\nthe popularity of graphics cards for neural network training exploded after the\nadvent of general purpose gpus. these gp-gpus could execute arbitrary code,\nnot just rendering subroutines. nvidia\u2019s cuda programming language provided\na way to write this arbitrary code in a c-like language. with their relatively\nconvenient programming model, massive parallelism, and high memory bandwidth,\n\n448\n\n "}, {"Page_number": 464, "text": "chapter 12. applications\n\ngp-gpus now offer an ideal platform for neural network programming. this\nplatform was rapidly adopted by deep learning researchers soon after it became\navailable (\n\nraina et al. 2009 ciresan et al. 2010\n\n).\n\n,\n\n;\n\n,\n\nwriting efficient code for gp-gpus remains a difficult task best left to spe-\ncialists.\u00a0the techniques required to obtain good performance on gpu are very\ndifferent from those used on cpu. for example, good cpu-based code is usually\ndesigned to read information from the cache as much as possible. on gpu, most\nwritable memory locations are not cached, so it can actually be faster to compute\nthe same value twice, rather than compute it once and read it back from memory.\ngpu code is also inherently multi-threaded and the different threads must be\ncoordinated with each other carefully. for example, memory operations are faster\nif they can be coalesced. coalesced reads or writes occur when several threads can\neach read or write a value that they need simultaneously, as part of a single memory\ntransaction. different models of gpus are able to coalesce different kinds of read\nor write patterns. typically, memory operations are easier to coalesce if among n\nthreads, thread i accesses byte i + j of memory, and j is a multiple of some power\nof 2.\u00a0the exact specifications differ between models of gpu. another common\nconsideration for gpus is making sure that each thread in a group executes the\nsame instruction simultaneously. this means that branching can be difficult on\ngpu. threads are divided into small groups called\n. each thread in a warp\nexecutes the same instruction during each cycle, so if different threads within the\nsame warp need to execute different code paths, these different code paths must\nbe traversed sequentially rather than in parallel.\n\nwarps\n\ndue to the difficulty of writing high performance gpu code, researchers should\nstructure their workflow to avoid needing to write new gpu code in order to test\nnew models or algorithms. typically, one can do this by building a software library\nof high performance operations like convolution and matrix multiplication, then\nspecifying models in terms of calls to this library of operations. for example, the\n) specifies all of its\nmachine learning library pylearn2 (goodfellow\nmachine learning algorithms in terms of calls to theano (\nbergstra et al. 2010\n;\nbastien\n), which provide these\nhigh-performance operations. this factored approach can also ease support for\nmultiple kinds of hardware. for example, the same theano program can run on\neither cpu or gpu, without needing to change any of the calls to theano itself.\nother libraries like tensorflow (\ncollobert et al.\n,\n2011b) provide similar features.\n\n) and cuda-convnet (\n\nabadi et al. 2015\n\nkrizhevsky 2010\n\n) and torch (\n\net al.,\n\net al.,\n\n2013c\n\n2012\n\n,\n\n,\n\n,\n\n449\n\n "}, {"Page_number": 465, "text": "chapter 12. applications\n\n12.1.3 large scale distributed implementations\n\nin many cases, the computational resources available on a single machine are\ninsufficient. we therefore want to distribute the workload of training and inference\nacross many machines.\n\ndistributing inference is simple, because each input example we want to process\n\ncan be run by a separate machine. this is known as data parallelism.\n\nit is also possible to get model parallelism,\u00a0where multiple machines work\ntogether on a single datapoint, with each machine running a different part of the\nmodel. this is feasible for both inference and training.\n\nt \u2212 1\n.\n\ndata parallelism during training is somewhat harder. we can increase the size\nof the minibatch used for a single sgd step, but usually we get less than linear\nreturns in terms of optimization performance. it would be better to allow multiple\nmachines to compute multiple gradient descent steps in parallel. unfortunately,\nthe standard definition of gradient descent is as a completely sequential algorithm:\nthe gradient at step is a function of the parameters produced by step\n\nt\n\n;\n\n2012\n\n2011\n\net al.,\n\n2001 recht\n\nthis can be solved using asynchronous stochastic gradient descent (bengio\net al.,\n).\u00a0in this approach, several processor cores share\nthe memory representing the parameters. each core reads parameters without a\nlock, then computes a gradient, then increments the parameters without a lock.\nthis reduces the average amount of improvement that each gradient descent step\nyields, because some of the cores overwrite each other\u2019s progress, but the increased\nrate of production of steps causes the learning process to be faster overall. dean\net al. (\n) pioneered the multi-machine implementation of this lock-free approach\nto gradient descent, where the parameters are managed by a parameter server\nrather than stored in shared memory. distributed asynchronous gradient descent\nremains the primary strategy for training large deep networks and is used by\nmost major deep learning groups in industry (\nchilimbi et al. 2014 wu et al.,\n2015). academic deep learning researchers typically cannot afford the same scale\nof distributed learning systems but some research has focused on how to build\ndistributed networks with relatively low-cost hardware available in the university\nsetting (\n\ncoates et al. 2013\n\n).\n\n,\n\n;\n\n,\n\n12.1.4 model compression\n\nin many commercial applications, it is much more important that the time and\nmemory cost of running inference in a machine learning model be low than that\nthe time and memory cost of training be low. for applications that do not require\n\n450\n\n "}, {"Page_number": 466, "text": "chapter 12. applications\n\npersonalization, it is possible to train a model once, then deploy it to be used by\nbillions of users. in many cases, the end user is more resource-constrained than\nthe developer. for example, one might train a speech recognition network with a\npowerful computer cluster, then deploy it on mobile phones.\n\n2006\n\na key strategy for reducing the cost of inference is model compression (bucilu\u02c7a\net al.,\n). the basic idea of model compression is to replace the original,\nexpensive model with a smaller model that requires less memory and runtime to\nstore and evaluate.\n\nmodel compression is applicable when the size of the original model is driven\nprimarily by a need to prevent overfitting. in most cases, the model with the\nlowest generalization error is an ensemble of several independently trained models.\nevaluating all n ensemble members is expensive. sometimes, even a single model\ngeneralizes better if it is large (for example, if it is regularized with dropout).\n\nthese large models learn some function f(x), but do so using many more\nparameters than are necessary for the task. their size is necessary only due to\nthe limited number of training examples. as soon as we have fit this function\nf (x), we can generate a training set containing infinitely many examples, simply\nby applying f to randomly sampled points x. we then train the new, smaller,\nmodel to match f (x) on these points. in order to most efficiently use the capacity\nof the new, small model, it is best to sample the new x points from a distribution\nresembling the actual test inputs that will be supplied to the model later. this can\nbe done by corrupting training examples or by drawing points from a generative\nmodel trained on the original training set.\n\nalternatively, one can train the smaller model only on the original training\npoints, but train it to copy other features of the model, such as its posterior\ndistribution over the incorrect classes (hinton\n\n2014 2015\n\net al.,\n\n).\n\n,\n\n12.1.5 dynamic structure\n\none strategy for accelerating data processing systems in general is to build systems\nthat have dynamic structure in the graph describing the computation needed to\nprocess an input. data processing systems can dynamically determine which\nsubset of many neural networks should be run on a given input. individual neural\nnetworks can also exhibit dynamic structure internally by determining which subset\nof features (hidden units) to compute given information from the input. this\nform of dynamic structure inside neural networks is sometimes called conditional\ncomputation (\n). since many components of the\narchitecture may be relevant only for a small amount of possible inputs, the system\n\nbengio 2013 bengio et al. 2013b\n\n,\n\n;\n\n,\n\n451\n\n "}, {"Page_number": 467, "text": "chapter 12. applications\n\ncan run faster by computing these features only when they are needed.\n\ndynamic structure of computations is a basic computer science principle applied\ngenerally throughout the software engineering discipline.\u00a0the simplest versions\nof dynamic structure applied to neural networks are based on determining which\nsubset of some group of neural networks (or other machine learning models) should\nbe applied to a particular input.\n\na venerable strategy for accelerating inference in a classifier is to use a cascade\nof classifiers. the cascade strategy may be applied when the goal is to detect the\npresence of a rare object (or event). to know for sure that the object is present,\nwe must use a sophisticated classifier with high capacity, that is expensive to run.\nhowever, because the object is rare, we can usually use much less computation\nto reject inputs as not containing the object. in these situations, we can train\na sequence of classifiers. the first classifiers in the sequence have low capacity,\nand are trained to have high recall. in other words, they are trained to make sure\nwe do not wrongly reject an input when the object is present. the final classifier\nis trained to have high precision. at test time, we run inference by running the\nclassifiers in a sequence, abandoning any example as soon as any one element in\nthe cascade rejects it. overall, this allows us to verify the presence of objects with\nhigh confidence, using a high capacity model, but does not force us to pay the cost\nof full inference for every example. there are two different ways that the cascade\ncan achieve high capacity. one way is to make the later members of the cascade\nindividually have high capacity. in this case, the system as a whole obviously has\nhigh capacity, because some of its individual members do.\u00a0it is also possible to\nmake a cascade in which every individual model has low capacity but the system\nas a whole has high capacity due to the combination of many small models. viola\nand jones 2001\n) used a cascade of boosted decision trees to implement a fast and\nrobust face detector suitable for use in handheld digital cameras. their classifier\nlocalizes a face using essentially a sliding window approach in which many windows\nare examined and rejected if they do not contain faces. another version of cascades\nuses the earlier models to implement a sort of hard attention mechanism: the\nearly members of the cascade localize an object and later members of the cascade\nperform further processing given the location of the object. for example, google\ntranscribes address numbers from street view imagery using a two-step cascade\nthat first locates the address number with one machine learning model and then\ntranscribes it with another (goodfellow\n\n2014d\n\net al.,\n\n).\n\n(\n\ndecision trees themselves are an example of dynamic structure, because each\nnode in the tree determines which of its subtrees should be evaluated for each input.\na simple way to accomplish the union of deep learning and dynamic structure\n\n452\n\n "}, {"Page_number": 468, "text": "chapter 12. applications\n\nis to train a decision tree in which each node uses a neural network to make the\nsplitting decision (\n), though this has typically not been\ndone with the primary goal of accelerating inference computations.\n\nguo and gelfand 1992\n\n,\n\n;\n\n1991\n\ngater\n\net al.,\n\ncollobert et al. 2001 2002\n\nin the same spirit, one can use a neural network, called the\n\nto select which\none out of several expert networks will be used to compute the output, given the\ncurrent input. the first version of this idea is called the mixture of experts (\nnowlan\n,\n1990 jacobs\n), in which the gater outputs a set of probabilities or\nweights (obtained via a softmax nonlinearity), one per expert, and the final output\nis obtained by the weighted combination of the output of the experts. in that\ncase, the use of the gater does not offer a reduction in computational cost, but if a\nsingle expert is chosen by the gater for each example, we obtain the hard mixture\nof experts (\n), which can considerably accelerate training\nand inference time. this strategy works well when the number of gating decisions is\nsmall because it is not combinatorial. but when we want to select different subsets\nof units or parameters, it is not possible to use a \u201csoft switch\u201d because it requires\nenumerating (and computing outputs for) all the gater configurations. to deal\nwith this problem, several approaches have been explored to train combinatorial\n) experiment with several estimators of the gradient\ngaters.\non the gating probabilities, while\n) use\nreinforcement learning techniques (policy gradient) to learn a form of conditional\ndropout on blocks of hidden units and get an actual reduction in computational\ncost without impacting negatively on the quality of the approximation.\n\nbengio et al. 2013b\n\nbengio et al. 2015a\n\nbacon et al. 2015\n\n) and\n\n(\n\n(\n\n(\n\n,\n\n,\n\nanother\u00a0kind of\u00a0dynamic structure\u00a0is a\u00a0switch,\u00a0where\u00a0a hidden\u00a0unit can\nreceive input from different units depending on the context. this dynamic routing\napproach can be interpreted as an attention mechanism (\nolshausen et al. 1993\n).\nso far, the use of a hard switch has not proven effective on large-scale applications.\ncontemporary approaches instead use a weighted average over many possible inputs,\nand thus do not achieve all of the possible computational benefits of dynamic\nstructure. contemporary attention mechanisms are described in sec.\n\n12.4.5.1\n.\n\n,\n\none major obstacle to using dynamically structured systems is the decreased\ndegree of parallelism that results from the system following different code branches\nfor different inputs. this means that few operations in the network can be described\nas matrix multiplication or batch convolution on a minibatch of examples. we\ncan write more specialized sub-routines that convolve each example with different\nkernels or multiply each row of a design matrix by a different set of columns\nof weights. unfortunately,\u00a0these more specialized subroutines are difficult to\nimplement efficiently. cpu implementations will be slow due to the lack of cache\ncoherence and gpu implementations will be slow due to the lack of coalesced\n\n453\n\n "}, {"Page_number": 469, "text": "chapter 12. applications\n\nmemory transactions and the need to serialize warps when members of a warp take\ndifferent branches. in some cases, these issues can be mitigated by partitioning the\nexamples into groups that all take the same branch, and processing these groups\nof examples simultaneously.\u00a0this can be an acceptable strategy for minimizing\nthe time required to process a fixed amount of examples in an offline setting. in\na real-time setting where examples must be processed continuously, partitioning\nthe workload can result in load-balancing issues. for example, if we assign one\nmachine to process the first step in a cascade and another machine to process\nthe last step in a cascade, then the first will tend to be overloaded and the last\nwill tend to be underloaded. similar issues arise if each machine is assigned to\nimplement different nodes of a neural decision tree.\n\n12.1.6 specialized hardware implementations of deep networks\n\nsince the early days of neural networks research, hardware designers have worked\non specialized hardware implementations that could speed up training and/or\ninference of neural network algorithms. see early and more recent reviews of\nspecialized hardware for deep networks (\nlindsey and lindblad 1994 beiu et al.\n,\n2003 misra and saha 2010\n\n).\n\n,\n\n;\n\n;\n\n,\n\n;\n\n;\n\n,\n\n,\n\net al.,\n\net al.,\n\n2012 chen\n\n2009 pham\n\ndifferent forms of specialized hardware (graf and jackel 1989 mead and\nismail 2012 kim\n, ) have\nbeen developed over the last decades, either with asics (application-specific inte-\ngrated circuit), either with digital (based on binary representations of numbers),\nanalog (graf and jackel 1989 mead and ismail 2012\n) (based on physical imple-\nmentations of continuous values as voltages or currents) or hybrid implementations\n(combining digital and analog components). in recent years more flexible fpga\n(field programmable gated array) implementations (where the particulars of the\ncircuit can be written on the chip after it has been built) have been developed.\n\n2014a b\n\net al.,\n\n;\n\n;\n\n,\n\n,\n\n;\n\n,\n\nthough software implementations on general-purpose processing units (cpus\nand gpus) typically use 32 or 64 bits of precision to represent floating point\nnumbers, it has long been known that it was possible to use less precision, at\nleast at inference time (holt and baker 1991 holi and hwang 1993 presley\nand haggard 1994 simard and graf 1994 wawrzynek\net al.,\n2007). this has become a more pressing issue in recent years as deep learning\nhas gained in popularity in industrial products, and as the great impact of faster\nhardware was demonstrated with gpus. another factor that motivates current\nresearch on specialized hardware for deep networks is that the rate of progress of\na single cpu or gpu core has slowed down, and most recent improvements in\ncomputing speed have come from parallelization across cores (either in cpus or\n\n1996 savich\n\net al.,\n\n,\n\n;\n\n,\n\n;\n\n;\n\n,\n\n;\n\n;\n\n454\n\n "}, {"Page_number": 470, "text": "chapter 12. applications\n\ngpus). this is very different from the situation of the 1990s (the previous neural\nnetwork era) where the hardware implementations of neural networks (which might\ntake two years from inception to availability of a chip) could not keep up with\nthe rapid progress and low prices of general-purpose cpus. building specialized\nhardware is thus a way to push the envelope further, at a time when new hardware\ndesigns are being developed for low-power devices such as phones, aiming for\ngeneral-public applications of deep learning (e.g., with speech, computer vision or\nnatural language).\n\n;\n\n;\n\n2015\n\net al.,\n\net al.,\n\net al.,\n\n2015 gupta\n\n2011 courbariaux\n\nrecent work on low-precision implementations of backprop-based neural nets\n(vanhoucke\n) suggests\nthat between 8 and 16 bits of precision can suffice for using or training deep\nneural networks with back-propagation.\u00a0what is clear is that more precision is\nrequired during training than at inference time, and that some forms of dynamic\nfixed point representation of numbers can be used to reduce how many bits are\nrequired per number. traditional fixed point numbers are restricted to a fixed\nrange (which corresponds to a given exponent in a floating point representation).\ndynamic fixed point representations share that range among a set of numbers\n(such as all the weights in one layer). using fixed point rather than floating point\nrepresentations and using less bits per number reduces the hardware surface area,\npower requirements and computing time needed for performing multiplications,\nand multiplications are the most demanding of the operations needed to use or\ntrain a modern deep network with backprop.\n\n12.2 computer vision\n\ncomputer vision has traditionally been one of the most active research areas for\ndeep learning applications, because vision is a task that is effortless for humans\nand many animals but challenging for computers (\n). many of\nthe most popular standard benchmark tasks for deep learning algorithms are forms\nof object recognition or optical character recognition.\n\nballard et al. 1983\n\n,\n\ncomputer vision is a very broad field encompassing a wide variety of ways\nof processing images, and an amazing diversity of applications.\u00a0applications of\ncomputer vision range from reproducing human visual abilities, such as recognizing\nfaces, to creating entirely new categories of visual abilities. as an example of\nthe latter category, one recent computer vision application is to recognize sound\nwaves from the vibrations they induce in objects visible in a video (\ndavis et al.\n,\n2014). most deep learning research on computer vision has not focused on such\nexotic applications that expand the realm of what is possible with imagery but\n\n455\n\n "}, {"Page_number": 471, "text": "chapter 12. applications\n\nrather a small core of ai goals aimed at replicating human abilities. most deep\nlearning for computer vision is used for object recognition or detection of some\nform, whether this means reporting which object is present in an image, annotating\nan image with bounding boxes around each object, transcribing a sequence of\nsymbols from an image, or labeling each pixel in an image with the identity of the\nobject it belongs to. because generative modeling has been a guiding principle\nof deep learning research, there is also a large body of work on image synthesis\nusing deep models. while image synthesis\nis usually not considered a\ncomputer vision endeavor, models capable of image synthesis are usually useful for\nimage restoration, a computer vision task involving repairing defects in images or\nremoving objects from images.\n\nex nihilo\n\n12.2.1 preprocessing\n\nmany application areas require sophisticated preprocessing because the original\ninput comes in a form that is difficult for many deep learning architectures to\nrepresent. computer vision usually requires relatively little of this kind of prepro-\ncessing. the images should be standardized so that their pixels all lie in the same,\nreasonable range, like [0,1] or [-1, 1]. mixing images that lie in [0,1] with images\nthat lie in [0, 255] will usually result in failure. formatting images to have the same\nscale is the only kind of preprocessing that is strictly necessary. many computer\nvision architectures require images of a standard size, so images must be cropped or\nscaled to fit that size. however, even this rescaling is not always strictly necessary.\nsome convolutional models accept variably-sized inputs and dynamically adjust\nthe size of their pooling regions to keep the output size constant (waibel et al.,\n1989). other convolutional models have variable-sized output that automatically\nscales in size with the input, such as models that denoise or label each pixel in an\nimage (\n\nhadsell et al. 2007\n\n).\n\n,\n\ndataset augmentation may be seen as a way of preprocessing the training set\nonly. dataset augmentation is an excellent way to reduce the generalization error\nof most computer vision models. a related idea applicable at test time is to show\nthe model many different versions of the same input (for example, the same image\ncropped at slightly different locations) and have the different instantiations of the\nmodel vote to determine the output. this latter idea can be interpreted as an\nensemble approach, and helps to reduce generalization error.\n\nother kinds of preprocessing are applied to both the train and the test set with\nthe goal of putting each example into a more canonical form in order to reduce the\namount of variation that the model needs to account for. reducing the amount of\nvariation in the data can both reduce generalization error and reduce the size of\n\n456\n\n "}, {"Page_number": 472, "text": "chapter 12. applications\n\nthe model needed to fit the training set. simpler tasks may be solved by smaller\nmodels, and simpler solutions are more likely to generalize well. preprocessing\nof this kind is usually designed to remove some kind of variability in the input\ndata that is easy for a human designer to describe and that the human designer\nis confident has no relevance to the task. when training with large datasets and\nlarge models, this kind of preprocessing is often unnecessary, and it is best to just\nlet the model learn which kinds of variability it should become invariant to. for\nexample, the alexnet system for classifying imagenet only has one preprocessing\nstep: subtracting the mean across training examples of each pixel (krizhevsky\net al.,\n\n2012\n\n).\n\n12.2.1.1 contrast normalization\n\none of the most obvious sources of variation that can be safely removed\u00a0for\nmany tasks is the amount of contrast in the image. contrast simply refers to the\nmagnitude of the difference between the bright and the dark pixels in an image.\nthere are many ways of quantifying the contrast of an image. in the context of\ndeep learning, contrast usually refers to the standard deviation of the pixels in an\nimage or region of an image. suppose we have an image represented by a tensor\nx \u2208 r r c\u00d7 \u00d73, with xi,j,1 being the red intensity at row i and column j, xi,j,2 giving\nthe green intensity and xi,j,3 giving the blue intensity. then the contrast of the\nentire image is given by\n\nvuut 1\n\n3rc\n\nrxi=1\n\ncxj=1\n\n3xk=1(cid:24)xi,j,k \u2212 \u00afx(cid:25)2\n\nwhere \u00afx is the mean intensity of the entire image:\n\n\u00afx =\n\n1\n3rc\n\nrxi=1\n\ncxj=1\n\n3xk=1\n\nxi,j,k.\n\n(12.1)\n\n(12.2)\n\nglobal contrast normalization (gcn) aims to prevent images from having\nvarying amounts of contrast by subtracting the mean from each image,\u00a0then\nrescaling it so that the\u00a0standard deviation\u00a0across its\u00a0pixels is\u00a0equal to some\nconstant s. this approach is complicated by the fact that no scaling factor can\nchange the contrast of a zero-contrast image (one whose pixels all have equal\nintensity). images with very low but non-zero contrast often have little information\ncontent. dividing by the true standard deviation usually accomplishes nothing\nmore than amplifying sensor noise or compression artifacts in such cases. this\n\n457\n\n "}, {"Page_number": 473, "text": "chapter 12. applications\n\nmotivates introducing a small, positive regularization parameter \u03bb to bias the\nestimate of the standard deviation. alternately, one can constrain the denominator\nto be at least (cid:115). given an input image x, gcn produces an output image x0,\ndefined such that\n\nx0i,j,k = s\n\nmax(cid:26)(cid:115),q\u03bb + 1\n\n3rcpr\n\nxi,j,k \u2212 \u00afx\nj=1p3\ni=1pc\n\nk=1(cid:24)x i,j,k \u2212 \u00afx(cid:25)2(cid:27).\n\n(12.3)\n\ndatasets consisting of large images cropped to interesting objects are unlikely\nto contain any images with nearly constant intensity. in these cases, it is safe\nto practically ignore the small denominator problem by setting \u03bb = 0 and avoid\ndivision by 0 in extremely rare cases by setting (cid:115) to an extremely low value like\n10\u22128.\u00a0this is the approach used by\n) on the cifar-10\ndataset. small images cropped randomly are more likely to have nearly constant\nintensity, making aggressive regularization more useful.\n) used\n(cid:115)\n\n= 0 and = 10 on small, randomly selected patches drawn from cifar-10.\n\ngoodfellow et al. 2013a\n\ncoates et al. 2011\n\n\u03bb\n\n(\n\n(\n\nthe scale parameter s can usually be set to , as done by\n\n),\nor chosen to make each individual pixel have standard deviation across examples\nclose to 1, as done by\n\ngoodfellow et al. 2013a\n\ncoates et al. 2011\n\n).\n\n1\n\n(\n\n(\n\n12.3\n\nis just a rescaling of the\n\nthe standard deviation in eq.\n\nl2 norm of the\nimage (assuming the mean of the image has already been removed). it is preferable\nto define gcn in terms of standard deviation rather than l 2 norm because the\nstandard deviation includes division by the number of pixels, so gcn based on\nstandard deviation allows the same s to be used regardless of image size. however,\nthe observation that the l2 norm is proportional to the standard deviation can\nhelp build a useful intuition. one can understand gcn as mapping examples to\na spherical shell. see fig.\nfor an illustration. this can be a useful property\nbecause neural networks are often better at responding to directions in space rather\nthan exact locations. responding to multiple distances in the same direction\nrequires hidden units with collinear weight vectors but different biases. such\ncoordination can be difficult for the learning algorithm to discover. additionally,\nmany shallow graphical models have problems with representing multiple separated\nmodes along the same line. gcn avoids these problems by reducing each example\nto a direction rather than a direction and a distance.\n\n12.1\n\ncounterintuitively, there is a preprocessing operation known as\n\nand it\nis not the same operation as gcn. sphering does not refer to making the data lie\non a spherical shell, but rather to rescaling the principal components to have equal\nvariance, so that the multivariate normal distribution used by pca has spherical\ncontours. sphering is more commonly known as\n\nwhitening\n.\n\nsphering\n\n458\n\n "}, {"Page_number": 474, "text": "chapter 12. applications\n\nraw input\n\ngcn, = 0\n\n\u03bb\n\n1 5.\n\n1\nx\n\n0 0.\n\n\u22121 5.\n\ngcn, = 10\n\n\u03bb\n\n\u22122\n\n\u22121 5\n.\n\n0 0\n.\nx 0\n\n1 5\n\n.\n\n\u22121 5\n.\n\n0 0\n.\nx0\n\n1 5\n\n.\n\n\u22121 5\n.\n\n1 5\n\n.\n\n0 0\n.\nx0\n\n(center)\n\ngcn with\n\nfigure 12.1: gcn maps examples onto a sphere.\u00a0(left) raw input data may have any\nnorm.\n\u03bb = 0 maps all non-zero examples perfectly onto a sphere.\nhere we use s = 1 and (cid:115) = 10\u22128 . because we use gcn based on normalizing the standard\ndeviation rather than the l2 norm, the resulting sphere is not the unit sphere. (right)\nregularized gcn, with \u03bb > 0, draws examples toward the sphere but does not completely\ndiscard the variation in their norm. we leave\n\nand the same as before.\n\ns\n\n(cid:115)\n\nglobal contrast normalization will often fail to highlight image features we\nwould like to stand out, such as edges and corners. if we have a scene with a large\ndark area and a large bright area (such as a city square with half the image in\nthe shadow of a building) then global contrast normalization will ensure there is a\nlarge difference between the brightness of the dark area and the brightness of the\nlight area. it will not, however, ensure that edges within the dark region stand out.\n\nthis motivates local contrast normalization. local contrast normalization\nensures that the contrast is normalized across each small window, rather than over\nthe image as a whole. see fig.\nfor a comparison of global and local contrast\nnormalization.\n\n12.2\n\nvarious definitions of local contrast normalization are possible. in all cases,\none modifies each pixel by subtracting a mean of nearby pixels and dividing by\na standard deviation of nearby pixels. in some cases, this is literally the mean\nand standard deviation of all pixels in a rectangular window centered on the\npixel to be modified (\n). in other cases, this is a weighted mean\nand weighted standard deviation using gaussian weights centered on the pixel to\nbe modified.\u00a0in the case of color images, some strategies process different color\nchannels separately while others combine information from different channels to\nnormalize each pixel (\n\nsermanet et al. 2012\n\npinto et al. 2008\n\n).\n\n,\n\n,\n\n459\n\n "}, {"Page_number": 475, "text": "chapter 12. applications\n\ninput image\n\ngcn\n\nlcn\n\nfigure 12.2: a comparison of global and local contrast normalization. visually, the effects\nof global contrast normalization are subtle. it places all images on roughly the same\nscale, which reduces the burden on the learning algorithm to handle multiple scales. local\ncontrast normalization modifies the image much more, discarding all regions of constant\nintensity. this allows the model to focus on just the edges. regions of fine texture,\nsuch as the houses in the second row, may lose some detail due to the bandwidth of the\nnormalization kernel being too high.\n\nlocal contrast normalization can usually be implemented efficiently by using\n) to compute feature maps of local means and\nseparable convolution (see sec.\nlocal standard deviations, then using element-wise subtraction and element-wise\ndivision on different feature maps.\n\n9.8\n\nlocal contrast normalization is a differentiable operation and can also be used as\na nonlinearity applied to the hidden layers of a network, as well as a preprocessing\noperation applied to the input.\n\nas with global contrast normalization, we typically need to regularize local\ncontrast normalization to avoid division by zero. in fact, because local contrast\nnormalization typically acts on smaller windows, it is even more important to\nregularize. smaller windows are more likely to contain values that are all nearly\nthe same as each other, and thus more likely to have zero standard deviation.\n\n12.2.1.2 dataset augmentation\n\nas described in sec.\n, it is easy to improve the generalization of a classifier\nby increasing the size of the training set by adding extra copies of the training\nexamples that have been modified with transformations that do not change the\n\n7.4\n\n460\n\n "}, {"Page_number": 476, "text": "chapter 12. applications\n\nclass. object recognition is a classification task that is especially amenable to\nthis form\u00a0of dataset\u00a0augmentation because\u00a0the class\u00a0is invariant\u00a0to so\u00a0many\ntransformations and the input can be easily transformed with many geometric\noperations. as described before, classifiers can benefit from random translations,\nrotations, and in some cases, flips of the input to augment the dataset. in specialized\ncomputer vision applications, more advanced transformations are commonly used\nfor dataset augmentation. these schemes include random perturbation of the\ncolors in an image (\n) and nonlinear geometric distortions of\nthe input (\n\nkrizhevsky et al. 2012\n\nlecun et al. 1998b\n\n).\n\n,\n\n,\n\n12.3 speech recognition\n\nthe task of speech recognition is to map an acoustic signal containing a spoken\nnatural language utterance into the corresponding sequence of words intended by\nthe speaker. let x = (x(1) , x(2), . . . , x( )t ) denote the sequence of acoustic input\nvectors (traditionally produced by splitting the audio into 20ms frames). most\nspeech recognition systems preprocess the input using specialized hand-designed\nfeatures, but some (\n) deep learning systems learn features\nfrom raw input. let y = ( y1, y 2, . . . , yn ) denote the target output sequence (usually\na sequence of words or characters). the automatic speech recognition (asr) task\nconsists of creating a function f\u2217asr that computes the most probable linguistic\nsequence\n\ngiven the acoustic sequence\n\njaitly and hinton 2011\n\nx\n\ny\n\n:\n\n,\n\nf\u2217asr(\n\nx\n\n) = arg max\n\ny\n\np \u2217(\n\ny x|\n\n= \n\nx\n\n)\n\n(12.4)\n\nwhere p \u2217 is the true conditional distribution relating the inputs x to the targets\ny.\n\n,\n\nbahl et al. 1987\n\nsince the 1980s and until about 2009\u20132012, state-of-the art speech recognition\nsystems primarily combined hidden markov models (hmms) and gaussian mixture\nmodels (gmms). gmms modeled the association between acoustic features and\nphonemes (\n), while hmms modeled the sequence of phonemes.\nthe gmm-hmm\u00a0model family treats\u00a0acoustic waveforms as being generated\nby the following process:\u00a0first an hmm generates a sequence of phonemes and\ndiscrete sub-phonemic states (such as the beginning, middle, and end of each\nphoneme), then a gmm transforms each discrete symbol into a brief segment of\naudio waveform. although gmm-hmm systems dominated asr until recently,\nspeech recognition was actually one of the first areas where neural networks were\napplied, and numerous asr systems from the late 1980s and early 1990s used\n\n461\n\n "}, {"Page_number": 477, "text": "chapter 12. applications\n\n;\n\n,\n\n;\n\n,\n\n;\n\n;\n\n,\n\n1996\n\net al.,\n\net al.,\n\net al.,\n\n1991 1992 konig\n\ngarofolo et al. 1993\n\n1989 robinson and\nneural nets (bourlard and wellekens 1989 waibel\nfallside 1991 bengio\n). at the time, the\nperformance of asr based on neural nets approximately matched the performance\nof gmm-hmm systems. for example, robinson and fallside 1991\n) achieved\n26% phoneme error rate on the timit (\n) corpus (with 39\nphonemes to discriminate between),\u00a0which was better than or comparable to\nhmm-based systems. since then, timit has been a benchmark for phoneme\nrecognition, playing a role similar to the role mnist plays for object recognition.\nhowever, because of the complex engineering involved in software systems for\nspeech recognition and the effort that had been invested in building these systems\non the basis of gmm-hmms, the industry did not see a compelling argument\nfor switching to neural networks. as a consequence, until the late 2000s, both\nacademic and industrial research in using neural nets for speech recognition mostly\nfocused on using neural nets to learn extra features for gmm-hmm systems.\n\n(\n\n,\n\niii\n\nlater, with much larger and deeper models and much larger datasets,\nrecognition accuracy was dramatically improved by using neural networks to\nreplace gmms for the task of associating acoustic features to phonemes (or sub-\nphonemic states). starting in 2009, speech researchers applied a form of deep\nlearning based on unsupervised learning to speech recognition. this approach\nto deep learning was based on training undirected probabilistic models called\nrestricted boltzmann machines (rbms) to model the input data. rbms will be\ndescribed in part\n. to solve speech recognition tasks, unsupervised pretraining\nwas used to build deep feedforward networks whose layers were each initialized\nby training an rbm. these networks take spectral acoustic representations in\na fixed-size input window (around a center frame) and predict the conditional\nprobabilities of hmm states for that center frame. training such deep networks\nhelped to significantly improve the recognition rate on timit (\nmohamed et al.\n,\n2009 2012a\n), bringing down the phoneme error rate from about 26% to 20.7%.\nsee\n) for an analysis of reasons for the success of these\nmodels. extensions to the basic phone recognition pipeline included the addition\nof speaker-adaptive features (\n) that further reduced the\nerror rate. this was quickly followed up by work to expand the architecture from\nphoneme recognition (which is what timit is focused on) to large-vocabulary\nspeech recognition (\n), which involves not just recognizing phonemes\nbut also recognizing sequences of words from a large vocabulary. deep networks\nfor speech recognition eventually shifted from being based on pretraining and\nboltzmann machines to being based on techniques such as rectified linear units and\ndropout (\n). by that time, several of the major\nspeech groups in industry had started exploring deep learning in collaboration with\n\n,\nmohamed et al. 2012b\n\nzeiler et al. 2013 dahl et al. 2013\n\nmohamed et al. 2011\n\ndahl et al. 2012\n\n(\n\n,\n\n,\n\n;\n\n,\n\n,\n\n462\n\n "}, {"Page_number": 478, "text": "chapter 12. applications\n\nacademic researchers.\n) describe the breakthroughs achieved\nby these collaborators, which are now deployed in products such as mobile phones.\n\nhinton et al. 2012a\n\n(\n\nlater, as these groups explored larger and larger labeled datasets and incorpo-\nrated some of the methods for initializing, training, and setting up the architecture\nof deep nets, they realized that the unsupervised pretraining phase was either\nunnecessary or did not bring any significant improvement.\n\nthese breakthroughs in recognition performance for word error rate in speech\nrecognition were unprecedented (around 30% improvement) and were following a\nlong period of about ten years during which error rates did not improve much with\nthe traditional gmm-hmm technology, in spite of the continuously growing size\nof training sets (see fig. 2.4 of deng and yu 2014\n)). this created a rapid shift in\nthe speech recognition community towards deep learning. in a matter of roughly\ntwo years, most of the industrial products for speech recognition incorporated deep\nneural networks and this success spurred a new wave of research into deep learning\nalgorithms and architectures for asr, which is still ongoing today.\n\n(\n\none of these innovations was the use of convolutional networks (\n\n,\nsainath et al.\n2013) that replicate weights across time and frequency, improving over the earlier\ntime-delay neural networks that replicated weights only across time. the new\ntwo-dimensional convolutional models regard the input spectrogram not as one\nlong vector but as an image, with one axis corresponding to time and the other to\nfrequency of spectral components.\n\nanother important push,\u00a0still ongoing, has been towards end-to-end deep\nlearning speech recognition systems that completely remove the hmm. the first\nmajor breakthrough in this direction came from graves\n) who trained a\n), using map inference over the frame-to-phoneme\ndeep lstm rnn (see sec.\nalignment, as in\ngraves et al.,\n2006 graves 2012\n) has state variables from\nseveral layers at each time step, giving the unfolded graph two kinds of depth:\nordinary depth due to a stack of layers, and depth due to time unfolding.\u00a0this\nwork brought the phoneme error rate on timit to a record low of 17.7%. see\npascanu\n) for other variants of deep rnns,\napplied in other settings.\n\n) and in the ctc framework (\ngraves\n\nlecun et al. 1998b\n). a deep rnn (\n\n10.10\n(\n\net al. (\n\net al. (\n\nchung\n\net al. (\n\n2014a\n\net al.,\n\n) and\n\n2014\n\n2013\n\n2013\n\n,\n\n;\n\nanother contemporary step toward end-to-end deep learning asr is to let the\nsystem learn how to \u201calign\u201d the acoustic-level information with the phonetic-level\ninformation (\n\nchorowski et al. 2014 lu et al. 2015\n\n).\n\n,\n\n;\n\n,\n\n463\n\n "}, {"Page_number": 479, "text": "chapter 12. applications\n\n12.4 natural language processing\n\nnatural language processing (nlp) is the use of human languages, such as english\nor french, by a computer. computer programs typically read and emit specialized\nlanguages designed to allow efficient and unambiguous parsing by simple programs.\nmore naturally occurring languages are often ambiguous and defy formal description.\nnatural language processing includes applications such as machine translation,\nin which the learner must read a sentence in one human language and emit an\nequivalent sentence in another human language. many nlp applications are based\non language models that define a probability distribution over sequences of words,\ncharacters or bytes in a natural language.\n\nas with the other applications discussed in this chapter, very generic neural\nnetwork techniques can be successfully applied to natural language processing.\nhowever, to achieve excellent performance and to scale well to large applications,\nsome domain-specific strategies become important. to build an efficient model of\nnatural language, we must usually use techniques that are specialized for processing\nsequential data. in many cases, we choose to regard natural language as a sequence\nof words, rather than a sequence of individual characters or bytes. because the total\nnumber of possible words is so large, word-based language models must operate on\nan extremely high-dimensional and sparse discrete space. several strategies have\nbeen developed to make models of such a space efficient, both in a computational\nand in a statistical sense.\n\n12.4.1\n\nn\n\n-grams\n\na language model defines a probability distribution over sequences of tokens in\na natural language. depending on how the model is designed, a token may be\na word, a character, or even a byte. tokens are always discrete entities. the\nearliest successful language models were based on models of fixed-length sequences\nof tokens called -grams. an -gram is a sequence of\n\ntokens.\n\nn\n\nn\n\nn\n\nmodels based on n-grams define the conditional probability of the n-th token\ngiven the preceding n \u2212 1 tokens. the model uses products of these conditional\ndistributions to define the probability distribution over longer sequences:\n\np x( 1, . . . , x\u03c4 ) = \n\n(p x1 , . . . , xn\u22121)\n\np x( t | xt n\u2212 +1, . . . , xt\u22121).\n\n(12.5)\n\nthis decomposition is justified by the chain rule of probability. the probability\ndistribution over the initial sequence p (x1, . . . , xn\u22121) may be modeled by a different\nmodel with a smaller value of\n\n.n\n\n\u03c4yt n=\n\n464\n\n "}, {"Page_number": 480, "text": "chapter 12. applications\n\ntraining n-gram models is straightforward because the maximum likelihood\nestimate can be computed simply by counting how many times each possible n\ngram occurs in the training set. models based on n-grams have been the core\nbuilding block of statistical language modeling for many decades (jelinek and\nmercer 1980 katz 1987 chen and goodman 1999\n\n).\n\n,\n\n;\n\n,\n\n;\n\n,\n\nfor small values of n , models have particular names: unigram for n=1, bigram\nfor n=2, and trigram for n=3. these names derive from the latin prefixes for the\ncorresponding numbers and the greek suffix \u201c-gram\u201d denoting something that is\nwritten.\n\nusually we train both an n-gram model and an n\u22121 gram model simultaneously.\n\nthis makes it easy to compute\n\np x( t | xt n\u2212 +1, . . . , xt) =\n\npn (xt n\u2212 +1, . . . , x t)\n\np n\u22121(xt n\u2212 +1, . . . , x t\u22121)\n\n(12.6)\n\nsimply by looking up two stored probabilities. for this to exactly reproduce\ninference in p n, we must omit the final character from each sequence when we\ntrain p n\u22121.\n\nas an example, we demonstrate how a trigram model computes the probability\nof the sentence \u201c the dog ran away.\u201d the first words of the sentence cannot be\nhandled by the default formula based on conditional probability because there is no\ncontext at the beginning of the sentence. instead, we must use the marginal prob-\nability over words at the start of the sentence. we thus evaluate p3(the dog ran).\nfinally, the last word may be predicted using the typical case, of using the con-\nditional distribution p(away dog ran\n, we\nobtain:\n\n). putting this together with eq.\n\n12.6\n\n|\n\np\n\n(\nthe dog ran away\n\n) = \n\np\n\n3(\n\nthe dog ran p 3(\n\n)\n\ndog ran away /p2(\n\n)\n\n)\ndog ran .\n(12.7)\n\na fundamental limitation of maximum likelihood for n-gram models is that pn\nas estimated from training set counts is very likely to be zero in many cases, even\nthough the tuple (xt n\u2212 +1, . . . , xt) may appear in the test set. this can cause two\ndifferent kinds of catastrophic outcomes. when pn\u22121 is zero, the ratio is undefined,\nso the model does not even produce a sensible output. when pn\u22121 is non-zero but\npn is zero, the test log-likelihood is \u2212\u221e.\u00a0to avoid such catastrophic outcomes,\nmost n-gram models employ some form of smoothing. smoothing techniques shift\nprobability mass from the observed tuples to unobserved ones that are similar.\nsee\n) for a review and empirical comparisons. one basic\ntechnique consists of adding non-zero probability mass to all of the possible next\nsymbol values. this method can be justified as bayesian inference with a uniform\n\nchen and goodman 1999\n\n(\n\n465\n\n "}, {"Page_number": 481, "text": "chapter 12. applications\n\nor dirichlet prior over the count parameters. another very popular idea is to form\na mixture model containing higher-order and lower-order n-gram models, with the\nhigher-order models providing more capacity and the lower-order models being\nmore likely to avoid counts of zero. back-off methods look-up the lower-order\nn-grams if the frequency of the context xt\u22121, . . . , x t n\u2212 +1 is too small to use the\nhigher-order model. more formally, they estimate the distribution over xt by using\ncontexts x t n k\u2212 + , . . . , xt\u22121, for increasing k, until a sufficiently reliable estimate is\nfound.\n\n|v n possible n-grams and |\n\nclassical n-gram models are particularly vulnerable to the curse of dimension-\n|v is often very large. even with a\nality. there are |\nmassive training set and modest n , most n-grams will not occur in the training set.\none way to view a classical n-gram model is that it is performing nearest-neighbor\nlookup. in other words, it can be viewed as a local non-parametric predictor,\nsimilar to k-nearest neighbors. the statistical problems facing these extremely\nlocal predictors are described in sec.\n. the problem for a language model is\neven more severe than usual, because any two different words have the same dis-\ntance from each other in one-hot vector space. it is thus difficult to leverage much\ninformation from any \u201cneighbors\u201d\u2014only training examples that repeat literally the\nsame context are useful for local generalization.\u00a0to overcome these problems, a\nlanguage model must be able to share knowledge between one word and other\nsemantically similar words.\n\n5.11.2\n\n;\n\n,\n\n;\n\n1998\n\net al.,\n\net al.,\n\n1992 ney and kneser 1993 niesler\n\nto improve the statistical efficiency of n-gram models, class-based language\nmodels (brown\n) introduce\nthe notion of word categories and then share statistical strength between words that\nare in the same category. the idea is to use a clustering algorithm to partition the\nset of words into clusters or classes, based on their co-occurrence frequencies with\nother words. the model can then use word class ids rather than individual word\nids to represent the context on the right side of the conditioning bar. composite\nmodels combining word-based and class-based models via mixing or back-off are\nalso possible. although word classes provide a way to generalize between sequences\nin which some word is replaced by another of the same class, much information is\nlost in this representation.\n\n12.4.2 neural language models\n\nneural language models or nlms are a class of language model designed to overcome\nthe curse of dimensionality problem for modeling natural language sequences by\nusing a distributed representation of words (\n). unlike class-\nbased n -gram models, neural language models are able to recognize that two words\n\nbengio et al. 2001\n\n,\n\n466\n\n "}, {"Page_number": 482, "text": "chapter 12. applications\n\nare similar without losing the ability to encode each word as distinct from the\nother. neural language models share statistical strength between one word (and\nits context) and other similar words and contexts. the distributed representation\nthe model learns for each word enables this sharing by allowing the model to treat\nwords that have features in common similarly. for example, if the word dog and\nthe word cat map to representations that share many attributes, then sentences\nthat contain the word cat can inform the predictions that will be made by the\nmodel for sentences that contain the word dog, and vice-versa. because there are\nmany such attributes, there are many ways in which generalization can happen,\ntransferring information from each training sentence to an exponentially large\nnumber of semantically related sentences. the curse of dimensionality requires the\nmodel to generalize to a number of sentences that is exponential in the sentence\nlength. the model counters this curse by relating each training sentence to an\nexponential number of similar sentences.\n\nwe sometimes call these word representations word embeddings. in this inter-\npretation, we view the raw symbols as points in a space of dimension equal to the\nvocabulary size. the word representations embed those points in a feature space\nof lower dimension. in the original space, every word is represented by a one-hot\nvector, so every pair of words is at euclidean distance \u221a2 from each other. in the\nembedding space, words that frequently appear in similar contexts (or any pair\nof words sharing some \u201cfeatures\u201d learned by the model) are close to each other.\nthis often results in words with similar meanings being neighbors. fig.\nzooms\nin on specific areas of a learned word embedding space to show how semantically\nsimilar words map to representations that are close to each other.\n\n12.3\n\nneural networks in other domains also define embeddings. for example, a\nhidden layer of a convolutional network provides an \u201cimage embedding.\u201d usually\nnlp practitioners are much more interested in this idea of embeddings because\nnatural language does not originally lie in a real-valued vector space. the hidden\nlayer has provided a more qualitatively dramatic change in the way the data is\nrepresented.\n\nthe basic idea of using distributed representations to improve models for\nnatural language processing is not restricted to neural networks. it may also be\nused with graphical models that have distributed representations in the form of\nmultiple latent variables (mnih and hinton 2007\n\n).\n\n,\n\n467\n\n "}, {"Page_number": 483, "text": "chapter 12. applications\n\n\u22126\n\u22127\n\u22128\n\u22129\n\u221210\n\u221211\n\u221212\n\u221213\n\u221214\n\nfrance\nchina\n\nrussian\n\nfrench\nenglish\n\ngermany\nontario\n\neurope\neu\nunionafrican\nafrica\nassembly\neuropean\n\nbritish\n\niraq\n\njapan\n\nnorth\n\ncanada\ncanadian\n\n22\n\n21\n\n20\n\n19\n\n18\n\n2009\n2008\n\n2004\n2003\n\n1995\n\n2006\n\n2005\n\n2002\n\n2007\n\n2001\n\n2000\n\n1999\n\n1996\n\n19971998\n\n\u2212\n34\n\n\u2212\n32\n\n\u2212\n30\n\nsouth\n\u2212\n28\n\n\u2212\n26\n\n17\n35 0 35 5 36 0 36 5 37 0 37 5 38 0\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nfigure 12.3: two-dimensional visualizations of word embeddings obtained from a neural\nmachine translation model (\n), zooming in on specific areas where\nsemantically related words have embedding vectors that are close to each other. countries\nappear on the left and numbers on the right. keep in mind that these embeddings are 2-d\nfor the purpose of visualization. in real applications, embeddings typically have higher\ndimensionality and can simultaneously capture many kinds of similarity between words.\n\nbahdanau et al. 2015\n\n,\n\n12.4.3 high-dimensional outputs\n\nin many natural language applications, we often want our models to produce\nwords (rather than characters) as the fundamental unit of the output. for large\nvocabularies, it can be very computationally expensive to represent an output\ndistribution over the choice of a word, because the vocabulary size is large. in many\napplications, v contains hundreds of thousands of words. the naive approach to\nrepresenting such a distribution is to apply an affine transformation from a hidden\nrepresentation to the output space, then apply the softmax function. suppose\nwe have a vocabulary v with size |\n|v . the weight matrix describing the linear\ncomponent of this affine transformation is very large, because its output dimension\nis |\n|v . this imposes a high memory cost to represent the matrix, and a high\ncomputational cost to multiply by it. because the softmax is normalized across all\n|\n|v outputs, it is necessary to perform the full matrix multiplication at training\ntime as well as test time\u2014we cannot calculate only the dot product with the weight\nvector for the correct output. the high computational costs of the output layer\nthus arise both at training time (to compute the likelihood and its gradient) and\nat test time (to compute probabilities for all or selected words). for specialized\nloss functions, the gradient can be computed efficiently (\n), but\nthe standard cross-entropy loss applied to a traditional softmax output layer poses\n\nvincent et al. 2015\n\n,\n\n468\n\n "}, {"Page_number": 484, "text": "chapter 12. applications\n\nmany difficulties.\n\nsuppose that h is the top hidden layer used to predict the output probabilities\n\u02c6y. if we parametrize the transformation from h to \u02c6y with learned weights w\nand learned biases b, then the affine-softmax output layer performs the following\ncomputations:\n\nwijhj \u2200 \u2208 {\n\ni\n\n|}\n1, . . . , v ,\n\n|\n\n(12.8)\n\n(12.9)\n\nai = bi +xj\n\n\u02c6yi =\n\neai\n|v\ni0=1 ea i0\n\n.\n\np|\n\nif h contains nh elements then the above operation is o(|\nthousands and |\ncomputation of most neural language models.\n\n|v nh). with nh in the\n|v in the hundreds of thousands, this operation dominates the\n\n12.4.3.1 use of a short list\n\n,\n\n,\n\nbengio et al. 2001 2003\n\nthe first neural language models (\n) dealt with the high cost\nof using a softmax over a large number of output words by limiting the vocabulary\nsize to 10,000 or 20,000 words. schwenk and gauvain 2002\nschwenk 2007\n)\nbuilt upon this approach by splitting the vocabulary v into a shortlist l of most\nfrequent words (handled by the neural net) and a tail t = v l\\ of more rare words\n(handled by an n-gram model).\u00a0to be able to combine the two predictions, the\nneural net also has to predict the probability that a word appearing after context\nc belongs to the tail list. this may be achieved by adding an extra sigmoid output\nunit to provide an estimate of p (i\n). the extra output can then be used to\nas follows:\nachieve an estimate of the probability distribution over all words in\n\n\u2208 |t\n\n) and\n\nc\n\nv\n\n(\n\n(\n\np y\n\n( = \n\n|\ni c\n\n) =1 i\u2208lp y\n\n( = \n+ 1 i\u2208tp y\n\n\u2208\n|\ni c, i\n|\ni c, i\n( = \n\n\u2212l)(1\n( \u2208 |t\np i\n\u2208 t)\n( \u2208 |t\np i\nc\n\nc\n\n))\n\n)\n\n(12.10)\n\n(12.11)\n\n|\n\n\u2208 l) is provided by the neural language model and p(y = i |\nwhere p (y = i c, i\nc, i \u2208 t) is provided by the n-gram model. with slight modification, this approach\ncan also work using an extra output value in the neural language model\u2019s softmax\nlayer, rather than a separate sigmoid unit.\n\nan obvious disadvantage of the short list approach is that the potential gener-\nalization advantage of the neural language models is limited to the most frequent\nwords, where, arguably, it is the least useful.\u00a0this disadvantage has stimulated\nthe exploration of alternative methods to deal with high-dimensional outputs,\ndescribed below.\n\n469\n\n "}, {"Page_number": 485, "text": "chapter 12. applications\n\n12.4.3.2 hierarchical softmax\n\n,\n\ngoodman 2001\n\na classical approach (\n) to reducing the computational burden\nof high-dimensional output layers over large vocabulary sets v is to decompose\nprobabilities hierarchically. instead of necessitating a number of computations\nproportional to |\n|v (and also proportional to the number of hidden units, nh),\nthe |\nmorin and\nbengio 2005\n) introduced this factorized approach to the context of neural language\n(\nmodels.\n\n|v factor can be reduced to as low as log|\n\nbengio 2002\n\n) and\n\n|v .\n\n(\n\none can think of this hierarchy as building categories of words, then categories\nof categories of words, then categories of categories of categories of words, etc.\nthese nested categories form a tree, with words at the leaves. in a balanced tree,\nthe tree has depth o(log|\n|v ). the probability of a choosing a word is given by the\nproduct of the probabilities of choosing the branch leading to that word at every\nnode on a path from the root of the tree to the leaf containing the word. fig. 12.4\nillustrates a simple example.\u00a0\n) also describe how to use\nmultiple paths to identify a single word in order to better model words that have\nmultiple meanings. computing the probability of a word then involves summation\nover all of the paths that lead to that word.\n\nmnih and hinton 2009\n\n(\n\nto predict the conditional probabilities required at each node of the tree, we\ntypically use a logistic regression model at each node of the tree, and provide the\nsame context c as input to all of these models. because the correct output is\nencoded in the training set, we can use supervised learning to train the logistic\nregression models. this is typically done using a standard cross-entropy loss,\ncorresponding to maximizing the log-likelihood of the correct sequence of decisions.\n|v\n|v ), its gradients may also be computed efficiently. this includes not\nrather than |\nonly the gradient with respect to the output parameters but also the gradients\nwith respect to the hidden layer activations.\n\nbecause the output log-likelihood can be computed efficiently (as low as log|\n\nit is possible but usually not practical to optimize the tree structure to minimize\nthe expected number of computations. tools from information theory specify how\nto choose the optimal binary code given the relative frequencies of the words. to\ndo so, we could structure the tree so that the number of bits associated with a word\nis approximately equal to the logarithm of the frequency of that word. however, in\npractice, the computational savings are typically not worth the effort because the\ncomputation of the output probabilities is only one part of the total computation\nin the neural language model. for example, suppose there are l fully connected\nhidden layers of width nh. let nb be the weighted average of the number of bits\n\n470\n\n "}, {"Page_number": 486, "text": "chapter 12. applications\n\n(0)\n\n(1)\n\n(0,0)\n\n(0,1)\n\n(1,0)\n\n(1,1)\n\nw0w0\n\nw1w1\n\nw2w2\n\nw3w3\n\nw4w4\n\nw5w5\n\nw6w6\n\nw7w7\n\n(0,0,0)\n\n(0,0,1)\n\n(0,1,0)\n\n(0,1,1)\n\n(1,0,0)\n\n(1,0,1)\n\n(1,1,0)\n\n(1,1,1)\n\n(0\n\nand\n\nfigure 12.4: illustration of a simple hierarchy of word categories, with 8 words w 0, . . . , w7\norganized into a three level hierarchy. the leaves of the tree represent actual specific words.\ninternal nodes represent groups of words. any node can be indexed by the sequence\nof binary decisions (0=left, 1=right) to reach the node from the root. super-class (0)\ncontains the classes (0, 0)\n,1), which respectively contain the sets of words {w0, w1}\n, 1), which\nand {w2, w3}, and similarly super-class\n(1)\nand w6 , w7). if the tree is sufficiently balanced,\nrespectively contain the words (w4, w 5)\nthe maximum depth (number of binary decisions) is on the order of the logarithm of\nthe number of words |\n|v words can be obtained by doing\n|v ) operations (one for each of the nodes on the path from the root). in this example,\no(log |\ncomputing the probability of a word y can be done by multiplying three probabilities,\nassociated with the binary decisions to move left or right at each node on the path from\nthe root to a node y. let bi(y) be the i-th binary decision when traversing the tree\ntowards the value y. the probability of sampling an output y decomposes into a product\nof conditional probabilities, using the chain rule for conditional probabilities, with each\nnode indexed by the prefix of these bits. for example, node (1, 0) corresponds to the\nprefix (b0 (w4) = 1, b1(w4 ) = 0), and the probability of w4 can be decomposed as follows:\n\n|v :\u00a0the choice of one out of |\n\ncontains the classes\n(\n\n(1 , 0)\n\nand\n\n(1\n\np\n\n( = y\n\nw\n\n4) =  (p b0 = 1, b 1 = 0, b2 = 0)\n\n=  (p b0 = 1) (p b 1 = 0 | b0 = 1) (p b 2 = 0 | b0 = 1, b1 = 0).\n\n(12.12)\n(12.13)\n\n471\n\n "}, {"Page_number": 487, "text": "chapter 12. applications\n\nrequired to identify a word, with the weighting given by the frequency of these\nwords. in this example, the number of operations needed to compute the hidden\nactivations grows as as o(ln2\nh) while the output computations grow as o(nhnb ).\nas long as nb \u2264 lnh, we can reduce computation more by shrinking nh than by\nshrinking nb. indeed, nb is often small. because the size of the vocabulary rarely\nexceeds a million words and log2(106) \u2248 20, it is possible to reduce nb to about\n,20\nbut nh is often much larger, around 103 or more. rather than carefully optimizing\n, one can instead define a tree with depth two\na tree with a branching factor of\nand a branching factor ofp|\n|v . such a tree corresponds to simply defining a set\nof mutually exclusive word classes. the simple approach based on a tree of depth\ntwo captures most of the computational benefit of the hierarchical strategy.\n\n2\n\nmorin and bengio 2005\n\none question that remains somewhat open is how to best define these word\nclasses, or how to define the word hierarchy in general. early work used existing\nhierarchies (\n) but the hierarchy can also be learned, ideally\njointly with the neural language model. learning the hierarchy is difficult. an exact\noptimization of the log-likelihood appears intractable because the choice of a word\nhierarchy is a discrete one, not amenable to gradient-based optimization. however,\none could use discrete optimization to approximately optimize the partition of\nwords into word classes.\n\n,\n\nan important advantage of the hierarchical softmax is that it brings computa-\ntional benefits both at training time and at test time, if at test time we want to\ncompute the probability of specific words.\n\nof course, computing the probability of all |\n\n|v words will remain expensive\neven with the hierarchical softmax. another important operation is selecting the\nmost likely word in a given context. unfortunately the tree structure does not\nprovide an efficient and exact solution to this problem.\n\na disadvantage is that in practice the hierarchical softmax tends to give worse\ntest results than sampling-based methods we will describe next. this may be due\nto a poor choice of word classes.\n\n12.4.3.3\n\nimportance sampling\n\none way to speed up the training of neural language models is to avoid explicitly\ncomputing the contribution of the gradient from all of the words that do not appear\nin the next position. every incorrect word should have low probability under the\nmodel. it can be computationally costly to enumerate all of these words. instead,\nit is possible to sample only a subset of the words. using the notation introduced\n\n472\n\n "}, {"Page_number": 488, "text": "chapter 12. applications\n\nin eq.\n\n12.8\n\n, the gradient can be written as follows:\n\n\u2202\n\np y c\nlog (\n\u2202\u03b8\n\n|\n\n)\n\n=\n\n=\n\n=\n\n=\n\n\u2202 log softmaxy( )a\n\n\u2202\u03b8\nea y\n\n\u2202\n\u2202\u03b8\n\u2202\n\u2202\u03b8\n\nlog\n\npi eai\n(ay \u2212 logxi\n\u2202\u03b8 \u2212xi\n\np y\n\n\u2202ay\n\neai )\n\n( = \n\n|\ni c\n\n(12.14)\n\n(12.15)\n\n(12.16)\n\n(12.17)\n\n)\n\n\u2202ai\n\u2202\u03b8\n\nwhere a is the vector of pre-softmax activations (or scores), with one element\nper word. the first term is the positive phase term (pushing ay up) while the\nsecond term is the negative phase term (pushing ai down for all i, with weight\np(i c|\n). since the negative phase term is an expectation, we can estimate it with\na monte carlo sample. however, that would require sampling from the model itself.\nsampling from the model requires computing p (i c|\n) for all i in the vocabulary,\nwhich is precisely what we are trying to avoid.\n\n,\n\n;\n\ninstead of sampling from the model, one can sample from another distribution,\ncalled the proposal distribution (denoted q ), and use appropriate weights to correct\nfor the bias introduced by sampling from the wrong distribution (bengio and\ns\u00e9n\u00e9cal 2003 bengio and s\u00e9n\u00e9cal 2008\n). this is an application of a more general\ntechnique called importance sampling, which will be described in more detail in\n. unfortunately, even exact importance sampling is not efficient because it\nsec.\nrequires computing weights pi/qi, where pi = p (i c|\n), which can only be computed\nif all the scores ai are computed. the solution adopted for this application is called\nbiased importance sampling, where the importance weights are normalized to sum\nto 1. when negative word ni is sampled, the associated gradient is weighted by\n\n17.2\n\n,\n\nwi =\n\n.\n\n(12.18)\n\npni/qn i\nj=1 pnj/qnj\n\npn\n\nthese weights are used to give the appropriate importance to the m negative\nsamples from q used to form the estimated negative phase contribution to the\ngradient:\n\n|\n\n|vxi=1\n\np i c( |\n\n)\n\n\u2202ai\n\u2202\u03b8 \u2248\n\n1\nm\n\nwi\n\n\u2202ani\n\u2202\u03b8\n\n.\n\n(12.19)\n\nmxi=1\n\na unigram or a bigram distribution works well as the proposal distribution q . it is\neasy to estimate the parameters of such a distribution from data. after estimating\nthe parameters, it is also possible to sample from such a distribution very efficiently.\n\n473\n\n "}, {"Page_number": 489, "text": "chapter 12. applications\n\n1\n\nimportance sampling is not only useful for speeding up models with large\nsoftmax outputs. more generally, it is useful for accelerating training with large\nsparse output layers, where the output is a sparse vector rather than a -of-\nn\nchoice. an example is a bag of words. a bag of words is a sparse vector v where vi\nindicates the presence or absence of word i from the vocabulary in the document.\nalternately, vi can indicate the number of times that word i appears.\u00a0machine\nlearning models that emit such sparse vectors can be expensive to train for a\nvariety of reasons. early in learning, the model may not actually choose to make\nthe output truly sparse. moreover, the loss function we use for training might\nmost naturally be described in terms of comparing every element of the output to\nevery element of the target. this means that it is not always clear that there is a\ncomputational benefit to using sparse outputs, because the model may choose to\nmake the majority of the output non-zero and all of these non-zero values need to\nbe compared to the corresponding training target, even if the training target is zero.\ndauphin\n) demonstrated that such models can be accelerated using\nimportance sampling. the efficient algorithm minimizes the loss reconstruction for\nthe \u201cpositive words\u201d (those that are non-zero in the target) and an equal number\nof \u201cnegative words.\u201d the negative words are chosen randomly, using a heuristic to\nsample words that are more likely to be mistaken.\u00a0the bias introduced by this\nheuristic oversampling can then be corrected using importance weights.\n\net al. (\n\n2011\n\nin all of these cases, the computational complexity of gradient estimation for\nthe output layer is reduced to be proportional to the number of negative samples\nrather than proportional to the size of the output vector.\n\n12.4.3.4 noise-contrastive estimation and ranking loss\n\nother approaches based on sampling have been proposed to reduce the computa-\ntional cost of training neural language models with large vocabularies. an early\nexample is the ranking loss proposed by collobert and weston 2008a\n), which\nviews the output of the neural language model for each word as a score and tries to\nmake the score of the correct word ay be ranked high in comparison to the other\nscores ai . the ranking loss proposed then is\n\n(\n\nl =xi\n\nmax(0 1, \u2212 ay + ai).\n\n(12.20)\n\nthe gradient is zero for the i-th term if the score of the observed word, ay, is\ngreater than the score of the negative word ai by a margin of 1. one issue with\nthis criterion is that it does not provide estimated conditional probabilities, which\n\n474\n\n "}, {"Page_number": 490, "text": "chapter 12. applications\n\nare useful in some applications, including speech recognition and text generation\n(including conditional text generation tasks such as translation).\n\na more recently used training objective for neural language model is noise-\ncontrastive estimation, which is introduced in sec.\n. this approach has been\nsuccessfully applied to neural language models (mnih and teh 2012 mnih and\nkavukcuoglu 2013\n\n18.6\n\n).\n\n,\n\n;\n\n,\n\n12.4.4 combining neural language models with -grams\n\nn\n\na major advantage of n-gram models over neural networks is that n-gram models\nachieve high model capacity (by storing the frequencies of very many tuples)\nwhile requiring very little computation to process an example (by looking up\nonly a few tuples that match the current context). if we use hash tables or trees\nto access the counts, the computation used for n-grams is almost independent\nof capacity. in comparison, doubling a neural network\u2019s number of parameters\ntypically also roughly doubles its computation time. exceptions include models\nthat avoid using all parameters on each pass. embedding layers index only a single\nembedding in each pass, so we can increase the vocabulary size without increasing\nthe computation time per example. some other models, such as tiled convolutional\nnetworks, can add parameters while reducing the degree of parameter sharing\nin order to maintain the same amount of computation. however, typical neural\nnetwork layers based on matrix multiplication use an amount of computation\nproportional to the number of parameters.\n\n,\n\n2001 2003\n\none easy way to add capacity is thus to combine both approaches in an ensemble\nconsisting of a neural language model and an n-gram language model (bengio\net al.,\n). as with any ensemble, this technique can reduce test error if\nthe ensemble members make independent mistakes. the field of ensemble learning\nprovides many ways of combining the ensemble members\u2019 predictions, including\nuniform weighting and weights chosen on a validation set. mikolov\n2011a\n)\nextended the ensemble to include not just two models but a large array of models.\nit is also possible to pair a neural network with a maximum entropy model and\ntrain both jointly (mikolov\n). this approach can be viewed as training\na neural network with an extra set of inputs that are connected directly to the\noutput, and not connected to any other part of the model. the extra inputs are\nindicators for the presence of particular n-grams in the input context, so these\nvariables are very high-dimensional and very sparse. the increase in model capacity\n|sv n parameters\u2014but\nis huge\u2014the new portion of the architecture contains up to |\nthe amount of added computation needed to process an input is minimal because\nthe extra inputs are very sparse.\n\net al. (\n\n2011b\n\net al.,\n\n475\n\n "}, {"Page_number": 491, "text": "chapter 12. applications\n\n12.4.5 neural machine translation\n\nmachine translation is the task of reading a sentence in one natural language and\nemitting a sentence with the equivalent meaning in another language.\u00a0machine\ntranslation systems often involve many components. at a high level, there is\noften one component that proposes many candidate translations. many of these\ntranslations will not be grammatical due to differences between the languages. for\nexample, many languages put adjectives after nouns, so when translated to english\ndirectly they yield phrases such as \u201capple red.\u201d the proposal mechanism suggests\nmany variants of the suggested translation, ideally including \u201cred apple.\u201d a second\ncomponent of the translation system, a language model, evaluates the proposed\ntranslations, and can score \u201cred apple\u201d as better than \u201capple red.\u201d\n\n,\n\n;\n\n2006 schwenk 2010\n\nthe earliest use of neural networks for machine translation was to upgrade the\nlanguage model of a translation system by using a neural language model (schwenk\net al.,\n). previously, most machine translation systems had\nused an n-gram model for this component. the n-gram based models used for\nmachine translation include not just traditional back-off n-gram models (jelinek\nand mercer 1980 katz 1987 chen and goodman 1999\n) but also maximum\n), in which an affine-softmax layer\nentropy language models (\npredicts the next word given the presence of frequent\n\n-grams in the context.\n\nberger et al. 1996\n\nn\n\n,\n\n;\n\n,\n\n;\n\n,\n\n,\n\n6.2.1.1\n\ntraditional language models simply report the probability of a natural language\nsentence. because machine translation involves producing an output sentence given\nan input sentence, it makes sense to extend the natural language model to be\nconditional. as described in sec.\n, it is straightforward to extend a model\nthat defines a marginal distribution over some variable to define a conditional\ndistribution over that variable given a context c, where c might be a single variable\nor a list of variables.\n) beat the state-of-the-art in some statistical\nmachine translation benchmarks by using an mlp to score a phrase t 1, t2 , . . . , tk\nin the target language given a phrase s1, s2 , . . . , sn in the source language. the\nmlp estimates p (t1, t2, . . . , tk | s1, s2, . . . , sn). the estimate formed by this mlp\nreplaces the estimate provided by conditional\n\ndevlin et al. 2014\n\n-gram models.\n\nn\n\n(\n\na drawback of the mlp-based approach is that it requires the sequences to be\npreprocessed to be of fixed length. to make the translation more flexible, we would\nlike to use a model that can accommodate variable length inputs and variable\nlength outputs. an rnn provides this ability. sec.\ndescribes several ways\nof constructing an rnn that represents a conditional distribution over a sequence\ngiven some input, and sec.\ndescribes how to accomplish this conditioning\nwhen the input is a sequence. in all cases, one model first reads the input sequence\nand emits a data structure that summarizes the input sequence. we call this\n\n10.2.4\n\n10.4\n\n476\n\n "}, {"Page_number": 492, "text": "chapter 12. applications\n\noutput\u00a0object\u00a0(english\u00a0\n\nsentence)\n\ndecoder\n\nintermediate,\u00a0semantic\u00a0representation\n\nencoder\n\nsource\u00a0object\u00a0(french\u00a0sentence\u00a0or\u00a0image)\n\nfigure 12.5: the encoder-decoder architecture to map back and forth between a surface\nrepresentation (such as a sequence of words or an image) and a semantic representation.\nby using the output of an encoder of data from one modality (such as the encoder mapping\nfrom french sentences to hidden representations capturing the meaning of sentences) as\nthe input to a decoder for another modality (such as the decoder mapping from hidden\nrepresentations capturing the meaning of sentences to english), we can train systems that\ntranslate from one modality to another. this idea has been applied successfully not just\nto machine translation but also to caption generation from images.\n\n,\n\nsummary the \u201ccontext\u201d c. the context c may be a list of vectors, or it may be a\nvector or tensor. the model that reads the input to produce c may be an rnn\n(\n) or a convolutional\n;\ncho et al. 2014a sutskever\nnetwork (kalchbrenner and blunsom 2013\n).\u00a0a second model, usually an rnn,\nthen reads the context c and generates a sentence in the target language. this\ngeneral idea of an encoder-decoder framework for machine translation is illustrated\nin fig.\n\n2014 jean\n,\n\net al.,\n\net al.,\n\n.\n12.5\n\n2014\n\n;\n\nin order to generate an entire sentence conditioned on the source sentence, the\nmodel must have a way to represent the entire source sentence.\u00a0earlier models\nwere only able to represent individual words or phrases.\u00a0from a representation\nlearning point of view, it can be useful to learn a representation in which sentences\nthat have the same meaning have similar representations regardless of whether\nthey were written in the source language or the target language. this strategy was\nexplored first using a combination of convolutions and rnns (kalchbrenner and\n). later work introduced the use of an rnn for scoring proposed\nblunsom 2013\ntranslations (\nsutskever\net al.\n).\n,\n\n) and for generating translated sentences (\n) scaled these models to larger vocabularies.\n\ncho et al. 2014a\n(\n2014\njean\n\n,\net al.\n\n2014\n\n,\n\n477\n\n "}, {"Page_number": 493, "text": "chapter 12. applications\n\n12.4.5.1 using an attention mechanism and aligning pieces of data\n\ncc\n\n+\n\n\u21b5(t(cid:129)1)\n\u21b5(t(cid:129)1)\n\n\u21b5 ( )t\u21b5 ( )t\n\nt\n\n\u21b5( +1)\nt\u21b5( +1)\n\n\u21e5\u21e5\n\n\u21e5\u21e5\n\n\u21e5\u21e5\n\nh(t(cid:129)1)\nh(t(cid:129)1)\n\nh( )th( )t\n\nt\n\nh( +1)\nth( +1)\n\n(\n\nbahdanau et al. 2015\n\nfigure 12.6: a modern attention mechanism, as introduced by\n), is\nessentially a weighted average. a context vector c is formed by taking a weighted average\nof feature vectors h( )t with weights \u03b1( )t . in some applications, the feature vectors h are\nhidden units of a neural network, but they may also be raw input to the model. the\nweights \u03b1( )t are produced by the model itself. they are usually values in the interval\n[0, 1] and are intended to concentrate around just one h( )t so that the weighted average\napproximates reading that one specific time step precisely. the weights \u03b1 ( )t are usually\nproduced by applying a softmax function to relevance scores emitted by another portion\nof the model. the attention mechanism is more expensive computationally than directly\nindexing the desired h( )t , but direct indexing cannot be trained with gradient descent. the\nattention mechanism based on weighted averages is a smooth, differentiable approximation\nthat can be trained with existing optimization algorithms.\n\n) and\n\n2014a\n\net al. (\n\nsutskever\n\nusing a fixed-size representation to capture all the semantic details of a very\nlong sentence of say 60 words is very difficult.\u00a0it can be achieved by training a\nsufficiently large rnn well enough and for long enough, as demonstrated by cho\net al. (\n). however, a more efficient approach is\nto read the whole sentence or paragraph (to get the context and the gist of what\nis being expressed), then produce the translated words one at a time, each time\nfocusing on a different part of the input sentence in order to gather the semantic\ndetails that are required to produce the next output word.\u00a0that is exactly the\nidea that\n) first introduced. the attention mechanism used\nto focus on specific parts of the input sequence at each time step is illustrated in\nfig.\n\nbahdanau et al. 2015\n\n12.6\n.\n\n2014\n\n(\n\nwe can think of an attention-based system as having three components:\n\n478\n\n "}, {"Page_number": 494, "text": "chapter 12. applications\n\n1. a process that \u201c reads\u201d raw data (such as source words in a source sentence),\nand converts them into distributed representations, with one feature vector\nassociated with each word position.\n\n2. a list of feature vectors storing the output of the reader. this can be\nunderstood as a \u201c\n\u201d\u00a0containing a sequence of facts, which can be\nretrieved later, not necessarily in the same order, without having to visit all\nof them.\n\nmemory\n\n3. a process that \u201c\n\n\u201d the content of the memory to sequentially perform\na task, at each time step having the ability put attention on the content of\none memory element (or a few, with a different weight).\n\nexploits\n\nthe third component generates the translated sentence.\n\nwhen words in a sentence written in one language are aligned with correspond-\ning words in a translated sentence in another language, it becomes possible to relate\nthe corresponding word embeddings. earlier work showed that one could learn a\nkind of translation matrix relating the word embeddings in one language with the\nword embeddings in another (ko\u010disk\u00fd\n), yielding lower alignment error\nrates than traditional approaches based on the frequency counts in the phrase table.\nthere is even earlier work on learning cross-lingual word vectors (klementiev et al.,\n2012). many extensions to this approach are possible. for example, more efficient\ncross-lingual alignment (\n\n) allows training on larger datasets.\n\ngouws et al. 2014\n\net al.,\n\n2014\n\n,\n\n12.4.6 historical perspective\n\n1986a\n\nthe idea of distributed representations for symbols was introduced by rumelhart\net al. (\n) in one of the first explorations of back-propagation, with symbols\ncorresponding to the identity of family members and the neural network capturing\nthe relationships between family members, with training examples forming triplets\nsuch as (colin, mother, victoria).\u00a0the first layer of the neural network learned\na representation of each family member. for example,\u00a0the features for colin\nmight represent which family tree colin was in, what branch of that tree he was\nin, what generation he was from, etc. one can think of the neural network as\ncomputing learned rules relating these attributes together in order to obtain the\ndesired predictions. the model can then make predictions such as inferring who is\nthe mother of colin.\n\nthe idea of forming an embedding for a symbol was extended to the idea of an\n). these embeddings were learned\n\nembedding for a word by deerwester\nusing the svd. later, embeddings would be learned by neural networks.\n\net al. (\n\n1990\n\n479\n\n "}, {"Page_number": 495, "text": "chapter 12. applications\n\nthe history of natural language processing is marked by transitions in the\npopularity of different ways of representing the input to the model. following\nthis early work on symbols or words, some of the earliest applications of neural\nnetworks to nlp (\n) represented\nthe input as a sequence of characters.\n\nmiikkulainen and dyer 1991 schmidhuber 1996\n\n,\n\n;\n\n,\n\n2001\n\nbengio\n\net al. (\n\n) returned the focus to modeling words and introduced\nneural language models, which produce interpretable word embeddings. these\nneural models have scaled up from defining representations of a small set of symbols\nin the 1980s to millions of words (including proper nouns and misspellings) in\nmodern applications. this computational scaling effort led to the invention of the\ntechniques described above in sec.\n\n12.4.3\n.\n\ninitially, the use of words as the fundamental units of language models yielded\nimproved language\u00a0modeling performance (\n). to this day,\nnew techniques continually push both character-based models (sutskever et al.,\n2011) and word-based models forward, with recent work (\n) even\nmodeling individual bytes of unicode characters.\n\nbengio et al. 2001\n\ngillick et al. 2015\n\n,\n\n,\n\nthe ideas behind neural language models have been extended into several\nnatural language processing applications, such as parsing (\nhenderson 2003 2004\n;\ncollobert 2011\n), part-of-speech tagging, semantic role labeling, chunking, etc,\nsometimes using a single multi-task learning architecture (collobert and weston,\n2008a collobert\n;\n) in which the word embeddings are shared across\ntasks.\n\net al.,\n\n2011a\n\n,\n\n,\n\n,\n\ntwo-dimensional visualizations of embeddings became a popular tool for an-\nalyzing language models following the development of the t-sne dimensionality\nreduction algorithm (van der maaten and hinton 2008\n) and its high-profile appli-\ncation to visualization word embeddings by joseph turian in 2009.\n\n,\n\n12.5 other applications\n\nin this section we cover a few other types of applications of deep learning that\nare different from the standard object recognition, speech recognition and natural\nlanguage processing tasks discussed above. part\nof this book will expand\nthat scope even further to include tasks requiring the ability to generate rich\nhigh-dimensional samples (unlike \u201cthe next word,\u201d in language models).\n\niii\n\n480\n\n "}, {"Page_number": 496, "text": "chapter 12. applications\n\n12.5.1 recommender systems\n\none of the major families of applications of machine learning in the information\ntechnology sector is the ability to make recommendations of items to potential\nusers or customers. two major types of applications can be distinguished: online\nadvertising and item recommendations (often these recommendations are still for\nthe purpose of selling a product). both rely on predicting the association between\na user and an item, either to predict the probability of some action (the user\nbuying the product, or some proxy for this action) or the expected gain (which\nmay depend on the value of the product) if an ad is shown or a recommendation is\nmade regarding that product to that user. the internet is currently financed in\ngreat part by various forms of online advertising.\u00a0there are major parts of the\neconomy that rely on online shopping.\u00a0companies including amazon and ebay\nuse machine learning, including deep learning, for their product recommendations.\nsometimes, the items are not products that are actually for sale. examples include\nselecting posts to display on social network news feeds, recommending movies to\nwatch, recommending jokes, recommending advice from experts, matching players\nfor video games, or matching people in dating services.\n\noften, this association problem is handled like a supervised learning problem:\ngiven some information about the item and about the user, predict the proxy of\ninterest (user clicks on ad, user enters a rating, user clicks on a \u201clike\u201d button, user\nbuys product, user spends some amount of money on the product, user spends\ntime visiting a page for the product, etc). this often ends up being either a\nregression problem (predicting some conditional expected value) or a probabilistic\nclassification problem (predicting the conditional probability of some discrete\nevent).\n\nthe early work on recommender systems relied on minimal information as\ninputs for these predictions: the user id and the item id. in this context, the only\nway to generalize is to rely on the similarity between the patterns of values of the\ntarget variable for different users or for different items. suppose that user 1 and\nuser 2 both like items a, b and c. from this, we may infer that user 1 and user 2\nhave similar tastes. if user 1 likes item d, then this should be a strong cue that\nuser 2 will also like d. algorithms based on this principle come under the name of\ncollaborative filtering. both non-parametric approaches (such as nearest-neighbor\nmethods based on the estimated similarity between patterns of preferences) and\nparametric methods are possible. parametric methods often rely on learning a\ndistributed representation (also called an embedding) for each user and for each\nitem. bilinear prediction of the target variable (such as a rating) is a simple\nparametric method that is highly successful and often found as a component of\n\n481\n\n "}, {"Page_number": 497, "text": "chapter 12. applications\n\nstate-of-the-art systems. the prediction is obtained by the dot product between\nthe user embedding and the item embedding (possibly corrected by constants that\ndepend only on either the user id or the item id). let \u02c6r be the matrix containing\nour predictions, a a matrix with user embeddings in its rows and b a matrix with\nitem embeddings in its columns. let b and c be vectors that contain respectively\na kind of bias for each user (representing how grumpy or positive that user is\nin general) and for each item (representing its general popularity). the bilinear\nprediction is thus obtained as follows:\n\n\u02c6ru,i = bu + ci +xj\n\na u,jbj,i.\n\n(12.21)\n\ntypically one wants to minimize the squared error between predicted ratings\n\u02c6ru,i and actual ratings ru,i. user embeddings and item embeddings can then be\nconveniently visualized when they are first reduced to a low dimension (two or\nthree), or they can be used to compare users or items against each other, just\nlike word embeddings.\u00a0one way to obtain these embeddings is by performing a\nsingular value decomposition of the matrix r of actual targets (such as ratings).\nthis corresponds to factorizing r = u dv 0 (or a normalized variant) into the\nproduct of two factors, the lower rank matrices a = u d and b = v 0. one\nproblem with the svd is that it treats the missing entries in an arbitrary way,\nas if they corresponded to a target value of 0. instead we would like to avoid\npaying any cost for the predictions made on missing entries. fortunately, the\nsum of squared errors on the observed ratings can also be easily minimized by\nboth\ngradient-based optimization. the svd and the bilinear prediction of eq.\nbennett and lanning\nperformed very well in the competition for the netflix prize (\n,\n2007), aiming at predicting ratings for films, based only on previous ratings by\na large set of anonymous users.\u00a0many machine learning experts participated in\nthis competition, which took place between 2006 and 2009. it raised the level of\nresearch in recommender systems using advanced machine learning and yielded\nimprovements in recommender systems. even though it did not win by itself,\nthe simple bilinear prediction or svd was a component of the ensemble models\npresented by most of the competitors, including the winners (\nt\u00f6scher et al. 2009\n;\nkoren 2009\n\n12.21\n\n).\n\n,\n\n,\n\nbeyond these bilinear models with distributed representations, one of the first\nuses of neural networks for collaborative filtering is based on the rbm undirected\n). rbms were an important element\nprobabilistic model (salakhutdinov\nof the ensemble of methods that won the netflix competition (t\u00f6scher\n2009\n;\nkoren 2009\n). more advanced variants on the idea of factorizing the ratings matrix\nhave also been explored in the neural networks community (salakhutdinov and\n\net al.,\n\net al.,\n\n2007\n\n,\n\n482\n\n "}, {"Page_number": 498, "text": "chapter 12. applications\n\nmnih 2008\n\n,\n\n).\n\nhowever, there is a basic limitation of collaborative filtering systems: when a\nnew item or a new user is introduced, its lack of rating history means that there\nis no way to evaluate its similarity with other items or users (respectively), or\nthe degree of association between, say, that new user and existing items. this\nis called the problem of cold-start recommendations. a general way of solving\nthe cold-start recommendation problem is to introduce extra information about\nthe individual users and items. for example, this extra information could be user\nprofile information or features of each item. systems that use such information are\ncalled content-based recommender systems. the mapping from a rich set of user\nfeatures or item features to an embedding can be learned through a deep learning\narchitecture (\n\nhuang et al. 2013 elkahky\n\net al.,\n\n2015\n\n).\n\n,\n\n;\n\nspecialized deep learning architectures such as convolutional networks have also\nbeen applied to learn to extract features from rich content such as from musical\naudio tracks, for music recommendation (van den o\u00f6rd\n). in that work,\nthe convolutional net takes acoustic features as input and computes an embedding\nfor the associated song. the dot product between this song embedding and the\nembedding for a user is then used to predict whether a user will listen to the song.\n\net al.,\n\n2013\n\n12.5.1.1 exploration versus exploitation\n\n,\n\n;\n\n,\n\nwhen making recommendations to users, an issue arises that goes beyond ordinary\nsupervised learning and into the realm of reinforcement learning. many recommen-\ndation problems are most accurately described theoretically as contextual bandits\n(\nlangford and zhang 2008 lu et al. 2010\n).\u00a0the issue is that when we use the\nrecommendation system to collect data, we get a biased and incomplete view of\nthe preferences of users: we only see the responses of users to the items they were\nrecommended and not to the other items. in addition, in some cases we may not\nget any information on users for whom no recommendation has been made (for\nexample, with ad auctions, it may be that the price proposed for an ad was below\na minimum price threshold, or does not win the auction, so the ad is not shown at\nall). more importantly, we get no information about what outcome would have\nresulted from recommending any of the other items. this would be like training a\nclassifier by picking one class \u02c6y for each training example x (typically the class\nwith the highest probability according to the model) and then only getting as\nfeedback whether this was the correct class or not. clearly, each example conveys\nless information than in the supervised case where the true label y is directly\naccessible, so more examples are necessary. worse, if we are not careful, we could\nend up with a system that continues picking the wrong decisions even as more\n\n483\n\n "}, {"Page_number": 499, "text": "chapter 12. applications\n\nand more data is collected, because the correct decision initially had a very low\nprobability: until the learner picks that correct decision, it does not learn about\nthe correct decision. this is similar to the situation in reinforcement learning\nwhere only the reward for the selected action is observed. in general, reinforcement\nlearning can involve a sequence of many actions and many rewards. the bandits\nscenario is a special case of reinforcement learning, in which the learner takes only\na single action and receives a single reward. the bandit problem is easier in the\nsense that the learner knows which reward is associated with which action. in\nthe general reinforcement learning scenario, a high reward or a low reward might\nhave been caused by a recent action or by an action in the distant past. the term\ncontextual bandits refers to the case where the action is taken in the context of\nsome input variable that can inform the decision. for example, we at least know\nthe user identity, and we want to pick an item. the mapping from context to\naction is also called a policy. the feedback loop between the learner and the data\ndistribution (which now depends on the actions of the learner) is a central research\nissue in the reinforcement learning and bandits literature.\n\nreinforcement learning requires choosing a tradeoff between exploration and\nexploitation. exploitation refers to taking actions that come from the current, best\nversion of the learned policy\u2014actions that we know will achieve a high reward.\nexploration refers to taking actions specifically in order to obtain more training\ndata. if we know that given context x, action a gives us a reward of 1, we do not\nknow whether that is the best possible reward. we may want to exploit our current\npolicy and continue taking action a in order to be relatively sure of obtaining a\nreward of 1. however, we may also want to explore by trying action a0. we do not\nknow what will happen if we try action a0 . we hope to get a reward of\n, but we\n2\n. either way, we at least gain some knowledge.\nrun the risk of getting a reward of\n0\n\nexploration can be implemented in many ways, ranging from occasionally\ntaking random actions intended to cover the entire space of possible actions, to\nmodel-based approaches that compute a choice of action based on its expected\nreward and the model\u2019s amount of uncertainty about that reward.\n\nmany factors determine the extent to which we prefer exploration or exploitation.\none of the most prominent factors is the time scale we are interested in.\u00a0if the\nagent has only a short amount of time to accrue reward, then we prefer more\nexploitation. if the agent has a long time to accrue reward, then we begin with\nmore exploration so that future actions can be planned more effectively with more\nknowledge. as time progresses and our learned policy improves, we move toward\nmore exploitation.\n\nsupervised\u00a0learning has\u00a0no tradeoff\u00a0between\u00a0exploration and\u00a0exploitation\n\n484\n\n "}, {"Page_number": 500, "text": "chapter 12. applications\n\nbecause the supervision signal always specifies which output is correct for each\ninput. there is no need to try out different outputs to determine if one is better\nthan the model\u2019s current output\u2014we always know that the label is the best output.\n\nanother difficulty arising in the context of reinforcement learning, besides the\nexploration-exploitation trade-off, is the difficulty of evaluating and comparing\ndifferent policies. reinforcement learning involves interaction between the learner\nand the environment. this feedback loop means that it is not straightforward to\nevaluate the learner\u2019s performance using a fixed set of test set input values. the\npolicy itself determines which inputs will be seen.\n) present\ntechniques for evaluating contextual bandits.\n\ndudik et al. 2011\n\n(\n\n12.5.2 knowledge representation, reasoning and question an-\n\nswering\n\n,\n\nrumelhart et al. 1986a\n\ndeep learning approaches have been very successful in language modeling, machine\ntranslation and natural language processing due to the use of embeddings for\nsymbols (\ndeerwester et al. 1990 bengio et al.\n,\n2001). these embeddings represent semantic knowledge about individual words\nand concepts. a research frontier is to develop embeddings for phrases and for\nrelations between words and facts. search engines already use machine learning for\nthis purpose but much more remains to be done to improve these more advanced\nrepresentations.\n\n) and words (\n\n,\n\n;\n\n12.5.2.1 knowledge, relations and question answering\n\nindexrelations one interesting research direction is determining how distributed\nrepresentations can be trained to capture the relations between two entities. these\nrelations allow us to formalize facts about objects and how objects interact with\neach other.\n\nin mathematics, a binary relation is a set of ordered pairs of objects. pairs\nthat are in the set are said to have the relation while those who are not in the set\ndo not. for example, we can define the relation \u201cis less than\u201d on the set of entities\n{1, 2, 3} by defining the set of ordered pairs s = {(1,2), (1, 3),(2 , 3)}. once this\nrelation is defined, we can use it like a verb. because (1, 2) \u2208 s, we say that 1 is\nless than 2. because (2,1) 6\u2208 s, we can not say that 2 is less than 1. of course, the\nentities that are related to one another need not be numbers. we could define a\nrelation\n\ncontaining tuples like (\n\nis_a_type_of\n\ndog mammal\n\n).\n\n,\n\nin the context of ai, we think of a relation as a sentence in a syntactically\n\n485\n\n "}, {"Page_number": 501, "text": "chapter 12. applications\n\nsimple and highly structured language. the relation plays the role of a verb,\nwhile two arguments to the relation play the role of its subject and object. these\nsentences take the form of a triplet of tokens\n\n(subject verb object)\n\n,\n\n,\n\nwith values\n\n(entityi , relationj, entity k).\n\n(12.22)\n\n(12.23)\n\nwe can also define an\n\nattribute\n\n, a concept analogous to a relation, but taking\n\nonly one argument:\n\n(entityi, attribute j).\n\n(12.24)\n\nfor example, we could define the has_fur attribute, and apply it to entities like\ndog.\n\nmany applications require representing relations and reasoning about them.\n\nhow should we best do this within the context of neural networks?\n\nmachine learning models of course require training data. we can infer relations\nbetween entities from training datasets consisting of unstructured natural language.\nthere are also structured databases that identify relations explicitly. a common\nstructure for these databases is the relational database, which stores this same\nkind of information,\u00a0albeit\u00a0not formatted as three token sentences. when\u00a0a\ndatabase is intended to convey commonsense knowledge about everyday life or\nexpert knowledge about an application area to an artificial intelligence system,\nwe call\u00a0the database a\u00a0knowledge base. knowledge bases range from\u00a0general\nones like freebase, opencyc, wordnet, or wikibase,1 etc. to more specialized\nknowledge bases, like geneontology.2 representations for entities and relations\ncan be learned by considering each triplet in a knowledge base as a training example\nand maximizing a training objective that captures their joint distribution (bordes\net al.,\n\n2013a\n\n).\n\nin addition to training data, we also need to define a model family to train.\na common approach is to extend neural language models to model entities and\nrelations. neural language models learn a vector that provides a distributed\nrepresentation of each word. they also learn about interactions between words,\nsuch as which word is likely to come after a sequence of words, by learning functions\nof these vectors. we can extend this approach to entities and relations by learning\nan embedding vector for each relation. in fact, the parallel between modeling\n\n1respectively available\u00a0from these\u00a0web\u00a0sites: freebase.com , cyc.com/opencyc , wordnet.\n\nprinceton.edu wikiba.se\n\n,\n\n2geneontology.org\n\n486\n\n "}, {"Page_number": 502, "text": "chapter 12. applications\n\n;\n\n,\n\n,\n\net al.,\n\nbordes et al. 2011 2012 wang\n\nknowledge bases\n2014a\n\nlanguage and modeling knowledge encoded as relations is so close that researchers\nand\nhave trained representations of such entities by using both\nnatural language sentences (\n) or\ncombining data from multiple relational databases (\n). many\npossibilities exist for the particular parametrization associated with such a model.\nearly work on learning about relations between entities (\npaccanaro and hinton\n,\n2000) posited highly constrained parametric forms (\u201clinear relational embeddings\u201d),\noften using a different form of representation for the relation than for the entities.\nfor example,\n) used vectors for\nentities and matrices for relations, with the idea that a relation acts like an operator\non entities. alternatively, relations can be considered as any other entity (bordes\net al.,\n), allowing us to make statements about relations, but more flexibility is\nput in the machinery that combines them in order to model their joint distribution.\n\npaccanaro and hinton 2000\n\nbordes et al. 2013b\n\nbordes et al. 2011\n\n) and\n\n2012\n\n(\n\n(\n\n,\n\na practical short-term application of such models is link prediction: predicting\nmissing arcs in the knowledge graph. this is a form of generalization to new\nfacts, based on old facts. most of the knowledge bases that currently exist have\nbeen constructed through manual labor, which tends to leave many and probably\nthe majority of true relations absent from the knowledge base. see wang et al.\n(\n2014b lin et al. 2015\n) for examples of such an\napplication.\n\ngarcia-duran et al. 2015\n\n) and\n\n),\n\n(\n\n(\n\nevaluating the performance of a model on a link prediction task is difficult\nbecause we have only a dataset of positive examples (facts that are known to\nbe true).\u00a0if the model proposes a fact that is not in the dataset, we are unsure\nwhether the model has made a mistake or discovered a new, previously unknown\nfact. the metrics are thus somewhat imprecise and are based on testing how the\nmodel ranks a held-out of set of known true positive facts compared to other facts\nthat are less likely to be true. a common way to construct interesting examples\nthat are probably negative (facts that are probably false) is to begin with a true\nfact and create corrupted versions of that fact, for example by replacing one entity\nin the relation with a different entity selected at random. the popular precision at\n10% metric counts how many times the model ranks a \u201ccorrect\u201d fact among the\ntop 10% of all corrupted versions of that fact.\n\nanother application of knowledge bases and distributed representations for\n),\nthem is word-sense disambiguation (navigli and velardi 2005 bordes\nwhich is the task of deciding which of the senses of a word is the appropriate one,\nin some context.\n\net al.,\n\n2012\n\n,\n\n;\n\neventually, knowledge of relations combined with a reasoning process and\nunderstanding of natural language could allow us to build a general question\n\n487\n\n "}, {"Page_number": 503, "text": "chapter 12. applications\n\nanswering system. a general question answering system must be able to process\ninput information and remember important facts, organized in a way that enables\nit to retrieve and reason about them later. this remains a difficult open problem\nwhich can only be solved in restricted \u201ctoy\u201d environments. currently, the best\napproach to remembering and retrieving specific declarative facts is to use an\n. memory networks were\nexplicit memory mechanism, as described in sec.\nfirst proposed to solve a toy question answering task (weston\n2014 kumar\net al. (\n) have proposed an extension that uses gru recurrent nets to read\nthe input into the memory and to produce the answer given the contents of the\nmemory.\n\net al.,\n\n10.12\n\n2015\n\n).\n\ndeep learning has been applied to many other applications besides the ones\ndescribed here, and will surely be applied to even more after this writing. it would\nbe impossible to describe anything remotely resembling a comprehensive coverage\nof such a topic. this survey provides a representative sample of what is possible\nas of this writing.\n\nii\n\nthis concludes part\n\n, which has described modern practices involving deep\nnetworks, comprising all of the most successful methods. generally speaking, these\nmethods involve using the gradient of a cost function to find the parameters of a\nmodel that approximates some desired function. with enough training data, this\napproach is extremely powerful. we now turn to part\n, in which we step into the\nterritory of research\u2014methods that are designed to work with less training data\nor to perform a greater variety of tasks, where the challenges are more difficult\nand not as close to being solved as the situations we have described so far.\n\niii\n\n488\n\n "}, {"Page_number": 504, "text": "part iii\n\ndeep learning research\n\n489\n\n "}, {"Page_number": 505, "text": "this part of the book describes the more ambitious and advanced approaches\n\nto deep learning, currently pursued by the research community.\n\nin the previous parts of the book, we have shown how to solve supervised\nlearning problems\u2014how to learn to map one vector to another, given enough\nexamples of the mapping.\n\nnot all problems we might want to solve fall into this category. we may\nwish to generate new examples, or determine how likely some point is, or handle\nmissing values and take advantage of a large set of unlabeled examples or examples\nfrom related tasks. a shortcoming of the current state of the art for industrial\napplications is that our learning algorithms require large amounts of supervised\ndata to achieve good accuracy.\nin this part of the book, we discuss some of\nthe speculative approaches to reducing the amount of labeled data necessary\nfor existing models to work well and be applicable across a broader range of\ntasks. accomplishing these goals usually requires some form of unsupervised or\nsemi-supervised learning.\n\nmany deep learning algorithms have been designed to tackle unsupervised\nlearning problems, but none have truly solved the problem in the same way that\ndeep learning has largely solved the supervised learning problem for a wide variety of\ntasks. in this part of the book, we describe the existing approaches to unsupervised\nlearning and some of the popular thought about how we can make progress in this\nfield.\n\na central cause of the difficulties with unsupervised learning is the high di-\nmensionality of the random variables being modeled. this brings two distinct\nchallenges: a statistical challenge and a computational challenge. the statistical\nchallenge regards generalization:\u00a0the number of configurations we may want to\ndistinguish can grow exponentially with the number of dimensions of interest, and\nthis quickly becomes much larger than the number of examples one can possibly\nhave (or use with bounded computational resources). the computational challenge\nassociated with high-dimensional distributions arises because many algorithms for\nlearning or using a trained model (especially those based on estimating an explicit\nprobability function) involve intractable computations that grow exponentially\nwith the number of dimensions.\n\nwith probabilistic models, this computational challenge arises from the need to\nperform intractable inference or simply from the need to normalize the distribution.\n\n\u2022 intractable inference:\n\n. it\nregards the question of guessing the probable values of some variables a,\ngiven other variables b, with respect to a model that captures the joint\n\ninference is discussed mostly in chapter\n\n19\n\n490\n\n "}, {"Page_number": 506, "text": "distribution between a, b and c. in order to even compute such conditional\nprobabilities one needs to sum over the values of the variables c, as well as\ncompute a normalization constant which sums over the values of a and c.\n\n18\n\n\u2022 intractable normalization constants (the partition function): the\npartition function is discussed mostly in chapter\n. normalizing constants\nof probability functions come up in inference (above) as well as in learning.\nmany probabilistic models involve such a normalizing constant. unfortu-\nnately, learning such a model often requires computing the gradient of the\nlogarithm of the partition function with respect to the model parameters.\nthat computation is generally as intractable as computing the partition\nfunction itself. monte carlo markov chain (mcmc) methods (chapter\n)17\nare often used to deal with the partition function (computing it or its gradi-\nent). unfortunately, mcmc methods suffer when the modes of the model\ndistribution are numerous and well-separated, especially in high-dimensional\nspaces (sec.\n\n17.5\n\n).\n\none way to confront these intractable computations is to approximate them,\nand many approaches have been proposed as discussed in this third part of the\nbook. another interesting way,\u00a0also discussed here,\u00a0would be to avoid these\nintractable computations altogether by design, and methods that do not require\nsuch computations are thus very appealing. several generative models have been\nproposed in recent years, with that motivation. a wide variety of contemporary\napproaches to generative modeling are discussed in chapter\n\n.20\n\niii\n\npart\n\nis the most important for a researcher\u2014someone who wants to un-\nderstand the breadth of perspectives that have been brought to the field of deep\nlearning, and push the field forward towards true artificial intelligence.\n\n491\n\n "}, {"Page_number": 507, "text": "chapter 13\n\nlinear factor models\n\nmany of the research frontiers in deep learning involve building a probabilistic model\nof the input, pmodel(x). such a model can, in principle, use probabilistic inference to\npredict any of the variables in its environment given any of the other variables. many\nx h|\n.\nof these models also have latent variables h, with pmodel( ) = \n)\nthese latent variables provide another means of representing the data. distributed\nrepresentations based\u00a0on latent\u00a0variables can obtain\u00a0all of the advantages of\nrepresentation learning that we have seen with deep feedforward and recurrent\nnetworks.\n\nehpmodel(\n\nx\n\nin this chapter, we describe some of the simplest probabilistic models with\nlatent variables: linear factor models. these models are sometimes used as building\nblocks of mixture models (hinton\n,\n1995a ghahramani and hinton 1996\n;\n). they\nroweis\nalso show many of the basic approaches necessary to build generative models that\nthe more advanced deep models will extend further.\n\n) or larger, deep probabilistic models (\n\net al.,\n\net al.,\n\net al.,\n\ntang\n\n2012\n\n2002\n\n;\n\na linear factor model is defined by the use of a stochastic, linear decoder\n\nfunction that generates\n\nx\n\nby adding noise to a linear transformation of\n\n.\nh\n\nthese models are interesting because they allow us to discover explanatory\nfactors that have a simple joint distribution. the simplicity of using a linear decoder\nmade these models some of the first latent variable models to be extensively studied.\n\na linear factor model describes the data generation process as follows. first,\n\nwe sample the explanatory factors\n\nh\n\nfrom a distribution\n\nwhere p(h) is a factorial distribution, with p(h) =\n\ni p(hi), so that it is easy to\n\nh \u223c p\n\n,\n( )h\n\n(13.1)\n\n492\n\nq\n\n "}, {"Page_number": 508, "text": "chapter 13. linear factor models\n\nsample from. next we sample the real-valued observable variables given the factors:\n\nx w h b\n\n+ + noise\n\n= \n\n(13.2)\n\nwhere the noise is typically gaussian and diagonal (independent across dimensions).\nthis is illustrated in fig.\n\n.\n13.1\n\nh1h1\n\nx1x1\n\nh2h2\n\nx2x2\n\nh3h3\n\nx3x3\n\nx\nx\n\n= w + +b\n= w + +b\n\nh\nh\n\nn ois e\nn ois e\n\nfigure 13.1: the directed graphical model describing the linear factor model family, in\nwhich we assume that an observed data vector x is obtained by a linear combination of\nindependent latent factors h, plus some noise. different models, such as probabilistic\npca, factor analysis or ica, make different choices about the form of the noise and of\nthe prior\n\n.\np( )h\n\n13.1 probabilistic pca and factor analysis\n\nprobabilistic pca (principal components analysis), factor analysis and other linear\nfactor models are special cases of the above equations (\n) and only\ndiffer in the choices made for the model\u2019s prior over latent variables h before\nobserving\n\nand noise distributions.\n\n13.2\n\n13.1\n\nand\n\nx\n\nin\n\nfactor analysis bartholomew 1987 basilevsky 1994\n\n(\n\n,\n\n;\n\n,\n\n), the latent variable\n\nprior is just the unit variance gaussian\n\nh\n\n\u223c n ( ;h , i)\n\n0\n\n(13.3)\n\nwhile the observed variables xi are assumed to be conditionally independent, given h.\nspecifically, the noise is assumed to be drawn from a diagonal covariance gaussian\nn]> a\ndistribution, with covariance matrix \u03c8 = diag(\u03c32), with \u03c32 = [\u03c32\nvector of per-variable variances.\n\n2 , . . . , \u03c32\n\n1, \u03c32\n\nthe role of the latent variables is thus to capture the dependencies between\nthe different observed variables x i. indeed, it can easily be shown that x is just a\nmultivariate normal random variable, with\n\nx \u223c n ( ;x b w w,\n\n> + )\u03c8 .\n\n493\n\n(13.4)\n\n "}, {"Page_number": 509, "text": "chapter 13. linear factor models\n\nin order to cast pca in a probabilistic framework,\u00a0we can make a slight\nmodification to the factor analysis model, making the conditional variances \u03c32\ni\nequal to each other. in that case the covariance of x is just w w > + \u03c32i , where\n\u03c32 is now a scalar. this yields the conditional distribution\n\n(13.5)\n\n(13.6)\n\nor equivalently\n\nx \u223c n ( ;x b w w,\n\n> + \u03c3 2i)\n\nh\nwhere z \u223c n(z ; 0, i) is gaussian noise.\niterative em algorithm for estimating the parameters\n\n= w + +b\n\ntipping and bishop 1999\n\u03c32 .\n\n(\nandw\n\nx\n\n\u03c3\n\nz\n\n) then show an\n\nthis probabilistic pca model takes advantage of the observation that most\nvariations in the data can be captured by the latent variables h, up to some\nsmall residual reconstruction error \u03c32. as shown by\n),\ntipping and bishop 1999\nprobabilistic pca becomes pca as \u03c3 \u2192 0. in that case, the conditional expected\nb\u2212 onto the space\nvalue of h given x becomes an orthogonal projection of x\nspanned by the\n\n, like in pca.\n\ncolumns of\n\nw\n\nd\n\n(\n\nas \u03c3 \u2192 0, the density model defined by probabilistic pca becomes very sharp\naround these d dimensions spanned by the columns of w . this can make the\nmodel assign very low likelihood to the data if the data does not actually cluster\nnear a hyperplane.\n\n13.2\n\nindependent component analysis (ica)\n\n;\n\n,\n\nindependent component analysis (ica) is among the oldest representation learning\n;\nherault\u00a0and ans 1984 jutten\u00a0and herault 1991 comon 1994\nalgorithms (\nhyv\u00e4rinen 1999 hyv\u00e4rinen\n).\n2003\nit is an approach to modeling linear factors that seeks to separate an observed\nsignal into many underlying signals that are scaled and added together to form\nthe observed data. these signals are intended to be fully independent, rather than\nmerely decorrelated from each other. 1\n\n;\n2001a hinton\n\n;\u00a0\n,\n2001 teh\n\n,\net al.,\n\n,\net al.,\n\net al.,\n\n;\n\n;\n\nmany different specific methodologies are referred to as ica. the variant\nthat is most similar to the other generative models we have described here is a\nvariant (\n) that trains a fully parametric generative model. the\nprior distribution over the underlying factors, p(h), must be fixed ahead of time by\nthe user. the model then deterministically generates x = w h. we can perform\n\npham et al. 1992\n\n,\n\n1see sec.\n\nvariables.\n\n3.8\n\nfor a discussion of the difference between uncorrelated variables and independent\n\n494\n\n "}, {"Page_number": 510, "text": "chapter 13. linear factor models\n\na nonlinear change of variables (using eq.\nmodel then proceeds as usual, using maximum likelihood.\n\n) to determine\n\n3.47\n\np(x). learning the\n\nthe motivation for this approach is that by choosing p(h) to be independent,\nwe can recover underlying factors that are as close as possible to independent.\nthis is commonly used, not to capture high-level abstract causal factors, but to\nrecover low-level signals that have been mixed together.\nin this setting, each\ntraining example is one moment in time, each xi is one sensor\u2019s observation of\nthe mixed signals, and each hi is one estimate of one of the original signals. for\nexample, we might have n people speaking simultaneously. if we have n different\nmicrophones placed in different locations, ica can detect the changes in the volume\nbetween each speaker as heard by each microphone, and separate the signals so\nthat each hi contains only one person speaking clearly. this is commonly used\nin neuroscience for electroencephalography, a technology for recording electrical\nsignals originating in the brain. many electrode sensors placed on the subject\u2019s\nhead are used to measure many electrical signals coming from the body. the\nexperimenter is typically only interested in signals from the brain, but signals from\nthe subject\u2019s heart and eyes are strong enough to confound measurements taken\nat the subject\u2019s scalp. the signals arrive at the electrodes mixed together, so\nica is necessary to separate the electrical signature of the heart from the signals\noriginating in the brain, and to separate signals in different brain regions from\neach other.\n\nas mentioned before, many variants of ica are possible. some add some noise\nin the generation of x rather than using a deterministic decoder. most do not\nuse the maximum likelihood criterion, but instead aim to make the elements of\nh = w \u22121x independent from each other. many criteria that accomplish this goal\nare possible. eq.\nw , which can be an\nexpensive and numerically unstable operation. some variants of ica avoid this\nproblematic operation by constraining\n\nrequires taking the determinant of\n\nto be orthonormal.\n\n3.47\n\nw\n\nall variants of ica require that p(h) be non-gaussian. this is because if p(h)\nis an independent prior with gaussian components, then w is not identifiable.\nwe can obtain the same distribution over p(x) for many values of w . this is very\ndifferent from other linear factor models like probabilistic pca and factor analysis,\nthat often require p(h) to be gaussian in order to make many operations on the\nmodel have closed form solutions. in the maximum likelihood approach where the\nuser explicitly specifies the distribution, a typical choice is to use p (hi ) = d\n\u03c3(hi).\ndhi\ntypical choices of these non-gaussian distributions have larger peaks near 0 than\ndoes the gaussian distribution, so we can also see most implementations of ica\nas learning sparse features.\n\n495\n\n "}, {"Page_number": 511, "text": "chapter 13. linear factor models\n\nmany variants of ica are not generative models in the sense that we use the\nphrase. in this book, a generative model either represents p(x) or can draw samples\nfrom it. many variants of ica only know how to transform between x and h, but\ndo not have any way of representing p(h), and thus do not impose a distribution\nover p(x ). for example, many ica variants aim to increase the sample kurtosis of\nh = w \u22121x, because high kurtosis indicates that p(h) is non-gaussian, but this is\naccomplished without explicitly representing p(h). this is because ica is more\noften used as an analysis tool for separating signals, rather than for generating\ndata or estimating its density.\n\n(\n\n14\n\n2000\n\n). another nonlinear extension of ica is the approach of\n\njust as pca can be generalized to the nonlinear autoencoders described in\nchapter\n, ica can be generalized to a nonlinear generative model, in which\nwe use a nonlinear function f to generate the observed data. see hyv\u00e4rinen\nand pajunen\u00a0 1999\n) for the initial\u00a0work\u00a0on nonlinear ica and\u00a0its successful\nlappalainen\nuse with ensemble learning by roberts and\u00a0everson\u00a0 2001\net al. (\nnonlinear\n), which stacks a\nindependent components estimation, or nice (\nseries of invertible transformations (encoder stages) that have the property that\nthe determinant of the jacobian of each transformation can be computed efficiently.\nthis makes it possible to compute the likelihood exactly and, like ica, attempts\nto transform the data into a space where it has a factorized marginal distribution,\nbut is more likely to succeed thanks to the nonlinear encoder. because the encoder\nis associated with a decoder that is its perfect inverse, it is straightforward to\ngenerate samples from the model (by first sampling from p(h) and then applying\nthe decoder).\n\ndinh et al. 2014\n\n) and\u00a0\n\n(\n\n,\n\n;\n\n,\n\net al.,\n\n2001b\n\nanother generalization of ica is to learn groups of features, with statistical\ndependence allowed within a group but discouraged between groups (hyv\u00e4rinen\nand hoyer 1999 hyv\u00e4rinen\n). when the groups of related units are\nchosen to be non-overlapping, this is called independent subspace analysis. it is also\npossible to assign spatial coordinates to each hidden unit and form overlapping\ngroups of spatially neighboring units. this encourages nearby units to learn similar\nfeatures. when applied to natural images, this topographic ica approach learns\ngabor filters, such that neighboring features have similar orientation, location or\nfrequency. many different phase offsets of similar gabor functions occur within\neach region, so that pooling over small regions yields translation invariance.\n\n13.3 slow feature analysis\n\nslow feature analysis (sfa) is a linear factor model that uses information from\n\n496\n\n "}, {"Page_number": 512, "text": "chapter 13. linear factor models\n\ntime signals to learn invariant features (\n\nwiskott and sejnowski 2002\n\n,\n\n).\n\nslow feature analysis is motivated by a general principle called the slowness\nprinciple. the idea is that the important characteristics of scenes change very\nslowly compared to the individual measurements that make up a description of a\nscene. for example, in computer vision, individual pixel values can change very\nrapidly. if a zebra moves from left to right across the image, an individual pixel\nwill rapidly change from black to white and back again as the zebra\u2019s stripes pass\nover the pixel. by comparison, the feature indicating whether a zebra is in the\nimage will not change at all, and the feature describing the zebra\u2019s position will\nchange slowly.\u00a0we therefore may wish to regularize our model to learn features\nthat change slowly over time.\n\nthe slowness principle predates slow feature analysis and has been applied\nhinton 1989 f\u00f6ldi\u00e1k 1989 mobahi et al. 2009\n;\nto a wide variety of models (\nbergstra and bengio 2009\n). in general, we can apply the slowness principle to any\ndifferentiable model trained with gradient descent. the slowness principle may be\nintroduced by adding a term to the cost function of the form\n\n,\n\n;\n\n,\n\n;\n\n,\n\n,\n\nl f( (x( +1)\n(, f x( )t ))\n)\n\nt\n\n(13.7)\n\n\u03bbxt\n\nwhere \u03bb is a hyperparameter determining the strength of the slowness regularization\nterm, t is the index into a time sequence of examples, f is the feature extractor\nto be regularized, and l is a loss function measuring the distance between f( x( )t )\nand f (x ( +1)\n\nis the mean squared difference.\n\n). a common choice for\n\nl\n\nt\n\nslow feature analysis is a particularly efficient application of the slowness\nprinciple. it is efficient because it is applied to a linear feature extractor, and can\nthus be trained in closed form. like some variants of ica, sfa is not quite a\ngenerative model per se, in the sense that it defines a linear map between input\nspace and feature space but does not define a prior over feature space and thus\ndoes not impose a distribution\n\non input space.\n\np( )x\n\nthe sfa algorithm (wiskott and sejnowski 2002\n\n,\n\n) consists of defining f (x; \u03b8 )\n\nto be a linear transformation, and solving the optimization problem\n\nsubject to the constraints\n\nand\n\net( (f x ( +1)\n\nt\n\nmin\n\n\u03b8\n\n)i \u2212 f (x( )t )i)2\n\netf (x( )t )i = 0\n\net[ (f x( )t )2\n\ni ] = 1.\n\n497\n\n(13.8)\n\n(13.9)\n\n(13.10)\n\n "}, {"Page_number": 513, "text": "chapter 13. linear factor models\n\nthe constraint that the learned feature have zero mean is necessary to make the\nproblem have a unique solution; otherwise we could add a constant to all feature\nvalues and obtain a different solution with equal value of the slowness objective.\nthe constraint that the features have unit variance is necessary to prevent the\npathological solution where all features collapse to . like pca, the sfa features\nare ordered, with the first feature being the slowest. to learn multiple features, we\nmust also add the constraint\n\n0\n\n\u2200i < j, et [ (f x( )t )if (x( )t )j ] = 0.\n\n(13.11)\n\nthis specifies that the learned features must be linearly decorrelated from each\nother. without this constraint, all of the learned features would simply capture the\none slowest signal. one could imagine using other mechanisms, such as minimizing\nreconstruction error,\u00a0to\u00a0force the\u00a0features to diversify,\u00a0but\u00a0this decorrelation\nmechanism admits a simple solution due to the linearity of sfa features. the sfa\nproblem may be solved in closed form by a linear algebra package.\n\nsfa is typically used to learn nonlinear features by applying a nonlinear basis\nexpansion to x before running sfa. for example, it is common to replace x by the\nquadratic basis expansion, a vector containing elements xi xj for all i and j. linear\nsfa modules may then be composed to learn deep nonlinear slow feature extractors\nby repeatedly learning a linear sfa feature extractor, applying a nonlinear basis\nexpansion to its output, and then learning another linear sfa feature extractor on\ntop of that expansion.\n\nwhen trained on small spatial patches of videos of natural scenes, sfa with\nquadratic basis expansions learns features that share many characteristics with\nthose of complex cells in v1 cortex (berkes and wiskott 2005\n). when trained\non videos of random motion within 3-d computer rendered environments, deep\nsfa learns features that share many characteristics with the features represented\nby neurons in rat brains that are used for navigation (franzius\n). sfa\nthus seems to be a reasonably biologically plausible model.\n\net al.,\n\n2007\n\n,\n\na major advantage of sfa is that it is possibly to theoretically predict which\nfeatures sfa will learn, even in the deep, nonlinear setting. to make such theoretical\npredictions, one must know about the dynamics of the environment in terms of\nconfiguration space\u00a0(e.g.,\u00a0in the\u00a0case of random\u00a0motion in the\u00a03-d rendered\nenvironment, the theoretical analysis proceeds from knowledge of the probability\ndistribution over position and velocity of the camera). given the knowledge of how\nthe underlying factors actually change, it is possible to analytically solve for the\noptimal functions expressing these factors. in practice, experiments with deep sfa\napplied to simulated data seem to recover the theoretically predicted functions.\n\n498\n\n "}, {"Page_number": 514, "text": "chapter 13. linear factor models\n\nthis is in comparison to other learning algorithms where the cost function depends\nhighly on specific pixel values, making it much more difficult to determine what\nfeatures the model will learn.\n\n2008\n\net al.,\n\ndeep sfa has also been used to learn features for object recognition and pose\nestimation (franzius\n). so far, the slowness principle has not become\nthe basis for any state of the art applications. it is unclear what factor has limited\nits performance. we speculate that perhaps the slowness prior is too strong, and\nthat, rather than imposing a prior that features should be approximately constant,\nit would be better to impose a prior that features should be easy to predict from\none time step to the next. the position of an object is a useful feature regardless of\nwhether the object\u2019s velocity is high or low, but the slowness principle encourages\nthe model to ignore the position of objects that have high velocity.\n\n13.4 sparse coding\n\n,\u00a0\n\nolshausen\u00a0and field 1996\n\nsparse\u00a0coding (\n)\u00a0is a\u00a0linear\u00a0factor model\u00a0that\u00a0has\nbeen heavily studied as an unsupervised feature learning and feature extraction\nmechanism.\u00a0strictly speaking, the term \u201csparse coding\u201d refers to the process of\ninferring the value of h in this model, while \u201csparse modeling\u201d refers to the process\nof designing and learning the model, but the term \u201csparse coding\u201d is often used to\nrefer to both.\n\nlike most other linear factor models, it uses a linear decoder plus noise to\nobtain reconstructions of x, as specified in eq.\n. more specifically, sparse\ncoding models typically assume that the linear factors have gaussian noise with\nisotropic precision :\u03b2\n\n13.2\n\nx h|\n(\np\n\n) = \n\n( ;\n\nn x w h b\n+\n,\n\n1\n\u03b2\n\ni).\n\n(13.12)\n\nthe distribution p(h) is chosen to be one with sharp peaks near 0 (olshausen\nand field 1996\n). common choices include factorized laplace, cauchy or factorized\nstudent-t distributions. for example, the laplace prior parametrized in terms of\nthe sparsity penalty coefficient\n\nis given by\n\n\u03bb\n\n,\n\np h( i) = laplace(hi; 0,\n\n2\n\u03bb\n\n) =\n\n\u03bb\n4\n\ne\u2212 1\n\n2 \u03bb h| i|\n\nand the student- prior by\n\nt\n\np h( i) \u221d\n\n1\n(1 + h 2\n\u03bd )\n499\n\ni\n\n.\n\n\u03bd+1\n\n2\n\n(13.13)\n\n(13.14)\n\n "}, {"Page_number": 515, "text": "chapter 13. linear factor models\n\ntraining sparse coding with maximum likelihood is intractable. instead, the\ntraining alternates between encoding the data and training the decoder to better\nreconstruct the data given the encoding. this approach will be justified further as\na principled approximation to maximum likelihood later, in sec.\n\n19.3\n.\n\nfor models such as pca, we have seen the use of a parametric encoder function\nthat predicts h and consists only of multiplication by a weight matrix. the encoder\nthat we use with sparse coding is not a parametric encoder. instead, the encoder\nis an optimization algorithm, that solves an optimization problem in which we seek\nthe single most likely code value:\n\nh\u2217 =  ( ) = arg max\n\nf x\n\nh\n\nh x|\n(\np\n)\n.\n\n(13.15)\n\n13.13\n\nand eq.\n\n13.12\n\n, this yields the following optimization\n\nwhen combined with eq.\nproblem:\n\narg max\n\nh\n\n= arg max\n\nh\n\n= arg min\n\nh\n\np(\n\nh x|\n)\np h x|\nlog (\n)\n||h 1 + \u03b2|| \u2212\n\n\u03bb||\n\nx w h 2\n||\n2 ,\n\n(13.16)\n\n(13.17)\n\n(13.18)\n\nwhere we have dropped terms not depending on h and divided by positive scaling\nfactors to simplify the equation.\n\ndue to the imposition of an l1 norm on h, that this procedure will yield a\n\nsparse h\u2217 (see sec.\n\n7.1.2\n\n).\n\nto train the model rather than just perform inference, we alternate between\nminimization with respect to h and minimization with respect to w. in this\npresentation, we treat \u03b2 as a hyperparameter. typically it is set to 1 because its\nrole in this optimization problem is shared with \u03bb and there is no need for both\nhyperparameters. in principle, we could also treat \u03b2 as a parameter of the model\nand learn it. our presentation here has discarded some terms that do not depend\non h but do depend on \u03b2. to learn \u03b2, these terms must be included, or \u03b2 will\ncollapse to .0\n\nnot all approaches to sparse coding explicitly build a p( h) and a p(x h|\n).\noften we are just interested in learning a dictionary of features with activation\nvalues that will often be zero when extracted using this inference procedure.\n\nif we sample h from a laplace prior, it is in fact a zero probability event for\nan element of h to actually be zero. the generative model itself is not especially\n) describe approximate\nsparse, only the feature extractor is.\n\ngoodfellow et al. 2013d\n\n(\n\n500\n\n "}, {"Page_number": 516, "text": "chapter 13. linear factor models\n\ninference in a different model family, the spike and slab sparse coding model, for\nwhich samples from the prior usually contain true zeros.\n\nthe sparse coding approach combined with the use of the non-parametric\nencoder can in principle minimize the combination of reconstruction error and\nlog-prior better than any specific parametric encoder. another advantage is that\nthere is no generalization error to the encoder. a parametric encoder must learn\nhow to map x to h in a way that generalizes. for unusual x that do not resemble\nthe training data, a learned, parametric encoder may fail to find an h that results\nin accurate reconstruction or a sparse code. for the vast majority of formulations\nof sparse coding models, where the inference problem is convex, the optimization\nprocedure will always find the optimal code (unless degenerate cases such as\nreplicated weight vectors occur). obviously, the sparsity and reconstruction costs\ncan still rise on unfamiliar points, but this is due to generalization error in the\ndecoder weights, rather than generalization error in the encoder. the lack of\ngeneralization error in sparse coding\u2019s optimization-based encoding process may\nresult in better generalization when sparse coding is used as a feature extractor for\na classifier than when a parametric function is used to predict the code. coates\nand ng 2011\n) demonstrated that sparse coding features generalize better for\nobject recognition tasks than the features of a related model based on a parametric\nencoder, the linear-sigmoid autoencoder. inspired by their work, goodfellow et al.\n(\n2013d\n) showed that a variant of sparse coding generalizes better than other feature\nextractors in the regime where extremely few labels are available (twenty or fewer\nlabels per class).\n\n(\n\n14\n\nthe primary disadvantage of the non-parametric encoder is that it requires\ngreater time to compute h given x because the non-parametric approach requires\nrunning an iterative algorithm. the parametric autoencoder approach, developed\nin chapter\u00a0 ,\u00a0uses only a fixed\u00a0number\u00a0of layers,\u00a0often only one. another\ndisadvantage is that it is not straight-forward to back-propagate through the\nnon-parametric encoder, which makes it difficult to pretrain a sparse coding model\nwith an unsupervised criterion and then fine-tune it using a supervised criterion.\nmodified versions of sparse coding that permit approximate derivatives do exist\nbut are not widely used (\n\nbagnell and bradley 2009\n\n).\n\n,\n\n13.2\n\nsparse coding, like other linear factor models, often produces poor samples,\n. this happens even when the model is able to reconstruct\nas shown in fig.\nthe data well and provide useful features for a classifier. the reason is that each\nindividual feature may be learned well, but the factorial prior on the hidden code\nresults in the model including random subsets of all of the features in each generated\nsample. this motivates the development of deeper models that can impose a non-\n\n501\n\n "}, {"Page_number": 517, "text": "chapter 13. linear factor models\n\nfigure 13.2:\u00a0example samples and weights from a spike and slab sparse coding model\ntrained on the mnist dataset. (left) the samples from the model do not resemble the\ntraining examples.\u00a0at first glance, one might assume the model is poorly fit.\nthe\nweight vectors of the model have learned to represent penstrokes and sometimes complete\ndigits. the model has thus learned useful features. the problem is that the factorial prior\nover features results in random subsets of features being combined. few such subsets\nare appropriate to form a recognizable mnist digit. this motivates the development of\ngenerative models that have more powerful distributions over their latent codes. figure\nreproduced with permission from goodfellow\n\net al. (\n\n(right)\n\n2013d\n\n).\n\nfactorial distribution on the deepest code layer, as well as the development of more\nsophisticated shallow models.\n\n13.5 manifold interpretation of pca\n\n,\n\nhinton et al. 1997\n\nlinear factor models including pca and factor analysis can be interpreted as\nlearning a manifold (\n). we can view probabilistic pca as\ndefining a thin pancake-shaped region of high probability\u2014a gaussian distribution\nthat is very narrow along some axes, just as a pancake is very flat along its vertical\naxis, but is elongated along other axes, just as a pancake is wide along its horizontal\naxes. this is illustrated in fig.\n. pca can be interpreted as aligning this\npancake with a linear manifold in a higher-dimensional space. this interpretation\napplies not just to traditional pca but also to any linear autoencoder that learns\nmatrices w and v with the goal of making the reconstruction of x lie as close to\nx as possible,\n\n13.3\n\nlet the encoder be\n\nh\n\n=  (f\n\nx w\n\n) = \n\n>(\n\nx \u00b5\u2212 .\n)\n\n(13.19)\n\n502\n\n "}, {"Page_number": 518, "text": "chapter 13. linear factor models\n\nthe encoder computes a low-dimensional representation of h. with the autoencoder\nview, we have a decoder computing the reconstruction\n\n\u02c6x\n\n=  (g\n\nh\n\n) =  +\n\nb v h\n.\n\n(13.20)\n\nfigure 13.3: flat gaussian capturing probability concentration near a low-dimensional\nmanifold. the figure shows the upper half of the \u201cpancake\u201d above the \u201cmanifold plane\u201d\nwhich goes through its middle. the variance in the direction orthogonal to the manifold is\nvery small (arrow pointing out of plane) and can be considered like \u201cnoise,\u201d while the other\nvariances are large (arrows in the plane) and correspond to \u201csignal,\u201d and a coordinate\nsystem for the reduced-dimension data.\n\nthe choices of linear encoder and decoder that minimize reconstruction error\n\ne[|| \u2212x\n\n\u02c6x|| 2]\n\n(13.21)\n\ncorrespond to v = w, \u00b5 = b = e[x] and the columns of w form an orthonormal\nbasis which spans the same subspace as the principal eigenvectors of the covariance\nmatrix\n\nc\n\n= \n\nx \u00b5 x \u00b5\n\n[(e \u2212 )( \u2212 )>].\n\n(13.22)\n\nin the case of pca, the columns of w are these eigenvectors, ordered by the\nmagnitude of the corresponding eigenvalues (which are all real and non-negative).\n\none can also show that eigenvalue \u03bbi of c corresponds to the variance of x\nin the direction of eigenvector v( )i . if x \u2208 rd and h \u2208 rd with d < d, then the\n\n503\n\n "}, {"Page_number": 519, "text": "chapter 13. linear factor models\n\noptimal reconstruction error (choosing\n\n\u00b5 b v\n\n,\n\n,\n\nand\n\nw\n\nas above) is\n\nmin [e || \u2212x \u02c6x||2] =\n\ndxi d= +1\n\n\u03bbi.\n\n(13.23)\n\nhence, if the covariance has rank d , the eigenvalues \u03bbd+1 to \u03bbd are 0 and recon-\nstruction error is 0.\n\nfurthermore, one can also show that the above solution can be obtained by\n, instead of\n\n, under orthonormal\n\nw\n\nh\n\nmaximizing the variances of the elements of\nminimizing reconstruction error.\n\nlinear factor models are some of the simplest generative models and some of the\nsimplest models that learn a representation of data. much as linear classifiers and\nlinear regression models may be extended to deep feedforward networks, these linear\nfactor models may be extended to autoencoder networks and deep probabilistic\nmodels that perform the same tasks but with a much more powerful and flexible\nmodel family.\n\n504\n\n "}, {"Page_number": 520, "text": "chapter 14\n\nautoencoders\n\nan autoencoder is a neural network that is trained to attempt to copy its input\nto its output. internally, it has a hidden layer h that describes a code used to\nrepresent the input. the network may be viewed as consisting of two parts: an\nencoder function h = f (x) and a decoder that produces a reconstruction r = g(h).\nthis architecture is presented in fig.\n. if an autoencoder succeeds in simply\nlearning to set g(f (x)) = x everywhere, then it is not especially useful. instead,\nautoencoders are designed to be unable to learn to copy perfectly. usually they are\nrestricted in ways that allow them to copy only approximately, and to copy only\ninput that resembles the training data. because the model is forced to prioritize\nwhich aspects of the input should be copied, it often learns useful properties of the\ndata.\n\n14.1\n\nx h|\n.\n)\n\nmodern\u00a0autoencoders\u00a0have generalized\u00a0the\u00a0idea of\u00a0an encoder\u00a0and\u00a0a de-\n) and\n\ncoder beyond deterministic functions to stochastic mappings pencoder(h x|\npdecoder(\n\n;\n\n,\n\n;\n\n,\n\nthe idea of autoencoders has been part of the historical landscape of neural\nnetworks for decades (\nlecun 1987 bourlard and kamp 1988 hinton and zemel\n,\n1994). traditionally,\u00a0autoencoders\u00a0were used\u00a0for dimensionality reduction or\nfeature learning. recently, theoretical connections between autoencoders and\nlatent variable models have brought autoencoders to the forefront of generative\nmodeling, as we will see in chapter\n. autoencoders may be thought of as being\na special case of feedforward networks, and may be trained with all of the same\ntechniques, typically minibatch gradient descent following gradients computed\nby back-propagation. unlike general feedforward networks, autoencoders may\nalso be trained using recirculation (\n), a learning\nalgorithm based on comparing the activations of the network on the original input\n\nhinton and mcclelland 1988\n\n20\n\n,\n\n505\n\n "}, {"Page_number": 521, "text": "chapter 14. autoencoders\n\nto the activations on the reconstructed input. recirculation is regarded as more\nbiologically plausible than back-propagation, but is rarely used for machine learning\napplications.\n\nhh\n\ng\n\nrr\n\nf\n\nxx\n\nfigure 14.1:\u00a0the general structure an autoencoder, mapping an input x to an output\n(called reconstruction) r through an internal representation or code h . the autoencoder\nhas two components: the encoder f (mapping x to h) and the decoder g (mapping h to\nr).\n\n14.1 undercomplete autoencoders\n\ncopying the input to the output may sound useless, but we are typically not\ninterested in the output of the\u00a0decoder.\ninstead,\u00a0we hope\u00a0that training the\nautoencoder to perform the input copying task will result in h taking on useful\nproperties.\n\none way to obtain useful features from the autoencoder is to constrain h to\nhave smaller dimension than x. an autoencoder whose code dimension is less\nthan the input dimension is called undercomplete. learning an undercomplete\nrepresentation forces the autoencoder to capture the most salient features of the\ntraining data.\n\nthe learning process is described simply as minimizing a loss function\n\nl , g f\n\n(x ( ( )))x\n\n(14.1)\n\nwhere l is a loss function penalizing g(f (x)) for being dissimilar from x, such as\nthe mean squared error.\n\nwhen the decoder is linear and l is the mean squared error, an undercomplete\nautoencoder learns to span the same subspace as pca. in this case, an autoencoder\ntrained to perform the copying task has learned the principal subspace of the\ntraining data as a side-effect.\n\nautoencoders with nonlinear encoder functions f and nonlinear decoder func-\ntions g can thus learn a more powerful nonlinear generalization of pca. unfortu-\n\n506\n\n "}, {"Page_number": 522, "text": "chapter 14. autoencoders\n\nnately, if the encoder and decoder are allowed too much capacity, the autoencoder\ncan learn to perform the copying task without extracting useful information about\nthe distribution of the data. theoretically, one could imagine that an autoencoder\nwith a one-dimensional code but a very powerful nonlinear encoder could learn to\nrepresent each training example x( )i with the code i. the decoder could learn to\nmap these integer indices back to the values of specific training examples.\u00a0this\nspecific scenario does not occur in practice, but it illustrates clearly that an autoen-\ncoder trained to perform the copying task can fail to learn anything useful about\nthe dataset if the capacity of the autoencoder is allowed to become too great.\n\n14.2 regularized autoencoders\n\nundercomplete autoencoders, with code dimension less than the input dimension,\ncan learn the most salient features of the data distribution.\u00a0we have seen that\nthese autoencoders fail to learn anything useful if the encoder and decoder are\ngiven too much capacity.\n\na similar problem occurs if the hidden code is allowed to have dimension equal\nto the input, and in the overcomplete case in which the hidden code has dimension\ngreater than the input. in these cases, even a linear encoder and linear decoder\ncan learn to copy the input to the output without learning anything useful about\nthe data distribution.\n\nideally, one could train any architecture of autoencoder successfully, choosing\nthe code dimension and the capacity of the encoder and decoder based on the\ncomplexity of distribution to be modeled. regularized autoencoders provide the\nability to do so. rather than limiting the model capacity by keeping the encoder\nand decoder shallow and the code size small, regularized autoencoders use a loss\nfunction that encourages the model to have other properties besides the ability\nto copy its input to its output. these other properties include sparsity of the\nrepresentation, smallness of the derivative of the representation, and robustness\nto noise or to missing inputs. a regularized autoencoder can be nonlinear and\novercomplete but still learn something useful about the data distribution even if\nthe model capacity is great enough to learn a trivial identity function.\n\nin addition to the methods described here which are most naturally interpreted\nas regularized autoencoders, nearly any generative model with latent variables\nand equipped with an inference procedure (for computing latent representations\ngiven input) may be viewed as a particular form of autoencoder. two generative\nmodeling approaches that emphasize this connection with autoencoders are the\n), such as the variational\ndescendants of the helmholtz machine (\n\nhinton et al. 1995b\n\n,\n\n507\n\n "}, {"Page_number": 523, "text": "chapter 14. autoencoders\n\n20.10.3\n\n) and the generative stochastic networks (sec.\n\nautoencoder (sec.\n).\n20.12\nthese models naturally learn high-capacity, overcomplete encodings of the input\nand do not require regularization for these encodings to be useful. their encodings\nare naturally useful because the models were trained to approximately maximize\nthe probability of the training data rather than to copy the input to the output.\n\n14.2.1 sparse autoencoders\n\na sparse autoencoder is simply an autoencoder whose training criterion involves a\nsparsity penalty \u03c9(h) on the code layer h, in addition to the reconstruction error:\n\nl , g f\n\n(x ( ( ))) + \u03c9( )\nh\n\nx\n\n(14.2)\n\nwhere g (h) is the decoder output and typically we have h = f (x ), the encoder\noutput.\n\nsparse autoencoders are typically used to learn features for another task such\nas classification. an autoencoder that has been regularized to be sparse must\nrespond to unique statistical features of the dataset it has been trained on, rather\nthan simply acting as an identity function. in this way, training to perform the\ncopying task with a sparsity penalty can yield a model that has learned useful\nfeatures as a byproduct.\n\nwe can\u00a0think of the\u00a0penalty \u03c9(h) simply\u00a0as a regularizer\u00a0term added to\na feedforward network whose primary task is to copy the input to the output\n(unsupervised learning objective) and possibly also perform some supervised task\n(with\u00a0a supervised\u00a0learning\u00a0objective) that depends\u00a0on these\u00a0sparse\u00a0features.\nunlike other regularizers such as weight decay,\u00a0there is not a straightforward\nbayesian interpretation to this regularizer. as described in sec.\n, training\nwith weight\u00a0decay and\u00a0other regularization penalties\u00a0can be interpreted as a\nmap approximation to bayesian inference, with the added regularizing penalty\ncorresponding to a prior probability distribution over the model parameters. in\nthis view, regularized maximum likelihood corresponds to maximizing p(\u03b8 x|\n),\nwhich is equivalent to maximizing log p(x \u03b8|\n) term\nis the usual data log-likelihood term and the log p(\u03b8) term, the log-prior over\nparameters, incorporates the preference over particular values of \u03b8. this view\nwas described in sec.\n. regularized autoencoders defy such an interpretation\nbecause the regularizer depends on the data and is therefore by definition not a\nprior in the formal sense of the word. we can still think of these regularization\nterms as implicitly expressing a preference over functions.\n\n) + log p(\u03b8).\u00a0the log p(x \u03b8|\n\n5.6.1\n\n5.6\n\nrather than thinking of the sparsity penalty as a regularizer for the copying\ntask, we can think of the entire sparse autoencoder framework as approximating\n\n508\n\n "}, {"Page_number": 524, "text": "chapter 14. autoencoders\n\nmaximum likelihood\u00a0training of a generative model\u00a0that has latent\u00a0variables.\nsuppose we have a model with visible variables x and latent variables h, with\nan explicit joint distribution pmodel(x h,\n). we refer to\npmodel(h) as the model\u2019s prior distribution over the latent variables, representing\nthe model\u2019s beliefs prior to seeing x. this is different from the way we have\npreviously used the word \u201cprior,\u201d to refer to the distribution p (\u03b8) encoding our\nbeliefs about the model\u2019s parameters before we have seen the training data. the\nlog-likelihood can be decomposed as\n\n) = pmodel(h)pmodel(x h|\n\nlog pmodel( ) = log\n\nx\n\nxh\n\npmodel (\n\n)h x,\n.\n\n(14.3)\n\nwe can think of the autoencoder as approximating this sum with a point estimate\nfor just one highly likely value for h. this is similar to the sparse coding generative\nmodel (sec.\nh being the output of the parametric encoder rather\nthan the result of an optimization that infers the most likely h. from this point of\nview, with this chosen , we are maximizing\n\n), but with\n\n13.4\n\nh\n\nlog pmodel(\n\nh x,\n\n) = log\n\npmodel( ) + log\n\nh\n\npmodel (\n\nx h|\n)\n.\n\n(14.4)\n\nthe log p model( )h term can be sparsity-inducing. for example, the laplace prior,\n\npmodel(hi ) =\n\ne\u2212 |\u03bb hi|,\n\n\u03bb\n2\n\n(14.5)\n\ncorresponds to an absolute value sparsity penalty. expressing the log-prior as an\nabsolute value penalty, we obtain\n\nh\n\n\u03c9( ) = \n\n\u03bbxi\n\u2212 log pmodel ( ) =h xi (cid:18)\u03bb h|\n\n|hi|\n\ni| \u2212 log\n\n\u03bb\n\n2(cid:19) = \u03c9( ) + const\n\nh\n\n(14.6)\n\n(14.7)\n\nwhere the constant term depends only on \u03bb and not h. we typically treat \u03bb as a\nhyperparameter and discard the constant term since it does not affect the parameter\nlearning. other priors such as the student-t prior can also induce sparsity. from\nthis point of view of sparsity as resulting from the effect of pmodel(h) on approximate\nmaximum likelihood learning, the sparsity penalty is not a regularization term at\nall.\u00a0it is just a consequence of the model\u2019s distribution over its latent variables.\nthis view provides a different motivation for training an autoencoder: it is a way\nof approximately training a generative model. it also provides a different reason for\n\n509\n\n "}, {"Page_number": 525, "text": "chapter 14. autoencoders\n\nwhy the features learned by the autoencoder are useful: they describe the latent\nvariables that explain the input.\n\n,\n\n,\n\nranzato et al. 2007a 2008\n\nearly work on sparse autoencoders (\n\n) explored\nvarious forms of sparsity and proposed a connection between the sparsity penalty\nand the log z term that arises when applying maximum likelihood to an undirected\nprobabilistic model p(x) = 1\nz \u02dcp(x). the idea is that minimizing log z prevents a\nprobabilistic model from having high probability everywhere, and imposing sparsity\non\u00a0an autoencoder\u00a0prevents the\u00a0autoencoder\u00a0from having\u00a0low\u00a0reconstruction\nerror everywhere.\nin this case,\u00a0the connection is on the level of an intuitive\nunderstanding of a general mechanism rather than a mathematical correspondence.\nthe interpretation of the sparsity penalty as corresponding to log pmodel(h) in a\ndirected model pmodel ( )h pmodel(\n\nis more mathematically straightforward.\n\none way to achieve actual zeros in h for sparse (and denoising) autoencoders\nwas introduced in\n). the idea is to use rectified linear units to\nproduce the code layer. with a prior that actually pushes the representations to\nzero (like the absolute value penalty), one can thus indirectly control the average\nnumber of zeros in the representation.\n\nglorot et al. 2011b\n\n(\n\nx h|\n)\n\n14.2.2 denoising autoencoders\n\nrather than adding a penalty\nto the cost function, we can obtain an autoencoder\nthat learns something useful by changing the reconstruction error term of the cost\nfunction.\n\n\u03c9 \n\ntraditionally, autoencoders minimize some function\n\nl , g f\n\n(x ( ( )))x\n\n(14.8)\n\nwhere l is a loss function penalizing g(f (x)) for being dissimilar from x, such as\nthe l2 norm of their difference.\u00a0this encourages g\nto learn to be merely an\nidentity function if they have the capacity to do so.\n\nf\u25e6\n\na denoising autoencoder\n\nor\n\ndae\n\ninstead minimizes\n\nl , g f\n\n(x ( (\u02dcx))),\n\n(14.9)\n\nwhere \u02dcx is a copy of x that has been corrupted by some form of noise. denoising\nautoencoders must therefore undo this corruption rather than simply copying their\ninput.\n\ndenoising training forces f and g to implicitly learn the structure of pdata(x),\n). denoising\n\nalain and\u00a0bengio 2013\n\nbengio et\u00a0al. 2013c\n\nas shown\u00a0by\u00a0\n\n) and\n\n(\n\n(\n\n510\n\n "}, {"Page_number": 526, "text": "chapter 14. autoencoders\n\nautoencoders thus provide yet another example of how useful properties can emerge\nas a byproduct of minimizing reconstruction error. they are also an example of\nhow overcomplete, high-capacity models may be used as autoencoders so long\nas care is taken to prevent them from learning the identity function. denoising\nautoencoders are presented in more detail in sec.\n\n.\n14.5\n\n14.2.3 regularizing by penalizing derivatives\n\nanother strategy for regularizing an autoencoder is to use a penalty\nautoencoders,\n\n\u03c9\n\nas in sparse\n\nbut with a different form of\n\n:\u03c9\n\nl , g f\n\n(x ( ( ))) + \u03c9(\n\nx\n\nh x)\n,\n\n,\n\n\u03c9(\n\nh x,\n\n) = \n\n\u03bbxi\n\n||\u2207xhi||2 .\n\n(14.10)\n\n(14.11)\n\nthis forces the model to learn a function that does not change much when x\nchanges slightly. because this penalty is applied only at training examples, it forces\nthe autoencoder to learn features that capture information about the training\ndistribution.\n\nan autoencoder regularized in this way is called a contractive autoencoder\ncae\n. this approach has theoretical connections to denoising autoencoders,\nor\nmanifold learning and probabilistic modeling. the cae is described in more detail\nin sec.\n\n.\n14.7\n\n14.3 representational power, layer size and depth\n\nautoencoders are often trained with only a single layer encoder and a single layer\ndecoder. however, this is not a requirement. in fact, using deep encoders and\ndecoders offers many advantages.\n\n6.4.1\n\nrecall from sec.\n\nthat there are many advantages to depth in a feedforward\nnetwork. because autoencoders are feedforward networks, these advantages also\napply to autoencoders. moreover, the encoder is itself a feedforward network as\nis the decoder, so each of these components of the autoencoder can individually\nbenefit from depth.\n\none major advantage of non-trivial depth is that the universal approximator\ntheorem guarantees that a feedforward neural network with at least one hidden\nlayer can represent an approximation of any function (within a broad class) to an\n\n511\n\n "}, {"Page_number": 527, "text": "chapter 14. autoencoders\n\narbitrary degree of accuracy, provided that it has enough hidden units. this means\nthat an autoencoder with a single hidden layer is able to represent the identity\nfunction along the domain of the data arbitrarily well. however, the mapping from\ninput to code is shallow. this means that we are not able to enforce arbitrary\nconstraints, such as that the code should be sparse. a deep autoencoder, with at\nleast one additional hidden layer inside the encoder itself, can approximate any\nmapping from input to code arbitrarily well, given enough hidden units.\n\ndepth can exponentially reduce the computational cost of representing some\nfunctions. depth can also exponentially decrease the amount of training data\nneeded to learn some functions. see sec.\nfor a review of the advantages of\ndepth in feedforward networks.\n\n6.4.1\n\nexperimentally, deep autoencoders yield much better compression than corre-\n\nsponding shallow or linear autoencoders (hinton and salakhutdinov 2006\n\n,\n\n).\n\na common strategy for training a deep autoencoder is to greedily pretrain\nthe deep architecture by training a stack of shallow autoencoders, so we often\nencounter shallow autoencoders, even when the ultimate goal is to train a deep\nautoencoder.\n\n14.4 stochastic encoders and decoders\n\nautoencoders are just feedforward networks. the same loss functions and output\nunit types that can be used for traditional feedforward networks are also used for\nautoencoders.\n\n6.2.2.4\n\nas described in sec.\n\n, a general strategy for designing the output units\nand the loss function of a feedforward network is to define an output distribution\np(y x|\n). in that setting, y\nwas a vector of targets, such as class labels.\n\n) and minimize the negative log-likelihood \u2212 log p(y x|\n\nin the case of an autoencoder, x is now the target as well as the input. however,\nwe can still apply the same machinery as before. given a hidden code h, we may\nthink of the decoder as providing a conditional distribution pdecoder (x h|\n).\u00a0we\nx h|\nmay then train the autoencoder by minimizing \u2212 log pdecoder(\n. the exact\n)\nform of this loss function will change depending on the form of pdecoder . as with\ntraditional feedforward networks, we usually use linear output units to parametrize\nthe mean of a gaussian distribution if x is real-valued. in that case, the negative\nlog-likelihood yields a mean squared error criterion. similarly, binary x values\ncorrespond to a bernoulli distribution whose parameters are given by a sigmoid\noutput unit, discrete x values correspond to a softmax distribution, and so on.\n\n512\n\n "}, {"Page_number": 528, "text": "chapter 14. autoencoders\n\ntypically, the output variables are treated as being conditionally independent\ngiven h so that this probability distribution is inexpensive to evaluate, but some\ntechniques such as mixture density outputs allow tractable modeling of outputs\nwith correlations.\n\nhh\n\npencoder(\n\nh x|\n)\n\npdecoder(\n\nx h|\n)\n\nxx\n\nrr\n\nfigure 14.2: the structure of a stochastic autoencoder, in which both the encoder and the\ndecoder are not simple functions but instead involve some noise injection, meaning that\n) for the encoder\ntheir output can be seen as sampled from a distribution, pencoder(h x|\nand pdecoder(\n\nfor the decoder.\n\nx h|\n)\n\nto make a more radical departure from the feedforward networks we have seen\npreviously, we can also generalize the notion of an encoding function f(x) to an\nencoding distribution pencoder(\n\n14.2\n.\n\nh x|\n, as illustrated in fig.\n)\n\nany latent variable model pmodel (\nh x|\n\npencoder(\n\n) = \n\npmodel(\n\nh x|\n)\n\n)h x,\n\ndefines a stochastic encoder\n\nand a stochastic decoder\n\npdecoder(\n\nx h|\n\n) = \n\npmodel(\n\nx h|\n)\n.\n\n(14.12)\n\n(14.13)\n\nin general, the encoder and decoder distributions are not necessarily conditional\ndistributions compatible with a unique joint distribution pmodel(x h,\n). alain et al.\n) showed that training the encoder and decoder as a denoising autoencoder\n(\n2015\nwill tend to make them compatible asymptotically (with enough capacity and\nexamples).\n\n14.5 denoising autoencoders\n\nthe denoising autoencoder (dae) is an autoencoder that receives a corrupted data\npoint as input and is trained to predict the original, uncorrupted data point as its\noutput.\n\nthe dae training\u00a0procedure is illustrated\u00a0in fig.\n\n. we\u00a0introduce a\n) which represents a conditional distribution\u00a0over\n\n14.3\n\ncorruption process c (\u02dcx\n\nx|\n\n513\n\n "}, {"Page_number": 529, "text": "chapter 14. autoencoders\n\nhh\n\ng\n\nll\n\nf\n\n\u02dcx\u02dcx\n\nxx\n\nc(\u02dcx x|\n)\n\nfigure 14.3: the computational graph of the cost function for a denoising autoencoder,\nwhich is trained to reconstruct the clean data point x from its corrupted version \u02dcx.\nthis is accomplished by minimizing the loss l = \u2212 log pdecoder(x h| = f(\u02dcx)), where\n\u02dcx is a corrupted version of the data example x, obtained through a given corruption\n). typically the distribution pdecoder is a factorial distribution whose mean\nprocess c(\u02dcx x|\nparameters are emitted by a feedforward network .g\n\ncorrupted samples \u02dcx, given a data sample x. the autoencoder then learns a\nestimated from training pairs x, \u02dcx),\nreconstruction distribution preconstruct(x | \u02dcx)\nas follows:\n\n(\n\n1.\u00a0sample a training example\n\nx\n\nfrom the training data.\n\n2.\u00a0sample a corrupted version \u02dcx from c(\u02dcx x| =  )x .\n3. use ( x, \u02dcx) as a training example for estimating the autoencoder reconstruction\n) with h the output of encoder\n\ndistribution preconstruct(x | \u02dcx) = pdecoder(x h|\nf ( \u02dcx) and p decoder typically defined by a decoder\n\n.\ng( )h\n\ntypically we can simply perform gradient-based approximate minimization (such\nas minibatch gradient descent) on the negative log-likelihood \u2212 log pdecoder(x h|\n).\nso long as the encoder is deterministic, the denoising autoencoder is a feedforward\nnetwork\u00a0and may\u00a0be\u00a0trained with exactly\u00a0the same\u00a0techniques\u00a0as any\u00a0other\nfeedforward network.\n\nwe can therefore view the dae as performing stochastic gradient descent on\n\nthe following expectation:\n\n\u2212 e\n\nx\u223c\u02c6pdata ( )x e\u02dcx\u223cc(\u02dcx|x) log pdecoder(\n\nx h|\n\n=  (\n\nf \u02dcx))\n\n(14.14)\n\nwhere \u02c6pdata( )x is the training distribution.\n\n514\n\n "}, {"Page_number": 530, "text": "chapter 14. autoencoders\n\n\u02dcx\ng f(cid:129)\n\nx\n\n\u02dcx\n\nc(\u02dcx x|\n)\n\nx\n\nfigure 14.4: a denoising autoencoder is trained to map a corrupted data point \u02dcx back to\nthe original data point x. we illustrate training examples x as red crosses lying near a\nlow-dimensional manifold illustrated with the bold black line. we illustrate the corruption\n) with a gray circle of equiprobable corruptions. a gray arrow demonstrates\nprocess c (\u02dcx x|\nhow one training example is transformed into one sample from this corruption process.\nwhen the denoising autoencoder is trained to minimize the average of squared errors\n||g( f (\u02dcx))\u2212 ||x 2, the reconstruction g(f (\u02dcx)) estimates e\nx, \u02dcx\u223cpdata ( ) (x c \u02dcx x| )[ x | \u02dcx]. the vector\ng(f(\u02dcx)) \u2212 \u02dcx points approximately towards the nearest point on the manifold, since g(f(\u02dcx))\nestimates the center of mass of the clean points x which could have given rise to \u02dcx. the\nautoencoder thus learns a vector field g(f(x))\u2212 x indicated by the green arrows. this\nvector field estimates the score \u2207 xlog pdata (x) up to a multiplicative factor that is the\naverage root mean square reconstruction error.\n\n515\n\n "}, {"Page_number": 531, "text": "chapter 14. autoencoders\n\n14.5.1 estimating the score\n\nhyv\u00e4rinen 2005\n\nscore matching (\n) is an alternative to maximum likelihood. it\nprovides a consistent estimator of probability distributions based on encouraging\nthe model to have the same score as the data distribution at every training point\nx. in this context, the score is a particular gradient field:\n\n,\n\n\u2207x log ( )\np x .\nscore\u00a0matching\u00a0is discussed further\u00a0in sec.\n. for\u00a0the\u00a0present\u00a0discussion\nregarding autoencoders, it is sufficient to understand that learning the gradient\nfield of log pdata is one way to learn the structure of pdata itself.\n\n(14.15)\n\n18.4\n\na very important property of daes is that their training criterion\u00a0(with\nconditionally gaussian p( x h|\n)) makes\u00a0the autoencoder\u00a0learn a vector\u00a0field\n(g(f( x)) \u2212 x) that estimates the score of the data distribution. this is illustrated\nin fig.\n\n.\n14.4\n\n,\n\nvincent 2011\n\ndenoising training of a specific kind of autoencoder (sigmoidal hidden units,\nlinear reconstruction units) using gaussian noise and mean squared error as the\nreconstruction cost is equivalent (\n) to training a specific kind of\nundirected probabilistic model called an rbm with gaussian visible units. this\nkind of model will be described in detail in sec.\n; for the present discussion\nit suffices to know that it is a model that provides an explicit pmodel (x; \u03b8). when\nthe rbm is trained using denoising score matching (\nkingma and lecun 2010\n),\nits learning algorithm is equivalent to denoising training in the corresponding\nautoencoder. with a fixed noise level, regularized score matching is not a consistent\nestimator; it instead recovers a blurred version of the distribution. however, if\nthe noise level is chosen to approach 0 when the number of examples approaches\ninfinity, then consistency is recovered.\u00a0denoising score matching is discussed in\nmore detail in sec.\n\n20.5.1\n\n18.5\n.\n\n,\n\nother connections between autoencoders and rbms exist. score matching\napplied to rbms yields a cost function that is identical to reconstruction error\ncombined with a regularization term similar to the contractive penalty of the\ncae (swersky\n) showed that an autoen-\ncoder gradient provides an approximation to contrastive divergence training of\nrbms.\n\nbengio and delalleau 2009\n\net al.,\n\n2011\n\n).\n\n(\n\nfor continuous-valued x, the denoising criterion with gaussian corruption and\nreconstruction distribution yields an estimator of the score that is applicable to\ngeneral encoder and decoder parametrizations (\n). this\nmeans a generic encoder-decoder architecture may be made to estimate the score\n\nalain and bengio 2013\n\n,\n\n516\n\n "}, {"Page_number": 532, "text": "chapter 14. autoencoders\n\nby training with the squared error criterion\n\nand corruption\n\n||g f( ( \u02dcx\n\n)) \u2212 || 2\n\nx\n\n(14.16)\n\nwith noise variance \u03c3 2. see fig.\n\nc(\u02dcx = \u02dcx x| ) = \n14.5\n\n; = \u00b5\n\n(n \u02dcx\nfor an illustration of how this works.\n\n\u03c3\u03c2 =  2i)\n\nx\n,\n\n(14.17)\n\nfigure 14.5: vector field learned by a denoising autoencoder around a 1-d curved manifold\nnear which the data concentrates in a 2-d space. each arrow is proportional to the\nreconstruction minus input vector of the autoencoder and points towards higher probability\naccording to the implicitly estimated probability distribution. the vector field has zeros\nat both maxima of the estimated density function (on the data manifolds) and at minima\nof that density function. for example, the spiral arm forms a one-dimensional manifold of\nlocal maxima that are connected to each other. local minima appear near the middle of\nthe gap between two arms. when the norm of reconstruction error (shown by the length\nof the arrows) is large, it means that probability can be significantly increased by moving\nin the direction of the arrow, and that is mostly the case in places of low probability.\nthe autoencoder maps these low probability points to higher probability reconstructions.\nwhere probability is maximal, the arrows shrink because the reconstruction becomes more\naccurate.\n\nin general, there is no guarantee that the reconstruction g(f (x)) minus the\ninput x corresponds to the gradient of any function, let alone to the score. that is\n\n517\n\n "}, {"Page_number": 533, "text": "chapter 14. autoencoders\n\n,\n\nvincent 2011\n\nwhy the early results (\n) are specialized to particular parametrizations\nwhere g(f(x)) \u2212 x may be obtained by taking the derivative of another function.\n) by\nkamyshanska and memisevic 2015\nidentifying a family of shallow autoencoders such that g(f( x))\u2212 x corresponds to\na score for all members of the family.\n\n) generalized the results of\n\nvincent 2011\n\n(\n\n(\n\nso far we have described only how the denoising autoencoder learns to represent\na probability distribution. more generally, one may want to use the autoencoder as\na generative model and draw samples from this distribution. this will be described\nlater, in sec.\n\n20.11\n.\n\n14.5.1.1 historical perspective\n\n(\n\n(\n\n(\n\n).\n\ngallinari et al. 1987 behnke 2001\n\n)\nlecun 1987\nthe idea of using mlps for denoising dates back to the work of\nand\n) also used recurrent networks to denoise\nimages. denoising autoencoders are, in some sense, just mlps trained to denoise.\nhowever, the name \u201cdenoising autoencoder\u201d refers to a model that is intended not\nmerely to learn to denoise its input but to learn a good internal representation\nas\u00a0a side effect\u00a0of learning to\u00a0denoise. this\u00a0idea came\u00a0much later\u00a0(vincent\net al.,\n). the learned representation may then be used to pretrain a\ndeeper unsupervised network or a supervised network. like sparse autoencoders,\nsparse coding, contractive autoencoders and other regularized autoencoders, the\nmotivation for daes was to allow the learning of a very high-capacity encoder\nwhile preventing the encoder and decoder from learning a useless identity function.\n\n2008 2010\n\n,\n\nprior to the introduction of the modern dae, inayoshi and kurita 2005\n)\nexplored some of the same goals with some of the same methods. their approach\nminimizes reconstruction error in addition to a supervised objective while injecting\nnoise in the hidden layer of a supervised mlp, with the objective to improve\ngeneralization by\u00a0introducing\u00a0the reconstruction\u00a0error and\u00a0the injected\u00a0noise.\nhowever, their method was based on a linear encoder and could not learn function\nfamilies as powerful as can the modern dae.\n\n(\n\n14.6 learning manifolds with autoencoders\n\nlike\u00a0many\u00a0other machine\u00a0learning\u00a0algorithms,\u00a0autoencoders\u00a0exploit the\u00a0idea\nthat data concentrates around a low-dimensional manifold or a small set of such\nmanifolds, as described in sec.\n. some machine learning algorithms exploit\nthis idea only insofar as that they learn a function that behaves correctly on the\nmanifold but may have unusual behavior if given an input that is off the manifold.\n\n5.11.3\n\n518\n\n "}, {"Page_number": 534, "text": "chapter 14. autoencoders\n\nautoencoders take this idea further and aim to learn the structure of the manifold.\n\nto understand how autoencoders do this, we must present some important\n\ncharacteristics of manifolds.\n\nan important characterization of a manifold is the set of its\n\n. at\na point x on a d-dimensional manifold, the tangent plane is given by d basis vectors\nthat span the local directions of variation allowed on the manifold. as illustrated\nin fig.\nx infinitesimally\nwhile staying on the manifold.\n\n, these local directions specify how one can change\n\ntangent planes\n\n14.6\n\nall autoencoder training procedures involve a compromise between two forces:\n\n1. learning a representation h of a training example x such that x can be\napproximately recovered from h through a decoder. the fact that x is drawn\nfrom the training data is crucial, because it means the autoencoder need\nnot successfully reconstruct inputs that are not probable under the data\ngenerating distribution.\n\n2.\u00a0satisfying the constraint or regularization penalty. this can be an architec-\ntural constraint that limits the capacity of the autoencoder, or it can be\na regularization term added to the reconstruction cost. these techniques\ngenerally prefer solutions that are less sensitive to the input.\n\nclearly, neither force alone would be useful\u2014copying the input to the output\nis not useful on its own, nor is ignoring the input. instead, the two forces together\nare useful because they force the hidden representation to capture information\nabout the structure of the data generating distribution. the important principle\nis that the autoencoder can afford to represent only the variations that are needed\nto reconstruct training examples. if the data generating distribution concentrates\nnear a low-dimensional manifold, this yields representations that implicitly capture\na local coordinate system for this manifold: only the variations tangent to the\nmanifold around x need to correspond to changes in h = f (x). hence the encoder\nlearns a mapping from the input space x to a representation space, a mapping that\nis only sensitive to changes along the manifold directions, but that is insensitive to\nchanges orthogonal to the manifold.\n\na one-dimensional example is illustrated in fig.\n\n, showing that by making\nthe reconstruction function insensitive to perturbations of the input around the\ndata points we recover the manifold structure.\n\n14.7\n\nto understand why autoencoders are useful for manifold learning, it is instruc-\ntive to compare them to other approaches.\u00a0what is most commonly learned to\ncharacterize a manifold is a representation of the data points on (or near) the\n\n519\n\n "}, {"Page_number": 535, "text": "chapter 14. autoencoders\n\nfigure 14.6:\u00a0an illustration of the concept of a tangent hyperplane. here we create a\none-dimensional manifold in 784-dimensional space. we take an mnist image with 784\npixels and transform it by translating it vertically.\u00a0the amount of vertical translation\ndefines a coordinate along a one-dimensional manifold that traces out a curved path\nthrough image space. this plot shows a few points along this manifold. for visualization,\nwe have projected the manifold into two dimensional space using pca. an n-dimensional\nmanifold has an n-dimensional tangent plane at every point. this tangent plane touches\nthe manifold exactly at that point and is oriented parallel to the surface at that point.\nit defines the space of directions in which it is possible to move while remaining on\nthe manifold. this one-dimensional manifold has a single tangent line. we indicate an\nexample tangent line at one point, with an image showing how this tangent direction\nappears in image space. gray pixels indicate pixels that do not change as we move along\nthe tangent line, white pixels indicate pixels that brighten, and black pixels indicate pixels\nthat darken.\n\n520\n\n "}, {"Page_number": 536, "text": "chapter 14. autoencoders\n\n)\nx\n(\nr\n\n1 0.\n\n0 8.\n\n0 6.\n\n0 4.\n\n0 2.\n\n0 0.\n\nidentity\noptimal reconstruction\n\nx0\n\nx1\n\nx\n\nx2\n\nfigure 14.7: if the autoencoder learns a reconstruction function that is invariant to small\nperturbations near the data points, it captures the manifold structure of the data. here\nthe manifold structure is a collection of\n-dimensional manifolds. the dashed diagonal\n0\nline indicates the identity function target for reconstruction. the optimal reconstruction\nfunction crosses the identity function wherever there is a data point. the horizontal\narrows at the bottom of the plot indicate the r(x)\u2212 x reconstruction direction vector\nat the base of the arrow, in input space, always pointing towards the nearest \u201cmanifold\u201d\n(a single datapoint, in the 1-d case). the denoising autoencoder explicitly tries to make\nthe derivative of the reconstruction function r(x) small around the data points. the\ncontractive autoencoder does the same for the encoder. although the derivative of r(x) is\nasked to be small around the data points, it can be large between the data points. the\nspace between the data points corresponds to the region between the manifolds, where\nthe reconstruction function must have a large derivative in order to map corrupted points\nback onto the manifold.\n\nmanifold. such a representation for a particular example is also called its em-\nbedding. it is typically given by a low-dimensional vector, with less dimensions\nthan the \u201cambient\u201d space of which the manifold is a low-dimensional subset. some\nalgorithms (non-parametric manifold learning algorithms, discussed below) directly\nlearn an embedding for each training example, while others learn a more general\nmapping, sometimes called an encoder, or representation function, that maps any\npoint in the ambient space (the input space) to its embedding.\n\nmanifold learning has mostly focused on unsupervised learning procedures that\nattempt to capture these manifolds. most of the initial machine learning research\non learning nonlinear manifolds has focused on non-parametric methods based on\nthe nearest-neighbor graph. this graph has one node per training example and\nedges connecting near neighbors to each other. these methods (sch\u00f6lkopf et al.,\n2000 brand 2003 belkin and\n1998 roweis and saul 2000 tenenbaum\n\net al.,\n\n;\n\n,\n\n;\n\n;\n\n,\n\n;\n\n521\n\n "}, {"Page_number": 537, "text": "chapter 14. autoencoders\n\nfigure 14.8: non-parametric manifold learning procedures build a nearest neighbor graph\nwhose nodes are training examples and arcs connect nearest neighbors. various procedures\ncan thus obtain the tangent plane associated with a neighborhood of the graph as well\nas a coordinate system that associates each training example with a real-valued vector\nposition, or embedding. it is possible to generalize such a representation to new examples\nby a form of interpolation. so long as the number of examples is large enough to cover\nthe curvature and twists of the manifold, these approaches work well. images from the\nqmul multiview face dataset (\n\ngong et al. 2000\n\n).\n\n,\n\n;\n;\n\n,\n,\n\nniyogi 2003 donoho and grimes 2003 weinberger and saul 2004 hinton and\nroweis 2003 van der maaten and hinton 2008\n) associate each of nodes with a\ntangent plane that spans the directions of variations associated with the difference\nvectors between the example and its neighbors, as illustrated in fig.\n\n.\n14.8\n\n,\n\n,\n\n;\n\n,\n\n;\n\na global coordinate system can then be obtained through an optimization or\nsolving a linear system. fig.\nillustrates how a manifold can be tiled by a\nlarge number of locally linear gaussian-like patches (or \u201cpancakes,\u201d because the\ngaussians are flat in the tangent directions).\n\n14.9\n\nhowever, there is a fundamental difficulty with such local non-parametric\napproaches to manifold learning, raised in\n): if the\nmanifolds are not very smooth (they have many peaks and troughs and twists),\none may need a very large number of training examples to cover each one of these\nvariations, with no chance to generalize to unseen variations. indeed, these methods\n\nbengio and monperrus 2005\n\n(\n\n522\n\n "}, {"Page_number": 538, "text": "chapter 14. autoencoders\n\n14.6\n\n) at each location are known, then they\nfigure 14.9: if the tangent planes (see fig.\ncan be tiled to form a global coordinate system or a density function. each local patch\ncan be thought of as a local euclidean coordinate system or as a locally flat gaussian, or\n\u201cpancake\u201d, with a very small variance in the directions orthogonal to the pancake and a\nvery large variance in the directions defining the coordinate system on the pancake. a\nmixture of these gaussians provides an estimated density function, as in the manifold\nparzen window algorithm (\n) or its non-local neural-net based\nvariant (\nbengio et al. 2006c\n\nvincent and bengio 2003\n).\n\n,\n\n,\n\ncan only generalize the shape of the manifold by interpolating between neighboring\nexamples. unfortunately, the manifolds involved in ai problems can have very\ncomplicated structure that can be difficult to capture from only local interpolation.\nconsider for example the manifold resulting from translation shown in fig.\n. if\nwe watch just one coordinate within the input vector, xi, as the image is translated,\nwe will observe that one coordinate encounters a peak or a trough in its value\nonce for every peak or trough in brightness in the image. in other words, the\ncomplexity of the patterns of brightness in an underlying image template drives\nthe complexity of the manifolds that are generated by performing simple image\ntransformations. this motivates the use of distributed representations and deep\nlearning for capturing manifold structure.\n\n14.6\n\n523\n\n "}, {"Page_number": 539, "text": "chapter 14. autoencoders\n\n14.7 contractive autoencoders\n\nthe contractive autoencoder (\n, ) introduces an explicit regularizer\non the code h = f (x), encouraging the derivatives of f to be as small as possible:\n\nrifai et al. 2011a b\n\n,\n\n\u03c9( ) = \n\nh\n\n.\n\n(14.18)\n\n2\n\nf\n\n\u03bb(cid:17)(cid:17)(cid:17)(cid:17)\n\n\u2202f ( )x\n\n\u2202x (cid:17)(cid:17)(cid:17)(cid:17)\n\nthe penalty \u03c9(h) is the squared frobenius norm (sum of squared elements) of the\njacobian matrix of partial derivatives associated with the encoder function.\n\n(\n\nalain and bengio 2013\n\nthere is a connection between the denoising autoencoder and the contractive\nautoencoder:\n) showed that in the limit of small gaussian\ninput\u00a0noise,\u00a0the\u00a0denoising\u00a0reconstruction error\u00a0is\u00a0equivalent to\u00a0a contractive\npenalty on the reconstruction function that maps x to r = g (f (x)). in other\nwords, denoising autoencoders make the reconstruction function resist small but\nfinite-sized perturbations of the input, while contractive autoencoders make the\nfeature extraction function resist infinitesimal perturbations of the input. when\nusing the jacobian-based contractive penalty to pretrain features f (x) for use\nwith a classifier, the best classification accuracy usually results from applying the\ncontractive penalty to f(x) rather than to g(f (x)). a contractive penalty on f (x)\nalso has close connections to score matching, as discussed in sec.\n\n14.5.1\n.\n\nthe name contractive arises from the way that the cae warps space. specifi-\ncally, because the cae is trained to resist perturbations of its input, it is encouraged\nto map a neighborhood of input points to a smaller neighborhood of output points.\nwe can think of this as contracting the input neighborhood to a smaller output\nneighborhood.\n\nto clarify, the cae is contractive only locally\u2014all perturbations of a training\npoint x are mapped near to f (x). globally, two different points x and x0 may be\nmapped to f(x ) and f (x0) points that are farther apart than the original points.\nit is plausible that f be expanding in-between or far from the data manifolds (see\nfor example what happens in the 1-d toy example of fig.\n\u03c9(h)\npenalty is applied to sigmoidal units, one easy way to shrink the jacobian is to\nmake the sigmoid units saturate to\n. this encourages the cae to encode\ninput points with extreme values of the sigmoid that may be interpreted as a\nbinary code. it also ensures that the cae will spread its code values throughout\nmost of the hypercube that its sigmoidal hidden units can span.\n\n). when the\n\n14.7\n\nor\n\n0\n\n1\n\nwe can think of the jacobian matrix j at a point x as approximating the\nnonlinear encoder f (x) as being a linear operator. this allows us to use the word\n\u201ccontractive\u201d more formally.\u00a0in the theory of linear operators, a linear operator\n\n524\n\n "}, {"Page_number": 540, "text": "chapter 14. autoencoders\n\nis said to be contractive if the norm of j x remains less than or equal to\nfor\nall unit-norm x. in other words, j is contractive if it shrinks the unit sphere.\nwe can think of the cae as penalizing the frobenius norm of the local linear\napproximation of f (x) at every training point x in order to encourage each of\nthese local linear operator to become a contraction.\n\n1\n\n14.6\n\nas described in sec.\n\n, regularized autoencoders learn manifolds by balancing\ntwo opposing forces. in the case of the cae, these two forces are reconstruction\nerror and the contractive penalty \u03c9(h). reconstruction error alone would encourage\nthe cae to learn an identity function. the contractive penalty alone would\nencourage the cae to learn features that are constant with respect to x. the\ncompromise between these two forces yields an autoencoder whose derivatives\n\u2202f ( )x\n\u2202x are mostly tiny. only a small number of hidden units, corresponding to a\n\nsmall number of directions in the input, may have significant derivatives.\n\n(\n\n(\n\nthe goal of the cae is to learn the manifold structure of the data. directions\nx with large j x rapidly change h, so these are likely to be directions which\napproximate the tangent planes of the manifold. experiments by\nrifai et al. 2011a\n)\n) show that training the cae results in most singular values\nand\nrifai et al. 2011b\nof j dropping below in magnitude and therefore becoming contractive. however,\n1\nsome singular values remain above , because the reconstruction error penalty\nencourages the cae to encode the directions with the most local variance. the\ndirections corresponding to the largest singular values are interpreted as the tangent\ndirections that the contractive autoencoder has learned. ideally, these tangent\ndirections should correspond to real variations in the data. for example, a cae\napplied to images should learn tangent vectors that show how the image changes as\nobjects in the image gradually change pose, as shown in fig.\n. visualizations of\nthe experimentally obtained singular vectors do seem to correspond to meaningful\ntransformations of the input image, as shown in fig.\n\n.\n14.10\n\n14.6\n\n1\n\n2011a\n\net al. (\n\none practical issue with the cae regularization criterion is that although it\nis cheap to compute in the case of a single hidden layer autoencoder, it becomes\nmuch more expensive in the case of deeper autoencoders. the strategy followed by\nrifai\n) is to separately train a series of single-layer autoencoders, each\ntrained to reconstruct the previous autoencoder\u2019s hidden layer. the composition\nof these autoencoders then forms a deep autoencoder. because each layer was\nseparately trained to be locally contractive, the deep autoencoder is contractive\nas well. the result is not the same as what would be obtained by jointly training\nthe entire architecture with a penalty on the jacobian of the deep model, but it\ncaptures many of the desirable qualitative characteristics.\n\nanother practical issue is that the contraction penalty can obtain useless results\n\n525\n\n "}, {"Page_number": 541, "text": "chapter 14. autoencoders\n\ninput\npoint\n\ntangent vectors\n\nlocal pca (no sharing across regions)\n\ncontractive autoencoder\n\nfigure 14.10: illustration of tangent vectors of the manifold estimated by local pca\nand by a contractive autoencoder. the location on the manifold is defined by the input\nimage of a dog drawn from the cifar-10 dataset.\u00a0the tangent vectors are estimated\nby the leading singular vectors of the jacobian matrix \u2202h\n\u2202x of the input-to-code mapping.\nalthough both local pca and the cae can capture local tangents, the cae is able to\nform more accurate estimates from limited training data because it exploits parameter\nsharing across different locations that share a subset of active hidden units.\u00a0the cae\ntangent directions typically correspond to moving or changing parts of the object (such as\nthe head or legs).\n\nif we do not impose some sort of scale on the decoder. for example, the encoder\ncould consist of multiplying the input by a small constant (cid:115) and the decoder\n, the encoder drives the\ncould consist of dividing the code by (cid:115). as (cid:115) approaches\n0\ncontractive penalty \u03c9(h) to approach without having learned anything about the\ndistribution. meanwhile, the decoder maintains perfect reconstruction. in rifai\net al. (\nf and g. both f and g are\nstandard neural network layers consisting of an affine transformation followed by\nan element-wise nonlinearity, so it is straightforward to set the weight matrix of g\nto be the transpose of the weight matrix of\n\n), this is prevented by tying the weights of\n\n2011a\n\n.f\n\n0\n\n14.8 predictive sparse decomposition\n\npredictive\u00a0sparse\u00a0decomposition\u00a0(psd) is\u00a0a model\u00a0that is\u00a0a\u00a0hybrid\u00a0of sparse\ncoding and parametric autoencoders (kavukcuoglu\n). a parametric\nencoder is trained to predict the output of iterative inference. psd has been\napplied to unsupervised feature learning for object recognition in images and video\n), as well\n(kavukcuoglu\nas for audio (\nf (x) and a\ndecoder g(h) that are both parametric. during training, h is controlled by the\n\n;\net al.,\nhenaff et al. 2011\n\n). the model consists of an encoder\n\n2009 2010 jarrett\n\n2009 farabet\n\net al.,\n\net al.,\n\net al.,\n\n2008\n\n2011\n\n,\n\n,\n\n;\n\n526\n\n "}, {"Page_number": 542, "text": "chapter 14. autoencoders\n\noptimization algorithm. training proceeds by minimizing\n\nx g( )h 2 + \u03bb|\n|| \u2212\n\n||\n\n|h 1 +\n\n\u03b3\n\n|| \u2212h\n\nf\n\nx ||2.\n( )\n\n(14.19)\n\nlike in sparse coding, the training algorithm alternates between minimization with\nrespect to h and minimization with respect to the model parameters. minimization\nwith respect to h is fast because f(x) provides a good initial value of h and the\ncost function constrains h to remain near f (x) anyway. simple gradient descent\ncan obtain reasonable values of\n\nin as few as ten steps.\n\nh\n\nthe training procedure used by psd is different from first training a sparse\ncoding model and then training f( x) to predict the values of the sparse coding\nfeatures. the psd training procedure regularizes the decoder to use parameters\nfor which\n\ncan infer good code values.\n\nf ( )x\n\npredictive sparse coding is an example of learned approximate inference. in sec.\n19.5, this topic is developed further. the tools presented in chapter make it\nclear that psd can be interpreted as training a directed sparse coding probabilistic\nmodel by maximizing a lower bound on the log-likelihood of the model.\n\n19\n\nin practical applications of psd, the iterative optimization is only used during\ntraining. the parametric encoder f is used to compute the learned features when\nthe model is deployed. evaluating f is computationally inexpensive compared to\ninferring h via gradient descent. because f is a differentiable parametric function,\npsd models may be stacked and used to initialize a deep network to be trained\nwith another criterion.\n\n14.9 applications of autoencoders\n\nautoencoders have been successfully applied to dimensionality reduction and infor-\nmation retrieval tasks. dimensionality reduction was one of the first applications\nof representation learning and deep learning. it was one of the early motivations\nfor studying autoencoders. for example, hinton and salakhutdinov 2006\n) trained\na stack of rbms and then used their weights to initialize a deep autoencoder\nwith gradually smaller hidden layers, culminating in a bottleneck of 30 units. the\nresulting code yielded less reconstruction error than pca into 30 dimensions and\nthe learned representation was qualitatively easier to interpret and relate to the\nunderlying categories, with these categories manifesting as well-separated clusters.\n\n(\n\nlower-dimensional representations can improve performance on many tasks,\nsuch as classification. models of smaller spaces consume less memory and runtime.\nmany forms of dimensionality reduction place semantically related examples near\n\n527\n\n "}, {"Page_number": 543, "text": "chapter 14. autoencoders\n\net al.\n). the hints provided by the mapping to the lower-dimensional space aid\n\neach other, as observed by salakhutdinov and hinton 2007b\n(\n2008\ngeneralization.\n\ntorralba\n\n) and\n\n(\n\none task that benefits even more than usual from dimensionality reduction\nis information retrieval, the task of finding entries in a database that resemble a\nquery entry.\u00a0this task derives the usual benefits from dimensionality reduction\nthat other tasks do, but also derives the additional benefit that search can become\nextremely efficient in certain kinds of low dimensional spaces. specifically,\u00a0if\nwe train the dimensionality reduction algorithm to produce a code that is low-\ndimensional and binary, then we can store all database entries in a hash table\nmapping binary code vectors to entries. this hash table allows us to perform\ninformation retrieval by returning all database entries that have the same binary\ncode as the\u00a0query. we can also search over\u00a0slightly less similar entries very\nefficiently, just by flipping individual bits from the encoding of the query.\u00a0this\napproach to information retrieval via dimensionality reduction and binarization\nis called\n), and has\nbeen applied to both textual input (salakhutdinov and hinton 2007b 2009b\n) and\nimages (torralba\n\nsemantic hashing salakhutdinov and hinton 2007b 2009b\n\n2008 krizhevsky and hinton 2011\n\n2008 weiss\n\net al.,\n\net al.,\n\n).\n\n(\n\n,\n\n,\n\n;\n\n;\n\n,\n\n,\n\n,\n\nto produce binary codes for semantic hashing, one typically uses an encoding\nfunction with sigmoids on the final layer. the sigmoid units must be trained to be\nsaturated to nearly 0 or nearly 1 for all input values. one trick that can accomplish\nthis is simply to inject additive noise just before the sigmoid nonlinearity during\ntraining. the magnitude of the noise should increase over time. to fight that\nnoise and preserve as much information as possible, the network must increase the\nmagnitude of the inputs to the sigmoid function, until saturation occurs.\n\nthe idea of learning a hashing function has been further explored in several\ndirections, including the idea of training the representations so as to optimize\na loss more directly linked to the task of finding nearby examples in the hash\ntable (\n\nnorouzi and fleet 2011\n\n).\n\n,\n\n528\n\n "}, {"Page_number": 544, "text": "chapter 15\n\nrepresentation learning\n\nin this chapter, we first discuss what it means to learn representations and how\nthe notion of representation can be useful to design deep architectures. we discuss\nhow learning algorithms share statistical strength across different tasks, including\nusing information from unsupervised tasks to perform supervised tasks. shared\nrepresentations are useful to handle multiple modalities or domains, or to transfer\nlearned knowledge to tasks for which few or no examples are given but a task\nrepresentation exists. finally, we step back and argue about the reasons for the\nsuccess of representation learning, starting with the theoretical advantages of\ndistributed representations (hinton\n) and deep representations and\nending with the more general idea of underlying assumptions about the data\ngenerating process, in particular about underlying causes of the observed data.\n\net al.,\n\n1986\n\nmany information processing tasks can be very easy or very difficult depending\non how the information is represented. this is a general principle applicable to\ndaily life, computer science in general, and to machine learning. for example, it\nis straightforward for a person to divide 210 by 6 using long division.\u00a0the task\nbecomes considerably less straightforward if it is instead posed using the roman\nnumeral representation of the numbers. most modern people asked to divide ccx\nby vi would begin by converting the numbers to the arabic numeral representation,\npermitting long division procedures that make use of the place value system. more\nconcretely, we can quantify the asymptotic runtime of various operations using\nappropriate or inappropriate representations. for example, inserting a number\ninto the correct position in a sorted list of numbers is an o(n) operation if the\nlist is represented as a linked list, but only o(log n) if the list is represented as a\nred-black tree.\n\nin the context of machine learning, what makes one representation better than\n\n529\n\n "}, {"Page_number": 545, "text": "chapter 15. representation learning\n\nanother? generally speaking, a good representation is one that makes a subsequent\nlearning task easier. the choice of representation will usually depend on the choice\nof the subsequent learning task.\n\nwe can think of feedforward networks trained by supervised learning as per-\nforming a kind of representation learning. specifically, the last layer of the network\nis typically a linear classifier, such as a softmax regression classifier. the rest of\nthe network learns to provide a representation to this classifier. training with a\nsupervised criterion naturally leads to the representation at every hidden layer (but\nmore so near the top hidden layer) taking on properties that make the classification\ntask easier. for example, classes that were not linearly separable in the input\nfeatures may become linearly separable in the last hidden layer. in principle, the\nlast layer could be another kind of model, such as a nearest neighbor classifier\n(salakhutdinov and hinton 2007a\n). the features in the penultimate layer should\nlearn different properties depending on the type of the last layer.\n\n,\n\nsupervised training of feedforward networks does not involve explicitly imposing\nany condition on the learned intermediate features. other kinds of representation\nlearning algorithms are often explicitly designed to shape the representation in\nsome particular way. for example, suppose we want to learn a representation that\nmakes density estimation easier. distributions with more independences are easier\nto model, so we could design an objective function that encourages the elements\nof the representation vector h to be independent. just like supervised networks,\nunsupervised deep learning algorithms have a main training objective but also\nlearn a representation as a side effect. regardless of how a representation was\nobtained, it can can be used for another task. alternatively, multiple tasks (some\nsupervised, some unsupervised) can be learned together with some shared internal\nrepresentation.\n\nmost representation learning problems face a tradeoff between preserving as\nmuch information about the input as possible and attaining nice properties (such\nas independence).\n\nrepresentation learning is particularly interesting because it provides one\nway to perform unsupervised and semi-supervised learning. we often have very\nlarge amounts of unlabeled training data and relatively little labeled training\ndata. training with supervised learning techniques on the labeled subset often\nresults in severe overfitting. semi-supervised learning offers the chance to resolve\nthis overfitting problem by also learning from the unlabeled data. specifically,\nwe can learn good representations for the unlabeled data, and then use these\nrepresentations to solve the supervised learning task.\n\nhumans and animals are able to learn from very few labeled examples. we do\n\n530\n\n "}, {"Page_number": 546, "text": "chapter 15. representation learning\n\nnot yet know how this is possible. many factors could explain improved human\nperformance\u2014for example, the brain may use very large ensembles of classifiers\nor bayesian inference techniques. one popular hypothesis is that the brain is\nable to leverage unsupervised or semi-supervised learning. there are many ways\nto leverage unlabeled data. in this chapter, we focus on the hypothesis that the\nunlabeled data can be used to learn a good representation.\n\n15.1 greedy layer-wise unsupervised pretraining\n\nunsupervised learning played a key historical role in the revival of deep neural\nnetworks, allowing for the first time to train a deep supervised network without\nrequiring architectural specializations like convolution or recurrence. we call this\nprocedure unsupervised pretraining, or more precisely, greedy layer-wise unsuper-\nvised pretraining. this procedure is a canonical example of how a representation\nlearned for one task (unsupervised learning, trying to capture the shape of the\ninput distribution) can sometimes be useful for another task (supervised learning\nwith the same input domain).\n\ngreedy layer-wise unsupervised pretraining relies on a single-layer represen-\ntation learning algorithm such as an rbm, a single-layer autoencoder, a sparse\ncoding model, or another model that learns latent representations. each layer is\npretrained using unsupervised learning, taking the output of the previous layer\nand producing as output a new representation of the data, whose distribution (or\nits relation to other variables such as categories to predict) is hopefully simpler.\nsee algorithm\n\nfor a formal description.\n\n15.1\n\n,\n\ngreedy layer-wise training procedures based on unsupervised criteria have long\nbeen used to sidestep the difficulty of jointly training the layers of a deep neural net\nfor a supervised task. this approach dates back at least as far as the neocognitron\n(fukushima 1975\n). the deep learning renaissance of 2006 began with the discovery\nthat this greedy learning procedure could be used to find a good initialization for\na joint learning procedure over all the layers, and that this approach could be used\nto successfully train even fully connected architectures (hinton\n2006 hinton\n;\nand salakhutdinov 2006 hinton 2006 bengio\n).\net al.,\n2007a\nprior to this discovery, only convolutional deep networks or networks whose depth\nresulted from recurrence were regarded as feasible to train. today, we now know\nthat greedy layer-wise pretraining is not required to train fully connected deep\narchitectures, but the unsupervised pretraining approach was the first method to\nsucceed.\n\net al.,\n2007 ranzato\n\net al.,\n\n,\n\n;\n\n,\n\n;\n\n;\n\ngreedy layer-wise pretraining is called greedy\n\nbecause it is a\n\ngreedy algorithm\n,\n\n531\n\n "}, {"Page_number": 547, "text": "chapter 15. representation learning\n\nlayer-wise\n\nmeaning that it optimizes each piece of the solution independently, one piece at a\ntime, rather than jointly optimizing all pieces. it is called\nbecause these\nindependent pieces are the layers of the network. specifically, greedy layer-wise\npretraining proceeds one layer at a time, training the k-th layer while keeping the\nprevious ones fixed. in particular, the lower layers (which are trained first) are not\nadapted after the upper layers are introduced. it is called unsupervised because each\nlayer is trained with an unsupervised representation learning algorithm. however\nit is also called pretraining, because it is supposed to be only a first step before\na joint training algorithm is applied to\nall the layers together. in the\ncontext of a supervised learning task, it can be viewed as a regularizer (in some\nexperiments, pretraining decreases test error without decreasing training error)\nand a form of parameter initialization.\n\nfine-tune\n\nit is common to use the word \u201cpretraining\u201d to refer not only to the pretraining\nstage itself but to the entire two phase protocol that combines the pretraining\nphase and a supervised learning phase. the supervised learning phase may involve\ntraining a simple classifier on top of the features learned in the pretraining phase,\nor it may involve supervised fine-tuning of the entire network learned in the\npretraining phase. no matter what kind of unsupervised learning algorithm or\nwhat model type is employed, in the vast majority of cases, the overall training\nscheme is nearly the same. while the choice of unsupervised learning algorithm\nwill obviously impact the details, most applications of unsupervised pretraining\nfollow this basic protocol.\n\ngreedy layer-wise unsupervised pretraining can also be used as initialization\nfor other unsupervised learning algorithms, such as deep autoencoders (hinton\nand salakhutdinov 2006\n) and probabilistic models with many layers of latent\nvariables. such models include deep belief networks (\n) and deep\nboltzmann machines (salakhutdinov and hinton 2009a\n). these deep generative\nmodels will be described in chapter\n\nhinton et al. 2006\n\n.20\n\n,\n\n,\n\n,\n\n8.7.4\n\nas discussed in sec.\n\nsuper-\nvised pretraining. this builds on the premise that training a shallow network is\neasier than training a deep one, which seems to have been validated in several\ncontexts (\n\n, it is also possible to have greedy layer-wise\n\nerhan et al. 2010\n\n).\n\n,\n\n15.1.1 when and why does unsupervised pretraining work?\n\non many tasks, greedy layer-wise unsupervised pretraining can yield substantial\nimprovements in test error for classification tasks. this observation was responsible\nfor the renewed interested in deep neural networks starting in 2006 (hinton et al.,\n\n532\n\n "}, {"Page_number": 548, "text": "chapter 15. representation learning\n\nalgorithm 15.1 greedy layer-wise unsupervised pretraining protocol.\ngiven the following:\u00a0unsupervised feature learning algorithm l, which takes a\ntraining set of examples and returns an encoder or feature function f. the raw\ninput data is x, with one row per example and f (1)(x) is the output of the first\nstage encoder on x and the dataset used by the second level unsupervised feature\nlearner. in the case where fine-tuning is performed, we use a learner t which takes\nan initial function f, input examples x (and in the supervised fine-tuning case,\n.\nassociated targets\nm\n\n), and returns a tuned function. the number of stages is\n\ny\n\nf \u2190 identity function\n\u02dcx x= \ndo\nfor\n\n= 1\n\nk\n, . . . , m\nf ( )k =  (l \u02dcx)\nf\u2190 ( )k \u25e6 f\nf\n\u02dcx \u2190 f ( )k ( \u02dcx)\nend for\nif fine-tuning then\n\u2190 t ( x y )\n\nf,\n\nf\n\n,\n\nend if\nreturn f\n\n;\n\n;\n\n(\n\net al.,\n\net al.,\n\n2007a\n\n2007 ranzato\n\nma et al. 2015\n\n2006 bengio\n). on many other tasks, however,\nunsupervised pretraining either does not confer a benefit or even causes noticeable\nharm.\n) studied the effect of pretraining on machine learning\nmodels for chemical activity prediction and found that, on average, pretraining was\nslightly harmful, but for many tasks was significantly helpful. because unsupervised\npretraining is sometimes helpful but often harmful it is important to understand\nwhen and why it works in order to determine whether it is applicable to a particular\ntask.\n\nat the outset, it is important to clarify that most of this discussion is restricted\nto greedy unsupervised pretraining in particular. there are other, completely\ndifferent paradigms for performing semi-supervised learning with neural networks,\nsuch as virtual adversarial training described in sec.\u00a0\n. it is also possible to\ntrain an autoencoder or generative model at the same time as the supervised model.\nexamples of this single-stage approach include the discriminative rbm (larochelle\nand bengio 2008\n), in which the total\nobjective is an explicit sum of the two terms (one using the labels and one only\nusing the input).\n\n) and the ladder network (\n\nrasmus et al. 2015\n\n7.13\n\n,\n\n,\n\nunsupervised pretraining combines two different ideas. first, it makes use of\n\n533\n\n "}, {"Page_number": 549, "text": "chapter 15. representation learning\n\nthe idea that the choice of initial parameters for a deep neural network can have\na significant regularizing effect on the model (and, to a lesser extent, that it can\nimprove optimization). second, it makes use of the more general idea that learning\nabout the input distribution can help to learn about the mapping from inputs to\noutputs.\n\nboth of these ideas involve many complicated interactions between several\n\nparts of the machine learning algorithm that are not entirely understood.\n\nthe first idea, that the choice of initial parameters for a deep neural network\ncan have a strong regularizing effect on its performance, is the least well understood.\nat the time that pretraining became popular, it was understood as initializing the\nmodel in a location that would cause it to approach one local minimum rather than\nanother.\u00a0today, local minima are no longer considered to be a serious problem\nfor neural network optimization. we now know that our standard neural network\ntraining procedures usually do not arrive at a critical point of any kind. it remains\npossible that pretraining initializes the model in a location that would otherwise\nbe inaccessible\u2014for example, a region that is surrounded by areas where the cost\nfunction varies so much from one example to another that minibatches give only\na very noisy estimate of the gradient, or a region surrounded by areas where the\nhessian matrix is so poorly conditioned that gradient descent methods must use\nvery small steps. however, our ability to characterize exactly what aspects of the\npretrained parameters are retained during the supervised training stage is limited.\nthis is one reason that modern approaches typically use simultaneous unsupervised\nlearning and supervised learning rather than two sequential stages. one may\nalso avoid struggling with these complicated ideas about how optimization in the\nsupervised learning stage preserves information from the unsupervised learning\nstage by simply freezing the\u00a0parameters for\u00a0the feature\u00a0extractors and\u00a0using\nsupervised learning only to add a classifier on top of the learned features.\n\nthe other idea, that a learning algorithm can use information learned in the\nunsupervised phase to perform better in the supervised learning stage, is better\nunderstood. the basic idea is that some features that are useful for the unsupervised\ntask may also be useful for the supervised learning task. for example, if we train\na generative model of images of cars and motorcycles, it will need to know about\nwheels, and about how many wheels should be in an image. if we are fortunate,\nthe representation of the wheels will take on a form that is easy for the supervised\nlearner to access. this is not yet understood at a mathematical, theoretical level,\nso it is not always possible to predict which tasks will benefit from unsupervised\nlearning in this way. many aspects of this approach are highly dependent on\nthe specific models used. for example, if we wish to add a linear classifier on\n\n534\n\n "}, {"Page_number": 550, "text": "chapter 15. representation learning\n\ntop of pretrained features, the features must make the underlying classes linearly\nseparable. these properties often occur naturally but do not always do so. this\nis another reason that simultaneous supervised and unsupervised learning can be\npreferable\u2014the constraints imposed by the output layer are naturally included\nfrom the start.\n\nfrom the point of view of unsupervised pretraining as learning a representation,\nwe can expect unsupervised pretraining to be more effective when the initial\nrepresentation is poor.\u00a0one key example of this is the use of word embeddings.\nwords represented by one-hot vectors are not very informative because every two\ndistinct one-hot vectors are the same distance from each other (squared l2 distance\nof ). learned word embeddings naturally encode similarity between words by their\ndistance from each other. because of this, unsupervised pretraining is especially\nuseful when processing words. it is less useful when processing images, perhaps\nbecause images already lie in a rich vector space where distances provide a low\nquality similarity metric.\n\n2\n\nfrom the point of view of unsupervised pretraining as a regularizer, we can\nexpect unsupervised pretraining to be most helpful when the number of labeled\nexamples is very small. because the source of information added by unsupervised\npretraining is the unlabeled data, we may also expect unsupervised pretraining\nto perform best\u00a0when the\u00a0number\u00a0of unlabeled\u00a0examples is\u00a0very\u00a0large. the\nadvantage of semi-supervised learning via unsupervised pretraining with many\nunlabeled examples and few labeled examples was made particularly clear in\n2011 with unsupervised pretraining winning two international transfer learning\ncompetitions (\n), in settings where the\nnumber of labeled examples in the target task was small (from a handful to dozens\nof examples per class). these effects were also documented in carefully controlled\nexperiments by paine\n\nmesnil et al. 2011 goodfellow et al. 2011\n\net al. (\n\n2014\n\n).\n\n,\n\n,\n\n;\n\nother factors are likely to be involved. for example, unsupervised pretraining\nis likely to be most useful when the function to be learned is extremely complicated.\nunsupervised learning differs from regularizers like weight decay because it does not\nbias the learner toward discovering a simple function but rather toward discovering\nfeature functions that are useful for the unsupervised learning task.\u00a0if the true\nunderlying functions are complicated and shaped by regularities of the input\ndistribution, unsupervised learning can be a more appropriate regularizer.\n\nthese caveats aside, we now analyze some success cases where unsupervised\npretraining is known to cause an improvement, and explain what is known about\nwhy this improvement occurs. unsupervised pretraining has usually been used\nto improve classifiers, and is usually most interesting from the point of view of\n\n535\n\n "}, {"Page_number": 551, "text": "chapter 15. representation learning\n\nwith\u00a0pretraining\nwithout\u00a0pretraining\n\n1500\n\n1000\n\n500\n\n0\n\n\u2212500\n\n\u22121000\n\n\u22121500\n\n\u22124000 \u22123000 \u22122000 \u22121000\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n(\n\nerhan et al. 2010\n\nfigure 15.1: visualization via nonlinear projection of the learning trajectories of different\nneural networks in function space (not parameter space, to avoid the issue of many-to-\none mappings from parameter vectors to functions), with different random initializations\nand with or without unsupervised pretraining. each point corresponds to a different\nneural network, at a particular time during its training process. this figure is adapted\n). a coordinate in function space is an infinite-\nwith permission from\ndimensional vector associating every input x with an output y.\n) made\na linear projection to high-dimensional space by concatenating the y for many specific x\npoints. they then made a further nonlinear projection to 2-d by isomap (tenenbaum\net al.,\n). color indicates time. all networks are initialized near the center of the plot\n(corresponding to the region of functions that produce approximately uniform distributions\nover the class y for most inputs). over time, learning moves the function outward, to\npoints that make strong predictions. training consistently terminates in one region when\nusing pretraining and in another, non-overlapping region when not using pretraining.\nisomap tries to preserve global relative distances (and hence volumes) so the small region\ncorresponding to pretrained models may indicate that the pretraining-based estimator\nhas reduced variance.\n\nerhan et al. 2010\n\n2000\n\n(\n\n536\n\n "}, {"Page_number": 552, "text": "chapter 15. representation learning\n\nreducing test set error. however, unsupervised pretraining can help tasks other\nthan classification, and can act to improve optimization rather than being merely\na regularizer. for example, it can improve both train and test reconstruction error\nfor deep autoencoders (hinton and salakhutdinov 2006\n\n).\n\n,\n\n2010\n\nerhan\n\net al. (\n\n) performed many experiments to explain several successes of\nunsupervised pretraining. both improvements to training error and improvements\nto test error may be explained in terms of unsupervised pretraining taking the\nparameters into a region that would otherwise be inaccessible. neural network\ntraining is non-deterministic, and converges to a different function every time it\nis run.\u00a0training may halt at a point where the gradient becomes small, a point\nwhere early stopping ends training to prevent overfitting, or at a point where the\ngradient is large but it is difficult to find a downhill step due to problems such as\nstochasticity or poor conditioning of the hessian.\u00a0neural networks that receive\nunsupervised pretraining consistently halt in the same region of function space,\nwhile neural networks without pretraining consistently halt in another region. see\nfig.\nfor a visualization of this phenomenon. the region where pretrained\nnetworks arrive is smaller, suggesting that pretraining reduces the variance of the\nestimation process, which can in turn reduce the risk of severe over-fitting. in\nother words, unsupervised pretraining initializes neural network parameters into\na region that they do not escape, and the results following this initialization are\nmore consistent and less likely to be very bad than without this initialization.\n\n15.1\n\n2010\n\nerhan\n\net al. (\n\n) also provide some answers as to when pretraining works\nbest\u2014the mean and variance of the test error were most reduced by pretraining for\ndeeper networks. keep in mind that these experiments were performed before the\ninvention and popularization of modern techniques for training very deep networks\n(rectified linear units, dropout and batch normalization) so less is known about the\neffect of unsupervised pretraining in conjunction with contemporary approaches.\n\nan important question is how unsupervised pretraining can act as a regularizer.\none hypothesis is that pretraining encourages the learning algorithm to discover\nfeatures that relate to the underlying causes that generate the observed data.\nthis is an important idea motivating many other algorithms besides unsupervised\npretraining, and is described further in sec.\n\n.\n15.3\n\ncompared to other ways of incorporating this belief by using unsupervised\nlearning, unsupervised pretraining has the disadvantage that it operates with\ntwo separate training phases. one reason that these two training phases are\ndisadvantageous is that there is not a single hyperparameter that predictably\nreduces or increases the strength of the regularization arising from the unsupervised\npretraining. instead, there are very many hyperparameters, whose effect may be\n\n537\n\n "}, {"Page_number": 553, "text": "chapter 15. representation learning\n\nmeasured after the fact but is often difficult to predict ahead of time. when we\nperform unsupervised and supervised learning simultaneously, instead of using the\npretraining strategy, there is a single hyperparameter, usually a coefficient attached\nto the unsupervised cost, that determines how strongly the unsupervised objective\nwill regularize the supervised model. one can always predictably obtain less\nregularization by decreasing this coefficient. in the case of unsupervised pretraining,\nthere is not a way of flexibly adapting the strength of the regularization\u2014either\nthe supervised model is initialized to pretrained parameters, or it is not.\n\nanother disadvantage of having two separate training phases is that each phase\nhas its own hyperparameters. the performance of the second phase usually cannot\nbe predicted during the first phase, so there is a long delay between proposing\nhyperparameters for the first phase and being able to update them using feedback\nfrom the second phase. the most principled approach is to use validation set error\nin the supervised phase in order to select the hyperparameters of the pretraining\nphase, as discussed in\n). in practice, some hyperparameters,\nlike the number of pretraining iterations, are more conveniently set during the\npretraining phase, using early stopping on the unsupervised objective, which is\nnot ideal but computationally much cheaper than using the supervised objective.\n\nlarochelle et al. 2009\n\n(\n\ntoday, unsupervised pretraining has been largely abandoned, except in the\nfield of natural language processing, where the natural representation of words as\none-hot vectors conveys no similarity information and where very large unlabeled\nsets are available. in that case, the advantage of pretraining is that one can pretrain\nonce on a huge unlabeled set (for example with a corpus containing billions of\nwords), learn a good representation (typically of words, but also of sentences), and\nthen use this representation or fine-tune it for a supervised task for which the\ntraining set contains substantially fewer examples. this approach was pioneered\nby by collobert and weston 2008b turian\net al.\n(\n2011a\n\n) and remains in common use today.\n\ncollobert\n\net al. (\n\n), and\n\n2010\n\n),\n\n(\n\ndeep learning techniques based on supervised learning, regularized with dropout\nor batch normalization, are able to achieve human-level performance on very many\ntasks, but only with extremely large labeled datasets. these same techniques\noutperform unsupervised pretraining on medium-sized datasets such as cifar-10\nand mnist, which have roughly 5,000 labeled examples per class. on extremely\nsmall datasets, such as the alternative splicing dataset, bayesian methods outper-\nform methods based on unsupervised pretraining (srivastava 2013\n). for these\nreasons, the popularity of unsupervised pretraining has declined. nevertheless,\nunsupervised pretraining remains an important milestone in the history of deep\nlearning research and continues to influence contemporary approaches. the idea of\n\n,\n\n538\n\n "}, {"Page_number": 554, "text": "chapter 15. representation learning\n\npretraining has been generalized to supervised pretraining discussed in sec.\n,\n8.7.4\nas a very common approach for transfer learning. supervised pretraining for\ntransfer learning is popular (\n) for use with\nconvolutional networks pretrained on the imagenet dataset. practitioners publish\nthe parameters of these trained networks for this purpose, just like pretrained word\nvectors are published for natural language tasks (\ncollobert et al. 2011a mikolov\net al.,\n\noquab et al. 2014 yosinski\n\n2013a\n\net al.,\n\n2014\n\n).\n\n,\n\n;\n\n,\n\n;\n\n15.2 transfer learning and domain adaptation\n\ntransfer learning and domain adaptation refer to the situation where what has been\nlearned in one setting (i.e., distribution p1) is exploited to improve generalization\nin another setting (say distribution p2). this generalizes the idea presented in the\nprevious section, where we transferred representations between an unsupervised\nlearning task and a supervised learning task.\n\nin transfer learning, the learner must perform two or more different tasks,\nbut we assume that many of the factors that explain the variations in p1 are\nrelevant to the variations that need to be captured for learning p2. this is typically\nunderstood in a supervised learning context, where the input is the same but the\ntarget may be of a different nature. for example, we may learn about one set of\nvisual categories, such as cats and dogs, in the first setting, then learn about a\ndifferent set of visual categories, such as ants and wasps, in the second setting. if\nthere is significantly more data in the first setting (sampled from p1), then that\nmay help to learn representations that are useful to quickly generalize from only\nvery few examples drawn from p2. many visual categories share low-level notions\nof edges and visual shapes, the effects of geometric changes, changes in lighting, etc.\nin general, transfer learning, multi-task learning (sec.\n), and domain adaptation\ncan be achieved via representation learning when there exist features that are\nuseful for the different settings or tasks, corresponding to underlying factors that\nappear in more than one setting. this is illustrated in fig.\n, with shared lower\nlayers and task-dependent upper layers.\n\n7.2\n\n7.7\n\nhowever,\u00a0sometimes,\u00a0what is shared\u00a0among the\u00a0different\u00a0tasks is not the\nsemantics of the input but the semantics of the output. for example, a speech\nrecognition system needs to produce valid sentences at the output layer, but\nthe earlier layers near the input may need to recognize very different versions of\nthe same phonemes or sub-phonemic vocalizations depending on which person\nis speaking. in cases like these, it makes more sense to share the upper layers\n(near the output) of the neural network, and have a task-specific preprocessing, as\n\n539\n\n "}, {"Page_number": 555, "text": "chapter 15. representation learning\n\nillustrated in fig.\n\n.\n15.2\n\nyy\n\nh(shared)\nh(shared)\n\nselection\u00a0switch\n\nh(1)h(1)\n\nh(2)h(2)\n\nh(3)h(3)\n\nx(1)x(1)\n\nx(2)x(2)\n\nx(3)x(3)\n\ny\n\nhas the same semantics for all tasks while the input variable\n\nfigure 15.2:\u00a0example architecture for multi-task or transfer learning when the output\nvariable\nhas a different\nmeaning (and possibly even a different dimension) for each task (or, for example, each\nuser), called x(1), x(2) and x (3) for three tasks. the lower levels (up to the selection\nswitch) are task-specific, while the upper levels are shared. the lower levels learn to\ntranslate their task-specific input into a generic set of features.\n\nx \n\ndomain adaptation\n\nin the related case of\n\n, the task (and the optimal input-to-\noutput mapping) remains the same between each setting, but the input distribution\nis slightly different. for example, consider the task of sentiment analysis, which\nconsists of determining whether a comment expresses positive or negative sentiment.\ncomments posted on the web come from many categories. a domain adaptation\nscenario can arise when a sentiment predictor trained on customer reviews of\nmedia content such as books, videos and music is later used to analyze comments\nabout consumer electronics such as televisions or smartphones. one can imagine\nthat there is an underlying function that tells whether any statement is positive,\nneutral or negative, but of course the vocabulary and style may vary from one\ndomain to another, making it more difficult to generalize across domains. simple\nunsupervised pretraining (with denoising autoencoders) has been found to be very\nsuccessful for sentiment analysis with domain adaptation (\n\nglorot et al. 2011b\n\n).\n\n,\n\na related problem is that of concept drift, which we can view as a form of transfer\nlearning due to gradual changes in the data distribution over time. both concept\ndrift and transfer learning can be viewed as particular forms of multi-task learning.\n\n540\n\n "}, {"Page_number": 556, "text": "chapter 15. representation learning\n\nwhile the phrase \u201cmulti-task learning\u201d typically refers to supervised learning tasks,\nthe more general notion of transfer learning is applicable to unsupervised learning\nand reinforcement learning as well.\n\nin all of these cases, the objective is to take advantage of data from the first\nsetting to extract information that may be useful when learning or even when\ndirectly making predictions in the second setting. the core idea of representation\nlearning is that the same representation may be useful in both settings. using the\nsame representation in both settings allows the representation to benefit from the\ntraining data that is available for both tasks.\n\n;\n\n,\n\n2011\n\nas mentioned before, unsupervised deep learning for transfer learning has found\nmesnil et al. 2011 goodfellow\nsuccess in some machine learning competitions (\net al.,\n). in the first of these competitions, the experimental setup is the\nfollowing. each participant is first given a dataset from the first setting (from\ndistribution p1), illustrating examples of some set of categories. the participants\nmust use this to learn a good feature space (mapping the raw input to some\nrepresentation), such that when we apply this learned transformation to inputs\nfrom the transfer setting (distribution p2), a linear classifier can be trained and\ngeneralize well from very few labeled examples. one of the most striking results\nfound in this competition is that as an architecture makes use of deeper and\ndeeper representations (learned in a purely unsupervised way from data collected\nin the first setting, p1), the learning curve on the new categories of the second\n(transfer) setting p2 becomes much better. for deep representations, fewer labeled\nexamples of the transfer tasks are necessary to achieve the apparently asymptotic\ngeneralization performance.\n\n, sometimes also called\n\ntwo extreme forms of transfer learning are one-shot learning\n\nzero-shot\nlearning\n. only one labeled example of\nthe transfer task is given for one-shot learning, while no labeled examples are given\nat all for the zero-shot learning task.\n\nzero-data learning\n\nand\n\n2006\n\net al.,\n\none-shot learning (fei-fei\n\n) is possible because the representation\nlearns to cleanly separate the underlying classes during the first stage. during the\ntransfer learning stage, only one labeled example is needed to infer the label of many\npossible test examples that all cluster around the same point in representation\nspace. this works to the extent that the factors of variation corresponding to\nthese invariances have been cleanly separated from the other factors, in the learned\nrepresentation space, and we have somehow learned which factors do and do not\nmatter when discriminating objects of certain categories.\n\nas an example of a zero-shot learning setting, consider the problem of having\na learner read a large collection of text and then solve object recognition problems.\n\n541\n\n "}, {"Page_number": 557, "text": "chapter 15. representation learning\n\nit may be possible to recognize a specific object class even without having seen an\nimage of that object, if the text describes the object well enough.\u00a0for example,\nhaving read that a cat has four legs and pointy ears, the learner might be able to\nguess that an image is a cat, without having seen a cat before.\n\n;\n\n2008\n\net al.\n,\n\net al.,\n\n2013b\n\n2009 socher\n\n) and zero-shot learning (\n\nzero-data learning (larochelle\n\npalatucci\net al.\n) are only possible because additional information\n,\nhas been exploited during training. we can think of the zero-data learning scenario\nas including three random variables: the traditional inputs x, the traditional\noutputs or targets y , and an additional random variable describing the task, t .\nthe model is trained to estimate the conditional distribution p(y x|\n, t ) where\nt is a description of the task we wish the model to perform.\u00a0in our example of\nrecognizing cats after having read about cats, the output is a binary variable y\nwith y = 1 indicating \u201cyes\u201d and y = 0 indicating \u201cno.\u201d the task variable t then\nrepresents questions to be answered such as \u201cis there a cat in this image?\u201d if we\nhave a training set containing unsupervised examples of objects that live in the\nsame space as t , we may be able to infer the meaning of unseen instances of t .\nin our example of recognizing cats without having seen an image of the cat, it is\nimportant that we have had unlabeled text data containing sentences such as \u201ccats\nhave four legs\u201d or \u201ccats have pointy ears.\u201d\n\nzero-shot learning requires t to be represented in a way that allows some sort\nof generalization. for example, t cannot be just a one-hot code indicating an\nobject category.\n) provide instead a distributed representation\nof object categories by using a learned word embedding for the word associated\nwith each category.\n\nsocher et al. 2013b\n\n(\n\n;\n\n2014\n\net al.,\n\net al.,\n\net al.,\n\n2013b gouws\n\na similar phenomenon happens in machine translation (klementiev\n\n2012\n;\nmikolov\n): we have words in one language, and\nthe relationships between words can be learned from unilingual corpora; on the\nother hand, we have translated sentences which relate words in one language with\nwords in the other. even though we may not have labeled examples translating\nword a in language x to word b in language y , we can generalize and guess a\ntranslation for word a because we have learned a distributed representation for\nwords in language x, a distributed representation for words in language y , and\ncreated a link (possibly two-way) relating the two spaces, via training examples\nconsisting of matched pairs of sentences in both languages. this transfer will be\nmost successful if all three ingredients (the two representations and the relations\nbetween them) are learned jointly.\n\nzero-shot learning is a particular form of transfer learning. the same principle\nexplains how one can perform multi-modal learning, capturing a representation in\n\n542\n\n "}, {"Page_number": 558, "text": "chapter 15. representation learning\n\nhx = fx ( )x\n\nhy = fy ( )y\n\nfx\n\nxtest\n\nfy\n\nx(cid:129)space\n\ny(cid:129)space\n\ny test\n\n) pairs in the training set\n\n(\nx y,\nfx : encoder function for x\nfy : encoder function for y\nrelationship\u00a0between\u00a0embedded\u00a0points\u00a0within\u00a0one\u00a0of\u00a0the\u00a0domains\n\nmaps\u00a0between\u00a0representation\u00a0spaces\u00a0\n\nfigure 15.3: transfer learning between two domains x and y enables zero-shot learning.\nlabeled or unlabeled examples of x allow one to learn a representation function fx and\nsimilarly with examples of y to learn fy. each application of the fx and f y functions\nappears as an upward arrow, with the style of the arrows indicating which function is\napplied. distance in hx space provides a similarity metric between any pair of points\nin x space that may be more meaningful than distance in x space. likewise, distance\nin hy space provides a similarity metric between any pair of points in y space. both\nof these similarity functions are indicated with dotted bidirectional arrows. labeled\nexamples (dashed horizontal lines) are pairs (x y, ) which allow one to learn a one-way\nor two-way map (solid bidirectional arrow) between the representations fx( x) and the\nrepresentations fy (y ) and anchor these representations to each other. zero-data learning\nis then enabled as follows. one can associate an image xtest to a word ytest, even if no\nimage of that word was ever presented, simply because word-representations fy (ytest)\nand image-representations fx (xtest) can be related to each other via the maps between\nrepresentation spaces. it works because, although that image and that word were never\npaired, their respective feature vectors fx(xtest ) and f y(ytest ) have been related to each\nother. figure inspired from suggestion by hrant khachatrian.\n\n543\n\n "}, {"Page_number": 559, "text": "chapter 15. representation learning\n\none modality, a representation in the other, and the relationship (in general a joint\ndistribution) between pairs (x y, ) consisting of one observation x in one modality\nand another observation y in the other modality (srivastava and salakhutdinov,\n2012). by learning all three sets of parameters (from x to its representation, from\ny to its representation, and the relationship between the two representations),\nconcepts in one representation are anchored in the other, and vice-versa, allowing\none to meaningfully\u00a0generalize to\u00a0new pairs. the procedure is\u00a0illustrated in\nfig.\n\n15.3\n.\n\n15.3 semi-supervised disentangling of causal factors\n\nan important question about representation learning is \u201cwhat makes one repre-\nsentation better than another?\u201d one hypothesis is that an ideal representation\nis one in which the features within the representation correspond to the under-\nlying causes of the observed data, with separate features or directions in feature\nspace corresponding to different causes, so that the representation disentangles the\ncauses from one another. this hypothesis motivates approaches in which we first\nseek a good representation for p(x).\u00a0such a representation may also be a good\nrepresentation for computing p(y x|\n) if y is among the most salient causes of\nx.\u00a0this idea has guided a large amount of deep learning research since at least\nthe 1990s (becker and hinton 1992 hinton and sejnowski 1999\n;\n), in more detail.\nfor other arguments about when semi-supervised learning can outperform pure\nsupervised learning, we refer the reader to sec. 1.2 of\n\nchapelle et al. 2006\n\n).\n\n(\n\n,\n\n,\n\nin other approaches to representation learning, we have often been concerned\nwith a representation that is easy to model\u2014for example, one whose entries are\nsparse, or independent from each other. a representation that cleanly separates\nthe underlying causal factors may not necessarily be one that is easy to model.\nhowever, a further part of the hypothesis motivating semi-supervised learning\nvia unsupervised representation learning is that for many ai tasks, these two\nproperties coincide:\u00a0once we are able to obtain the underlying explanations for\nwhat we observe, it generally becomes easy to isolate individual attributes from\nthe others. specifically, if a representation h represents many of the underlying\ncauses of the observed x, and the outputs y are among the most salient causes,\nthen it is easy to predict\n\nfrom .\nh\n\ny\n\nfirst, let us see how semi-supervised learning can fail because unsupervised\nlearning of p(x) is of no help to learn p(y x|\n). consider for example the case\nwhere p(x) is uniformly distributed and we want to learn f (x) = e[y | x]. clearly,\ny  x|\n.\nobserving a training set of\n)\n\nvalues alone gives us no information about\n\np(\n\nx\n\n544\n\n "}, {"Page_number": 560, "text": "chapter 15. representation learning\n\nmixture model\n\ny=1\n\ny=2\n\ny=3\n\n)\nx\n(\np\n\nx\n\nfigure 15.4: example of a density over x that is a mixture over three\u00a0components.\nthe component identity is an underlying explanatory factor, y. because the mixture\ncomponents (e.g.,\u00a0natural object classes in image data) are statistically salient, just\nmodeling p( x) in an unsupervised way with no labeled example already reveals the factor\ny.\n\nnext, let us see a simple example of how semi-supervised learning can succeed.\nconsider the situation where x arises from a mixture, with one mixture component\nper value of y, as illustrated in fig.\n. if the mixture components are well-\nseparated, then modeling p(x) reveals precisely where each component is, and a\nsingle labeled example of each class will then be enough to perfectly learn p (y x|\n).\nbut more generally, what could make\n\nbe tied together?\n\np( )x\n\n15.4\n\nand\n\np(\n\ny  x|\n)\n\nif y is closely associated with one of the causal factors of x, then p(x) and\nx|\n) will\u00a0be strongly tied,\u00a0and unsupervised representation learning that\np(y\ntries to disentangle the underlying factors of variation is likely to be useful as a\nsemi-supervised learning strategy.\n\nconsider the assumption that y is one of the causal factors of x, and let\nh represent all those factors. the true generative process can be conceived as\nstructured according to this directed graphical model, with\n:\nx\n\nas the parent of\n\nh \n\np\n\n(h x) =  (\np\n\n,\n\nx h|\n)\np\n\n( )h\n.\n\nas a consequence, the data has marginal probability\n\np( ) = \n\nx\n\nx h|\n(\nehp\n)\n.\n\n(15.1)\n\n(15.2)\n\nfrom this straightforward observation, we conclude that the best possible model\nof x (from a generalization point of view) is the one that uncovers the above \u201ctrue\u201d\n\n545\n\n "}, {"Page_number": 561, "text": "chapter 15. representation learning\n\nstructure, with h as a latent variable that explains the observed variations in x.\nthe \u201cideal\u201d representation learning discussed above should thus recover these latent\nfactors. if y is one of these (or closely related to one of them), then it will be\nvery easy to learn to predict y from such a representation. we also see that the\nconditional distribution of y given x is tied by bayes rule to the components in\nthe above equation:\n\np(\n\ny x|\n\n) =\n\n( )y\n\nx y|\np\n)\n(\np\np( )x\n\n.\n\n(15.3)\n\nthus the marginal p(x) is intimately tied to the conditional p(y x|\n) and knowledge\nof the structure of the former should be helpful to learn the latter. therefore, in\nsituations respecting these assumptions, semi-supervised learning should improve\nperformance.\n\nan important research problem regards the fact that most observations are\nformed by an extremely large number of underlying causes. suppose y =hi, but\nthe unsupervised learner does not know which hi. the brute force solution is for\nan unsupervised learner to learn a representation that captures all the reasonably\nsalient generative factors hj and disentangles them from each other, thus making\nit easy to predict\n\nfrom , regardless of which h\n\ni is associated with .y\n\nh\n\ny\n\nin practice, the brute force solution is not feasible because it is not possible\nto capture all or most of the factors of variation that influence an observation.\nfor example, in a visual scene, should the representation always encode all of\nthe smallest objects in the background? it is a well-documented psychological\nphenomenon that human beings fail to perceive changes in their environment that\nare not immediately relevant to the task they are performing\u2014see, e.g., simons\nand levin 1998\n). an important research frontier in semi-supervised learning is\ndetermining what to encode in each situation. currently, two of the main strategies\nfor dealing with a large number of underlying causes are to use a supervised learning\nsignal at the same time as the unsupervised learning signal so that the model will\nchoose to capture the most relevant factors of variation, or to use much larger\nrepresentations if using purely unsupervised learning.\n\n(\n\nan emerging strategy for unsupervised learning is to modify the definition of\nwhich underlying causes are most salient. historically, autoencoders and generative\nmodels have been trained to optimize a fixed criterion, often similar to mean\nsquared error. these fixed criteria determine which causes are considered salient.\nfor example, mean squared error applied to the pixels of an image implicitly\nspecifies that an underlying cause is only salient if it significantly changes the\nbrightness of a large number of pixels. this can be problematic if the task we\nfor an example\nwish to solve involves interacting with small objects. see fig.\n\n15.5\n\n546\n\n "}, {"Page_number": 562, "text": "chapter 15. representation learning\n\ninput\n\nreconstruction\n\nfigure 15.5: an autoencoder trained with mean squared error for a robotics task has\nfailed to reconstruct a ping pong ball. the existence of the ping pong ball and all of its\nspatial coordinates are important underlying causal factors that generate the image and\nare relevant to the robotics task.\u00a0unfortunately, the autoencoder has limited capacity,\nand the training with mean squared error did not identify the ping pong ball as being\nsalient enough to encode. images graciously provided by chelsea finn.\n\nof a robotics task in which an autoencoder has failed to learn to encode a small\nping pong ball. this same robot is capable of successfully interacting with larger\nobjects, such as baseballs, which are more salient according to mean squared error.\n\ngoodfellow et al. 2014c\n\nother definitions of salience are possible. for example, if a group of pixels\nfollow a highly recognizable pattern, even if that pattern does not involve extreme\nbrightness or darkness, then that pattern could be considered extremely salient.\none way to implement such a definition of salience is to use a recently developed\napproach called generative adversarial networks (\n). in\nthis approach, a generative model is trained to fool a feedforward classifier. the\nfeedforward classifier attempts to recognize all samples from the generative model\nas being fake, and all samples from the training set as being real. in this framework,\nany structured pattern that the feedforward network can recognize is highly salient.\nthe generative adversarial network will be described in more detail in sec.\n20.10.4\n.\nfor the purposes of the present discussion, it is sufficient to understand that they\nlearn how to determine what is salient.\n) showed that models\ntrained to generate images of human heads will often neglect to generate the ears\nwhen trained with mean squared error, but will successfully generate the ears when\ntrained with the adversarial framework. because the ears are not extremely bright\nor dark compared to the surrounding skin, they are not especially salient according\nto mean squared error loss, but their highly recognizable shape and consistent\n\nlotter et al. 2015\n\n(\n\n,\n\n547\n\n "}, {"Page_number": 563, "text": "chapter 15. representation learning\n\nground truth\n\nmse\n\nadversarial\n\n(center)\n\nfigure 15.6: predictive generative networks provide an example of the importance of\nlearning which features are salient.\u00a0in this example, the predictive generative network\nhas been trained to predict the appearance of a 3-d model of a human head at a specific\nviewing angle. (left) ground truth. this is the correct image, that the network should\nemit.\u00a0\nimage produced by a predictive generative network trained with mean\nsquared error alone. because the ears do not cause an extreme difference in brightness\ncompared to the neighboring skin, they were not sufficiently salient for the model to learn\nto represent them.\nimage produced by a model trained with a combination of\nmean squared error and adversarial loss.\u00a0using this learned cost function, the ears are\nsalient because they follow a predictable pattern. learning which underlying causes are\nimportant and relevant enough to model is an important active area of research. figures\ngraciously provided by\n\nlotter et al. 2015\n\n(right)\n\n).\n\n(\n\nposition means that a feedforward network can easily learn to detect them, making\nthem highly salient under the generative adversarial framework. see fig. 15.6\nfor example images. generative adversarial networks are only one step toward\ndetermining which factors should be represented. we expect that future research\nwill discover better ways of determining which factors to represent, and develop\nmechanisms for representing different factors depending on the task.\n\n2012\n\n), is that if the true generative process has\n\na benefit of learning the underlying causal factors, as pointed out by sch\u00f6lkopf\net al. (\nx as an effect and y as\na cause, then modeling p(x  y|\n) is robust to changes in p(y).\u00a0if the cause-effect\nrelationship was reversed, this would not be true, since by bayes rule, p (x y|\n)\nwould be sensitive to changes in p( y). very often, when we consider changes in\ndistribution due to different domains, temporal non-stationarity, or changes in\nthe nature of the task, the causal mechanisms remain invariant (\u201cthe laws\nof the universe are constant\u201d) while the marginal distribution over the underlying\ncauses can change. hence, better generalization and robustness to all kinds of\nchanges can be expected via learning a generative model that attempts to recover\n\n548\n\n "}, {"Page_number": 564, "text": "chapter 15. representation learning\n\nthe causal factors\n\nh\n\nand\n\np(\n\nx h|\n.\n)\n\n15.4 distributed representation\n\ndistributed representations of concepts\u2014representations composed of many ele-\nments that can be set separately from each other\u2014are one of the most important\ntools for representation learning. distributed representations are powerful because\nthey can use n features with k values to describe k n different concepts. as we\nhave seen throughout this book, both neural networks with multiple hidden units\nand probabilistic models with multiple latent variables make use of the strategy of\ndistributed representation.\u00a0we now introduce an additional observation.\u00a0many\ndeep learning algorithms are motivated by the assumption that the hidden units\ncan learn to represent the underlying causal factors that explain the data, as\ndiscussed in sec.\n. distributed representations are natural for this approach,\nbecause each direction in representation space can correspond to the value of a\ndifferent underlying configuration variable.\n\n15.3\n\n15.7\n\nan example of a distributed representation is a vector of n binary features,\nwhich can take 2 n configurations, each potentially corresponding to a different\nregion in input space, as illustrated in fig.\n. this can be compared with\na symbolic representation, where the input is associated with a single symbol or\ncategory. if there are n symbols in the dictionary, one can imagine n feature\ndetectors, each corresponding to the detection of the presence of the associated\ncategory. in that case only n different configurations of the representation space\nare possible, carving n different regions in input space, as illustrated in fig.\n.\n15.8\nsuch a symbolic representation is also called a one-hot representation, since it can\nbe captured by a binary vector with n bits that are mutually exclusive (only one\nof them can be active). a symbolic representation is a specific example of the\nbroader class of non-distributed representations, which are representations that\nmay contain many entries but without significant meaningful separate control over\neach entry.\n\nexamples of learning algorithms\u00a0based on non-distributed representations\n\ninclude:\n\n\u2022 clustering methods, including the k-means algorithm: each input point is\n\nassigned to exactly one cluster.\n\n\u2022 k-nearest neighbors algorithms: one or a few templates or prototype examples\nare associated with a given input. in the case of k > 1, there are multiple\n\n549\n\n "}, {"Page_number": 565, "text": "chapter 15. representation learning\n\nh2\n\nh3\n\nh = [1,\n\n,0 0]>\n\nh = [1,\n\n,1 0]>\n\nh = [1,\n\n,0 1]>\n\nh = [1,\n\n,1 1]>\n\nh = [0,\n\n,1 0]>\n\nh = [0,\n\n,1 1]>\n\nh = [0,\n\n,0 1]>\n\nh1\n\n2 \u2229 h+\n. in the general case of\n\nfigure 15.7: illustration of how a learning algorithm based on a distributed representation\nbreaks up the input space into regions. in this example, there are three binary features\nh1, h2 , and h3 .\u00a0each feature is defined by thresholding the output of a learned, linear\ntransformation. each feature divides r2 into two half-planes. let h+\ni be the set of input\npoints for which hi = 1 and h\u2212i be the set of input points for which hi = 0. in this\nillustration, each line represents the decision boundary for one hi, with the corresponding\narrow pointing to the h+\ni side of the boundary. the representation as a whole takes\non a unique value at each possible intersection of these half-planes. for example, the\nrepresentation value [1,1, 1]> corresponds to the region h+\n3 . compare this to\nthe non-distributed representations in fig.\nd input dimensions,\na distributed representation divides r d by intersecting half-spaces rather than half-planes.\nthe distributed representation with n features assigns unique codes to o(nd ) different\nregions, while the nearest neighbor algorithm with n examples assigns unique codes to only\nn regions. the distributed representation is thus able to distinguish exponentially many\nmore regions than the non-distributed one. keep in mind that not all h values are feasible\n(there is no h = 0 in this example) and that a linear classifier on top of the distributed\nrepresentation is not able to assign different class identities to every neighboring region;\neven a deep linear-threshold network has a vc dimension of only o(w\nwlog ) where w\n). the combination of a powerful representation\nis the number of weights (\nlayer and a weak classifier layer can be a strong regularizer; a classifier trying to learn\nthe concept of \u201cperson\u201d versus \u201cnot a person\u201d does not need to assign a different class to\nan input represented as \u201cwoman with glasses\u201d than it assigns to an input represented as\n\u201cman without glasses.\u201d this capacity constraint encourages each classifier to focus on few\nhi and encourages\n\nto learn to represent the classes in a linearly separable way.\n\nsontag 1998\n\n1 \u2229 h+\n\n15.8\n\nh\n\n,\n\n550\n\n "}, {"Page_number": 566, "text": "chapter 15. representation learning\n\nvalues describing each input, but they can not be controlled separately from\neach other, so this does not qualify as a true distributed representation.\n\n\u2022 decision trees: only one leaf (and the nodes on the path from root to leaf) is\n\nactivated when an input is given.\n\n\u2022 gaussian mixtures and mixtures of experts: the templates (cluster centers)\nor experts are now associated with a degree of activation. as with the\nk-nearest neighbors algorithm, each input is represented with multiple values,\nbut those values cannot readily be controlled separately from each other.\n\n\u2022 kernel machines with a gaussian kernel (or other similarly local kernel):\nalthough the degree of activation of each \u201csupport vector\u201d or template example\nis now continuous-valued, the same issue arises as with gaussian mixtures.\n\n\u2022 language or translation models based on n-grams. the set of contexts\n(sequences of symbols) is partitioned according to a tree structure of suffixes.\na leaf may correspond to the last two words being w 1 and w2, for example.\nseparate parameters are estimated for each leaf of the tree (with some sharing\nbeing possible).\n\nfor some of these non-distributed algorithms, the output is not constant by\nparts but instead interpolates between neighboring regions. the relationship\nbetween the number of parameters (or examples) and the number of regions they\ncan define remains linear.\n\nan important related concept that distinguishes a distributed representation\nfrom a symbolic one is that generalization arises due to shared attributes\nbetween different concepts. as pure symbols, \u201ccat\u201d and \u201c dog\u201d are as far from each\nother as any other two symbols. however, if one associates them with a meaningful\ndistributed representation, then many of the things that can be said about cats\ncan generalize to dogs and vice-versa. for example, our distributed representation\nmay contain entries such as \u201c has_fur\u201d or \u201c number_of_legs\u201d that have the same\nvalue for the embedding of both \u201c cat\u201d and \u201c dog.\u201d neural language models that\noperate on distributed representations of words generalize much better than other\nmodels that operate directly on one-hot representations of words, as discussed\nin sec.\nsimilarity space, in which\nsemantically close concepts (or inputs) are close in distance, a property that is\nabsent from purely symbolic representations.\n\n. distributed representations induce a rich\n\n12.4\n\nwhen and why can there be a statistical advantage from using a distributed\nrepresentation as part of a learning algorithm?\u00a0distributed representations can\n\n551\n\n "}, {"Page_number": 567, "text": "chapter 15. representation learning\n\nfigure 15.8: illustration of how the nearest neighbor algorithm breaks up the input space\ninto different regions. the nearest neighbor algorithm provides an example of a learning\nalgorithm based on a non-distributed representation. different non-distributed algorithms\nmay have different geometry, but they typically break the input space into regions, with\na separate set of parameters for each region. the advantage of a non-distributed\napproach is that, given enough parameters, it can fit the training set without solving a\ndifficult optimization algorithm, because it is straightforward to choose a different output\nindependently for each region. the disadvantage is that such non-distributed models\ngeneralize only locally via the smoothness prior, making it difficult to learn a complicated\nfunction with more peaks and troughs than the available number of examples. contrast\nthis with a distributed representation, fig.\n\n15.7\n.\n\n552\n\n "}, {"Page_number": 568, "text": "chapter 15. representation learning\n\nhave a statistical advantage when an apparently complicated structure can be\ncompactly represented using a small number of parameters. some traditional non-\ndistributed learning algorithms generalize only due to the smoothness assumption,\nv\u2248 , then the target function f to be learned has the\nwhich states that if u\nproperty that f(u) \u2248 f(v), in general. there are many ways of formalizing such an\nassumption, but the end result is that if we have an example (x, y ) for which we\nknow that f (x) \u2248 y, then we choose an estimator \u02c6f that approximately satisfies\nthese constraints while changing as little as possible when we move to a nearby\ninput x + (cid:115). this assumption is clearly very useful, but it suffers from the curse of\ndimensionality:\u00a0in order to learn a target function that increases and decreases\nmany times in many different regions,1 we may need a number of examples that is\nat least as large as the number of distinguishable regions. one can think of each of\nthese regions as a category or symbol: by having a separate degree of freedom for\neach symbol (or region), we can learn an arbitrary decoder mapping from symbol\nto value.\u00a0however, this does not allow us to generalize to new symbols for new\nregions.\n\nif we are lucky, there may be some regularity in the target function, besides being\nsmooth. for example, a convolutional network with max-pooling can recognize an\nobject regardless of its location in the image, even though spatial translation of\nthe object may not correspond to smooth transformations in the input space.\n\nlet us examine a special case of a distributed representation learning algorithm,\nthat extracts binary features by thresholding linear functions of the input. each\nbinary feature in this representation divides rd into a pair of half-spaces,\u00a0as\nillustrated in fig.\nn\nof the corresponding half-spaces determines how many regions this distributed\nrepresentation learner can distinguish. how many regions are generated by an\narrangement of n hyperplanes in rd ? by applying a general result concerning the\nintersection of hyperplanes (\n2014b\n)\nthat the number of regions this binary feature representation can distinguish is\n\n. the exponentially large number of intersections of\n\n), one can show (\n\nzaslavsky 1975\n\npascanu\n\net al.,\n\n15.7\n\n,\n\nj(cid:19) = \ndxj=0(cid:18)n\n\n(o n d).\n\n(15.4)\n\ntherefore, we see a growth that is exponential in the input size and polynomial in\nthe number of hidden units.\n\n1 potentially, we may want to learn a function whose behavior is distinct in exponentially many\nregions: in a d-dimensional space with at least 2 different values to distinguish per dimension, we\nmight want\n\n2d different regions, requiring o(2 d) training examples.\n\nto differ in\n\nf\n\n553\n\n "}, {"Page_number": 569, "text": "chapter 15. representation learning\n\nthis provides a geometric argument to explain the generalization power of\ndistributed representation: with o (nd) parameters (for n linear-threshold features\nin rd ) we can distinctly represent o (nd) regions in input space. if instead we made\nno assumption at all about the data, and used a representation with one unique\nsymbol for each region, and separate parameters for each symbol to recognize its\ncorresponding portion of rd, then specifying o(nd) regions would require o(nd )\nexamples. more generally, the argument in favor of the distributed representation\ncould be extended to the case where instead of using linear threshold units we\nuse nonlinear, possibly continuous, feature extractors for each of the attributes in\nthe distributed representation. the argument in this case is that if a parametric\ntransformation with k parameters can learn about r regions in input space, with\nr(cid:129) , and if obtaining such a representation was useful to the task of interest, then\nk\nwe could potentially generalize much better in this way than in a non-distributed\nsetting where we would need o(r) examples to obtain the same features and\nassociated partitioning of the input space into r regions. using fewer parameters to\nrepresent the model means that we have fewer parameters to fit, and thus require\nfar fewer training examples to generalize well.\n\n,\n\nwlog\n\na further part of the argument for why models based on distributed represen-\ntations generalize well is that their capacity remains limited despite being able to\ndistinctly encode so many different regions. for example, the vc dimension of a\nneural network of linear threshold units is only o(w\n), where w is the number\n). this limitation arises because, while we can assign very\nof weights (sontag 1998\nmany unique codes to representation space, we cannot use absolutely all of the code\nspace, nor can we learn arbitrary functions mapping from the representation space\nh to the output y using a linear classifier. the use of a distributed representation\ncombined with a linear classifier thus expresses a prior belief that the classes to\nbe recognized are linearly separable as a function of the underlying causal factors\ncaptured by h .\u00a0we will typically want to learn categories such as the set of all\nimages of all green objects or the set of all images of cars, but not categories that\nrequire nonlinear, xor logic. for example, we typically do not want to partition\nthe data into the set of all red cars and green trucks as one class and the set of all\ngreen cars and red trucks as another class.\n\n(\n\nzhou et al. 2015\n\nthe ideas discussed so far have been abstract, but they may be experimentally\nvalidated.\n)\u00a0find that hidden units in a deep\u00a0convolutional\nnetwork trained on the imagenet and places benchmark datasets learn features\nthat are very often interpretable, corresponding to a label that humans would\nnaturally assign. in practice it is certainly not always the case that hidden units\nlearn something that has a simple linguistic name, but it is interesting to see this\nemerge near the top levels of the best computer vision deep networks. what such\n\n554\n\n "}, {"Page_number": 570, "text": "chapter 15. representation learning\n\n-\n\n+\n\n=\n\nfigure 15.9: a generative model has learned a distributed representation that disentangles\nthe concept of gender from the concept of wearing glasses.\u00a0if we begin with the repre-\nsentation of the concept of a man with glasses, then subtract the vector representing the\nconcept of a man without glasses, and finally add the vector representing the concept\nof a woman without glasses, we obtain the vector representing the concept of a woman\nwith glasses. the generative model correctly decodes all of these representation vectors to\nimages that may be recognized as belonging to the correct class. images reproduced with\npermission from\n\nradford et al. 2015\n\n).\n\n(\n\n15.9\n\n2015\n\nfeatures have in common is that one could imagine learning about each of them\nwithout having to see all the configurations of all the others. radford\net al. (\n) demonstrated that a generative model can learn a representation of\nimages of faces, with separate directions in representation space capturing different\nunderlying factors of variation. fig.\ndemonstrates that one direction in\nrepresentation space corresponds to whether the person is male or female, while\nanother corresponds to whether the person is wearing glasses. these features were\ndiscovered automatically, not fixed a priori.\u00a0there is no need to have labels for\nthe hidden unit classifiers: gradient descent on an objective function of interest\nnaturally learns semantically interesting features, so long as the task requires\nsuch features.\u00a0we can learn about the distinction between male and female, or\nabout the presence or absence of glasses, without having to characterize all of\nthe configurations of the n \u2212 1 other features by examples covering all of these\ncombinations of values. this form of statistical separability is what allows one to\ngeneralize to new configurations of a person\u2019s features that have never been seen\nduring training.\n\n555\n\n "}, {"Page_number": 571, "text": "chapter 15. representation learning\n\n15.5 exponential gains from depth\n\n6.4.1\n\nwe have seen in sec.\nthat multilayer perceptrons are universal approximators,\nand that some functions can be represented by exponentially smaller deep networks\ncompared to shallow networks. this decrease in model size leads to improved\nstatistical efficiency. in this section, we describe how similar results apply more\ngenerally to other kinds of models with distributed hidden representations.\n\n15.4\n\nin sec.\n\n, we saw an example of a generative model that learned about the\nexplanatory factors underlying images of faces, including the person\u2019s gender and\nwhether they are wearing glasses. the generative model that accomplished this\ntask was based on a deep neural network. it would not be reasonable to expect a\nshallow network, such as a linear network, to learn the complicated relationship\nbetween these abstract explanatory factors and the pixels in the image. in this and\nother ai tasks, the factors that can be chosen almost independently in order to\ngenerate data are more likely to be very high-level and related in highly nonlinear\nways to the input. we argue that this demands deep distributed representations,\nwhere the higher level features (seen as functions of the input) or factors (seen as\ngenerative causes) are obtained through the composition of many nonlinearities.\n\nit has been proven in many different settings that organizing computation\nthrough the composition of many nonlinearities and a hierarchy of reused features\ncan give an exponential boost to statistical efficiency, on top of the exponential\nboost given by using a distributed representation. many kinds of networks (e.g.,\nwith saturating nonlinearities, boolean gates, sum/products, or rbf units) with\na single hidden layer can be shown to be universal approximators. a model\nfamily that is a universal approximator can approximate a large class of functions\n(including all continuous functions) up to any non-zero tolerance level, given enough\nhidden units.\u00a0however, the required number of hidden units may be very large.\ntheoretical results concerning the expressive power of deep architectures state that\nthere are families of functions that can be represented efficiently by an architecture\nof depth k, but would require an exponential number of hidden units (with respect\nto the input size) with insufficient depth (depth 2 or depth\n\n6.4.1\n\nin sec.\n\n, we saw that deterministic feedforward networks are universal\napproximators of functions. many structured probabilistic models with a single\nhidden layer of latent variables, including restricted boltzmann machines and deep\nbelief networks, are universal approximators of probability distributions (le roux\nand bengio 2008 2010 mont\u00fafar and ay 2011 mont\u00fafar 2014 krause\net al.,\n2013).\n\n,\n\n,\n\n;\n\n,\n\n;\n\n,\n\n;\n\nk \u2212 1\n).\n\nin sec.\n\n6.4.1\n\n, we saw that a sufficiently deep feedforward network can have an\n\n556\n\n "}, {"Page_number": 572, "text": "chapter 15. representation learning\n\nor\n\nspn\n\nexponential advantage over a network that is too shallow. such results can also\nbe obtained for other models such as probabilistic models. one such probabilistic\nmodel is the sum-product network\n). these\nmodels use polynomial circuits to compute the probability distribution over a\nset of random variables.\n) showed that there exist\nprobability distributions for which a minimum depth of spn is required to avoid\nneeding an exponentially large model. later,\nmartens and medabalimi 2014\n)\nshowed that there are significant differences between every two finite depths of\nspn, and that some of the constraints used to make spns tractable may limit\ntheir representational power.\n\ndelalleau and bengio 2011\n\n(poon and domingos 2011\n\n(\n\n(\n\n,\n\nanother interesting development is a set of theoretical results for the expressive\npower of families of deep circuits related to convolutional nets, highlighting an\nexponential advantage for the deep circuit even when the shallow circuit is allowed\nto only approximate the function computed by the deep circuit (\ncohen et al.\n,\n2015). by comparison, previous theoretical work made claims regarding only the\ncase where the shallow circuit must exactly replicate particular functions.\n\n15.6 providing clues to discover underlying causes\n\nto close this chapter, we come back to one of our original questions: what makes\none representation better than another? one answer, first introduced in sec.\n15.3\n,\nis that an ideal representation is one that disentangles the underlying causal factors\nof variation that generated the data, especially those factors that are relevant to our\napplications. most strategies for representation learning are based on introducing\nclues that help the learning to find these underlying factors of variations. the clues\ncan help the learner separate these observed factors from the others. supervised\nlearning provides a very strong clue: a label y, presented with each x, that usually\nspecifies the value of at least one of the factors of variation directly. more generally,\nto make use of abundant unlabeled data, representation learning makes use of\nother, less direct, hints about the underlying factors. these hints take the form of\nimplicit prior beliefs that we, the designers of the learning algorithm, impose in\norder to guide the learner. results such as the no free lunch theorem show that\nregularization strategies are necessary to obtain good generalization. while it is\nimpossible to find a universally superior regularization strategy, one goal of deep\nlearning is to find a set of fairly generic regularization strategies that are applicable\nto a wide variety of ai tasks, similar to the tasks that people and animals are able\nto solve.\n\nwe provide here a list of these generic regularization strategies. the list is\n\n557\n\n "}, {"Page_number": 573, "text": "chapter 15. representation learning\n\nclearly not exhaustive, but gives some concrete examples of ways that learning\nalgorithms can be encouraged to discover features that correspond to underlying\nfactors. this list was introduced in sec. 3.1 of\n) and has been\npartially expanded here.\n\nbengio et al. 2013d\n\n(\n\n\u2022 smoothness: this is the assumption that f(x + (cid:115)d) \u2248 f(x) for unit d\n. this assumption allows the learner to generalize from training\nand small\n(cid:115)\nexamples to nearby points in input space. many machine learning algorithms\nleverage this idea, but it is insufficient to overcome the curse of dimensionality.\n\n\u2022 linearity: many learning algorithms assume that relationships between\nsome variables are linear. this allows the algorithm to make predictions even\nvery far from the observed data, but can sometimes lead to overly extreme\npredictions. most simple machine learning algorithms that do not make the\nsmoothness assumption instead make the linearity assumption. these are\nin fact different assumptions\u2014linear functions with large weights applied\nto high-dimensional spaces may not be very smooth. see goodfellow et al.\n(\n) for a further discussion of the limitations of the linearity assumption.\n2014b\n\n\u2022 multiple explanatory factors: many representation learning algorithms\nare motivated by the assumption that the data is generated by multiple\nunderlying explanatory factors, and that most tasks can be solved easily\ngiven the state of each of these factors.\u00a0sec.\u00a0\ndescribes how this view\nmotivates semi-supervised learning via representation learning. learning\nthe structure of p(x) requires learning some of the same features that are\nuseful for modeling p(y x|\n) because both refer to the same underlying\n15.4\nexplanatory factors. sec.\ndescribes how this view motivates the use of\ndistributed representations, with separate directions in representation space\ncorresponding to separate factors of variation.\n\n15.3\n\n\u2022 causal factors: the model is constructed in such a way that it treats the\nfactors of variation described by the learned representation h as the causes\n, this\nof the observed data x, and not vice-versa. as discussed in sec.\nis advantageous for semi-supervised learning and makes the learned model\nmore robust when the distribution over the underlying causes changes or\nwhen we use the model for a new task.\n\n15.3\n\n\u2022 depth, or a hierarchical organization of explanatory factors:\u00a0high-\nlevel, abstract concepts can be defined in terms of simple concepts, forming a\nhierarchy. from another point of view, the use of a deep architecture expresses\nour belief that the task should be accomplished via a multi-step program,\n\n558\n\n "}, {"Page_number": 574, "text": "chapter 15. representation learning\n\nwith each step referring back to the output of the processing accomplished\nvia previous steps.\n\n\u2022 shared factors across tasks: in the context where we have many tasks,\ncorresponding to different y i variables sharing the same input x or where\neach task is associated with a subset or a function f ( )i (x) of a global input\nx, the assumption is that each y i is associated with a different subset from a\ncommon pool of relevant factors h. because these subsets overlap, learning\nall the p (yi | x) via a shared intermediate representation p (h  x|\n) allows\nsharing of statistical strength between the tasks.\n\n\u2022 manifolds: probability mass concentrates, and the regions in which it con-\ncentrates are locally connected and occupy a tiny volume. in the continuous\ncase, these regions can be approximated by low-dimensional manifolds with\na much smaller dimensionality than the original space where the data lives.\nmany machine learning algorithms behave sensibly only on this manifold\n(\ngoodfellow et al. 2014b\n). some machine learning algorithms, especially\nautoencoders, attempt to explicitly learn the structure of the manifold.\n\n,\n\n\u2022 natural clustering: many machine learning algorithms assume that each\nconnected manifold in the input space may be assigned to a single class. the\ndata may lie on many disconnected manifolds, but the class remains constant\nwithin each one of these.\u00a0this assumption motivates a variety of learning\nalgorithms, including tangent propagation, double backprop, the manifold\ntangent classifier and adversarial training.\n\n\u2022 temporal\u00a0and\u00a0spatial\u00a0coherence: slow feature\u00a0analysis and\u00a0related\nalgorithms make the assumption that the most important explanatory factors\nchange slowly over time, or at least that it is easier to predict the true\nunderlying explanatory factors than to predict raw observations such as pixel\nvalues. see sec.\n\nfor further description of this approach.\n\n13.3\n\n\u2022 sparsity:\u00a0most features should presumably not be relevant to describing\nmost inputs\u2014there is no need to use a feature that detects elephant trunks\nwhen representing an image of a cat. it is therefore reasonable to impose a\nprior that any feature that can be interpreted as \u201cpresent\u201d or \u201cabsent\u201d should\nbe absent most of the time.\n\n\u2022 simplicity of factor dependencies: in good high-level representations,\nthe factors are related to each other through simple dependencies. the\n\nsimplest possible is marginal independence, p(h) =q ip (hi), but linear\n\n559\n\n "}, {"Page_number": 575, "text": "chapter 15. representation learning\n\ndependencies or those captured by a shallow autoencoder are also reasonable\nassumptions. this can be seen in many laws of physics, and is assumed\nwhen plugging a linear predictor or a factorized prior on top of a learned\nrepresentation.\n\nthe concept of representation learning ties together all of the many forms\nof deep learning. feedforward and recurrent networks, autoencoders and deep\nprobabilistic models all learn and exploit representations. learning\u00a0the best\npossible representation remains an exciting avenue of research.\n\n560\n\n "}, {"Page_number": 576, "text": "chapter 16\n\nstructured probabilistic models\nfor deep learning\n\ndeep learning draws upon many modeling formalisms that researchers can use to\nguide their design efforts and describe their algorithms. one of these formalisms is\nthe idea of structured probabilistic models. we have already discussed structured\nprobabilistic models briefly in sec.\n. that brief presentation was sufficient to\nunderstand how to use structured probabilistic models as a language to describe\nsome of the algorithms in part\n, structured probabilistic models\nare a key ingredient of many of the most important research topics in deep learning.\nin order to prepare to discuss these research ideas, this chapter describes structured\nprobabilistic models in much greater detail.\u00a0this chapter is intended to be self-\ncontained;\u00a0the reader does not need to review the earlier introduction before\ncontinuing with this chapter.\n\n. now, in part\n\n3.14\n\niii\n\nii\n\na structured probabilistic model is a way of describing a probability distribution,\nusing a graph to describe which random variables in the probability distribution\ninteract with each other directly. here we use \u201cgraph\u201d in the graph theory sense\u2014a\nset of vertices connected to one another by a set of edges. because the structure of\nthe model is defined by a graph, these models are often also referred to as graphical\nmodels.\n\nthe graphical models research community is large and has developed many\ndifferent models, training algorithms, and inference algorithms. in this chapter, we\nprovide basic background on some of the most central ideas of graphical models,\nwith an emphasis on the concepts that have proven most useful to the deep learning\nresearch community. if you already have a strong background in graphical models,\nyou may wish to skip most of this chapter. however, even a graphical model expert\n\n561\n\n "}, {"Page_number": 577, "text": "chapter 16. structured probabilistic models for deep learning\n\nmay benefit from reading the final section of this chapter, sec.\n, in which we\nhighlight some of the unique ways that graphical models are used for deep learning\nalgorithms. deep learning practitioners tend to use very different model structures,\nlearning algorithms and inference procedures than are commonly used by the rest\nof the graphical models research community. in this chapter, we identify these\ndifferences in preferences and explain the reasons for them.\n\n16.7\n\nin this chapter we first describe the challenges of building large-scale proba-\nbilistic models.\u00a0next, we describe how to use a graph to describe the structure\nof a probability distribution. while this approach allows us to overcome many\nchallenges, it is not without its own complications. one of the major difficulties in\ngraphical modeling is understanding which variables need to be able to interact\ndirectly, i.e., which graph structures are most suitable for a given problem.\u00a0we\noutline two approaches to resolving this difficulty by learning about the dependen-\ncies in sec.\n. finally, we close with a discussion of the unique emphasis that\ndeep learning practitioners place on specific approaches to graphical modeling in\nsec.\n\n16.7\n.\n\n16.5\n\n16.1 the challenge of unstructured modeling\n\nthe goal of deep learning is to scale machine learning to the kinds of challenges\nneeded to solve artificial intelligence. this means being able to understand high-\ndimensional data with rich structure. for example, we would like ai algorithms to\nbe able to understand natural images,1 audio waveforms representing speech, and\ndocuments containing multiple words and punctuation characters.\n\nclassification algorithms can take an input from such a rich high-dimensional\ndistribution and summarize it with a categorical label\u2014what object is in a photo,\nwhat word is spoken in a recording, what topic a document is about. the process\nof classification discards most of the information in the input and produces a\nsingle output (or a probability distribution over values of that single output). the\nclassifier is also often able to ignore many parts of the input. for example, when\nrecognizing an object in a photo, it is usually possible to ignore the background of\nthe photo.\n\nit is possible to ask probabilistic models to do many other tasks. these tasks are\noften more expensive than classification. some of them require producing multiple\noutput values. most require a complete understanding of the entire structure of\n\n1 a natural image is an image that might be captured by a camera in a reasonably ordinary\n\nenvironment, as opposed to a synthetically rendered image, a screenshot of a web page, etc.\n\n562\n\n "}, {"Page_number": 578, "text": "chapter 16. structured probabilistic models for deep learning\n\nthe input, with no option to ignore sections of it. these tasks include the following:\n\n\u2022 density estimation: given an input x, the machine learning system returns an\nestimate of the true density p(x) under the data generating distribution. this\nrequires only a single output, but it does require a complete understanding\nof the entire input. if even one element of the vector is unusual, the system\nmust assign it a low probability.\n\n\u2022 denoising: given a damaged or incorrectly observed input \u02dcx, the machine\nlearning system returns an estimate of the original or correct x. for example,\nthe machine learning system might be asked to remove dust or scratches\nfrom an old photograph. this requires multiple outputs (every element of the\nestimated clean example x) and an understanding of the entire input (since\neven one damaged area will still reveal the final estimate as being damaged).\n\n\u2022 missing value imputation: given the observations of some elements of x,\nthe model is asked to return estimates of or a probability distribution over\nsome or all of the unobserved elements of x. this requires multiple outputs.\nbecause the model could be asked to restore any of the elements of x, it\nmust understand the entire input.\n\n\u2022 sampling: the model generates new samples from the distribution p(x).\napplications include speech synthesis, i.e. producing new waveforms that\nsound like natural human speech. this requires multiple output values and a\ngood model of the entire input. if the samples have even one element drawn\nfrom the wrong distribution, then the sampling process is wrong.\n\nfor an example of a sampling task using small natural images, see fig.\n\n16.1\n.\n\nmodeling a rich distribution over thousands or millions of random variables is a\nchallenging task, both computationally and statistically. suppose we only wanted\nto model binary variables. this is the simplest possible case, and yet already it\npixel color (rgb) image, there are 3072\nseems overwhelming. for a small, 32 \u00d7 32\npossible binary images of this form. this number is over 10800 times larger than\nthe estimated number of atoms in the universe.\n\n2\n\nin general, if we wish to model a distribution over a random vector x containing\nn discrete variables capable of taking on k values each, then the naive approach of\nrepresenting p (x) by storing a lookup table with one probability value per possible\noutcome requires kn parameters!\n\nthis is not feasible for several reasons:\n\n563\n\n "}, {"Page_number": 579, "text": "chapter 16. structured probabilistic models for deep learning\n\nfigure 16.1: probabilistic modeling of natural images. (top) example 32 \u00d7 32 pixel color\nimages from the cifar-10 dataset (\nsamples\ndrawn from a structured probabilistic model trained on this dataset. each sample appears\nat the same position in the grid as the training example that is closest to it in euclidean\nspace. this comparison allows us to see that the model is truly synthesizing new images,\nrather than memorizing the training data. contrast of both sets of images has been\nadjusted for display. figure reproduced with permission from\n\nkrizhevsky and hinton 2009\n\ncourville et al. 2011\n\n(bottom)\n\n).\n\n).\n\n(\n\n,\n\n564\n\n "}, {"Page_number": 580, "text": "chapter 16. structured probabilistic models for deep learning\n\n\u2022 memory: the cost of storing the representation: for all but very\nsmall values of n and k, representing the distribution as a table will require\ntoo many values to store.\n\n\u2022 statistical efficiency: as the number of parameters in a model increases,\nso does the amount of training data needed to choose the values of those\nparameters using a statistical estimator. because the table-based model\nhas an astronomical number of parameters, it will require an astronomically\nlarge training set to fit accurately. any such model will overfit the training\nset very badly unless additional assumptions are made linking the different\nentries in the table (for example, like in back-off or smoothed n -gram models,\nsec.\n\n12.4.1\n\n).\n\n\u2022 runtime: the cost of inference: suppose we want to perform an inference\ntask where we use our model of the joint distribution p (x) to compute some\nother distribution, such as the marginal distribution p (x 1) or the conditional\ndistribution p (x2 | x1). computing these distributions will require summing\nacross the entire table, so the runtime of these operations is as high as the\nintractable memory cost of storing the model.\n\n\u2022 runtime: the cost of sampling:\u00a0likewise, suppose we want to draw a\nsample from the model. the naive way to do this is to sample some value\nu \u223c u (0, 1), then iterate through the table adding up the probability values\nuntil they exceed u and return the outcome whose probability value was\nadded last. this requires reading through the whole table in the worst case,\nso it has the same exponential cost as the other operations.\n\nthe problem with the table-based approach is that we are explicitly modeling\nevery possible kind of interaction between every possible subset of variables. the\nprobability distributions we encounter in real tasks are much simpler than this.\nusually, most variables influence each other only indirectly.\n\nfor example, consider modeling the finishing times of a team in a relay race.\nsuppose the team consists of three runners: alice, bob and carol. at the start of\nthe race, alice carries a baton and begins running around a track. after completing\nher lap around the track, she hands the baton to bob. bob then runs his own\nlap and hands the baton to carol, who runs the final lap. we can model each of\ntheir finishing times as a continuous random variable. alice\u2019s finishing time does\nnot depend on anyone else\u2019s, since she goes first. bob\u2019s finishing time depends\non alice\u2019s, because bob does not have the opportunity to start his lap until alice\nhas completed hers.\u00a0if alice finishes faster, bob will finish faster, all else being\n\n565\n\n "}, {"Page_number": 581, "text": "chapter 16. structured probabilistic models for deep learning\n\nequal. finally, carol\u2019s finishing time depends on both her teammates. if alice is\nslow, bob will probably finish late too. as a consequence, carol will have quite a\nlate starting time and thus is likely to have a late finishing time as well. however,\ncarol\u2019s finishing time depends only indirectly on alice\u2019s finishing time via bob\u2019s.\nif we already know bob\u2019s finishing time, we will not be able to estimate carol\u2019s\nfinishing time better by finding out what alice\u2019s finishing time was. this means\nwe can model the relay race using only two interactions: alice\u2019s effect on bob and\nbob\u2019s effect on carol. we can omit the third, indirect interaction between alice\nand carol from our model.\n\nstructured probabilistic models provide a formal framework for modeling only\ndirect interactions between random variables. this allows the models to have\nsignificantly fewer parameters which can in turn be estimated reliably from less\ndata.\u00a0these smaller models also have dramatically reduced computational cost\nin terms of storing the model, performing inference in the model, and drawing\nsamples from the model.\n\n16.2 using graphs to describe model structure\n\nstructured probabilistic models use graphs (in the graph theory sense of \u201cnodes\u201d or\n\u201cvertices\u201d connected by edges) to represent interactions between random variables.\neach node represents a random variable. each edge represents a direct interaction.\nthese direct interactions imply other, indirect interactions, but only the direct\ninteractions need to be explicitly modeled.\n\nthere is\u00a0more than one way to\u00a0describe\u00a0the interactions\u00a0in a probability\ndistribution using a graph. in the following sections we describe some of the most\npopular and useful approaches. graphical models can be largely divided into\ntwo categories: models based on directed acyclic graphs, and models based on\nundirected graphs.\n\n16.2.1 directed models\n\none kind of structured probabilistic model is the directed graphical model, otherwise\nknown as the belief network\n\nbayesian network\n\n2 (pearl 1985\n\nor\n\n).\n\n,\n\ndirected graphical models are called \u201cdirected\u201d because their edges are directed,\n\n2 judea pearl suggested using the term \u201cbayesian network\u201d when one wishes to \u201cemphasize\nthe judgmental\u201d nature of the values computed by the network, i.e. to highlight that they usually\nrepresent degrees of belief rather than frequencies of events.\n\n566\n\n "}, {"Page_number": 582, "text": "chapter 16. structured probabilistic models for deep learning\n\nalice\n\nbob\n\ncarol\n\nt0t0\n\nt1t1\n\nt2t2\n\nfigure 16.2: a directed graphical model depicting the relay race example. alice\u2019s finishing\ntime t0 influences bob\u2019s finishing time t 1, because bob does not get to start running until\nalice finishes. likewise, carol only gets to start running after bob finishes, so bob\u2019s\nfinishing time t1 directly influences carol\u2019s finishing time t2.\n\nthat is, they point from one vertex to another. this direction is represented in\nthe drawing with an arrow. the direction of the arrow indicates which variable\u2019s\nprobability distribution is defined in terms of the other\u2019s. drawing an arrow from\na to b means that we define the probability distribution over b via a conditional\ndistribution, with a as one of the variables on the right side of the conditioning\nbar. in other words, the distribution over b depends on the value of a.\n\ncontinuing with the relay race example from sec.\n\n, suppose we name alice\u2019s\nfinishing time t0 , bob\u2019s finishing time t1, and carol\u2019s finishing time t 2. as we saw\nearlier, our estimate of t1 depends on t0. our estimate of t2 depends directly on t1\nbut only indirectly on t0 . we can draw this relationship in a directed graphical\nmodel, illustrated in fig.\n\n.\n16.2\n\n16.1\n\nformally, a directed graphical model defined on variables x is defined by a\ndirected acyclic graph g whose vertices are the random variables in the model, and\na set of local conditional probability distributions p(xi | p ag (xi)) where p ag(xi)\ngives the parents of xi in . the probability distribution over\n\nis given by\n\nx \n\ng\n\np( ) = \u03c0\n\nx\n\ni p(xi | p ag(xi)).\n\n(16.1)\n\nin our relay race example, this means that, using the graph drawn in fig.\n\n16.2\n,\n\np(t0, t 1, t2) =  (p t0) (p t1 | t0 ) (p t2 | t1).\n\n(16.2)\n\nthis is our first time seeing a structured probabilistic model in action. we\ncan examine the cost of using it, in order to observe how structured modeling has\nmany advantages relative to unstructured modeling.\n\nsuppose we represented time by discretizing time ranging from minute 0 to\nminute 10 into 6 second chunks. this would make t0 , t1 and t2 each be discrete\nvariables with 100 possible values. if we attempted to represent p(t0, t1, t2) with a\ntable, it would need to store 999,999 values (100 values of t0 \u00d7 100 values of t1 \u00d7\n100 values of t2, minus 1, since the probability of one of the configurations is made\n\n567\n\n "}, {"Page_number": 583, "text": "chapter 16. structured probabilistic models for deep learning\n\nredundant by the constraint that the sum of the probabilities be 1). if instead, we\nonly make a table for each of the conditional probability distributions, then the\ndistribution over t0 requires 99 values, the table defining t1 given t0 requires 9900\nvalues, and so does the table defining t2 given t1. this comes to a total of 19,899\nvalues. this means that using the directed graphical model reduced our number of\nparameters by a factor of more than 50!\n\nin general, to model n discrete variables each having k values, the cost of the\nsingle table approach scales like o(k n), as we have observed before. now suppose\nwe build a directed graphical model over these variables. if m is the maximum\nnumber of variables appearing (on either side of the conditioning bar) in a single\nconditional probability distribution, then the cost of the tables for the directed\nmodel scales like o( km). as long as we can design a model such that m << n, we\nget very dramatic savings.\n\nin other words, so long as each variable has few parents in the graph, the\ndistribution can be represented with very few parameters.\u00a0some restrictions on\nthe graph structure, such as requiring it to be a tree, can also guarantee that\noperations like computing marginal or conditional distributions over subsets of\nvariables are efficient.\n\nit is important to realize what kinds of information can and cannot be encoded in\nthe graph. the graph encodes only simplifying assumptions about which variables\nare conditionally independent from each other. it is also possible to make other\nkinds of simplifying assumptions.\u00a0for example, suppose we assume bob always\nruns the same regardless of how alice performed. (in reality, alice\u2019s performance\nprobably influences bob\u2019s performance\u2014depending on bob\u2019s personality, if alice\nruns especially fast in a given race, this might encourage bob to push hard and\nmatch her exceptional performance, or it might make him overconfident and lazy).\nthen the only effect alice has on bob\u2019s finishing time is that we must add alice\u2019s\nfinishing time to the total amount of time we think bob needs to run. this\nobservation allows us to define a model with o(k ) parameters instead of o(k 2).\nhowever, note that t0 and t1 are still directly dependent with this assumption,\nbecause t1 represents the absolute time at which bob finishes, not the total time\nhe himself spends running. this means our graph must still contain an arrow from\nt0 to t1. the assumption that bob\u2019s personal running time is independent from\nall other factors cannot be encoded in a graph over t 0, t1 , and t2. instead, we\nencode this information in the definition of the conditional distribution itself. the\nk\u00d7 \u22121 element table indexed by t0 and t1\nconditional distribution is no longer a k\nbut is now a slightly more complicated formula using only k \u2212 1 parameters. the\ndirected graphical model syntax does not place any constraint on how we define\n\n568\n\n "}, {"Page_number": 584, "text": "chapter 16. structured probabilistic models for deep learning\n\nour conditional distributions. it only defines which variables they are allowed to\ntake in as arguments.\n\n16.2.2 undirected models\n\ndirected graphical models give us one language for describing structured proba-\nbilistic models. another popular language is that of undirected models, otherwise\nknown as markov random fields (mrfs) or\nmarkov networks kindermann 1980\n).\nas their name implies, undirected models use graphs whose edges are undirected.\n\n(\n\n,\n\ndirected models are most naturally applicable to situations where there is\na clear reason to draw each arrow in one particular direction. often these are\nsituations where we understand the causality and the causality only flows in one\ndirection. one such situation is the relay race example. earlier runners affect the\nfinishing times of later runners; later runners do not affect the finishing times of\nearlier runners.\n\nnot all situations we might want to model have such a clear direction to their\ninteractions. when the interactions seem to have no intrinsic direction, or to\noperate in both directions, it may be more appropriate to use an undirected model.\n\nas an example of such a situation, suppose we want to model a distribution\nover three binary variables: whether or not you are sick, whether or not your\ncoworker is sick, and whether or not your roommate is sick. as in the relay race\nexample, we can make simplifying assumptions about the kinds of interactions\nthat take place. assuming that your coworker and your roommate do not know\neach other, it is very unlikely that one of them will give the other a disease such as\na cold directly. this event can be seen as so rare that it is acceptable not to model\nit. however, it is reasonably likely that either of them could give you a cold, and\nthat you could pass it on to the other. we can model the indirect transmission of\na cold from your coworker to your roommate by modeling the transmission of the\ncold from your coworker to you and the transmission of the cold from you to your\nroommate.\n\nin this case, it is just as easy for you to cause your roommate to get sick as\nit is for your roommate to make you sick, so there is not a clean, uni-directional\nnarrative on which to base the model. this motivates using an undirected model.\nas with directed models, if two nodes in an undirected model are connected by an\nedge, then the random variables corresponding to those nodes interact with each\nother directly.\u00a0unlike directed models, the edge in an undirected model has no\narrow, and is not associated with a conditional probability distribution.\n\nwe denote the random variable representing your health as hy, the random\n\n569\n\n "}, {"Page_number": 585, "text": "chapter 16. structured probabilistic models for deep learning\n\nhrhr\n\nhyhy\n\nhchc\n\nfigure 16.3: an undirected graph representing how your roommate\u2019s health hr, your\nhealth hy , and your work colleague\u2019s health h c affect each other. you and your roommate\nmight infect each other with a cold, and you and your work colleague might do the same,\nbut assuming that your roommate and your colleague do not know each other, they can\nonly infect each other indirectly via you.\n\nvariable representing your roommate\u2019s health as hr, and the random variable\nrepresenting your colleague\u2019s health as hc. see fig.\nfor a drawing of the graph\nrepresenting this scenario.\n\n16.3\n\nformally, an undirected graphical model is a structured probabilistic model\ndefined on an undirected graph g. for each clique c in the graph,3 a factor \u03c6(c)\n(also called a clique potential) measures the affinity of the variables in that clique\nfor being in each of their possible joint states. the factors are constrained to be\nnon-negative. together they define an unnormalized probability distribution\n\n\u02dcp( ) = \u03c0\n\nx\n\nc\u2208g \u03c6\n\n( )c\n.\n\n(16.3)\n\nthe unnormalized probability distribution is efficient to work with so long as\nall the cliques are small. it encodes the idea that states with higher affinity are\nmore likely. however, unlike in a bayesian network, there is little structure to the\ndefinition of the cliques, so there is nothing to guarantee that multiplying them\ntogether will yield a valid probability distribution. see fig.\nfor an example of\nreading factorization information from an undirected graph.\n\n16.4\n\nour example of the cold spreading between you, your roommate, and your\ncolleague contains two cliques. one clique contains hy and hc. the factor for this\nclique can be defined by a table, and might have values resembling these:\n\nhy = 0 hy = 1\n\nhc = 0\nhc = 1\n\n2\n1\n\n1\n10\n\na state of 1 indicates good health, while a state of 0 indicates poor health\n(having been\u00a0infected with a\u00a0cold). both of\u00a0you\u00a0are usually healthy,\u00a0so\u00a0the\n\n3a clique of the graph is a subset of nodes that are all connected to each other by an edge of\n\nthe graph.\n\n570\n\n "}, {"Page_number": 586, "text": "chapter 16. structured probabilistic models for deep learning\n\ncorresponding state has the highest affinity.\u00a0the state where only one of you is\nsick has the lowest affinity, because this is a rare state. the state where both of\nyou are sick (because one of you has infected the other) is a higher affinity state,\nthough still not as common as the state where both are healthy.\n\nto complete the model, we would need to also define a similar factor for the\n\nclique containing hy and hr.\n\n16.2.3 the partition function\n\nwhile the unnormalized probability distribution is guaranteed to be non-negative\neverywhere, it is not guaranteed to sum or integrate to 1. to obtain a valid\nprobability distribution, we must use the corresponding normalized probability\ndistribution:4\n\np( ) =x\n\n\u02dcp( )x\n\n(16.4)\n\nwhere z is the value that results in the probability distribution summing or\nintegrating to 1:\n\n1\nz\n\nz =z \u02dcp\n\nd .\n( )x x\n\n(16.5)\n\nyou can think of z as a constant when the \u03c6 functions are held constant. note\nthat if the \u03c6 functions have parameters, then z is a function of those parameters.\nit is common in the literature to write z with its arguments omitted to save space.\nthe normalizing constant z is known as the partition function, a term borrowed\nfrom statistical physics.\n\nsince z is an integral or sum over all possible joint assignments of the state x\nit is often intractable to compute.\u00a0in order to be able to obtain the normalized\nprobability distribution of an undirected model,\u00a0the model structure and the\ndefinitions of the \u03c6 functions must be conducive to computing z efficiently. in\nthe context of deep learning, z is usually intractable.\u00a0due to the intractability\nof computing z exactly, we must resort to approximations. such approximate\nalgorithms are the topic of chapter\n\n.18\n\none important consideration to keep in mind when designing undirected models\nis that it is possible to specify the factors in such a way that z does not exist.\nthis happens if some of the variables in the model are continuous and the integral\nof \u02dcp over their domain diverges. for example, suppose we want to model a single\n\n4a distribution defined by normalizing a product of clique potentials is also called a gibbs\n\ndistribution.\n\n571\n\n "}, {"Page_number": 587, "text": "chapter 16. structured probabilistic models for deep learning\n\nscalar variable x\n\n\u2208 r\n\nwith a single clique potential\n\nz =z x2dx.\n\n\u03c6 x\n\n( ) =  2. in this case,\n\nx\n\n(16.6)\n\nsince this integral diverges, there is no probability distribution corresponding to\nthis choice of \u03c6(x).\u00a0sometimes the choice of some parameter of the \u03c6 functions\ndetermines whether\u00a0the probability\u00a0distribution is defined. for example,\u00a0for\n\n\u03c6(x; \u03b2) = exp(cid:18)\u2212\u03b2x2(cid:19), the \u03b2 parameter determines whether z exists. positive \u03b2\n\nresults in a gaussian distribution over x but all other values of \u03b2 make \u03c6 impossible\nto normalize.\n\none key difference between directed modeling and undirected modeling is that\ndirected models are defined directly in terms of probability distributions from\nthe start, while undirected models are defined more loosely by \u03c6 functions that\nare then converted into probability distributions. this changes the intuitions one\nmust develop in order to work with these models. one key idea to keep in mind\nwhile working with undirected models is that the domain of each of the variables\nhas dramatic effect on the kind of probability distribution that a given set of \u03c6\nfunctions corresponds to. for example, consider an n-dimensional vector-valued\nrandom variable x and an undirected model parametrized by a vector of biases\nb. suppose we have one clique for each element of x, \u03c6( )i (xi) = exp(bixi). what\nkind of probability distribution does this result in? the answer is that we do\nnot have enough information, because we have not yet specified the domain of x.\nif x \u2208 rn, then the integral defining z diverges and no probability distribution\nexists. if x \u2208 { 0, 1}n, then p (x ) factorizes into n independent distributions, with\np(x i = 1) = sigmoid (bi). if the domain of x is the set of elementary basis vectors\n({[1, 0, . . . , 0], [0, 1, . . . , 0], . . . , [0,0, . . . , 1]} ) then p(x) = softmax(b), so a large\nvalue of b i actually reduces p(x j = 1) for j 6= i.\u00a0often, it is possible to leverage\nthe effect of a carefully chosen domain of a variable in order to obtain complicated\nbehavior from a relatively simple set of \u03c6 functions. we will explore a practical\napplication of this idea later, in sec.\n\n20.6\n.\n\n16.2.4 energy-based models\n\nmany interesting theoretical results about undirected models depend on the as-\nsumption that \u2200x, \u02dcp(x) > 0. a convenient way to enforce this condition is to use\nan energy-based model (ebm) where\n\nand e(x) is known as the energy function. because exp(z) is positive for all z, this\nguarantees that no energy function will result in a probability of zero for any state x.\n\n\u02dcp\n\n( ) = exp(\nx\n\n\u2212 ( ))x\ne\n\n(16.7)\n\n572\n\n "}, {"Page_number": 588, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\nd\n\nb\n\ne\n\nc\n\nf\n\nfigure\n1\nz \u03c6a b, (a b,\ntions.\n\n16.4:\n)\u03c6 b c, (b c, )\u03c6a d, (a d,\n\ngraph implies\n\ncan be written as\n)\u03c6b e, (b e, )\u03c6 e f, (e f, ) for an appropriate choice of the \u03c6 func-\n\np(a b c d e f\n, )\n\nthis\n\nthat\n\n,\n\n,\n\n,\n\n,\n\nbeing completely free to choose the energy function makes learning simpler. if we\nlearned the clique potentials directly, we would need to use constrained optimization\nto arbitrarily impose some specific minimal probability value. by learning the\nenergy function, we can use unconstrained optimization.5 the probabilities in an\nenergy-based model can approach arbitrarily close to zero but never reach it.\n\n,\n\n;\n\n;\n\n16.7\n\net al.,\n\n1983 ackley\n\n1985 hinton\n\nis an example of a\n\nany distribution of the form given by eq.\n\nboltzmann\ndistribution. for this reason, many energy-based models are called boltzmann\net al.,\nmachines (fahlman\n1984 hinton\nand sejnowski 1986\n). there is no accepted guideline for when to call a model an\nenergy-based model and when to call it a boltzmann machine. the term boltzmann\nmachine was first introduced to describe a model with exclusively binary variables,\nbut today many models such as the mean-covariance restricted boltzmann machine\nincorporate real-valued variables as well. while boltzmann machines were originally\ndefined to encompass both models with and without latent variables, the term\nboltzmann machine is today most often used to designate models with latent\nvariables, while boltzmann machines without latent variables are more often called\nmarkov random fields or log-linear models.\n\net al.,\n\n;\n\ncliques in an undirected graph correspond to factors of the unnormalized\nprobability function. because exp(a) exp(b) = exp(a + b), this means that different\ncliques in the undirected graph correspond to the different terms of the energy\nfunction. in other words, an energy-based model is just a special kind of markov\nnetwork: the exponentiation makes each term in the energy function correspond\nto a factor for a different clique. see fig.\nfor an example of how to read the\nform of the energy function from an undirected graph structure. one can view an\nenergy-based model with multiple terms in its energy function as being a product\nof experts (\n). each term in the energy function corresponds to another\n\nhinton 1999\n\n16.5\n\n,\n\n5for some models, we may still need to use constrained optimization to make sure\n\nz\n\nexists.\n\n573\n\n "}, {"Page_number": 589, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\nd\n\nb\n\ne\n\nc\n\nf\n\nfigure\u00a016.5: this graph\u00a0implies that e(a b c d e f\neb c, (b c, ) + e a d, (a d,\nenergy functions. note that we can obtain the \u03c6 functions in fig.\nto the exponential of the corresponding negative energy, e.g., \u03c6a b, (a b, ) = exp (\n\n) +\n) + eb e, (b e, ) + ee f, (e f, ) for an appropriate choice of the per-clique\nby setting each\n\u03c6\n\u2212e a b,\n.\n))\n\n, ) can\u00a0be\u00a0written as ea b, (a b,\n\n16.4\n\n(\n\n,\n\n,\n\n,\n\n,\n\nfactor in the probability distribution. each term of the energy function can be\nthought of as an \u201cexpert\u201d that determines whether a particular soft constraint\nis satisfied. each expert may enforce only one constraint that concerns only\na low-dimensional projection of the random variables, but when combined by\nmultiplication of probabilities, the experts together enforce a complicated high-\ndimensional constraint.\n\n16.7\n\none part\u00a0of the\u00a0definition of an energy-based model\u00a0serves\u00a0no functional\npurpose from a machine learning point of view: the \u2212 sign in eq.\n. this\n\u2212 sign could be incorporated into the definition of e, or for many functions e\nthe learning algorithm could simply learn parameters with opposite sign. the \u2212\nsign is present primarily to preserve compatibility between the machine learning\nliterature and the physics literature. many advances in probabilistic modeling\nwere originally developed by statistical physicists, for whom e refers to actual,\nphysical energy and does not have arbitrary sign. terminology such as \u201cenergy\u201d\nand \u201cpartition function\u201d remains associated with these techniques, even though\ntheir mathematical applicability is broader than the physics context in which they\nwere developed. some machine learning researchers (e.g.,\n), who\nreferred to negative energy as\n) have chosen to emit the negation, but this\nis not the standard convention.\n\nsmolensky 1986\n\nharmony\n\n(\n\nmany algorithms that operate on probabilistic models do not need to compute\npmodel(x ) but only log \u02dcpmodel (x). for energy-based models with latent variables h,\nthese algorithms are sometimes phrased in terms of the negative of this quantity,\ncalled the free energy:\n\nf\n\n\u2212\n( ) = \nx\n\nlogxh\n\nexp (\n\n(\n\n\u2212e x h,\n))\n\n.\n\n(16.8)\n\nin this book, we usually prefer the more general log \u02dcpmodel( )x formulation.\n\n574\n\n "}, {"Page_number": 590, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\ns\n\nb\n\na\n\ns\n\nb\n\n(a)\n\n(b)\n\nfigure 16.6: (a) the path between random variable a and random variable b through s is\nactive, because s is not observed. this means that a and b are not separated. (b) here s\nis shaded in, to indicate that it is observed. because the only path between a and b is\nthrough s, and that path is inactive, we can conclude that a and b are separated given s.\n\n16.2.5 separation and d-separation\n\nthe edges in a graphical model tell us which variables directly interact. we often\nneed to know which variables indirectly interact. some of these indirect interactions\ncan be enabled or disabled by observing other variables. more formally, we would\nlike to know which subsets of variables are conditionally independent from each\nother, given the values of other subsets of variables.\n\nidentifying the conditional independences in a graph is very simple in the case\nof undirected models. in this case, conditional independence implied by the graph\nis called separation. we say that a set of variables a is separated from another set\nof variables b given a third set of variables s if the graph structure implies that a\nis independent from b given s. if two variables a and b are connected by a path\ninvolving only unobserved variables, then those variables are not separated. if no\npath exists between them, or all paths contain an observed variable, then they are\nseparated. we refer to paths involving only unobserved variables as \u201cactive\u201d and\npaths including an observed variable as \u201cinactive.\u201d\n\nwhen we draw a graph, we can indicate observed variables by shading them\nfor a depiction of how active and inactive paths in an undirected\nfor an example of reading\n\nin. see fig.\nmodel look when drawn in this way. see fig.\nseparation from an undirected graph.\n\n16.7\n\n16.6\n\nsimilar\u00a0concepts apply\u00a0to directed\u00a0models,\u00a0except\u00a0that\u00a0in the\u00a0context\u00a0of\ndirected models, these concepts are referred to as d-separation. the \u201cd\u201d stands for\n\u201cdependence.\u201d d-separation for directed graphs is defined the same as separation\nfor undirected graphs: we say that a set of variables a is d-separated from another\nset of variables b given a third set of variables s if the graph structure implies\nthat\n\nis independent from given .\ns\n\na\n\nb\n\nas with undirected models, we can examine the independences implied by the\ngraph by looking at what active paths exist in the graph. as before, two variables\nare dependent if there is an active path between them, and d-separated if no such\n\n575\n\n "}, {"Page_number": 591, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\nd\n\nb\n\nc\n\nfigure 16.7: an example of reading separation properties from an undirected graph. here\nb is shaded to indicate that it is observed. because observing b blocks the only path from\na to c , we say that a and c are separated from each other given b. the observation of b\nalso blocks one path between a and d , but there is a second, active path between them.\ntherefore, a and d are not separated given b.\n\npath exists. in directed nets, determining whether a path is active is somewhat\nmore complicated. see fig.\nfor a guide to identifying active paths in a directed\nmodel. see fig.\n\nfor an example of reading some properties from a graph.\n\n16.9\n\n16.8\n\nit is important to remember that separation and d-separation tell us only about\nthose conditional independences that are implied by the graph. there is no\nrequirement that the graph imply all independences that are present. in particular,\nit is always legitimate to use the complete graph (the graph with all possible edges)\nto represent any distribution. in fact, some distributions contain independences\nthat are not possible to represent with existing graphical notation. context-specific\nindependences are independences that are present dependent on the value of some\nvariables in the network. for example, consider a model of three binary variables:\na, b and c. suppose that when a is 0, b and c are independent, but when a is 1, b\nis deterministically equal to c . encoding the behavior when a = 1 requires an edge\nconnecting b and c. the graph then fails to indicate that b and c are independent\nwhen a\n\n.= 0\n\nin general, a graph will never imply that an independence exists when it does\n\nnot. however, a graph may fail to encode an independence.\n\n16.2.6 converting between undirected and directed graphs\n\nwe often refer to a specific machine learning model as being undirected or directed.\nfor example, we typically refer to rbms as undirected and sparse coding as directed.\nthis choice of wording can be somewhat misleading, because no probabilistic\nmodel is inherently directed or undirected. instead, some models are most easily\ndescribed using a directed graph, or most easily described using an undirected\ngraph.\n\n576\n\n "}, {"Page_number": 592, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\na\n\ns\n\ns\n\n(a)\n\nb\n\nb\n\na\n\nb\n\na\n\na\n\ns\n\n(c)\n\nb\n\nb\n\ns\n\n(b)\n\ns\n\nc\n\n(d)\n\n(a)\n\nany path with arrows proceeding directly from\n\nfigure 16.8: all of the kinds of active paths of length two that can exist between random\nvariables a and b.\na to b or vice versa.\nthis kind of path becomes blocked if s is observed.\u00a0we have already seen this kind of\npath in the relay race example. (b) a and b are connected by a common cause s. for\nexample, suppose s is a variable indicating whether or not there is a hurricane and a and\nb measure the wind speed at two different nearby weather monitoring outposts. if we\nobserve very high winds at station a, we might expect to also see high winds at b. this\nkind of path can be blocked by observing s. if we already know there is a hurricane, we\nexpect to see high winds at b, regardless of what is observed at a. a lower than expected\nwind at a (for a hurricane) would not change our expectation of winds at b (knowing\nthere is a hurricane). however, if s is not observed, then a and b are dependent, i.e.,\nthe path is active. (c) a and b are both parents of s. this is called a v-structure\nthe\ncollider case. the v-structure causes a and b to be related by the explaining away effect.\nin this case, the path is actually active when s is observed. for example, suppose s is a\nvariable indicating that your colleague is not at work. the variable a represents her being\nsick, while b represents her being on vacation. if you observe that she is not at work,\nyou can presume she is probably sick or on vacation, but it is not especially likely that\nboth have happened at the same time. if you find out that she is on vacation, this fact\nis sufficient to\n(d)\nthe explaining away effect happens even if any descendant of s is observed! for example,\nsuppose that c is a variable representing whether you have received a report from your\ncolleague. if you notice that you have not received the report, this increases your estimate\nof the probability that she is not at work today, which in turn makes it more likely that\nshe is either sick or on vacation. the only way to block a path through a v-structure is\nto observe none of the descendants of the shared child.\n\nher absence. you can infer that she is probably not also sick.\n\nexplain\n\nor\n\n577\n\n "}, {"Page_number": 593, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\nd\n\nc\n\nb\n\ne\n\nfigure 16.9: from this graph, we can read out several d-separation properties. examples\ninclude:\n\n\u2022 a and b are d-separated given the empty set.\n\u2022 a and e are d-separated given c.\n\u2022 d and e are d-separated given c.\n\nwe can also see that some variables are no longer d-separated when we observe some\nvariables:\n\n\u2022 a and b are not d-separated given c.\n\u2022 a and b are not d-separated given d.\n\n578\n\n "}, {"Page_number": 594, "text": "chapter 16. structured probabilistic models for deep learning\n\nfigure 16.10: examples of complete graphs, which can describe any probability distribution.\nhere we show examples with four random variables. (left) the complete undirected graph.\nin the undirected case, the complete graph is unique. (right) a complete directed graph.\nin the directed case, there is not a unique complete graph. we choose an ordering of the\nvariables and draw an arc from each variable to every variable that comes after it in the\nordering. there are thus a factorial number of complete graphs for every set of random\nvariables. in this example we order the variables from left to right, top to bottom.\n\ndirected models and undirected models both have their advantages and disad-\nvantages. neither approach is clearly superior and universally preferred. instead,\nwe should choose which language to use for each task. this choice will partially\ndepend on which probability distribution we wish to describe. we may choose to\nuse either directed modeling or undirected modeling based on which approach can\ncapture the most independences in the probability distribution or which approach\nuses the fewest edges to describe the distribution. there are other factors that\ncan affect the decision of which language to use. even while working with a single\nprobability distribution, we may sometimes switch between different modeling\nlanguages. sometimes a different language becomes more appropriate if we observe\na certain subset of variables, or if we wish to perform a different computational\ntask. for example, the directed model description often provides a straightforward\napproach to efficiently draw samples from the model (described in sec.\n) while\nthe undirected model formulation is often useful for deriving approximate inference\nprocedures (as we will see in chapter\n, where the role of undirected models is\nhighlighted in eq.\n\n19.56\n\n16.3\n\n19\n\n).\n\nevery probability distribution can be represented by either a directed model\nor by an undirected model.\nin the worst case,\u00a0one can always represent any\ndistribution by using a \u201ccomplete graph.\u201d in the case of a directed model, the\ncomplete graph is any directed acyclic graph where we impose some ordering on\nthe random variables, and each variable has all other variables that precede it in\nthe ordering as its ancestors in the graph. for an undirected model, the complete\ngraph is simply a graph containing a single clique encompassing all of the variables.\nsee fig.\n\nfor an example.\n\n16.10\n\n579\n\n "}, {"Page_number": 595, "text": "chapter 16. structured probabilistic models for deep learning\n\nof course, the utility of a graphical model is that the graph implies that some\nvariables do not interact directly. the complete graph is not very useful because it\ndoes not imply any independences.\n\nwhen we represent a probability distribution with a graph, we want to choose\na graph that implies as many independences as possible, without implying any\nindependences that do not actually exist.\n\nfrom this point of view, some distributions can be represented more efficiently\nusing directed models, while other distributions can be represented more efficiently\nusing\u00a0undirected models.\nin\u00a0other\u00a0words, directed\u00a0models\u00a0can\u00a0encode\u00a0some\nindependences that undirected models cannot encode, and vice versa.\n\ndirected models are able to use one specific kind of substructure that undirected\nmodels cannot represent perfectly. this substructure is called an immorality. the\nstructure occurs when two random variables a and b are both parents of a third\nrandom variable c, and there is no edge directly connecting a and b in either\ndirection. (the name \u201cimmorality\u201d may seem strange; it was coined in the graphical\nmodels literature as a joke about unmarried parents.) to convert a directed model\nwith graph d into an undirected model, we need to create a new graph u.\u00a0for\nevery pair of variables x and y, we add an undirected edge connecting x and y to\nu if there is a directed edge (in either direction) connecting x and y in d or if x\nand y are both parents in d of a third variable z.\u00a0the resulting u is known as\na moralized graph. see fig.\nfor examples of converting directed models to\nundirected models via moralization.\n\n16.11\n\nd\n\nlikewise, undirected models can include substructures that no directed model\ncan represent perfectly. specifically, a directed graph\ncannot capture all of the\nconditional independences implied by an undirected graph u if u contains a loop\nof length greater than three, unless that loop also contains a chord. a loop is\na sequence of variables connected by undirected edges, with the last variable in\nthe sequence connected back to the first variable in the sequence. a chord is a\nconnection between any two non-consecutive variables in the sequence defining a\nloop. if u has loops of length four or greater and does not have chords for these\nloops, we must add the chords before we can convert it to a directed model. adding\nthese chords discards some of the independence information that was encoded in\nu. the graph formed by adding chords to u is known as a chordal\ntriangulated\ngraph, because all the loops can now be described in terms of smaller, triangular\nloops. to build a directed graph d from the chordal graph, we need to also assign\ndirections to the edges. when doing so, we must not create a directed cycle in\nd, or the result does not define a valid directed probabilistic model. one way\nto assign directions to the edges in d is to impose an ordering on the random\n\nor\n\n580\n\n "}, {"Page_number": 596, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\nb\n\nc\n\na\n\nb\n\nc\n\na\n\nb\n\nh1h1\n\nh2h2\n\nh3h3\n\nc\n\nv1v1\n\nv2v2\n\nv3v3\n\na\n\nb\n\nh1h1\n\nh2h2\n\nh3h3\n\nc\n\nv1v1\n\nv2v2\n\nv3v3\n\n(center)\n\nfigure 16.11:\u00a0examples of converting directed models (top row) to undirected models\n(bottom row) by constructing moralized graphs. (left) this simple chain can be converted\nto a moralized graph merely by replacing its directed edges with undirected edges. the\nresulting undirected model implies exactly the same set of independences and conditional\nindependences.\nthis\u00a0graph is the simplest directed model that cannot\u00a0be\nconverted to an undirected model without losing some independences. this graph consists\nentirely of a single immorality. because a and b are parents of c, they are connected by an\nactive path when c is observed. to capture this dependence, the undirected model must\ninclude a clique encompassing all three variables. this clique fails to encode the fact that\nin general, moralization may add many edges to the graph, thus losing many\na b\u22a5 .\nimplied independences. for example, this sparse coding graph requires adding moralizing\nedges between every pair of hidden units, thus introducing a quadratic number of new\ndirect dependences.\n\n(right)\n\n581\n\n "}, {"Page_number": 597, "text": "chapter 16. structured probabilistic models for deep learning\n\na\n\nd\n\nb\n\nc\n\na\n\nd\n\nb\n\nc\n\na\n\nd\n\nb\n\nc\n\nfigure 16.12: converting an undirected model to a directed model. (left) this undirected\nmodel cannot be converted directed to a directed model because it has a loop of length four\nwith no chords. specifically, the undirected model encodes two different independences that\n\u22a5 | { , }.\u00a0(center)\nno directed model can capture simultaneously: a c\nto convert the undirected model to a directed model, we must triangulate the graph,\nby ensuring that all loops of greater than length three have a chord.\u00a0to do so, we can\neither add an edge connecting a and c or we can add an edge connecting b and d. in this\nexample, we choose to add the edge connecting a and c. (right) to finish the conversion\nprocess, we must assign a direction to each edge. when doing so, we must not create any\ndirected cycles. one way to avoid directed cycles is to impose an ordering over the nodes,\nand always point each edge from the node that comes earlier in the ordering to the node\nthat comes later in the ordering. in this example, we use the variable names to impose\nalphabetical order.\n\n\u22a5 | { , } and b d\n\nb d\n\na c\n\nvariables, then point each edge from the node that comes earlier in the ordering to\nthe node that comes later in the ordering. see fig.\n\nfor a demonstration.\n\n16.12\n\n16.2.7 factor graphs\n\nfactor\u00a0graphs are another way of drawing undirected models that resolve an\nambiguity in the graphical representation of standard undirected model syntax.\nin an undirected model, the scope of every \u03c6 function must be a subset of some\nclique in the graph. however, it is not necessary that there exist any \u03c6 whose\nscope contains the entirety of every clique. factor graphs explicitly represent the\nscope of each \u03c6 function. specifically, a factor graph is a graphical representation\nof an undirected model that consists of a bipartite undirected graph. some of the\nnodes are drawn as circles. these nodes correspond to random variables as in a\nstandard undirected model.\u00a0the rest of the nodes are drawn as squares.\u00a0these\nnodes correspond to the factors \u03c6 of the unnormalized probability distribution.\nvariables and factors may be connected with undirected edges. a variable and a\nfactor are connected in the graph if and only if the variable is one of the arguments\nto the factor in the unnormalized probability distribution. no factor may be\nconnected to another factor in the graph, nor can a variable be connected to a\n\n582\n\n "}, {"Page_number": 598, "text": "chapter 16. structured probabilistic models for deep learning\n\nvariable. see fig.\nin the interpretation of undirected networks.\n\n16.13\n\nfor an example of how factor graphs can resolve ambiguity\n\na\n\nb\n\na\n\na\n\nb\n\nc\n\nf1f1\n\nc\n\nf1f1\n\nb\n\nf3f3\n\nf2f2\n\nc\n\nfigure 16.13: an example of how a factor graph can resolve ambiguity in the interpretation\nof undirected networks. (left) an undirected network with a clique involving three\nvariables: a, b and c.\na factor graph corresponding to the same undirected\nmodel. this factor graph has one factor over all three variables. (right) another valid\nfactor graph for the same undirected model. this factor graph has three factors, each over\nonly two variables. representation, inference, and learning are all asymptotically cheaper\nin (c) compared to (b), even though both require the same undirected graph to represent.\n\n(center)\n\n16.3 sampling from graphical models\n\ngraphical models also facilitate the task of drawing samples from a model.\n\none advantage of directed graphical models is that a simple and efficient\nprocedure called ancestral sampling can produce a sample from the joint distribution\nrepresented by the model.\n\nthe basic idea is to sort the variables xi in the graph into a topological ordering,\nso that for all i and j, j is greater than i if xi is a parent of xj . the variables\ncan then be sampled in this order. in other words, we first sample x1 \u223c p (x 1),\nthen sample p (x2 | p ag(x2)), and so on, until finally we sample p (xn | p ag (x n)).\nso long as each conditional distribution p(xi | p ag(xi )) is easy to sample from,\nthen the whole model is easy to sample from. the topological sorting operation\nguarantees that we can read the conditional distributions in eq.\nand sample\nfrom them in order. without the topological sorting, we might attempt to sample\na variable before its parents are available.\n\n16.1\n\nfor some graphs, more than one topological ordering is possible. ancestral\n\nsampling may be used with any of these topological orderings.\n\nancestral sampling is generally very fast (assuming sampling from each condi-\n\ntional is easy) and convenient.\n\n583\n\n "}, {"Page_number": 599, "text": "chapter 16. structured probabilistic models for deep learning\n\none drawback to ancestral sampling is that it only applies to directed graphical\nmodels. another drawback is that it does not support every conditional sampling\noperation. when we wish to sample from a subset of the variables in a directed\ngraphical model, given some other variables, we often require that all the condition-\ning variables come earlier than the variables to be sampled in the ordered graph.\nin this case, we can sample from the local conditional probability distributions\nspecified by the model distribution. otherwise, the conditional distributions we\nneed to sample from are the posterior distributions given the observed variables.\nthese posterior distributions are usually not explicitly specified and parametrized\nin the model. inferring these posterior distributions can be costly. in models where\nthis is the case, ancestral sampling is no longer efficient.\n\nunfortunately, ancestral sampling is only applicable to directed models. we\ncan sample from undirected models by converting them to directed models, but this\noften requires solving intractable inference problems (to determine the marginal\ndistribution over the root nodes of the new directed graph) or requires introducing\nso many edges that the resulting directed model becomes intractable. sampling\nfrom an undirected model without first converting it to a directed model seems to\nrequire resolving cyclical dependencies. every variable interacts with every other\nvariable, so there is no clear beginning point for the sampling process. unfortunately,\ndrawing samples from an undirected graphical model is an expensive, multi-pass\nprocess. the conceptually simplest approach is\n. suppose we\nhave a graphical model over an n-dimensional vector of random variables x. we\niteratively visit each variable xi and draw a sample conditioned on all of the other\n| x\u2212i). due to the separation properties of the graphical\nvariables, from p (xi\nmodel, we can equivalently condition on only the neighbors of xi . unfortunately,\nafter we have made one pass through the graphical model and sampled all n\nvariables, we still do not have a fair sample from p(x). instead, we must repeat the\nprocess and resample all n variables using the updated values of their neighbors.\nasymptotically, after many repetitions, this process converges to sampling from\nthe correct distribution. it can be difficult to determine when the samples have\nreached a sufficiently accurate approximation of the desired distribution. sampling\ntechniques for undirected models are an advanced topic, covered in more detail in\nchapter\n\ngibbs sampling\n\n.17\n\n16.4 advantages of structured modeling\n\nthe primary advantage of using structured probabilistic models is that they allow\nus to dramatically reduce the cost of representing probability distributions as well\n\n584\n\n "}, {"Page_number": 600, "text": "chapter 16. structured probabilistic models for deep learning\n\nas learning and inference. sampling is also accelerated in the case of directed\nmodels, while the situation can be complicated with undirected models. the\nprimary mechanism that allows all of these operations to use less runtime and\nmemory is choosing to not model certain interactions. graphical models convey\ninformation by leaving edges out. anywhere there is not an edge, the model\nspecifies the assumption that we do not need to model a direct interaction.\n\na less quantifiable benefit of using structured probabilistic models is that\nthey allow us to explicitly separate representation of knowledge from learning of\nknowledge or inference given existing knowledge. this makes our models easier to\ndevelop and debug. we can design, analyze, and evaluate learning algorithms and\ninference algorithms that are applicable to broad classes of graphs. independently,\nwe can design models that capture the relationships we believe are important in our\ndata. we can then combine these different algorithms and structures and obtain\na cartesian product of different possibilities. it would be much more difficult to\ndesign end-to-end algorithms for every possible situation.\n\n16.5 learning about dependencies\n\na good generative model needs to accurately capture the distribution over the\nobserved or \u201cvisible\u201d\u00a0variables v. often the different elements of v are highly\ndependent on each other. in the context of deep learning, the approach most\ncommonly used to model these dependencies is to introduce several latent or\n\u201chidden\u201d variables, h. the model can then capture dependencies between any pair\nof variables v i and vj indirectly, via direct dependencies between vi and h, and\ndirect dependencies between\n\nand v\n\nh\n\nj.\n\na good model of v which did not contain any latent variables would need to\nhave very large numbers of parents per node in a bayesian network or very large\ncliques in a markov network. just representing these higher order interactions is\ncostly\u2014both in a computational sense, because the number of parameters that\nmust be stored in memory scales exponentially with the number of members in a\nclique, but also in a statistical sense, because this exponential number of parameters\nrequires a wealth of data to estimate accurately.\n\nwhen the model is intended to capture dependencies between visible variables\nwith direct connections, it is usually infeasible to connect all variables, so the graph\nmust be designed to connect those variables that are tightly coupled and omit\nedges between other variables. an entire field of machine learning called structure\nlearning is devoted to this problem for a good reference on structure learning, see\n). most structure learning techniques are a form of\n(koller and friedman 2009\n\n,\n\n585\n\n "}, {"Page_number": 601, "text": "chapter 16. structured probabilistic models for deep learning\n\ngreedy search. a structure is proposed, a model with that structure is trained,\nthen given a score. the score rewards high training set accuracy and penalizes\nmodel complexity. candidate structures with a small number of edges added or\nremoved are then proposed as the next step of the search. the search proceeds to\na new structure that is expected to increase the score.\n\nusing latent variables instead of adaptive structure avoids the need to perform\ndiscrete searches and multiple rounds of training. a fixed structure over visible\nand hidden variables can use direct interactions between visible and hidden units\nto impose indirect interactions between visible units. using simple parameter\nlearning techniques we can learn a model with a fixed structure that imputes the\nright structure on the marginal\n\n.\np( )v\n\n3.9.6\n\nlatent variables have advantages beyond their role in efficiently capturing p(v).\nthe new variables h also provide an alternative representation for v. for example,\nas discussed in sec.\n, the mixture of gaussians model learns a latent variable\nthat corresponds to which category of examples the input was drawn from. this\nmeans that the latent variable in a mixture of gaussians model can be used to do\nclassification. in chapter\nwe saw how simple probabilistic models like sparse\ncoding learn latent variables that can be used as input features for a classifier,\nor as coordinates along a manifold. other models can be used in this same way,\nbut deeper models and models with different kinds of interactions can create even\nricher descriptions of the input. many approaches accomplish feature learning\nby learning latent variables. often, given some model of v and h, experimental\nobservations show that e[h v|\n] or argmaxhp(h v, ) is a good feature mapping for\nv.\n\n14\n\n16.6\n\ninference and approximate inference\n\none of the main ways we can use a probabilistic model is to ask questions about\nhow variables are related to each other. given a set of medical tests, we can ask\nwhat disease a patient might have. in a latent variable model, we might want to\nextract features e[h v|\n] describing the observed variables v. sometimes we need\nto solve such problems in order to perform other tasks. we often train our models\nusing the principle of maximum likelihood. because\n\nlog ( ) = \n\np v\n\ne\n\nh\n\nh\u223cp( |v) [log (\n\np h v, \u2212\n\n)\n\nlog (\n\np h v|\n\n)]\n\n,\n\n(16.9)\n\nwe often want to compute p (h | v) in order to implement a learning rule. all of\nthese are examples of inference problems in which we must predict the value of\n\n586\n\n "}, {"Page_number": 602, "text": "chapter 16. structured probabilistic models for deep learning\n\nsome variables given other variables, or predict the probability distribution over\nsome variables given the value of other variables.\n\nunfortunately, for most interesting deep models, these inference problems are\nintractable, even when we use a structured graphical model to simplify them. the\ngraph structure allows us to represent complicated, high-dimensional distributions\nwith a reasonable number of parameters, but the graphs used for deep learning are\nusually not restrictive enough to also allow efficient inference.\n\nit is straightforward to see that computing the marginal probability of a general\ngraphical model is #p hard. the complexity class #p is a generalization of the\ncomplexity class np. problems in np require determining only whether a problem\nhas a solution and finding a solution if one exists. problems in #p require counting\nthe number of solutions. to construct a worst-case graphical model, imagine that\nwe define a graphical model over the binary variables in a 3-sat problem. we\ncan impose a uniform distribution over these variables. we can then add one\nbinary latent variable per clause that indicates whether each clause is satisfied.\nwe can then add another latent variable indicating whether all of the clauses are\nsatisfied. this can be done without making a large clique, by building a reduction\ntree of latent variables, with each node in the tree reporting whether two other\nvariables are satisfied. the leaves of this tree are the variables for each clause.\nthe root of the tree reports whether the entire problem is satisfied.\u00a0due to the\nuniform distribution over the literals, the marginal distribution over the root of the\nreduction tree specifies what fraction of assignments satisfy the problem. while\nthis is a contrived worst-case example, np hard graphs commonly arise in practical\nreal-world scenarios.\n\nthis motivates\u00a0the use\u00a0of approximate inference.\n\nin the\u00a0context\u00a0of deep\nlearning, this usually refers to variational inference, in which we approximate the\ntrue distribution p(h | v) by seeking an approximate distribution q(h v| ) that is as\nclose to the true one as possible. this and other techniques are described in depth\nin chapter\n\n.19\n\n16.7 the deep learning approach to structured prob-\n\nabilistic models\n\ndeep learning practitioners generally use the same basic computational tools as\nother machine learning practitioners who work with structured probabilistic models.\nhowever, in the context of deep learning, we usually make different design decisions\nabout how to combine these tools, resulting in overall algorithms and models that\n\n587\n\n "}, {"Page_number": 603, "text": "chapter 16. structured probabilistic models for deep learning\n\nhave a very different flavor from more traditional graphical models.\n\ndeep learning does not always involve especially deep graphical models. in the\ncontext of graphical models, we can define the depth of a model in terms of the\ngraphical model graph rather than the computational graph. we can think of a\nlatent variable h i as being at depth j if the shortest path from h i to an observed\nvariable is j steps. we usually describe the depth of the model as being the greatest\ndepth of any such h i. this kind of depth is different from the depth induced by\nthe computational graph. many generative models used for deep learning have no\nlatent variables or only one layer of latent variables, but use deep computational\ngraphs to define the conditional distributions within a model.\n\ndeep learning essentially always makes use of the idea of distributed represen-\ntations. even shallow models used for deep learning purposes (such as pretraining\nshallow models that will later be composed to form deep ones) nearly always\nhave a single, large layer of latent variables. deep learning models typically have\nmore latent variables than observed variables. complicated nonlinear interactions\nbetween variables are accomplished via indirect connections that flow through\nmultiple latent variables.\n\nby contrast, traditional graphical models usually contain mostly variables that\nare at least occasionally observed, even if many of the variables are missing at\nrandom from some training examples. traditional models mostly use higher-order\nterms and structure learning to capture complicated nonlinear interactions between\nvariables. if there are latent variables, they are usually few in number.\n\nthe way that latent variables are designed also differs in deep learning. the\ndeep learning practitioner typically does not intend for the latent variables to\ntake on any specific semantics ahead of time\u2014the training algorithm is free to\ninvent the concepts it needs to model a particular dataset. the latent variables are\nusually not very easy for a human to interpret after the fact, though visualization\ntechniques may allow some rough characterization of what they represent. when\nlatent variables are used in the context of traditional graphical models, they are\noften designed with some specific semantics in mind\u2014the topic of a document,\nthe intelligence of a student, the disease causing a patient\u2019s symptoms, etc. these\nmodels are often much more interpretable by human practitioners and often have\nmore theoretical guarantees, yet are less able to scale to complex problems and are\nnot reusable in as many different contexts as deep models.\n\nanother obvious difference is the kind of connectivity typically used in the\ndeep learning approach. deep graphical models typically have large groups of units\nthat are all connected to other groups of units, so that the interactions between\ntwo groups may be described by a single matrix. traditional graphical models\n\n588\n\n "}, {"Page_number": 604, "text": "chapter 16. structured probabilistic models for deep learning\n\nhave very few connections and the choice of connections for each variable may be\nindividually designed. the design of the model structure is tightly linked with\nthe choice of inference algorithm. traditional approaches to graphical models\ntypically aim to maintain the tractability of exact inference. when this constraint\nis too limiting, a popular approximate inference algorithm is an algorithm called\nloopy belief propagation. both of these approaches often work well with very\nsparsely connected graphs. by comparison, models used in deep learning tend to\nconnect each visible unit vi to very many hidden units hj, so that h can provide a\ndistributed representation of vi (and probably several other observed variables too).\ndistributed representations have many advantages, but from the point of view\nof graphical models and computational complexity, distributed representations\nhave the disadvantage of usually yielding graphs that are not sparse enough for\nthe traditional techniques of exact inference and loopy belief propagation to be\nrelevant. as a consequence, one of the most striking differences between the larger\ngraphical models community and the deep graphical models community is that\nloopy belief propagation is almost never used for deep learning. most deep models\nare instead designed to make gibbs sampling or variational inference algorithms\nefficient. another consideration is that deep learning models contain a very large\nnumber of latent variables, making efficient numerical code essential. this provides\nan additional motivation, besides the choice of high-level inference algorithm, for\ngrouping the units into layers with a matrix describing the interaction between\ntwo layers. this allows the individual steps of the algorithm to be implemented\nwith efficient matrix product operations, or sparsely connected generalizations, like\nblock diagonal matrix products or convolutions.\n\nfinally, the deep learning approach to graphical modeling is characterized by\na marked tolerance of the unknown. rather than simplifying the model until\nall quantities we might want can be computed exactly, we increase the power of\nthe model until it is just barely possible to train or use. we often use models\nwhose marginal distributions cannot be computed, and are satisfied simply to draw\napproximate samples from these models. we often train models with an intractable\nobjective function that we cannot even approximate in a reasonable amount of\ntime, but we are still able to approximately train the model if we can efficiently\nobtain an estimate of the gradient of such a function. the deep learning approach\nis often to figure out what the minimum amount of information we absolutely\nneed is, and then to figure out how to get a reasonable approximation of that\ninformation as quickly as possible.\n\n589\n\n "}, {"Page_number": 605, "text": "chapter 16. structured probabilistic models for deep learning\n\nh1h1\n\nh2h2\n\nh3h3\n\nh4h4\n\nv1v1\n\nv2v2\n\nv3v3\n\nfigure 16.14: an rbm drawn as a markov network.\n\n16.7.1 example: the restricted boltzmann machine\n\n,\n\n) or\n\n(rbm) (\n\nharmonium\n\nsmolensky 1986\n\nthe restricted boltzmann machine\nis the\nquintessential example of how graphical models are used for deep learning. the\nrbm is not itself a deep model. instead, it has a single layer of latent variables\nthat may be used to learn a representation for the input. in chapter\n, we will\nsee how rbms can be used to build many deeper models. here, we show how the\nrbm exemplifies many of the practices used in a wide variety of deep graphical\nmodels:\u00a0its units are organized into large groups called layers, the connectivity\nbetween layers is described by a matrix, the connectivity is relatively dense, the\nmodel is designed to allow efficient gibbs sampling, and the emphasis of the model\ndesign is on freeing the training algorithm to learn latent variables whose semantics\nwere not specified by the designer. later, in sec.\n, we will revisit the rbm in\nmore detail.\n\n20.2\n\n20\n\nthe canonical rbm is an energy-based model with binary visible and hidden\n\nunits. its energy function is\n\ne ,(v h\n\n) = \u2212 > v\n\nb\n\nc\u2212 > h v\u2212 > w h,\n\n(16.10)\n\nwhere b, c, and w are unconstrained, real-valued, learnable parameters. we can\nsee that the model is divided into two groups of units: v and h, and the interaction\nbetween them is described by a matrix w . the model is depicted graphically\nin fig.\n. as this figure makes clear, an important aspect of this model is\nthat there are no direct interactions between any two visible units or between any\ntwo hidden units (hence the \u201crestricted,\u201d a general boltzmann machine may have\narbitrary connections).\n\n16.14\n\nthe restrictions on the rbm structure yield the nice properties\n\nand\n\np(\n\nh v|\n\n) = \u03c0\n\nip(hi | v)\n\np(\n\nv h|\n\n) = \u03c0\n\nip(v i | h).\n\n590\n\n(16.11)\n\n(16.12)\n\n "}, {"Page_number": 606, "text": "chapter 16. structured probabilistic models for deep learning\n\n(\n\n).\u00a0\n\nlisa 2008\n\nfigure 16.15: samples from a trained rbm, and its weights. image reproduced with\npermission from\n(left) samples from a model trained on mnist, drawn\nusing gibbs sampling. each column is a separate gibbs sampling process. each row\nrepresents the output of another 1,000 steps of gibbs sampling. successive samples are\nhighly correlated with one another.\nthe corresponding weight vectors. compare\nthis to the samples and weights of a linear factor model, shown in fig.\u00a0\n. the samples\nhere are much better because the rbm prior p (h) is not constrained to be factorial. the\nrbm can learn which features should appear together when sampling. on the other hand,\nthe rbm posterior\nis not,\nh v|\nso the sparse coding model may be better for feature extraction. other models are able\nto have both a non-factorial\n\nis factorial, while the sparse coding posterior\n\nand a non-factorial\n\nh v|\n\n(right)\n\n13.2\n\np( )h\n\np(\n\np(\n\np(\n\n.\n)\n\n)\n\n)\n\nh v|\n\nthe individual conditionals are simple to compute as well. for the binary rbm\nwe obtain:\n\np (hi = 1 \n\n) = \n\np (hi = 0 \n\n) = 1\n\n| v\n| v\n\n\u03c3(cid:20)v>w:,i + bi(cid:21) ,\n\u2212 \u03c3(cid:20)v>w:,i + bi(cid:21) .\n\n(16.13)\n\n(16.14)\n\ntogether these properties allow for efficient block gibbs sampling, which alternates\nbetween sampling all of h simultaneously and sampling all of v simultaneously.\nsamples generated by gibbs sampling from an rbm model are shown in fig.\n16.15.\n\nsince the energy function itself is just a linear function of the parameters, it is\n\neasy to take derivatives of the energy function. for example,\n\n\u2202\n\n\u2202wi,j\n\ne ,(v h) = \u2212vi hj .\n\n(16.15)\n\nthese two properties\u2014efficient gibbs sampling and efficient derivatives\u2014make\n, we will see that undirected models may be\n\ntraining convenient.\u00a0in chapter\ntrained by computing such derivatives applied to samples from the model.\n\n18\n\ntraining the model induces a representation h of the data v. we can often use\nh\u223cp( |v) [\n\n]h as a set of features to describe\n\n.v\n\neh\n\n591\n\n "}, {"Page_number": 607, "text": "chapter 16. structured probabilistic models for deep learning\n\noverall, the rbm demonstrates the typical deep learning approach to graph-\nical models:\u00a0representation learning accomplished via layers of latent variables,\ncombined with efficient interactions between layers parametrized by matrices.\n\nthe language of graphical models provides an elegant, flexible and clear language\nfor describing probabilistic models. in the chapters ahead, we use this language,\namong other perspectives, to describe a wide variety of deep probabilistic models.\n\n592\n\n "}, {"Page_number": 608, "text": "chapter 17\n\nmonte carlo methods\n\nrandomized algorithms fall into two rough categories: las vegas algorithms and\nmonte carlo algorithms. las vegas algorithms always return precisely the correct\nanswer (or report that they failed). these algorithms consume a random amount\nof resources, usually memory or time. in contrast, monte carlo algorithms return\nanswers with a random amount of error. the amount of error can typically be\nreduced by expending more resources (usually running time and memory). for any\nfixed computational budget, a monte carlo algorithm can provide an approximate\nanswer.\n\nmany problems in machine learning are so difficult that we can never expect to\nobtain precise answers to them. this excludes precise deterministic algorithms and\nlas vegas algorithms. instead, we must use deterministic approximate algorithms\nor monte carlo approximations. both approaches are ubiquitous in machine\nlearning. in this chapter, we focus on monte carlo methods.\n\n17.1 sampling and monte carlo methods\n\nmany important technologies used to accomplish machine learning goals are based\non drawing samples from some probability distribution and using these samples to\nform a monte carlo estimate of some desired quantity.\n\n17.1.1 why sampling?\n\nthere are many reasons that we may wish to draw samples from a probability\ndistribution. sampling provides a flexible way to approximate many sums and\n\n593\n\n "}, {"Page_number": 609, "text": "chapter 17. monte carlo methods\n\nintegrals at reduced cost. sometimes we use this to provide a significant speedup to\na costly but tractable sum, as in the case when we subsample the full training cost\nwith minibatches. in other cases, our learning algorithm requires us to approximate\nan intractable sum or integral, such as the gradient of the log partition function of\nan undirected model. in many other cases, sampling is actually our goal, in the\nsense that we want to train a model that can sample from the training distribution.\n\n17.1.2 basics of monte carlo sampling\n\nwhen a sum or an integral cannot be computed exactly (for example the sum\nhas an exponential number of terms and no exact simplification is known) it is\noften possible to approximate it using monte carlo sampling. the idea is to view\nthe sum or integral as if it was an expectation under some distribution and to\napproximate the expectation by a corresponding average. let\n\nor\n\ns =xx\ns =z p\n\np\n\n( )x ( ) = \n\nx\n\nf\n\ne\n\np[ ( )]\n\nf x\n\n( )x ( )x x = \n\nd\n\nf\n\ne\n\np[ ( )]\n\nf x\n\n(17.1)\n\n(17.2)\n\nbe the sum or integral to estimate, rewritten as an expectation, with the constraint\nthat p is a probability distribution (for the sum) or a probability density (for the\nintegral) over random variable\n\n.x\n\nwe can approximate s by drawing n samples x(1), . . . , x( )n from p and then\n\nforming the empirical average\n\n\u02c6sn =\n\n1\nn\n\nnxi=1\n\nf (x( )i ).\n\n(17.3)\n\nthis approximation is justified by a few different properties. the first trivial\nobservation is that the estimator \u02c6s is unbiased, since\n\ne[\u02c6sn] =\n\n1\nn\n\nnxi=1\n\ne[ (f x( )i )] =\n\n1\nn\n\nnxi=1\n\ns\n\ns.= \n\n(17.4)\n\nbut in addition, the law of large numbers states that if the samples x( )i are i.i.d.,\nthen the average converges almost surely to the expected value:\n\n\u02c6sn = s,\n\nlim\nn\u2192\u221e\n\n594\n\n(17.5)\n\n "}, {"Page_number": 610, "text": "chapter 17. monte carlo methods\n\nprovided that the variance of the individual terms, var[f (x( )i )], is bounded. to see\nthis more clearly, consider the variance of \u02c6sn as n increases. the variance var[\u02c6sn]\ndecreases and converges to 0, so long as var[ (f x( )i )] < \u221e:\n\nvar[ ( )]\n\nf x\n\nvar[\u02c6sn] =\n\n1\nn2\n\nnxi=1\n\n=\n\nvar[ ( )]\n\nf x\nn\n\n.\n\n(17.6)\n\n(17.7)\n\nthis convenient result also tells us how to estimate the uncertainty in a monte\ncarlo average or equivalently the amount of expected error of the monte carlo\napproximation. we compute both the empirical average of the f(x( )i ) and their\nempirical variance,1 and then divide the estimated variance by the number of\nsamples n to obtain an estimator of var[\u02c6s n]. the central limit theorem tells us that\nthe distribution of the average, \u02c6sn, converges to a normal distribution with mean s\nf x\nand variance var[ ( )]\n. this allows us to estimate confidence intervals around the\nn\nestimate \u02c6s n, using the cumulative distribution of the normal density.\n\nhowever, all this relies on our ability to easily sample from the base distribution\np(x), but doing so is not always possible. when it is not feasible to sample from\np, an alternative is to use importance sampling, presented in sec.\n. a more\ngeneral approach is to form a sequence of estimators that converge towards the\ndistribution of interest. that is the approach of monte carlo markov chains\n(sec.\n\n17.2\n\n17.3\n\n).\n\n17.2\n\nimportance sampling\n\nan important step in the decomposition of the integrand (or summand) used by\nthe monte carlo method in eq.\nis deciding which part of the integrand should\nplay the role the probability p(x ) and which part of the integrand should play the\nrole of the quantity f(x) whose expected value (under that probability distribution)\nis to be estimated. there is no unique decomposition because p(x)f(x) can always\nbe rewritten as\n\n17.2\n\np\n\n( )x ( ) = \nq\n\nx\n\nf\n\n( )x\n\n,\n\n(17.8)\n\np\n\nf\n( )x ( )x\nq( )x\n\nwhere we now sample from q and average pf\nq . in many cases, we wish to compute\nan expectation for a given p and an f, and the fact that the problem is specified\n\n1the unbiased estimator of the variance is often preferred, in which the sum of squared\n\ndifferences is divided by\n\nn \u2212 1\n\ninstead of\n\n.\nn\n\n595\n\n "}, {"Page_number": 611, "text": "chapter 17. monte carlo methods\n\nfrom the start as an expectation suggests that this p and f would be a natural\nchoice of decomposition. however, the original specification of the problem may\nnot be the the optimal choice in terms of the number of samples required to obtain\na given level of accuracy.\u00a0fortunately, the form of the optimal choice q\u2217 can be\nderived easily. the optimal q\u2217 corresponds to what is called optimal importance\nsampling.\n\nbecause of the identity shown in eq.\n\n17.8\n\n, any monte carlo estimator\n\n\u02c6sp =\n\n1\nn\n\nnxi\n\n,=1 x( )i \u223cp\n\nf (x( )i )\n\ncan be transformed into an importance sampling estimator\n\n\u02c6sq =\n\n1\nn\n\nnxi\n\n,=1 x( )i \u223cq\n\np(x( )i ) (f x( )i )\n\nq(x( )i )\n\n.\n\n(17.9)\n\n(17.10)\n\nwe see readily that the expected value of the estimator does not depend on :q\n\neq [\u02c6sq] = eq[\u02c6sp] = s.\n\n(17.11)\n\nhowever, the variance of an importance sampling estimator can be greatly sensitive\nto the choice of\n\n. the variance is given by\n\nq\n\nvar[\u02c6sq ] = var[\n\np\n\nf\n( )x ( )x\nq( )x\n\n]/n.\n\nthe minimum variance occurs when\n\nisq\n\nq\u2217 ( ) =x\n\n( )x | ( )x |\np\n\nf\nz\n\n,\n\n(17.12)\n\n(17.13)\n\nwhere z is the normalization constant, chosen so that q\u2217(x) sums or integrates to\n1 as appropriate. better importance sampling distributions put more weight where\nthe integrand is larger. in fact, when f(x) does not change sign, var [\u02c6sq\u2217 ] = 0,\nmeaning that a single sample is sufficient when the optimal distribution is\nused. of course, this is only because the computation of q\u2217 has essentially solved\nthe original problem, so it is usually not practical to use this approach of drawing\na single sample from the optimal distribution.\n\nany choice of sampling distribution q is valid (in the sense of yielding the\ncorrect expected value) and q\u2217 is the optimal one (in the sense of yielding minimum\nvariance). sampling from q\u2217 is usually infeasible, but other choices of q can be\nfeasible while still reducing the variance somewhat.\n\n596\n\n "}, {"Page_number": 612, "text": "chapter 17. monte carlo methods\n\nanother approach is to use biased importance sampling, which has the\nadvantage of not requiring normalized p or q. in the case of discrete variables, the\nbiased importance sampling estimator is given by\n\ni=1\n\ni=1\n\n\u02c6sbis =p n\npn\n=p n\npn\n=p n\npn\n\ni=1\n\np(x( )i )\nq(x( )i )\n\nf (x( )i )\n\np(x( )i )\nq(x( )i )\n\ni=1\n\np(x( )i )\n\u02dcq(x( )i ) f (x( )i )\n\np(x( )i )\n\u02dcq(x( )i )\n\ni=1\n\n\u02dcp(x( )i )\n\u02dcq(x( )i ) f (x( )i )\n\n\u02dcp(x( )i )\n\u02dcq(x( )i )\n\ni=1\n\n(17.14)\n\n(17.15)\n\n,\n\n(17.16)\n\nwhere \u02dcp and \u02dcq are the unnormalized forms of p and q and the x( )i are the samples\nfrom q. this estimator is biased because e[ \u02c6sbis ] 6= s, except asymptotically when\nn \u2192 \u221e and the denominator of eq.\nconverges to 1. hence this estimator is\ncalled asymptotically unbiased.\n\n17.14\n\nf\n\nq( )x\n\n17.12\n\n( )x | ( )x |\n\nq for which p\n\n, we see that if there are samples of\n\nalthough a good choice of q can greatly improve the efficiency of monte carlo\nestimation, a poor choice of q can make the efficiency much worse.\u00a0going back\nto eq.\nis large,\nthen the variance of the estimator can get very large. this may happen when\nq(x) is tiny while neither p (x) nor f (x) are small enough to cancel it. the q\ndistribution is usually chosen to be a very simple distribution so that it is easy\nto sample from. when x is high-dimensional, this simplicity in q causes it to\n| poorly. when q(x( )i ) (cid:129) p(x( )i )|f (x( )i )|, importance sampling\nmatch p or p f|\ncollects useless samples (summing tiny numbers or zeros). on the other hand, when\nq(x( )i ) (cid:128) p(x( )i )|f (x( )i )|, which will happen more rarely, the ratio can be huge.\nbecause these latter events are rare, they may not show up in a typical sample,\nyielding typical underestimation of s , compensated rarely by gross overestimation.\nsuch very large or very small numbers are typical when x is high dimensional,\nbecause in high dimension the dynamic range of joint probabilities can be very\nlarge.\n\nin spite of this danger, importance sampling and its variants have been found\nvery useful in many machine learning algorithms, including deep learning algorithms.\nfor example, see the use of importance sampling to accelerate training in neural\nlanguage models with\u00a0a large vocabulary (sec.\n) or other neural nets\nwith a large number of outputs. see also how importance sampling has been\nused to estimate a partition function (the normalization constant of a probability\n\n12.4.3.3\n\n597\n\n "}, {"Page_number": 613, "text": "chapter 17. monte carlo methods\n\n18.7\n\n, and to estimate the log-likelihood in deep directed models\ndistribution) in sec.\nsuch as the variational autoencoder, in sec.\n. importance sampling may\nalso be used to improve the estimate of the gradient of the cost function used to\ntrain model parameters with stochastic gradient descent, particularly for models\nsuch as classifiers where most of the total value of the cost function comes from a\nsmall number of misclassified examples. sampling more difficult examples more\nfrequently can reduce the variance of the gradient in such cases (\n\nhinton 2006\n\n20.10.3\n\n).\n\n,\n\n17.3 markov chain monte carlo methods\n\nmarkov chain\n\nin many cases, we wish to use a monte carlo technique but there is no tractable\nmethod for drawing exact samples from the distribution pmodel (x) or from a good\n(low variance) importance sampling distribution q (x).\nin the context of deep\nlearning, this most often happens when pmodel(x) is represented by an undirected\nmodel. in these cases, we introduce a mathematical tool called a\nto\napproximately sample from pmodel(x). the family of algorithms that use markov\nchains to perform monte carlo estimates is called markov chain monte carlo\nmethods (mcmc). markov chain monte carlo methods for machine learning are\ndescribed at greater length in koller and friedman 2009\n).\u00a0the most standard,\ngeneric guarantees for mcmc techniques are only applicable when the model\ndoes not assign zero probability to any state. therefore, it is most convenient\nto\u00a0present\u00a0these\u00a0techniques\u00a0as sampling\u00a0from an\u00a0energy-based model\u00a0(ebm)\np(x) \u221d\n. in the ebm formulation, every\nstate is guaranteed to have non-zero probability. mcmc methods are in fact\nmore broadly applicable and can be used with many probability distributions that\ncontain zero probability states. however, the theoretical guarantees concerning the\nbehavior of mcmc methods must be proven on a case-by-case basis for different\nfamilies of such distributions. in the context of deep learning, it is most common\nto rely on the most general theoretical guarantees that naturally apply to all\nenergy-based models.\n\nexp ( e( ))x as described in sec.\n\n16.2.4\n\n\u2212\n\n(\n\nto understand why drawing samples from an energy-based model is difficult,\na b . in order\nconsider an ebm over just two variables, defining a distribution\nto sample a, we must draw a from p(a b|\n), and in order to sample b , we must\ndraw it from p(b\n). it seems to be an intractable chicken-and-egg problem.\ndirected models avoid this because their graph is directed and acyclic. to perform\nancestral sampling one simply samples each of the variables in topological order,\nconditioning on each variable\u2019s parents, which are guaranteed to have already been\n). ancestral sampling defines an efficient, single-pass method of\nsampled (sec.\n\np( , )\n\na|\n\n16.3\n\n598\n\n "}, {"Page_number": 614, "text": "chapter 17. monte carlo methods\n\nobtaining a sample.\n\nin an ebm, we can avoid this chicken and egg problem by sampling using a\nmarkov chain. the core idea of a markov chain is to have a state x that begins\nas an arbitrary value. over time, we randomly update x repeatedly. eventually\nx becomes (very nearly) a fair sample from p( x). formally, a markov chain is\ndefined by a random state x and a transition distribution t (x0 | x) specifying\nthe probability that a random update will go to state x0 if it starts in state x.\nrunning the markov chain means repeatedly updating the state x to a value x0\nsampled from t (x0 | x).\n\nto gain some theoretical understanding of how mcmc methods work, it is\nuseful to reparametrize the problem. first, we restrict our attention to the case\nwhere the random variable x has countably many states. in this case, we can\nrepresent the state as just a positive integer x. different integer values of x map\nback to different states\n\nin the original problem.\n\nx\n\nconsider what happens when we run infinitely many markov chains in parallel.\nall of the states of the different markov chains are drawn from some distribution\nq( )t (x), where t indicates the number of time steps that have elapsed. at the\nbeginning, q(0) is some distribution that we used to arbitrarily initialize x for each\nmarkov chain. later, q( )t\nis influenced by all of the markov chain steps that have\nrun so far. our goal is for q( )t ( )x to converge to\n\n.\np x( )\n\nbecause we have reparametrized the problem in terms of positive integer x, we\n\ncan describe the probability distribution\n\nq\n\nusing a vector\n\nv\n\n, with\n\nq\n\n( = x\ni\n) =  i.\n\nv\n\n(17.17)\n\nconsider what happens when we update a single markov chain\u2019s state x to a\n\nnew state x0 . the probability of a single state landing in state x0 is given by\n\nt\n\nq( +1)\n\n(x0) =xx\n\nq( )t ( ) (x t x0 | x .)\n\n(17.18)\n\nusing our integer parametrization, we can represent the effect of the transition\n\nt\n\na\n\noperator\n\nusing a matrix\n\nso that\n. we define\na\nai,j =  (t x0 = \ni | x\n. rather than writing it in\nusing this definition, we can now rewrite eq.\nterms of q and t to understand how a single state is updated, we may now use v\nand a to describe how the entire distribution over all the different markov chains\nrun in parallel shifts as we apply an update:\n\n(17.19)\n\n17.18\n\n=  )\nj .\n\nv( )t = av(\n\nt\u2212 .\n1)\n\n599\n\n(17.20)\n\n "}, {"Page_number": 615, "text": "chapter 17. monte carlo methods\n\napplying the markov chain update repeatedly corresponds to multiplying by the\nmatrix a repeatedly. in other words, we can think of the process as exponentiating\nthe matrix\n\n:a\n\nv( )t = at v(0).\n\n(17.21)\n\nthe matrix a has special structure because each of its columns represents a\nprobability distribution. such matrices are called stochastic matrices. if there is\na non-zero probability of transitioning from any state x to any other state x0 for\nsome power t, then the perron-frobenius theorem (\nperron 1907 frobenius 1908\n)\nguarantees that the largest eigenvalue is real and equal to . over time, we can\nsee that all of the eigenvalues are exponentiated:\n\n1\n\n,\n\n;\n\n,\n\nv ( )t =(cid:19)v\n\n\u03bb v\n\ndiag( ) \u22121(cid:20)t\n\nv(0) = \n\nv diag \u03bb tv \u22121v (0).\n\n( )\n\n(17.22)\n\nthis process causes all of the eigenvalues that are not equal to\nto decay to\nzero. under\u00a0some additional\u00a0mild\u00a0conditions, a is\u00a0guaranteed\u00a0to\u00a0have only\none eigenvector with eigenvalue\nstationary\ndistribution\n\n. the process thus converges to a\n\n, sometimes also called the\n\nequilibrium distribution\n\n. at convergence,\n\n1\n\n1\n\nv0 = \n\n= av\n\nv,\n\n(17.23)\n\nand this same condition holds for every additional step. this is an eigenvector\nequation. to be a stationary point, v must be an eigenvector with corresponding\neigenvalue . this condition guarantees that once we have reached the stationary\ndistribution, repeated applications of the transition sampling procedure do not\nchange the\nover the states of all the various markov chains (although\ntransition operator does change each individual state, of course).\n\ndistribution\n\n1\n\nif we have chosen t correctly, then the stationary distribution q will be equal\nto the distribution p we wish to sample from. we will describe how to choose t\nshortly, in sec.\n\n.\n17.4\n\nharris chain\n\nmost properties of markov chains with countable states can be generalized\nto continuous variables. in this situation, some authors call the markov chain\na\nbut we use the term markov chain to describe both conditions.\nin general, a markov chain with transition operator t will converge, under mild\nconditions, to a fixed point described by the equation\nq0 (x0) = ex\u223cqt (x0 | x),\n\n(17.24)\n\nwhich in the discrete case is just rewriting eq.\nx is discrete, the\nexpectation corresponds to a sum, and when x is continuous, the expectation\ncorresponds to an integral.\n\n. when\n\n17.23\n\n600\n\n "}, {"Page_number": 616, "text": "chapter 17. monte carlo methods\n\nburning in\n\nregardless of whether the state is continuous or discrete, all markov chain\nmethods consist of repeatedly applying stochastic updates until eventually the state\nbegins to yield samples from the equilibrium distribution. running the markov\nchain until it reaches its equilibrium distribution is called \u201c\n\u201d the markov\nchain. after the chain has reached equilibrium, a sequence of infinitely many\nsamples may be drawn from from the equilibrium distribution. they are identically\ndistributed but any two successive samples will be highly correlated with each other.\na finite sequence of samples may thus not be very representative of the equilibrium\ndistribution. one way to mitigate this problem is to return only every n successive\nsamples, so that our estimate of the statistics of the equilibrium distribution is\nnot as biased by the correlation between an mcmc sample and the next several\nsamples. markov chains are thus expensive to use because of the time required to\nburn in to the equilibrium distribution and the time required to transition from\none sample to another reasonably decorrelated sample after reaching equilibrium.\nif one desires truly independent samples, one can run multiple markov chains\nin parallel. this approach uses extra parallel computation to eliminate latency.\nthe strategy of using only a single markov chain to generate all samples and the\nstrategy of using one markov chain for each desired sample are two extremes; deep\nlearning practitioners usually use a number of chains that is similar to the number\nof examples in a minibatch and then draw as many samples as are needed from\nthis fixed set of markov chains. a commonly used number of markov chains is 100.\n\nmixing time\n\nanother difficulty is that we do not know in advance how many steps the\nmarkov chain must run before reaching its equilibrium distribution. this length of\ntime is called the\n. it is also very difficult to test whether a markov\nchain has reached equilibrium. we do not have a precise enough theory for guiding\nus in answering this question. theory tells us that the chain will converge, but not\nmuch more. if we analyze the markov chain from the point of view of a matrix a\nacting on a vector of probabilities v, then we know that the chain mixes when at\nhas effectively lost all of the eigenvalues from a besides the unique eigenvalue of\n.1\nthis means that the magnitude of the second largest eigenvalue will determine the\nmixing time. however, in practice, we cannot actually represent our markov chain\nin terms of a matrix. the number of states that our probabilistic model can visit\nis exponentially large in the number of variables, so it is infeasible to represent\nv, a, or the eigenvalues of a.\u00a0due to these and other obstacles, we usually do\nnot know whether a markov chain has mixed. instead, we simply run the markov\nchain for an amount of time that we roughly estimate to be sufficient, and use\nheuristic methods to determine whether the chain has mixed. these heuristic\nmethods include manually inspecting samples or measuring correlations between\nsuccessive samples.\n\n601\n\n "}, {"Page_number": 617, "text": "chapter 17. monte carlo methods\n\n17.4 gibbs sampling\n\nso far we have described how to draw samples from a distribution q(x) by repeatedly\nx\u2190 0 \u223c t (x0 | x). however, we have not described how to ensure that\nupdating x\nq(x) is a useful distribution. two basic approaches are considered in this book.\nthe first one is to derive t from a given learned pmodel , described below with the\ncase of sampling from ebms. the second one is to directly parametrize t and\nlearn it, so that its stationary distribution implicitly defines the pmodel of interest.\nexamples of this second approach are discussed in sec.\n\nand sec.\n\n.\n20.13\n\n20.12\n\nin the context of deep learning, we commonly use markov chains to draw\nsamples from an energy-based model defining a distribution pmodel(x). in this case,\nwe want the q (x) for the markov chain to be pmodel(x). to obtain the desired\nq( )x , we must choose an appropriate t (x0 | x).\n\ngibbs sampling\n\na conceptually simple and effective approach to building a markov chain\nthat samples from p model(x) is to use\n, in which sampling from\nt (x0 | x) is accomplished by selecting one variable xi and sampling it from pmodel\nconditioned on its neighbors in the undirected graph g defining the structure of\nthe energy-based model. it is also possible to sample several variables at the same\ntime so long as they are conditionally independent given all of their neighbors. as\nshown in the rbm example in sec.\n, all of the hidden units of an rbm may\nbe sampled simultaneously because they are conditionally independent from each\nother given all of the visible units. likewise, all of the visible units may be sampled\nsimultaneously because they are conditionally independent from each other given\nall of the hidden units. gibbs sampling approaches that update many variables\nsimultaneously in this way are called block gibbs sampling.\n\n16.7.1\n\nalternate approaches to designing markov chains to sample from p model are\npossible. for example, the metropolis-hastings algorithm is widely used in other\ndisciplines. in the context of the deep learning approach to undirected modeling,\nit is rare to use any approach other than gibbs sampling. improved sampling\ntechniques are one possible research frontier.\n\n17.5 the challenge of mixing between separated modes\n\nmix\n\nthe primary difficulty involved with mcmc methods is that they have a tendency\nto\npoorly. ideally, successive samples from a markov chain designed to sample\nfrom p(x ) would be completely independent from each other and would visit many\ndifferent regions in x space proportional to their probability. instead, especially\nin high dimensional cases, mcmc samples become very correlated. we refer\n\n602\n\n "}, {"Page_number": 618, "text": "chapter 17. monte carlo methods\n\n1)\n\nto such behavior as slow mixing or even failure to mix. mcmc methods with\nslow mixing can be seen as inadvertently performing something resembling noisy\ngradient descent on the energy function, or equivalently noisy hill climbing on the\nprobability, with respect to the state of the chain (the random variables being\nsampled).\u00a0the chain tends to take small steps (in the space of the state of the\nt\u2212 to a configuration x( )t , with the energy\nmarkov chain), from a configuration x (\nt\u2212 ), with a\ne(x( )t ) generally lower or approximately equal to the energy e (x(\npreference for moves that yield lower energy configurations. when starting from a\nrather improbable configuration (higher energy than the typical ones from p(x)),\nthe chain tends to gradually reduce the energy of the state and only occasionally\nmove to another mode. once the chain has found a region of low energy (for\nexample, if the variables are pixels in an image, a region of low energy might be\na connected manifold of images of the same object), which we call a mode, the\nchain will tend to walk around that mode (following a kind of random walk). once\nin a while it will step out of that mode and generally return to it or (if it finds\nan escape route) move towards another mode. the problem is that successful\nescape routes are rare for many interesting distributions, so the markov chain will\ncontinue to sample the same mode longer than it should.\n\n1)\n\n17.4\n\nthis is very clear when we consider the gibbs sampling algorithm (sec.\n\n).\nin this context, consider the probability of going from one mode to a nearby\nmode within a given number of steps. what will determine that probability is\nthe shape of the \u201cenergy barrier\u201d between these modes. transitions between two\nmodes that are separated by a high energy barrier (a region of low probability)\nare exponentially less likely (in terms of the height of the energy barrier). this is\nillustrated in fig.\n. the problem arises when there are multiple modes with\nhigh probability that are separated by regions of low probability, especially when\neach gibbs sampling step must update only a small subset of variables whose\nvalues are largely determined by the other variables.\n\n17.1\n\nas a simple example, consider an energy-based model over two variables a and\nand . if e (a b, ) = \u2212wab\nb, which are both binary with a sign, taking on values \u22121 \nfor some large positive number w, then the model expresses a strong belief that a\nand b have the same sign. consider updating b using a gibbs sampling step with\na = 1.\u00a0the conditional distribution over b is given by p(b = 1| a = 1) = \u03c3(w).\nif w is large, the sigmoid saturates, and the probability of also assigning b to be\n1 is close to 1. likewise, if a = \u22121, the probability of assigning b to be \u22121 is\nclose to 1. according to pmodel (a b, ), both signs of both variables are equally likely.\naccording to pmodel(a b|\n), both variables should have the same sign. this means\nthat gibbs sampling will only very rarely flip the signs of these variables.\n\n1\n\n603\n\n "}, {"Page_number": 619, "text": "chapter 17. monte carlo methods\n\n(center)\n\nfigure 17.1: paths followed by gibbs sampling for three distributions, with the markov\nchain initialized at the mode in both cases. (left) a multivariate normal distribution\nwith two independent variables. gibbs sampling mixes well because the variables are\nindependent.\na multivariate normal distribution with highly correlated variables.\nthe correlation between variables makes it difficult for the markov chain to mix. because\neach variable must be updated conditioned on the other, the correlation reduces the rate\nat which the markov chain can move away from the starting point. (right) a mixture of\ngaussians with widely separated modes that are not axis-aligned. gibbs sampling mixes\nvery slowly because it is difficult to change modes while altering only one variable at a\ntime.\n\nin more practical scenarios, the challenge is even greater because we care not\nonly about making transitions between two modes but more generally between\nall the many modes that a real model might contain. if several such transitions\nare difficult because of the difficulty of mixing between modes, then it becomes\nvery expensive to obtain a reliable set of samples covering most of the modes, and\nconvergence of the chain to its stationary distribution is very slow.\n\nsometimes this problem can be resolved by finding groups of highly dependent\nunits and updating all of them simultaneously in a block.\u00a0unfortunately, when\nthe dependencies are complicated, it can be computationally intractable to draw a\nsample from the group. after all, the problem that the markov chain was originally\nintroduced to solve is this problem of sampling from a large group of variables.\n\nin the context of models with latent variables, which define a joint distribution\npmodel(x h, ), we often draw samples of x by alternating between sampling from\npmodel(x h|\n). from the point of view of mixing\nrapidly, we would like pmodel(h x|\n) to have very high entropy. however, from the\npoint of view of learning a useful representation of h, we would like h to encode\n\n) and sampling from p model(h x|\n\n604\n\n "}, {"Page_number": 620, "text": "chapter 17. monte carlo methods\n\nfigure 17.2: an illustration of the slow mixing problem in deep probabilistic models.\neach panel should be read left to right, top to bottom. (left) consecutive samples from\ngibbs sampling applied to a deep boltzmann machine trained on the mnist dataset.\nconsecutive samples are similar to each other. because the gibbs sampling is performed\nin a deep graphical model, this similarity is based more on semantic rather than raw visual\nfeatures, but it is still difficult for the gibbs chain to transition from one mode of the\ndistribution to another, for example by changing the digit identity.\nconsecutive\nancestral samples from a generative adversarial network. because ancestral sampling\ngenerates each sample independently from the others, there is no mixing problem.\n\n(right)\n\nenough information about x to reconstruct it well, which implies that h and x\nshould have very high mutual information. these two goals are at odds with each\nother. we often learn generative models that very precisely encode x into h but\nare not able to mix very well. this situation arises frequently with boltzmann\nmachines\u2014the sharper the distribution a boltzmann machine learns, the harder\nit is for a markov chain sampling from the model distribution to mix well. this\nproblem is illustrated in fig.\n\n17.2\n.\n\nall this could make mcmc methods less useful when the distribution of interest\nhas a manifold structure with a separate manifold for each class: the distribution\nis concentrated around many modes and these modes are separated by vast regions\nof high energy. this type of distribution is what we expect in many classification\nproblems and would make mcmc methods converge very slowly because of poor\nmixing between modes.\n\n17.5.1 tempering to mix between modes\n\nwhen a distribution has sharp peaks of high probability surrounded by regions of\nlow probability, it is difficult to mix between the different modes of the distribution.\n\n605\n\n "}, {"Page_number": 621, "text": "chapter 17. monte carlo methods\n\nseveral techniques for faster mixing are based on constructing alternative versions\nof the target distribution in which the peaks are not as high and the surrounding\nvalleys are not as low. energy-based models provide a particularly simple way to\ndo so. so far, we have described an energy-based model as defining a probability\ndistribution\n\nenergy-based models may be augmented with an extra parameter \u03b2 controlling\nhow sharply peaked the distribution is:\n\np\n\nx \u221d\n( ) \n\nexp (\n\n\u2212 ( ))x\ne\n\n.\n\n(17.25)\n\nx \u221d\np \u03b2( ) \n\nexp (\n\n\u2212\u03b2e x .\n( ))\n\n(17.26)\n\nthe \u03b2 parameter is often described as being the reciprocal of the temperature,\nreflecting the origin of energy-based models in statistical physics. when the\ntemperature falls to zero and\nrises to infinity, the energy-based model becomes\ndeterministic. when the temperature rises to infinity and \u03b2 falls to zero, the\ndistribution (for discrete\n\n) becomes uniform.\n\nx\n\n\u03b2\n\ntypically, a model is trained to be evaluated at \u03b2 = 1. however, we can make\nuse of other temperatures, particularly those where \u03b2 < 1. tempering is a general\nstrategy of mixing between modes of p 1 rapidly by drawing samples with\n.\n\u03b2 < 1\n\n,\n\n,\n\n,\n\niba 2001\n\nneal 1994\n\nmarkov chains based on tempered transitions (\n\n) temporarily sample\nfrom higher-temperature distributions in order to mix to different modes, then\nresume sampling from the unit temperature distribution. these techniques have\nbeen applied to models such as rbms (salakhutdinov 2010\n). another approach is\nto use parallel tempering (\n), in which the markov chain simulates many\ndifferent states in parallel, at different temperatures. the highest temperature\nstates mix slowly, while the lowest temperature states, at temperature , provide\naccurate samples from the model. the transition operator includes stochastically\nswapping states between two different temperature levels, so that a sufficiently high-\nprobability sample from a high-temperature slot can jump into a lower temperature\nslot. this approach has also been applied to rbms (\ndesjardins et al. 2010 cho\net al.,\n). although tempering is a promising approach, at this point it has not\nallowed researchers to make a strong advance in solving the challenge of sampling\nfrom complex ebms. one possible reason is that there are critical temperatures\naround which the temperature transition must be very slow (as the temperature is\ngradually reduced) in order for tempering to be effective.\n\n2010\n\n1\n\n,\n\n;\n\n17.5.2 depth may help mixing\n\nwhen drawing samples from a latent variable model p(h x,\np(h x|\n\n) encodes x too well, then sampling from p(x h|\n\n), we have seen that if\n) will not change x very\n\n606\n\n "}, {"Page_number": 622, "text": "chapter 17. monte carlo methods\n\nx\n\nh\n\ninto\n\nmuch and mixing will be poor. one way to resolve this problem is to make h be a\ndeep representation, that encodes\nin such a way that a markov chain in\nthe space of h can mix more easily. many representation learning algorithms, such\nas autoencoders and rbms, tend to yield a marginal distribution over h that is\nmore uniform and more unimodal than the original data distribution over x . it can\nbe argued that this arises from trying to minimize reconstruction error while using\nall of the available representation space, because minimizing reconstruction error\nover the training examples will be better achieved when different training examples\nare easily distinguishable from each other in h-space, and thus well separated.\nbengio\n) observed that deeper stacks of regularized autoencoders or\nrbms yield marginal distributions in the top-level h-space that appeared more\nspread out and more uniform, with less of a gap between the regions corresponding\nto different modes (categories, in the experiments). training an rbm in that\nhigher-level space allowed gibbs sampling to mix faster between modes. it remains\nhowever unclear how to exploit this observation to help better train and sample\nfrom deep generative models.\n\net al. (\n\n2013a\n\ndespite the difficulty of mixing, monte carlo techniques are useful and are\noften the best tool available. indeed, they are the primary tool used to confront\nthe intractable partition function of undirected models, discussed next.\n\n607\n\n "}, {"Page_number": 623, "text": "chapter 18\n\nconfronting the partition\nfunction\n\n16.2.2\n\nin sec.\nwe saw that many probabilistic models (commonly known as undi-\nrected graphical models) are defined by an unnormalized probability distribution\n\u02dcp(x; \u03b8). we must normalize \u02dcp by dividing by a partition function z(\u03b8) in order to\nobtain a valid probability distribution:\n\np( ; ) =\n\nx \u03b8\n\n1\n\nz( )\u03b8\n\n\u02dcp\n\n( ; )x \u03b8\n.\n\n(18.1)\n\nthe partition function is an integral (for continuous variables) or sum (for discrete\nvariables) over the unnormalized probability of all states:\n\nor\n\nz \u02dcp\nxx\n\n( )x x\n\nd\n\n\u02dcp\n\n.\n( )x\n\n(18.2)\n\n(18.3)\n\nthis operation is intractable for many interesting models.\n\n20\n\nas we will see in chapter\n\n, several deep learning models are designed to\nhave a tractable normalizing constant, or are designed to be used in ways that do\nnot involve computing p(x) at all.\u00a0however, other models directly confront the\nchallenge of intractable partition functions. in this chapter, we describe techniques\nused for training and evaluating models that have intractable partition functions.\n\n608\n\n "}, {"Page_number": 624, "text": "chapter 18. confronting the partition function\n\n18.1 the log-likelihood gradient\n\nwhat\u00a0makes\u00a0learning\u00a0undirected models\u00a0by\u00a0maximum\u00a0likelihood\u00a0particularly\ndifficult is that the partition function depends on the parameters. the gradient of\nthe log-likelihood with respect to the parameters has a term corresponding to the\ngradient of the partition function:\n\n\u2207\u03b8 log \u02dcp( ; )x \u03b8 \u2212 \u2207\u03b8 log ( )z \u03b8 .\nthis is a well-known decomposition into the positive phase\n\n\u2207\u03b8 log ( ; ) = \n\np x \u03b8\n\nand\n\n(18.4)\n\nnegative phase\n\nof learning.\n\nfor most undirected models of interest, the negative phase is difficult. models\nwith no latent variables or with few interactions between latent variables typically\nhave a tractable positive phase. the quintessential example of a model with a\nstraightforward positive phase and difficult negative phase is the rbm, which has\nhidden units that are conditionally independent from each other given the visible\nunits. the case where the positive phase is difficult, with complicated interactions\nbetween latent variables, is primarily covered in chapter\n. this chapter focuses\non the difficulties of the negative phase.\n\n19\n\nlet us look more closely at the gradient of\n\nlog z\n\n:\n\n\u2202\n\u2202\u03b8\n\n=\n\nlog z\n\n\u2202\n\u2202\u03b8 z\nz\n\n\u2202\n\n=\n\n\u2202\u03b8p x \u02dcp( )x\n=px\n\nz\n\u2202\n\u2202\u03b8 \u02dcp( )x\nz\n\n.\n\n(18.5)\n\n(18.6)\n\n(18.7)\n\n(18.8)\n\nfor models that guarantee p(x) > 0 for all x, we can substitute exp (log \u02dcp( ))x\n\nfor \u02dcp( )x :\n\np x\n\n\u2202\n\u2202\u03b8 exp (log \u02dcp( ))x\n\nz\n\n= px exp (log \u02dcp( ))x \u2202\n\n= px \u02dcp( )x \u2202\n\nz\n\u2202\u03b8 log \u02dcp( )x\nz\n609\n\n\u2202\u03b8 log \u02dcp( )x\n\n(18.9)\n\n(18.10)\n\n(18.11)\n\n "}, {"Page_number": 625, "text": "chapter 18. confronting the partition function\n\n= xx\n\np( )x\n\n\u2202\n\u2202\u03b8\n\nlog \u02dcp( )x\n\n= e\n\nx\u223cp( )\nx\n\n\u2202\n\u2202\u03b8\n\nlog \u02dcp\n\n( )x\n.\n\n(18.12)\n\n(18.13)\n\nthis derivation made use of summation over discrete x, but a similar result\nin the continuous version of the\napplies using integration over continuous x.\nderivation, we use leibniz\u2019s rule for differentiation under the integral sign to obtain\nthe identity\n\n\u2202\n\n\u2202\u03b8 z \u02dcp\n\nd( )x x =z \u2202\n\n\u2202\u03b8\n\n\u02dcp\n\n( )x x\nd .\n\n(18.14)\n\nthis identity is only applicable under certain regularity conditions on \u02dcp and \u2202\n\u2202\u03b8 \u02dcp(x).\nin measure theoretic terms, the conditions are: (i) \u02dcp must be a lebesgue-integrable\nfunction of x for every value of \u03b8 ; (ii) \u2202\n\u2202\u03b8 \u02dcp(x) must exist for all \u03b8 and almost\nall x; (iii) there must exist an integrable function r(x) that bounds \u2202\n\u2202\u03b8 \u02dcp(x) (i.e.\n| \u2202\n\u2202\u03b8 \u02dcp(x)| \u2264 r(x) for all \u03b8 and almost all x). fortunately, most machine learning\nmodels of interest have these properties.\n\nthis identity\n\n\u2207\u03b8 log = z\n\ne\n\nx\n\nx\u223cp( )\u2207\u03b8 log \u02dcp( )x\n\n(18.15)\n\nis the basis for a variety of monte carlo methods for approximately maximizing\nthe likelihood of models with intractable partition functions.\n\nthe monte carlo approach to learning undirected models provides an intuitive\nframework in which we can think of both the positive phase and the negative\nphase. in the positive phase, we increase log \u02dcp(x) for x drawn from the data. in\nthe negative phase, we decrease the partition function by decreasing log \u02dcp(x) drawn\nfrom the model distribution.\n\nin the deep learning literature, it is common to parametrize log \u02dcp in terms of\nan energy function (eq.\u00a0\n). in this case, we can interpret the positive phase\nas pushing down on the energy of training examples and the negative phase as\npushing up on the energy of samples drawn from the model, as illustrated in fig.\n18.1.\n\n16.7\n\n18.2 stochastic maximum likelihood and contrastive\n\ndivergence\n\nthe naive way of implementing eq.\nis to compute it by burning in a set\nof markov chains from a random initialization every time the gradient is needed.\n\n18.15\n\n610\n\n "}, {"Page_number": 626, "text": "chapter 18. confronting the partition function\n\nwhen learning is performed using stochastic gradient descent, this means the\nchains must be burned in once per gradient step. this approach leads to the\ntraining procedure presented in algorithm\n. the high cost of burning in the\nmarkov chains in the inner loop makes this procedure computationally infeasible,\nbut this procedure is the starting point that other more practical algorithms aim\nto approximate.\n\n18.1\n\nalgorithm 18.1 a naive mcmc algorithm for maximizing the log-likelihood\nwith an intractable partition function using gradient ascent.\n\n(cid:115)\n\nset , the step size, to a small positive number.\nset k, the number of gibbs steps, high enough to allow burn in. perhaps 100 to\ntrain an rbm on a small image patch.\nwhile not converged do\nsample a minibatch of\ng \u2190 1\ninitialize a set of m samples {\u02dcx(1), . . . , \u02dcx(\n)m } to random values (e.g., from\na uniform or normal distribution, or possibly a distribution with marginals\nmatched to the model\u2019s marginals).\nfor\n\n)m } from the training set.\n\ni=1 \u2207\u03b8 log \u02dcp(x( )i ; )\u03b8 .\n\nmpm\n\n{x(1), . . . , x(\n\nexamples\n\nm\n\ni\nfor\n\ndo\nm\n\nk\n= 1 to\n\n= 1 to\nj\n\u02dcx( )j \u2190 gibbs_update(\u02dcx( )j ).\n\ndo\n\nend for\n\nend for\ng\u2190 \u2212 1\ng\n\u03b8\u2190 + (cid:115) .g\n\u03b8\nend while\n\nmpm\n\ni=1 \u2207\u03b8 log \u02dcp(\u02dcx( )i ; )\u03b8 .\n\nwe can view the mcmc approach to maximum likelihood as trying to achieve\nbalance between two forces, one pushing up on the model distribution where the\ndata occurs, and another pushing down on the model distribution where the model\nsamples occur. fig.\nillustrates this process. the two forces correspond to\nmaximizing log \u02dcp and minimizing log z . several approximations to the negative\nphase are possible. each of these approximations can be understood as making\nthe negative phase computationally cheaper but also making it push down in the\nwrong locations.\n\n18.1\n\nbecause the negative phase involves drawing samples from the model\u2019s distri-\nbution, we can think of it as finding points that the model believes in strongly.\nbecause the negative phase acts to reduce the probability of those points, they\nare generally considered to represent the model\u2019s incorrect beliefs about the world.\n\n611\n\n "}, {"Page_number": 627, "text": "chapter 18. confronting the partition function\n\nthe positive phase\n\nthe negative phase\n\npmodel( )x\npdata( )x\n\npmodel ( )x\npdata( )x\n\n)\nx\n(\np\n\n)\nx\n(\np\n\nx\n\nx\n\n18.1\n\nas having a \u201cpositive phase\u201d and \u201cnegative phase.\u201d\nfigure 18.1: the view of algorithm\n(left) in the positive phase, we sample points from the data distribution, and push up on\ntheir unnormalized probability. this means points that are likely in the data get pushed\nup on more.\u00a0(right) in the negative phase, we sample points from the model distribution,\nand push down on their unnormalized probability. this counteracts the positive phase\u2019s\ntendency to just add a large constant to the unnormalized probability everywhere. when\nthe data distribution and the model distribution are equal, the positive phase has the\nsame chance to push up at a point as the negative phase has to push down. when this\noccurs, there is no longer any gradient (in expectation) and training must terminate.\n\n,\n\ncrick and mitchison 1983\n\nthey are frequently referred to in the literature as \u201challucinations\u201d\u00a0or \u201cfantasy\nparticles.\u201d in fact, the negative phase has been proposed as a possible explanation\nfor dreaming in humans and other animals (\n), the idea\nbeing that the brain maintains a probabilistic model of the world and follows the\ngradient of log \u02dcp while experiencing real events while awake and follows the negative\ngradient of log \u02dcp to minimize log z while sleeping and experiencing events sampled\nfrom the current model. this view explains much of the language used to describe\nalgorithms with a positive and negative phase, but it has not been proven to be\ncorrect with neuroscientific experiments. in machine learning models, it is usually\nnecessary to use the positive and negative phase simultaneously, rather than in\nseparate time periods of wakefulness and rem sleep. as we will see in sec.\n19.5\n,\nother machine learning algorithms draw samples from the model distribution for\nother purposes and such algorithms could also provide an account for the function\nof dream sleep.\n\ngiven this understanding of the role of the positive and negative phase of\n.\n18.1\nlearning, we can attempt to design a less expensive alternative to algorithm\nthe main cost of the naive mcmc algorithm is the cost of burning in the markov\n\n612\n\n "}, {"Page_number": 628, "text": "chapter 18. confronting the partition function\n\nchains from a random initialization at each step. a natural solution is to initialize\nthe markov chains from a distribution that is very close to the model distribution,\nso that the burn in operation does not take as many steps.\n\n,\n\n,\n\n). this approach is presented as algorithm\n\nthe contrastive divergence (cd, or cd-k to indicate cd with k gibbs steps)\nalgorithm initializes the markov chain at each step with samples from the data\ndistribution (hinton 2000 2010\n18.2\n.\nobtaining samples from the data distribution is free, because they are already\navailable in the data set. initially, the data distribution is not close to the model\ndistribution, so the negative phase is not very accurate. fortunately, the positive\nphase can still accurately increase the model\u2019s probability of the data. after the\npositive phase has had some time to act, the model distribution is closer to the\ndata distribution, and the negative phase starts to become accurate.\n\nalgorithm 18.2 the contrastive divergence algorithm, using gradient ascent as\nthe optimization procedure.\n\n(cid:115)\n\nset , the step size, to a small positive number.\nset k, the number of gibbs steps, high enough to allow a markov chain sampling\nfrom p(x; \u03b8) to mix when initialized from pdata. perhaps 1-20 to train an rbm\non a small image patch.\nwhile not converged do\nsample a minibatch of\nmpm\ng \u2190 1\nfor\n= 1 to\ni\nm\n\u02dcx( )i \u2190 x( )i .\n= 1 to\nk\ni\n= 1 to\nfor\nj\n\u02dcx( )j \u2190 gibbs_update(\u02dcx( )j ).\n\n)m } from the training set.\n\ni=1 \u2207\u03b8 log \u02dcp(x( )i ; )\u03b8 .\n\n{x(1), . . . , x(\n\nend for\nfor\n\nexamples\n\ndo\nm\n\ndo\n\ndo\n\nm\n\nend for\n\nend for\ng\u2190 \u2212 1\ng\n\u03b8\u2190 + (cid:115) .g\n\u03b8\nend while\n\nmpm\n\ni=1 \u2207\u03b8 log \u02dcp(\u02dcx( )i ; )\u03b8 .\n\nof course, cd is still an approximation to the correct negative phase. the\nmain way that cd qualitatively fails to implement the correct negative phase\nis that it fails to suppress regions of high probability that are far from actual\ntraining examples. these regions that have high probability under the model but\nlow probability under the data generating distribution are called spurious modes.\n\n613\n\n "}, {"Page_number": 629, "text": "chapter 18. confronting the partition function\n\na spurious mode\n\npmodel ( )x\npdata( )x\n\n)\nx\n(\np\n\nx\n\nfigure 18.2: an illustration of how the negative phase of contrastive divergence (algorithm\n18.2) can fail to suppress spurious modes. a spurious mode is a mode that is present in\nthe model distribution but absent in the data distribution. because contrastive divergence\ninitializes its markov chains from data points and runs the markov chain for only a\nfew steps, it is unlikely to visit modes in the model that are far from the data points.\nthis means that when sampling from the model, we will sometimes get samples that do\nnot resemble the data.\u00a0it also means that due to wasting some of its probability mass\non these modes, the model will struggle to place high probability mass on the correct\nmodes. for the purpose of visualization, this figure uses a somewhat simplified concept\nof distance\u2014the spurious mode is far from the correct mode along the number line in\nr. this corresponds to a markov chain based on making local moves with a single x\nvariable in r. for most deep probabilistic models, the markov chains are based on gibbs\nsampling and can make non-local moves of individual variables but cannot move all of\nthe variables simultaneously. for these problems, it is usually better to consider the edit\ndistance between modes, rather than the euclidean distance. however, edit distance in a\nhigh dimensional space is difficult to depict in a 2-d plot.\n\n18.2\n\nillustrates why this happens. essentially, it is because modes in the\nfig.\nmodel distribution that are far from the data distribution will not be visited by\nmarkov chains initialized at training points, unless\n\nis very large.\n\nk\n\n(\n\ncarreira-perpi\u00f1an and hinton 2005\n\n) showed experimentally that the cd\nestimator is biased for rbms and fully visible boltzmann machines, in that it\nconverges to different points than the maximum likelihood estimator. they argue\nthat because the bias is small, cd could be used as an inexpensive way to initialize\na model that could later be fine-tuned via more expensive mcmc methods. bengio\nand delalleau 2009\n) showed that cd can be interpreted as discarding the smallest\nterms of the correct mcmc update gradient, which explains the bias.\n\n(\n\ncd is useful for training shallow models like rbms. these can in turn be\n\n614\n\n "}, {"Page_number": 630, "text": "chapter 18. confronting the partition function\n\nstacked to initialize deeper models like dbns or dbms.\u00a0however, cd does not\nprovide much help for training deeper models directly. this is because it is difficult\nto obtain samples of the hidden units given samples of the visible units. since the\nhidden units are not included in the data, initializing from training points cannot\nsolve the problem. even if we initialize the visible units from the data, we will still\nneed to burn in a markov chain sampling from the distribution over the hidden\nunits conditioned on those visible samples.\n\nthe cd algorithm can be thought of as penalizing the model for having a\nmarkov chain that changes the input rapidly when the input comes from the data.\nthis means training with cd somewhat resembles autoencoder training. even\nthough cd is more biased than some of the other training methods, it can be\nuseful for pretraining shallow models that will later be stacked. this is because\nthe earliest models in the stack are encouraged to copy more information up to\ntheir latent variables, thereby making it available to the later models. this should\nbe thought of more of as an often-exploitable side effect of cd training rather than\na principled design advantage.\n\nsutskever and tieleman 2010\n\n) showed that the cd update direction is not the\ngradient of any function. this allows for situations where cd could cycle forever,\nbut in practice this is not a serious problem.\n\n(\n\na different strategy that resolves many of the problems with cd is to initialize\nthe markov chains at each gradient step with their states from the previous gradient\nstep. this approach was first discovered under the name stochastic maximum\nlikelihood (sml) in the applied mathematics and statistics community (younes,\n1998) and later independently rediscovered under the name persistent contrastive\ndivergence (pcd, or pcd-k to indicate the use of k gibbs steps per update) in\nthe deep learning community (\n.\u00a0the basic\nidea of this approach is that, so long as the steps taken by the stochastic gradient\nalgorithm are small, then the model from the previous step will be similar to the\nmodel from the current step. it follows that the samples from the previous model\u2019s\ndistribution will be very close to being fair samples from the current model\u2019s\ndistribution, so a markov chain initialized with these samples will not require much\ntime to mix.\n\n).\u00a0see algorithm\n\ntieleman 2008\n\n18.3\n\n,\n\nbecause each markov chain is continually updated throughout the learning\nprocess, rather than restarted at each gradient step, the chains are free to wander\nfar enough to find all of the model\u2019s modes. sml is thus considerably more\nresistant to forming models with spurious modes than cd is. moreover, because\nit is possible to store the state of all of the sampled variables, whether visible or\nlatent, sml provides an initialization point for both the hidden and visible units.\n\n615\n\n "}, {"Page_number": 631, "text": "chapter 18. confronting the partition function\n\net al. (\n\ncd is only able to provide an initialization for the visible units, and therefore\nrequires burn-in for deep models. sml is able to train deep models efficiently.\nmarlin\n) compared sml to many of the other criteria presented in\nthis chapter. they found that sml results in the best test set log-likelihood for\nan rbm, and that if the rbm\u2019s hidden units are used as features for an svm\nclassifier, sml results in the best classification accuracy.\n\n2010\n\nsml is vulnerable to becoming inaccurate if the stochastic gradient algorithm\ncan move the model faster than the markov chain can mix between steps. this\ncan happen if k is too small or (cid:115) is too large. the permissible range of values is\nunfortunately highly problem-dependent. there is no known way to test formally\nwhether the chain is successfully mixing between steps. subjectively, if the learning\nrate is too high for the number of gibbs steps, the human operator will be able\nto observe that there is much more variance in the negative phase samples across\ngradient steps rather than across different markov chains. for example, a model\ntrained on mnist might sample exclusively 7s on one step. the learning process\nwill then push down strongly on the mode corresponding to 7s, and the model\nmight sample exclusively 9s on the next step.\n\nalgorithm 18.3 the stochastic maximum likelihood / persistent contrastive\ndivergence algorithm using gradient ascent as the optimization procedure.\n\n(cid:115)\n\nset , the step size, to a small positive number.\nset k, the number of gibbs steps, high enough to allow a markov chain sampling\nfrom p(x; \u03b8 + (cid:115)g) to burn in, starting from samples from p(x; \u03b8). perhaps 1 for\nrbm on a small image patch, or 5-50 for a more complicated model like a dbm.\ninitialize a set of m samples { \u02dcx(1) , . . . , \u02dcx(\n)m } to random values (e.g., from a\nuniform or normal distribution, or possibly a distribution with marginals matched\nto the model\u2019s marginals).\nwhile not converged do\nsample a minibatch of\nmpm\ng \u2190 1\n= 1 to\nfor\ni\nfor\nj\n\u02dcx( )j \u2190 gibbs_update(\u02dcx( )j ).\n\ni=1 \u2207\u03b8 log \u02dcp(x( )i ; )\u03b8 .\nk\n= 1 to\n\n)m } from the training set.\n\n{x(1), . . . , x(\n\nexamples\n\ndo\nm\n\ndo\n\nm\n\nend for\n\nend for\ng\u2190 \u2212 1\ng\n\u03b8\u2190 + (cid:115) .g\n\u03b8\nend while\n\nmpm\n\ni=1 \u2207\u03b8 log \u02dcp(\u02dcx( )i ; )\u03b8 .\n\n616\n\n "}, {"Page_number": 632, "text": "chapter 18. confronting the partition function\n\ncare must be taken when evaluating the samples from a model trained with\nsml. it is necessary to draw the samples starting from a fresh markov chain\ninitialized from a random starting point after the model is done training. the\nsamples present in the persistent negative chains used for training have been\ninfluenced by several recent versions of the model, and thus can make the model\nappear to have greater capacity than it actually does.\n\n(\n\nberglund and raiko 2013\n\n) performed experiments to examine the bias and\nvariance in the estimate of the gradient provided by cd and sml. cd proves to\nhave lower variance than the estimator based on exact sampling. sml has higher\nvariance. the cause of cd\u2019s low variance is its use of the same training points\nin both the positive and negative phase. if the negative phase is initialized from\ndifferent training points, the variance rises above that of the estimator based on\nexact sampling.\n\nall of these methods based on using mcmc to draw samples from the model\ncan in principle be used with almost any variant of mcmc. this means that\ntechniques such as sml can be improved by using any of the enhanced mcmc\ntechniques described in chapter\ndesjardins et al.\n,\n2010 cho\n\n, such as parallel tempering (\n\net al.,\n\n2010\n\n17\n\n).\n\n;\n\none approach to accelerating mixing during learning relies not on changing the\nmonte carlo sampling technology but rather on changing the parametrization of\nthe model and the cost function. fast pcd or fpcd (tieleman and hinton 2009\n)\ninvolves replacing the parameters\n\nof a traditional model with an expression\n\n\u03b8\n\n,\n\n\u03b8\n\n\u03b8=  (\n\nslow + \u03b8(\n\n)\n\n)\nfast .\n\n(18.16)\n\nthere are now twice as many parameters as before, and they are added together\nelement-wise to provide the parameters used by the original model definition. the\nfast copy of the parameters is trained with a much larger learning rate, allowing\nit to adapt rapidly in response to the negative phase of learning and push the\nmarkov chain to new territory. this forces the markov chain to mix rapidly, though\nthis effect only occurs during learning while the fast weights are free to change.\ntypically one also applies significant weight decay to the fast weights, encouraging\nthem to converge to small values, after only transiently taking on large values long\nenough to encourage the markov chain to change modes.\n\none key benefit to the mcmc-based methods described in this section is that\nthey provide an estimate of the gradient of log z , and thus we can essentially\ndecompose the problem into the log \u02dcp contribution and the log z contribution.\nwe can then use any other method to tackle log \u02dcp(x), and just add our negative\nphase gradient onto the other method\u2019s gradient. in particular, this means that\n\n617\n\n "}, {"Page_number": 633, "text": "chapter 18. confronting the partition function\n\nour positive phase can make use of methods that provide only a lower bound on\n\u02dcp. most of the other methods of dealing with log z presented in this chapter are\nincompatible with bound-based positive phase methods.\n\n18.3 pseudolikelihood\n\nmonte carlo approximations to the partition function and its gradient directly\nconfront the partition function. other approaches sidestep the issue, by training\nthe model without computing the partition function. most of these approaches are\nbased on the observation that it is easy to compute ratios of probabilities in an\nundirected probabilistic model. this is because the partition function appears in\nboth the numerator and the denominator of the ratio and cancels out:\n\np( )x\np( )y\n\n=\n\n1\nz \u02dcp( )x\n1\nz \u02dcp( )y\n\n=\n\n\u02dcp( )x\n\u02dcp( )y\n\n.\n\n(18.17)\n\nthe pseudolikelihood is based on the observation that conditional probabilities\ntake this ratio-based form, and thus can be computed without knowledge of the\npartition function. suppose that we partition x into a, b and c, where a contains\nthe variables we want to find the conditional distribution over, b contains the\nvariables we want to condition on, and c contains the variables that are not part\nof our query.\n\np(\n\na b|\n\n) =\n\np ,(a b)\np( )b\n\n=\n\np ,(a b)\n\npa c, p ,\n\n(a b c)\n\n,\n\n=\n\n\u02dcp ,(a b)\n\np a c, \u02dcp ,\n\n(a b c)\n\n,\n\n.\n\n(18.18)\n\nthis quantity requires marginalizing out a, which can be a very efficient operation\nprovided that a and c do not contain very many variables. in the extreme case, a\ncan be a single variable and c can be empty, making this operation require only as\nmany evaluations of \u02dcp as there are values of a single random variable.\n\nunfortunately, in order to compute the log-likelihood, we need to marginalize\nout large sets of variables. if there are n variables total, we must marginalize a set\nof size\n\n. by the chain rule of probability,\n\np x 1) + log (p x2 | x1) +\n\n\u00b7\u00b7\u00b7\n\n+ (\n\np xn | x1:\n\n1n\u2212 ).\n\n(18.19)\n\nin this case, we have made a maximally small, but c can be as large as x2:n . what\nif we simply move c into b to reduce the computational cost? this yields the\n) objective function, based on predicting the value of\npseudolikelihood (\n\nbesag 1975\n\n,\n\n618\n\nn \u2212 1\nlog ( ) = log (\n\np x\n\n "}, {"Page_number": 634, "text": "chapter 18. confronting the partition function\n\nfeature xi given all of the other features x\u2212i:\n\nnxi=1\n\nlog (p x i | x\u2212i).\n\n(18.20)\n\nif each random variable has k different values, this requires only k n\u00d7 evaluations\nof \u02dcp to compute, as opposed to the kn evaluations needed to compute the partition\nfunction.\n\nthis may look like an unprincipled hack, but it can be proven that estimation\nmase 1995\n).\nby maximizing the pseudolikelihood is asymptotically consistent (\nof course, in the case of datasets that do not approach the large sample limit,\npseudolikelihood may display different behavior from the maximum likelihood\nestimator.\n\n,\n\n,\n\nit is possible to trade computational complexity for deviation from maximum\nlikelihood behavior by using the generalized pseudolikelihood estimator (huang and\nogata 2002\n). the generalized pseudolikelihood estimator uses m different sets\ns( )i , i = 1, . . . , m of indices of variables that appear together on the left side of the\nconditioning bar. in the extreme case of m = 1 and s(1) = 1, . . . , n the generalized\npseudolikelihood recovers the log-likelihood. in the extreme case of m = n and\ns( )i = { }i , the generalized pseudolikelihood recovers the pseudolikelihood. the\ngeneralized pseudolikelihood objective function is given by\n\nmxi=1\n\nlog (p xs( )i\n\n| x\u2212s( )i ).\n\n(18.21)\n\nthe performance of pseudolikelihood-based approaches depends largely on how\nthe model will be used. pseudolikelihood tends to perform poorly on tasks that\nrequire a good model of the full joint p(x), such as density estimation and sampling.\nhowever, it can perform better than maximum likelihood for tasks that require only\nthe conditional distributions used during training, such as filling in small amounts\nof missing values. generalized pseudolikelihood techniques are especially powerful if\nthe data has regular structure that allows the s index sets to be designed to capture\nthe most important correlations while leaving out groups of variables that only\nhave negligible correlation. for example, in natural images, pixels that are widely\nseparated in space also have weak correlation, so the generalized pseudolikelihood\ncan be applied with each\n\nset being a small, spatially localized window.\n\ns\n\none weakness of the pseudolikelihood estimator is that it cannot be used with\nother approximations that provide only a lower bound on \u02dcp(x), such as variational\n\u02dcp appears in the\ninference, which will be covered in chapter\n\n. this is because\n\n19\n\n619\n\n "}, {"Page_number": 635, "text": "chapter 18. confronting the partition function\n\ndenominator. a lower bound on the denominator provides only an upper bound on\nthe expression as a whole, and there is no benefit to maximizing an upper bound.\nthis makes it difficult to apply pseudolikelihood approaches to deep models such\nas deep boltzmann machines, since variational methods are one of the dominant\napproaches to approximately marginalizing out the many layers of hidden variables\nthat interact with each other.\u00a0however, pseudolikelihood is still useful for deep\nlearning, because it can be used to train single layer models, or deep models using\napproximate inference methods that are not based on lower bounds.\n\npseudolikelihood has a much greater cost per gradient step than sml, due to\nits explicit computation of all of the conditionals. however, generalized pseudo-\nlikelihood and similar criteria can still perform well if only one randomly selected\nconditional is computed per example (\n), thereby bringing\nthe computational cost down to match that of sml.\n\ngoodfellow et al. 2013b\n\n,\n\nthough the pseudolikelihood estimator does not explicitly minimize log z, it\ncan still be thought of as having something resembling a negative phase. the\ndenominators of each conditional distribution result in the learning algorithm\nsuppressing the probability of all states that have only one variable differing from\na training example.\n\nsee marlin and de freitas 2011\n\n(\n\n) for a theoretical analysis of the asymptotic\n\nefficiency of pseudolikelihood.\n\n18.4 score matching and ratio matching\n\n,\n\nhyv\u00e4rinen 2005\n\n) provides another consistent means of training a\nscore matching (\nmodel without estimating z or its derivatives. the name score matching comes\nfrom terminology in which the derivatives of a log density with respect to its\nargument, \u2207x log p( x), are called its score. the strategy used by score matching is\nto minimize the expected squared difference between the derivatives of the model\u2019s\nlog density with respect to the input and the derivatives of the data\u2019s log density\nwith respect to the input:\n1\n2||\u2207x log pmodel( ; )x \u03b8 \u2212 \u2207x log pdata( )x ||2\n1\nepdata( )x l ,(x \u03b8)\n2\n\nl ,(x \u03b8) =\n\nj( ) =\u03b8\n\n(18.22)\n\n(18.23)\n\n2\n\n\u03b8\u2217 = min\n\u03b8\n\nj( )\u03b8\n\n(18.24)\n\nthis objective function avoids the difficulties associated with differentiating\nthe partition function z because z is not a function of x and therefore \u2207xz = 0.\n\n620\n\n "}, {"Page_number": 636, "text": "chapter 18. confronting the partition function\n\ninitially, score matching appears to have a new difficulty:\u00a0computing the score\nof the data distribution requires knowledge of the true distribution generating\nthe training data, pdata. fortunately, minimizing the expected value of\nis\nequivalent to minimizing the expected value of\n\nl ,(x \u03b8)\n\n\u02dcl ,(x \u03b8) =\n\nnxj=1\u00a0 \u22022\n\n\u2202x2\nj\n\nlog pmodel( ; ) +\n\nx \u03b8\n\nwhere\n\nn\n\nis the dimensionality of\n\n.\nx\n\n1\n\n2(cid:25) \u2202\n\n\u2202xj\n\nlog pmodel( ; )x \u03b8 (cid:26)2!\n\n(18.25)\n\nbecause score matching requires taking derivatives with respect to x, it is not\napplicable to models of discrete data. however, the latent variables in the model\nmay be discrete.\n\nlike the pseudolikelihood, score matching only works when we are able to\nevaluate log \u02dcp(x) and its derivatives directly. it is not compatible with methods\nthat only provide a lower bound on log \u02dcp(x), because score matching requires\nthe derivatives and second derivatives of log \u02dcp(x) and a lower bound conveys no\ninformation about its derivatives. this means that score matching cannot be\napplied to estimating models with complicated interactions between the hidden\nunits, such as sparse coding models or deep boltzmann machines. while score\nmatching can be used to pretrain the first hidden layer of a larger model, it has\nnot been applied as a pretraining strategy for the deeper layers of a larger model.\nthis is probably because the hidden layers of such models usually contain some\ndiscrete variables.\n\nwhile score matching does not explicitly have a negative phase, it can be\nviewed as a version of contrastive divergence using a specific kind of markov chain\n(\n). the markov chain in this case is not gibbs sampling, but\nhyv\u00e4rinen 2007a\nrather a different approach that makes local moves guided by the gradient. score\nmatching is equivalent to cd with this type of markov chain when the size of the\nlocal moves approaches zero.\n\n,\n\n(\n\nlyu 2009\n\n) generalized score matching to the discrete case (but made an error\nmarlin et al. 2010 marlin et al. 2010\n)\nin their derivation that was corrected by\nfound that generalized score matching (gsm) does not work in high dimensional\ndiscrete spaces where the observed probability of many events is 0.\n\n)).\n\n(\n\n(\n\na more successful approach to extending the basic ideas of score matching\nto discrete data is ratio matching (\n). ratio matching applies\nspecifically to binary data. ratio matching consists of minimizing the average over\n\nhyv\u00e4rinen 2007b\n\n,\n\n621\n\n "}, {"Page_number": 637, "text": "chapter 18. confronting the partition function\n\nexamples of the following objective function:\n\nl(\n\n)rm (\n\nx \u03b8,\n\n) =\n\nnxj=1\n\n1\n\n1 + pmodel ( ; )x \u03b8\npmodel( ( ) ); )\nf x ,j \u03b8\n\n(cid:27)(cid:28)\n\n(cid:29)(cid:30)\n\n2\n\n,\n\n(18.26)\n\nf\n\nx \n\n, j\n(x )\n\nreturns with the bit at position\n\nflipped. ratio matching avoids\nwhere\nthe partition function using the same trick as the pseudolikelihood estimator: in a\nratio of two probabilities, the partition function cancels out.\nmarlin et al. 2010\n)\nfound that ratio matching outperforms sml, pseudolikelihood and gsm in terms\nof the ability of models trained with ratio matching to denoise test set images.\n\nj\n\n(\n\nlike the pseudolikelihood estimator, ratio matching requires n evaluations of \u02dcp\nper data point, making its computational cost per update roughly n times higher\nthan that of sml.\n\nas with the pseudolikelihood estimator, ratio matching can be thought of as\npushing down on all fantasy states that have only one variable different from a\ntraining example. since ratio matching applies specifically to binary data, this\nmeans that it acts on all fantasy states within hamming distance 1 of the data.\n\nratio matching can also be useful as the basis for dealing with high-dimensional\nsparse data, such as word count vectors. this kind of data poses a challenge for\nmcmc-based methods because the data is extremely expensive to represent in\ndense format, yet the mcmc sampler does not yield sparse values until the model\nhas learned to represent the sparsity in the data distribution. dauphin and bengio\n(\n) overcame this issue by designing an unbiased stochastic approximation to\n2013\nratio matching. the approximation evaluates only a randomly selected subset of\nthe terms of the objective, and does not require the model to generate complete\nfantasy samples.\n\nsee marlin and de freitas 2011\n\n(\n\n) for a theoretical analysis of the asymptotic\n\nefficiency of ratio matching.\n\n18.5 denoising score matching\n\nin some cases we may wish to regularize score matching, by fitting a distribution\n\npsmoothed ( ) =x\n\nz pdata( ) (\n\ny q x y|\n\n)\ndy\n\n(18.27)\n\nrather than the true pdata. the distribution q(x y|\none that forms\n\nby adding a small amount of noise to .\ny\n\nx\n\n) is a corruption process, usually\n\n622\n\n "}, {"Page_number": 638, "text": "chapter 18. confronting the partition function\n\ndenoising score matching is especially useful because in practice we usually do\nnot have access to the true pdata but rather only an empirical distribution defined\nby samples from it. any consistent estimator will, given enough capacity, make\npmodel into a set of dirac distributions centered on the training points. smoothing\nby q helps to reduce this problem, at the loss of the asymptotic consistency property\ndescribed in sec.\n) introduced a procedure for\nperforming regularized score matching with the smoothing distribution q being\nnormally distributed noise.\n\n5.4.5 kingma and lecun 2010\n\n(\n\n.\n\n14.5.1\u00a0\n\nrecall\u00a0from sec.\n\nthat several autoencoder\u00a0training\u00a0algorithms\u00a0are\nequivalent to score matching or denoising score matching. these autoencoder\ntraining algorithms are therefore\u00a0a way of\u00a0overcoming the partition\u00a0function\nproblem.\n\n18.6 noise-contrastive estimation\n\nmost techniques for estimating models with intractable partition functions do not\nprovide an estimate of the partition function. sml and cd estimate only the\ngradient of the log partition function, rather than the partition function itself.\nscore matching and pseudolikelihood avoid computing quantities related to the\npartition function altogether.\n\nnoise-contrastive estimation (nce) (gutmann and hyvarinen 2010\n\n) takes a\ndifferent strategy. in this approach, the probability distribution estimated by the\nmodel is represented explicitly as\n\n,\n\nlog p model( ) = log \u02dc\n\nx\n\npmodel( ; ) +\n\nx \u03b8\n\nc,\n\n(18.28)\n\nwhere c is explicitly introduced as an approximation of \u2212 log z (\u03b8). rather than\nestimating only \u03b8, the noise contrastive estimation procedure treats c as just\nanother parameter and estimates \u03b8 and c simultaneously, using the same algorithm\nfor both. the resulting log pmodel (x) thus may not correspond exactly to a valid\nprobability distribution, but will become closer and closer to being valid as the\nestimate of\n\n1\nimproves.\n\nc\n\nsuch an approach would not be possible using maximum likelihood as the\ncriterion for the estimator. the maximum likelihood criterion would choose to set\nto create a valid probability distribution.\nc\n\narbitrarily high, rather than setting\n\nc\n\n1nce is also applicable to problems with a tractable partition function, where there is no\nneed to introduce the extra parameter c. however, it has generated the most interest as a means\nof estimating models with difficult partition functions.\n\n623\n\n "}, {"Page_number": 639, "text": "chapter 18. confronting the partition function\n\nnce works by reducing the unsupervised learning problem of estimating p(x)\nto that of learning a probabilistic binary classifier in which one of the categories\ncorresponds to the data generated by the model. this supervised learning problem\nis constructed in such a way that maximum likelihood estimation in this supervised\nlearning problem defines an asymptotically consistent estimator of the original\nproblem.\n\nspecifically, we introduce a second distribution, the noise distribution pnoise(x).\nthe noise distribution should be tractable to evaluate and to sample from.\u00a0we\ncan now construct a model over both x and a new, binary class variable y . in the\nnew joint model, we specify that\n\nand\n\npjoint( = 1) =\n\ny\n\n1\n2\n\n,\n\npjoint(\n\nx | y\n\n= 1) = \n\npmodel ( )x ,\n\npjoint(\n\nx | y\n\n= 0) = \n\npnoise( )x .\n\n(18.29)\n\n(18.30)\n\n(18.31)\n\nin other words, y is a switch variable that determines whether we will generate x\nfrom the model or from the noise distribution.\n\nx | y\n\nwe can construct a similar joint model of training data. in this case, the\nor from the noise\n2, ptrain (x | y = 1) = pdata(x ),\u00a0and\n\nswitch variable determines whether we draw x from the\ndistribution. formally, ptrain(y = 1) = 1\nptrain(\n\npnoise( )x .\n\n= 0) = \n\ndata\n\nwe can now just use standard maximum likelihood learning on the supervised\n\nlearning problem of fitting p joint to ptrain :\n\n\u03b8, c = arg max\n\n\u03b8,c\n\nex,\n\npy\u223c train log pjoint (\n\ny | x .\n)\n\n(18.32)\n\nthe distribution pjoint is essentially a logistic regression model applied to the\n\ndifference in log probabilities of the model and the noise distribution:\n\npjoint( = 1 \n\ny\n\n) =\n\n| x\n\npmodel ( )x\n\npmodel( ) +x\n\npnoise( )x\n\n=\n\n1\n\n1 + pnoise( )x\npmodel ( )x\n\n=\n\n1\n\n1 + exp(cid:23)log pnoise ( )x\npmodel( )x (cid:24)\n\n624\n\n(18.33)\n\n(18.34)\n\n(18.35)\n\n "}, {"Page_number": 640, "text": "chapter 18. confronting the partition function\n\n= \u03c3(cid:25)\u2212 log\n\npnoise( )x\n\npmodel( )x (cid:26)\n\n=  (log\n\n\u03c3\n\npmodel( )\n\nx \u2212\n\nlog\n\npnoise ( ))x .\n\n(18.36)\n\n(18.37)\n\nnce is thus simple to apply so long as log \u02dcp model is easy to back-propagate\nthrough, and, as specified above, pnoise is easy to evaluate (in order to evaluate\npjoint) and sample from (in order to generate the training data).\n\nnce is most successful when applied to problems with few random variables,\nbut can work well even if those random variables can take on a high number of\nvalues. for example, it has been successfully applied to modeling the conditional\ndistribution over a word given the context of the word (mnih and kavukcuoglu,\n2013). though the word may be drawn from a large vocabulary, there is only one\nword.\n\nwhen nce is applied to problems with many random variables, it becomes less\nefficient. the logistic regression classifier can reject a noise sample by identifying\nany one variable whose value is unlikely. this means that learning slows down\ngreatly after pmodel has learned the basic marginal statistics. imagine learning a\nmodel of images of faces, using unstructured gaussian noise as pnoise . if pmodel\nlearns about eyes, it can reject almost all unstructured noise samples without\nhaving learned anything about other facial features, such as mouths.\n\nthe constraint that pnoise must be easy to evaluate and easy to sample from\ncan be overly restrictive. when pnoise is simple, most samples are likely to be too\nobviously distinct from the data to force pmodel to improve noticeably.\n\nlike score matching and pseudolikelihood, nce does not work if only a lower\nbound on \u02dcp is available. such a lower bound could be used to construct a lower\nbound on pjoint(y = 1 | x), but it can only be used to construct an upper bound on\npjoint(y = 0 | x), which appears in half the terms of the nce objective. likewise,\na lower bound on pnoise is not useful, because it provides only an upper bound on\npjoint( = 1 \n\ny\n\nwhen the model distribution is copied to define a new noise distribution before\neach gradient step, nce defines a procedure called self-contrastive estimation,\nwhose\u00a0expected\u00a0gradient is equivalent\u00a0to the\u00a0expected\u00a0gradient of maximum\nlikelihood (goodfellow 2014\n). the special case of nce where the noise samples\nare\u00a0those generated by the model\u00a0suggests\u00a0that maximum likelihood\u00a0can\u00a0be\ninterpreted as a procedure that forces a model to constantly learn to distinguish\nreality from its own evolving beliefs, while noise contrastive estimation achieves\nsome reduced computational cost by only forcing the model to distinguish reality\nfrom a fixed baseline (the noise model).\n\n,\n\n| x .\n)\n\n625\n\n "}, {"Page_number": 641, "text": "chapter 18. confronting the partition function\n\nusing the supervised task of classifying between training samples and generated\nsamples (with the model energy function used in defining the classifier) to provide\na gradient on the model was introduced earlier in various forms (welling et al.,\n2003b bengio 2009\n\n).\n\n;\n\n,\n\nnoise contrastive estimation is based on the idea that a good generative model\nshould be able to distinguish data from noise. a closely related idea is that\na good generative model should be able to generate samples that no classifier\ncan distinguish from data. this idea yields generative adversarial networks (sec.\n20.10.4).\n\n18.7 estimating the partition function\n\nwhile much of this chapter is dedicated to describing methods that avoid needing\nto compute the intractable partition function z (\u03b8 ) associated with an undirected\ngraphical model, in this section we discuss several methods for directly estimating\nthe partition function.\n\nestimating the partition function can be important because we require it if\nwe wish to compute the normalized likelihood of data. this is often important in\nevaluating the model, monitoring training performance, and comparing models to\neach other.\n\nfor example, imagine we have two models: model ma defining a probabil-\nity distribution pa(x; \u03b8a) = 1\n\u02dcpa(x;\u03b8a) and model m b defining a probability\nza\ndistribution pb(x; \u03b8b) = 1\n\u02dcpb(x;\u03b8b ). a common way to compare the models\nzb\nis to evaluate and compare the likelihood that both models assign to an i.i.d.\ntest dataset. suppose the test set consists of m examples {x(1), . . . , x(\n)m }. if\nqi pa(x( )i ; \u03b8a) >qi p b(x( )i ; \u03b8 b) or equivalently if\n\nlog pb(x( )i ; \u03b8b) \n\n0> ,\n\n(18.38)\n\nxi\n\nlog pa(x( )i ; \u03b8a) \u2212xi\n\nthen we say that ma is a better model than mb (or, at least, it is a better model\nof the test set), in the sense that it has a better test log-likelihood. unfortunately,\ntesting whether this condition holds requires knowledge of the partition function.\nunfortunately, eq.\nseems to require evaluating the log probability that\nthe model assigns to each point, which in turn requires evaluating the partition\nfunction. we can simplify the situation slightly by re-arranging eq.\ninto a\n\n18.38\n\n18.38\n\n626\n\n "}, {"Page_number": 642, "text": "chapter 18. confronting the partition function\n\nform where we need to know only the ratio of the two model\u2019s partition functions:\n\nxi\n\nlog pa(x ( )i ; \u03b8a)\u2212xi\n\nlog p b(x( )i ; \u03b8 b) =xi\n\nz(\u03b8a)\nz(\u03b8b)\n(18.39)\nwe can thus determine whether ma is a better model than m b without knowing\nthe partition function of either model but only their ratio. as we will see shortly,\nwe can estimate this ratio using importance sampling, provided that the two models\nare similar.\n\n\u02dcpb(x( )i ; \u03b8b)!\u2212m log\n\n\u00a0 log\n\n\u02dcpa(x( )i ; \u03b8a)\n\n.\n\nif, however, we wanted to compute the actual probability of the test data under\neither ma or mb , we would need to compute the actual value of the partition\nfunctions. that said, if we knew the ratio of two partition functions, r = z(\u03b8b)\nz(\u03b8a) ,\nand we knew the actual value of just one of the two, say z(\u03b8a), we could compute\nthe value of the other:\n\nz(\u03b8b ) = \n\n(rz \u03b8a) =\n\nz(\u03b8b )\nz(\u03b8a )\n\nz(\u03b8a).\n\n(18.40)\n\na simple\u00a0way to estimate the partition\u00a0function is to\u00a0use a\u00a0monte\u00a0carlo\nmethod such as simple importance sampling. we present the approach in terms\nof continuous variables using integrals, but it can be readily applied to discrete\nvariables by replacing the integrals with summation. we use a proposal distribution\np0(x) = 1\n\u02dcp 0(x) which supports tractable sampling and tractable evaluation of\nz0\nboth the partition function z0 and the unnormalized distribution \u02dcp0 ( )x .\n\np0 ( )x\n\nz1 =z \u02dcp1( )x dx\n=z p0 ( )x\n= z0z p0( )x\nkxk=1\n\n\u02c6z1 =\n\nz0\nk\n\n\u02dcp1( )x dx\n\n\u02dcp 1( )x\n\u02dcp 0( )x\n\n\u02dcp1 (x( )k )\n\u02dcp0 (x( )k )\n\n(18.41)\n\n(18.42)\n\n(18.43)\n\n(18.44)\n\ndx\n\n. . x( )k \u223c p 0\ns t :\n\nin the last line, we make a monte carlo estimator, \u02c6z 1, of the integral using samples\ndrawn from p0 (x) and then weight each sample with the ratio of the unnormalized\n\u02dcp 1 and the proposal p 0 .\n\nwe see also that this approach allows us to estimate the ratio between the\n\n627\n\n "}, {"Page_number": 643, "text": "chapter 18. confronting the partition function\n\npartition functions as\n\n1\nk\n\nkxk=1\n\n\u02dcp1(x( )k )\n\u02dcp0(x( )k )\n\n. . x( )k \u223c p0.\ns t :\n\n(18.45)\n\nthis value can then be used directly to compare two models as described in eq.\n18.39.\n\nif the distribution p0 is close to p1, eq.\n\n18.44\ncan be an effective way of\nestimating the partition function (minka 2005\n). unfortunately, most of the time\np1 is both complicated (usually multimodal) and defined over a high dimensional\nspace. it is difficult to find a tractable p0 that is simple enough to evaluate while\nstill being close enough to p1 to result in a high quality approximation. if p0 and\np1 are not close, most samples from p0 will have low probability under p1 and\ntherefore make (relatively) negligible contribution to the sum in eq.\n\n.\n18.44\n\n,\n\nhaving few\u00a0samples with\u00a0significant\u00a0weights in this sum will result in\u00a0an\nestimator that is of poor quality due to high variance. this can be understood\nquantitatively through an estimate of the variance of our estimate \u02c6z1 :\n\n\u02c6var(cid:23) \u02c6z1(cid:24) =\n\nz0\nk2\n\nkxk=1\u00a0 \u02dcp 1(x( )k )\n\n\u02dcp 0(x( )k ) \u2212 \u02c6z1!2\n\n.\n\n(18.46)\n\nthis quantity is largest when there is significant deviation in the values of the\nimportance weights \u02dcp1 (x( )k )\n\u02dcp0 (x( )k )\n\n.\n\nwe now turn to two related strategies developed to cope with the challeng-\ning task of estimating partition functions for complex distributions over high-\ndimensional spaces: annealed importance sampling and bridge sampling. both\nstart with the simple importance sampling strategy introduced above and both\nattempt to overcome the problem of the proposal p0 being too far from p1 by\nintroducing intermediate distributions that attempt to\np0\nand p1 .\n\nbridge the gap\n\nbetween\n\n18.7.1 annealed importance sampling\n\nin situations where dkl(p0kp 1) is large (i.e., where there is little overlap between\np0 and p1), a strategy called annealed importance sampling (ais) attempts to\nbridge the gap by introducing intermediate distributions (\n;\njarzynski 1997 neal\n,\n<\u00b7\u00b7\u00b7\n2001). consider a sequence of distributions p\u03b70, . . . , p\u03b7n, with 0 = \u03b70 < \u03b71 <\n\u03b7n\u22121 < \u03b7n = 1 so that the first and last distributions in the sequence are p0 and p 1\nrespectively.\n\n,\n\n628\n\n "}, {"Page_number": 644, "text": "chapter 18. confronting the partition function\n\nthis approach allows us to estimate the partition function of a multimodal\ndistribution defined over a high-dimensional space (such as the distribution defined\nby a trained rbm). we begin with a simpler model with a known partition function\n(such as an rbm with zeroes for weights) and estimate the ratio between the two\nmodel\u2019s partition functions.\u00a0the estimate of this ratio is based on the estimate\nof the ratios of a sequence of many similar distributions, such as the sequence of\nrbms with weights interpolating between zero and the learned weights.\n\nwe can now write the ratio z1\nz0\n\nas\n\nz1\nz0\n\nz\u03b7 n\u22121\nz\u03b7 n\u22121\nz\u03b7n\u22121\nz\u03b7n\u22122\n\nz\u03b71\nz\u03b71 \u00b7\u00b7\u00b7\nz\u03b72\nz\u03b71 \u00b7\u00b7\u00b7\nz\u03b7j+1\nz\u03b7j\n\n=\n\n=\n\n=\n\nz1\nz0\nz\u03b71\nz0\n\nn\u22121yj=0\n\nz1\nz \u03b7n\u22121\n\n(18.47)\n\n(18.48)\n\n(18.49)\n\n1, are sufficiently\n\nprovided the distributions p\u03b7j and p\u03b7j+1, for all 0 \u2264 \u2264 \u2212\nclose, we can reliably estimate each of the factors\nsampling and then use these to obtain an estimate of z1\nz0\n\nj\nz\u03b7j+1\nz\u03b7j\n\nn\n\n.\n\nusing simple importance\n\nwhere do these intermediate distributions come from? just as the original\nproposal distribution p0 is a design choice, so is the sequence of distributions\np\u03b71 . . . p\u03b7 n\u22121. that is, it can be specifically constructed to suit the problem domain.\none general-purpose and popular choice for the intermediate distributions is to\nuse the weighted geometric average of the target distribution p1 and the starting\nproposal distribution (for which the partition function is known) p0:\n\np\u03b7j \u221d p\u03b7j\n\n1 p1\u2212\u03b7j\n\n0\n\n(18.50)\n\nin order to sample from these intermediate distributions, we define a series of\nmarkov chain transition functions t\u03b7j ( x0 | x) that define the conditional probability\ndistribution of transitioning to x0 given we are currently at x. the transition\noperator t\u03b7j (x0 | x) is defined to leave p\u03b7j ( )x invariant:\nz p\u03b7j(x0)t\u03b7 j (x x|\n0) dx0\n\np\u03b7j ( ) =x\n\n(18.51)\n\nthese transitions may be constructed as any markov chain monte carlo method\n(e.g., metropolis-hastings, gibbs), including methods involving multiple passes\nthrough all of the random variables or other kinds of iterations.\n\n629\n\n "}, {"Page_number": 645, "text": "chapter 18. confronting the partition function\n\nthe ais sampling strategy is then to generate samples from p0 and then use\nthe transition operators to sequentially generate samples from the intermediate\ndistributions until we arrive at samples from the target distribution p1:\n\n\u2022 for k\n\n= 1\n\n. . . k\n\n\u2013 sample x( )k\n\u2013 sample x( )k\n\u2013 . . .\n\u2013 sample x( )k\n\u2013 sample x( )k\n\n\u03b71 \u223c p 0( )x\n\u03b72 \u223c t\u03b71 (x( )k\n\n\u03b72 | x( )k\n\u03b7 1 )\n\n\u03b7n\u22121 \u223c t\u03b7n\u22122 (x( )k\n\u03b7n \u223c t\u03b7n\u22121(x ( )k\n\n\u03b7 n\u22121 | x( )k\n\u03b7n | x( )k\n\u03b7n\u22121 )\n\n\u03b7n\u22122 )\n\n\u2022 end\nfor sample k, we can derive the importance weight by chaining together the\nimportance weights for the jumps between the intermediate distributions given in\neq.\n\n18.49\n.\n\nw( )k =\n\n\u02dcp\u03b7 1(x( )k\n\u03b71 )\n\u02dcp0(x ( )k\n0 )\n\n\u02dcp\u03b7 2(x( )k\n\u03b7 2 )\n\u02dcp\u03b7 1(x( )k\n\u03b7 1 )\n\n. . .\n\n\u02dcp1(x( )k\n1 )\n\u02dcp\u03b7n\u22121 (x( )k\n\u03b7 n\u22121 )\n\n(18.52)\n\nto avoid computational issues such as overflow, it is probably best to do the\ncomputation in log space:\n\nlog w ( )k = log \u02dcp\u03b71 ( )\n\nx \u2212\n\nlog \u02dc\n\np0( ) +x\n\n. . .\n\n(18.53)\n\nwith the sampling procedure thus defined and the importance weights given in eq.\n18.52, the estimate of the ratio of partition functions is given by:\n\nz1\nz0 \u2248\n\n1\nk\n\nw( )k\n\nkxk=1\n\n(18.54)\n\nin order to verify that this procedure defines a valid importance sampling\nscheme, we can show (\n) that the ais procedure corresponds to simple\nimportance sampling on an extended state space with points sampled over the\nproduct space [x\u03b7 1, . . . , x\u03b7n\u22121 , x 1]. to do this, we define the distribution over the\nextended space as:\n\nneal 2001\n\n,\n\n\u02dcp(x\u03b7 1, . . . , x\u03b7 n\u22121, x1)\n=\u02dcp1(x1) \u02dct\u03b7n\u22121 (x\u03b7n\u22121 | x1) \u02dct\u03b7n\u22122 (x\u03b7n\u22122 | x\u03b7n\u22121 ) . . . \u02dct\u03b71 (x\u03b71 | x\u03b72 ),\n\n(18.55)\n\n(18.56)\n\n630\n\n "}, {"Page_number": 646, "text": "chapter 18. confronting the partition function\n\nwhere \u02dcta is the reverse of the transition operator defined by t a (via an application\nof bayes\u2019 rule):\n\n\u02dcta(x0 | x) =\n\npa(x0 )\npa( )x\n\nta(x x|\n\n0) =\n\n\u02dcpa(x0)\n\u02dcpa( )x\n\nta(x x|\n\n0).\n\n(18.57)\n\nplugging the above into the expression for the joint distribution on the extended\nstate space given in eq.\n\n, we get:\n\n18.56\n\n\u02dcp(x\u03b71 , . . . , x\u03b7n\u22121, x 1)\n\n= \u02dcp1(x1)\n\n\u02dcp\u03b7n\u22121(x\u03b7 n\u22121)\n\u02dcp\u03b7n\u22121 (x1)\n\nt \u03b7n\u22121(x1 | x\u03b7n\u22121 )\n\n=\n\n\u02dcp 1(x1)\n\u02dcp \u03b7n\u22121(x1)\n\nt\u03b7n\u22121(x 1 | x\u03b7n\u22121 ) \u02dcp\u03b7 1(x \u03b71)\n\n(18.58)\n\n\u02dcp\u03b7i (x\u03b7i)\n\u02dcp\u03b7i (x\u03b7i+1)\n\nt\u03b7i(x\u03b7i+1 | x\u03b7 i)\n\n(18.59)\n\n\u02dcp\u03b7i+1(x \u03b7i+1)\n\u02dcp\u03b7i (x\u03b7i+1)\n\nt\u03b7i(x\u03b7i+1 | x\u03b7i ).\n(18.60)\n\nn\u22122yi=1\nn\u22122yi=1\n\nwe now have means of generating samples from the joint proposal distribution\nq over the extended sample via a sampling scheme given above, with the joint\ndistribution given by:\n\nq(x\u03b7 1, . . . , x\u03b7n\u22121, x 1) = p 0(x\u03b71)t\u03b71 (x\u03b72 | x\u03b71) . . . t\u03b7n\u22121(x1 | x \u03b7n\u22121).\n\n(18.61)\n\nwe have a joint distribution on the extended space given by eq.\u00a0\n. taking\nq(x\u03b7 1, . . . , x\u03b7n\u22121 , x1) as the proposal distribution on the extended state space from\nwhich we will draw samples, it remains to determine the importance weights:\n\n18.60\n\nw( )k =\n\n\u02dcp(x\u03b71 , . . . , x\u03b7 n\u22121 , x1)\nq(x\u03b7 1 , . . . , x\u03b7 n\u22121 , x1)\n\n=\n\n\u02dcp 1(x( )k\n)\n1\n\u02dcp\u03b7n\u22121(x( )k\n\n\u03b7 n\u22121)\n\n. . .\n\n\u02dcp\u03b72(x( )k\n\u03b72 )\n\u02dcp1(x( )k\n\u03b71 )\n\n\u02dcp\u03b71 (x( )k\n\u03b71 )\n\u02dcp0(x( )k\n0 )\n\n.\n\n(18.62)\n\nthese weights are the same as proposed for ais. thus we can interpret ais as\nsimple importance sampling applied to an extended state and its validity follows\nimmediately from the validity of importance sampling.\n\nannealed importance sampling (ais) was first discovered by\n\njarzynski 1997\n)\nand then again, independently, by\n). it is currently the most common\nway of estimating the partition function for undirected probabilistic models. the\nreasons for this may have more to do with the publication of an influential paper\n(salakhutdinov and murray 2008\n) describing its application to estimating the\npartition function of restricted boltzmann machines and deep belief networks than\n\nneal 2001\n\n(\n\n(\n\n,\n\n631\n\n "}, {"Page_number": 647, "text": "chapter 18. confronting the partition function\n\nwith any inherent advantage the method has over the other method described\nbelow.\n\na discussion of the properties of the ais estimator (e.g..\n\nits variance and\n\nefficiency) can be found in\n\nneal 2001\n\n(\n\n).\n\n18.7.2 bridge sampling\n\n(\n\nbennett 1976\n\nbridge sampling\n) is another method that, like ais, addresses the\nshortcomings of importance sampling. rather than chaining together a series of\nintermediate distributions, bridge sampling relies on a single distribution p\u2217, known\nas the bridge, to interpolate between a distribution with known partition function,\np0, and a distribution p1 for which we are trying to estimate the partition function\nz1 .\n\nbridge sampling estimates the ratio z1 /z0 as the ratio of the expected impor-\n\n(18.63)\n\ntance weights between \u02dcp0 and \u02dcp\u2217 and between \u02dcp1 and \u02dcp\u2217:\n\u02dcp\u2217(x( )k\n1 )\n\u02dcp1(x( )k\n1 )\n\n\u02dcp\u2217 (x( )k\n0 )\n\u02dcp0 (x( )k\n\n0 ), kxk=1\n\nz1\nz0 \u2248\n\nkxk=1\n\nif the bridge distribution p\u2217 is chosen carefully to have a large overlap of support\nwith both p0 and p1 , then bridge sampling can allow the distance between two\ndistributions (or more formally, d kl(p0kp1 )) to be much larger than with standard\nimportance sampling.\n\nit can be shown that the optimal bridging distribution is given by p (\n)opt\n\u2217\n\n(x) \u221d\n\u02dcp 0( )\u02dcx p 1( )x\nr\u02dcp0( )+\u02dcx p1 ( )x where r = z1/z0 . at first, this appears to be an unworkable solution\nas it would seem to require the very quantity we are trying to estimate, z1/z0.\nhowever, it is possible to start with a coarse estimate of r and use the resulting\nbridge distribution to refine our estimate iteratively (\n). that is, we\niteratively re-estimate the ratio and use each iteration to update the value of\n\nneal 2005\n\n.r\n\n,\n\nlinked importance sampling both ais and bridge sampling have their ad-\nvantages. if dkl(p0kp 1) is not too large (because p0 and p1 are sufficiently close)\nbridge sampling can be a more effective means of estimating the ratio of partition\nfunctions than ais. if, however, the two distributions are too far apart for a single\ndistribution p\u2217 to bridge the gap then one can at least use ais with potentially\nmany intermediate distributions to span the distance between p0 and p1. neal\n(\n) showed how his linked importance sampling method leveraged the power of\n2005\nthe bridge sampling strategy to bridge the intermediate distributions used in ais\nto significantly improve the overall partition function estimates.\n\n632\n\n "}, {"Page_number": 648, "text": "chapter 18. confronting the partition function\n\nestimating the partition function while training while ais has become\naccepted as the standard method for estimating the partition function for many\nundirected models, it is sufficiently computationally intensive that it remains\ninfeasible to use during training. however, alternative strategies that have been\nexplored to maintain an estimate of the partition function throughout training\n\n2011\n\net al. (\n\nusing a combination of bridge sampling, short-chain ais and parallel tempering,\ndesjardins\n) devised a scheme to track the partition function of an\nrbm throughout the training process. the strategy is based on the maintenance of\nindependent estimates of the partition functions of the rbm at every temperature\noperating in the parallel tempering scheme. the authors combined bridge sampling\nestimates of the ratios of partition functions of neighboring chains (i.e.\nfrom\nparallel tempering) with ais estimates across time to come up with a low variance\nestimate of the partition functions at every iteration of learning.\n\nthe tools described in this chapter provide many different ways of overcoming\nthe problem of intractable partition functions, but there can be several other\ndifficulties involved in training and using generative models. foremost among these\nis the problem of intractable inference, which we confront next.\n\n633\n\n "}, {"Page_number": 649, "text": "chapter 19\n\napproximate inference\n\nmany probabilistic models are difficult to train because it is difficult to perform\ninference in them. in the context of deep learning, we usually have a set of visible\nvariables v and a set of latent variables h. the challenge of inference usually\nrefers to the difficult problem of computing p(h v|\n) or taking expectations with\nrespect to it. such operations are often necessary for tasks like maximum likelihood\nlearning.\n\nmany simple graphical models with only one hidden layer, such as restricted\nboltzmann machines and probabilistic pca, are defined in a way that makes\ninference operations like computing p(h v|\n), or taking expectations with respect\nto it, simple. unfortunately, most graphical models with multiple layers of hidden\nvariables have intractable posterior distributions. exact inference requires an\nexponential amount of time in these models. even some models with only a single\nlayer, such as sparse coding, have this problem.\n\nin this chapter, we introduce several of the techniques for confronting these\nintractable inference problems. later, in chapter\n, we will describe how to use\nthese techniques to train probabilistic models that would otherwise be intractable,\nsuch as deep belief networks and deep boltzmann machines.\n\n20\n\nintractable inference problems in deep learning usually arise from interactions\nbetween latent variables in a structured graphical model. see fig.\nfor some\nexamples. these interactions may be due to direct interactions in undirected\nmodels or \u201cexplaining away\u201d interactions between mutual ancestors of the same\nvisible unit in directed models.\n\n19.1\n\n634\n\n "}, {"Page_number": 650, "text": "chapter 19. approximate inference\n\n,\n\na\n\nfigure 19.1: intractable inference problems in deep learning are usually the result of\ninteractions between latent variables in a structured graphical model. these can be due to\nedges directly connecting one latent variable to another, or due to longer paths that are\nactivated when the child of a v-structure is observed. (left)\nsemi-restricted boltzmann\nmachine (osindero and hinton 2008\n) with connections between hidden units. these\ndirect connections between latent variables make the posterior distribution intractable\ndue to large cliques of latent variables.\na deep boltzmann machine, organized\ninto layers of variables without intra-layer connections, still has an intractable posterior\ndistribution due to the connections between layers.\nthis directed model has\ninteractions between latent variables when the visible variables are observed, because\nevery two latent variables are co-parents. some probabilistic models are able to provide\ntractable inference over the latent variables despite having one of the graph structures\ndepicted above. this is possible if the conditional probability distributions are chosen to\nintroduce additional independences beyond those described by the graph. for example,\nprobabilistic pca has the graph structure shown in the right, yet still has simple inference\ndue to special properties of the specific conditional distributions it uses (linear-gaussian\nconditionals with mutually orthogonal basis vectors).\n\n(center)\n\n(right)\n\n635\n\n "}, {"Page_number": 651, "text": "chapter 19. approximate inference\n\n19.1\n\ninference as optimization\n\nmany approaches to confronting the problem of difficult inference make use of\nthe observation that exact inference can be described as an optimization problem.\napproximate inference algorithms may then be derived by approximating the\nunderlying optimization problem.\n\nto construct the optimization problem, assume we have a probabilistic model\nconsisting of observed variables v and latent variables h. we would like to compute\nthe log probability of the observed data, log p(v ; \u03b8). sometimes it is too difficult\nto compute log p(v; \u03b8) if it is costly to marginalize out h. instead, we can compute\na lower bound l(v \u03b8,\n, q) on log p(v;\u03b8 ).\u00a0this bound is called the evidence lower\nbound (elbo). another commonly used name for this lower bound is the negative\nvariational free energy. specifically, the evidence lower bound is defined to be\n\n, q\n\nl\n(\nv \u03b8,\n\nkp h v|\nq h v|\n)\n.\nis an arbitrary probability distribution over\nh\n\np v \u03b8 dkl ( (\n\n) = log ( ; )\n\n\u2212\n\n(\n\nwhere\n\nq\n\n; ))\n\u03b8\n\n(19.1)\n\nbecause the difference between log p(v) and l(v \u03b8,\n\n, q) is given by the kl\ndivergence and because the kl divergence is always non-negative, we can see that\nl always has at most the same value as the desired log probability. the two are\nequal if and only if\n\nis the same distribution as\n\np(\n\n.\n)\n\nq\n\nsurprisingly, l can be considerably easier to compute for some distributions q.\nsimple algebra shows that we can rearrange l into a much more convenient form:\n\nh v|\n\nl\n(\nv \u03b8,\n\n, q\n\n) = log ( ; )\np v \u03b8\n\n\u2212\n\ndkl ( (\n\n= log ( ; )\n\np v \u03b8 \u2212 eh\u223cq log\n\n= log ( ; )\n\np v \u03b8 \u2212 eh\u223cq log\n\n; ))\n\u03b8\n\nq h v|\nkp h v|\n)\n(\nh v|\n)\nq(\nh v|\np(\n)\nh v|\nq(\n)\n,(h v \u03b8; )\np\np( ; )v \u03b8\n\n= log ( ; )\n= \u2212 eh\u223cq [log (\n\np v \u03b8 \u2212 eh\u223cq [log (\nq h v| \u2212\n\n)\n\n)\n\nq h v| \u2212\np h v,\nlog (\n\n; )]\n\u03b8 .\n\nlog (\n\np h v,\n\n; ) + log ( ; )]\n\u03b8\n\np v \u03b8\n\n(19.2)\n\n(19.3)\n\n(19.4)\n\n(19.5)\n\n(19.6)\n\nthis yields the more canonical definition of the evidence lower bound,\n\nv \u03b8,\n\nl(\n\n, q\n\n) = \n\neh\u223cq [log (\n\np h v,\n\n)] + ( )\nh q .\n\n(19.7)\n\nfor an appropriate choice of q, l is tractable to compute. for any choice\n) that are better\n\nof q, l provides a lower bound on the likelihood. for q(h v|\n\n636\n\n "}, {"Page_number": 652, "text": "chapter 19. approximate inference\n\napproximations of p(h v|\ncloser to log p(v).\u00a0when q( h v|\nl(\n\n) = log ( ; )\np v \u03b8 .\n\nv \u03b8,\n\n, q\n\n), the lower bound l will be tighter, in other words,\n), the approximation is perfect, and\n\n) = p (h v|\n\nwe can thus think of inference as the procedure for finding the q that maximizes\nl. exact inference maximizes l perfectly by searching over a family of functions\nq that includes p(h v|\n). throughout this chapter, we will show how to derive\ndifferent forms of approximate inference by using approximate optimization to\nfind q. we can make the optimization procedure less expensive but approximate\nby restricting the family of distributions q the optimization is allowed to search\nover or by using an imperfect optimization procedure that may not completely\nmaximize\n\nbut merely increase it by a significant amount.\n\nno matter what choice of q we use, l is a lower bound. we can get tighter\nor looser bounds that are cheaper or more expensive to compute depending on\nhow we choose to approach this optimization problem.\u00a0we can obtain a poorly\nmatched q but reduce the computational cost by using an imperfect optimization\nprocedure, or by using a perfect optimization procedure over a restricted family of\nq distributions.\n\nl\n\n19.2 expectation maximization\n\nthe first algorithm we introduce based on maximizing a lower bound l is the\nexpectation maximization (em) algorithm, a popular training algorithm for models\nwith latent variables. we describe here a view on the em algorithm developed by\nneal and hinton 1999\n). unlike most of the other algorithms we describe in this\nchapter, em is not an approach to approximate inference, but rather an approach\nto learning with an approximate posterior.\n\n(\n\nthe em algorithm consists of alternating between two steps until convergence:\n\n\u2022 the\n\ne-step\n\n(expectation step): let\n\n\u03b8(0) denote the value of the parameters\n| v( )i ; \u03b8(0)) for all indices\nat the beginning of the step. set q(h( )i\ni of the training examples v( )i we want to train on (both batch and minibatch\nvariants are valid).\u00a0by this we mean q is defined in terms of the current\nparameter value of \u03b8(0); if we vary \u03b8 then p(h v|\n; \u03b8) will change but q(h v|\n)\nwill remain equal to p(\n\n| v) = p(h( )i\n\n\u03b8(0)).\n\n;\n\nh v|\n\n\u2022 the m-step (maximization step): completely or partially maximize\n\nl(v( )i ,\n\n, q\u03b8 )\n\n(19.8)\n\nxi\n\n637\n\n "}, {"Page_number": 653, "text": "chapter 19. approximate inference\n\nwith respect to\n\n\u03b8\n\nusing your optimization algorithm of choice.\n\nthis can be viewed as a coordinate ascent algorithm to maximize l. on one\nstep, we maximize l with respect to q, and on the other, we maximize l with\nrespect to .\u03b8\n\nstochastic gradient ascent on latent variable models can be seen as a special\ncase of the em algorithm where the m step consists of taking a single gradient\nstep. other variants of the em algorithm can make much larger steps. for some\nmodel families, the m step can even be performed analytically, jumping all the\nway to the optimal solution for\n\ngiven the current\n\n\u03b8\n\nq\n\n.\n\neven though the e-step involves exact inference, we can think of the em\nalgorithm as using approximate inference in some sense. specifically, the m-step\nassumes that the same value of q can be used for all values of \u03b8. this will introduce\na gap between l and the true log p(v) as the m-step moves further and further\naway from the value \u03b8 (0) used in the e-step. fortunately, the e-step reduces the\ngap to zero again as we enter the loop for the next time.\n\nthe em algorithm contains a few different insights. first, there is the basic\nstructure of the learning process, in which we update the model parameters to\nimprove the likelihood of a completed dataset, where all missing variables have\ntheir values provided by an estimate of the posterior distribution. this particular\ninsight is not unique to the em algorithm. for example, using gradient descent to\nmaximize the log-likelihood also has this same property; the log-likelihood gradient\ncomputations require taking expectations with respect to the posterior distribution\nover the hidden units.\u00a0another key insight in the em algorithm is that we can\ncontinue to use one value of q even after we have moved to a different value of \u03b8.\nthis particular insight is used throughout classical machine learning to derive large\nm-step updates. in the context of deep learning, most models are too complex\nto admit a tractable solution for an optimal large m-step update, so this second\ninsight which is more unique to the em algorithm is rarely used.\n\n19.3 map inference and sparse coding\n\nwe usually use the term inference to refer to computing the probability distribution\nover one set of variables given another. when training probabilistic models with\nlatent variables, we are usually interested in computing p(h v|\n). an alternative\nform of inference is to compute the single most likely value of the missing variables,\nrather than to infer the entire distribution over their possible values. in the context\n\n638\n\n "}, {"Page_number": 654, "text": "chapter 19. approximate inference\n\nof latent variable models, this means computing\n\nh\u2217 = arg max\n\nh\n\nh v|\n(\np\n\n.\n)\n\n(19.9)\n\nthis is known as maximum a posteriori inference, abbreviated map inference.\n\nmap inference is usually not thought of as approximate inference\u2014it does\ncompute the exact most likely value of h\u2217. however, if we wish to develop a\nlearning process based on maximizing l(v h,\n, q), then it is helpful to think of map\ninference as a procedure that provides a value of q. in this sense, we can think of\nmap inference as approximate inference, because it does not provide the optimal\nq.\n\nrecall from sec.\n\n19.1\n\nthat exact inference consists of maximizing\n\nv \u03b8,\n\nl(\n\n, q\n\n) = \n\neh\u223cq [log (\n\np h v,\n\n)] + ( )\nh q\n\n(19.10)\n\nwith respect to q over an unrestricted family of probability distributions, using\nan exact optimization algorithm. we can derive map inference as a form of\napproximate inference by restricting the family of distributions q may be drawn\nfrom. specifically, we require\n\nto take on a dirac distribution:\n\nq\n\nq\n\nh v|\n(\n\n) = \n\u03b4\n\nh \u00b5\u2212\n(\n)\n.\n\n(19.11)\n\nthis means that we can now control q entirely via \u00b5. dropping terms of l that\ndo not vary with , we are left with the optimization problem\n\n\u00b5\n\n\u00b5\u2217 = max\n\n\u00b5\n\nlog ( = \n\n)\np h \u00b5 v,\n,\n\nwhich is equivalent to the map inference problem\n\nh\u2217 = max\nh\n\nh v|\n(\np\n\n.\n)\n\n(19.12)\n\n(19.13)\n\nwe can thus justify a learning procedure similar to em, in which we alternate\nbetween performing map inference to infer h\u2217 and then update \u03b8 to increase\nlog p(h\u2217, v). as with em, this is a form of coordinate ascent on l, where we\nalternate between using\u00a0inference to\u00a0optimize l with respect\u00a0to q and using\nparameter updates to optimize l with respect to \u03b8. the procedure as a whole can\nbe justified by the fact that l is a lower bound on log p(v ). in the case of map\ninference, this justification is rather vacuous, because the bound is infinitely loose,\ndue to the dirac distribution\u2019s differential entropy of negative infinity. however,\nadding noise to would make the bound meaningful again.\n\n\u00b5\n\n639\n\n "}, {"Page_number": 655, "text": "chapter 19. approximate inference\n\nmap inference is commonly used in deep learning as both a feature extractor\n\nand a learning mechanism. it is primarily used for sparse coding models.\n\nrecall from sec.\n\nthat sparse coding is a linear factor model that imposes a\nsparsity-inducing prior on its hidden units. a common choice is a factorial laplace\nprior, with\n\n13.4\n\np h( i) =\n\ne\u2212 1\n\n2 \u03bb h| i|.\n\n(19.14)\n\n\u03bb\n2\n\nthe visible units are then generated by performing a linear transformation and\nadding noise:\n\nx h|\n(\np\n\n) = \n\n( ;\n\nn x w h b i )\n, \u03b2 .\n\n+\n\n(19.15)\n\ncomputing or even representing p(h v|\n\n) is difficult. every pair of variables hi\nand hj are both parents of v. this means that when v is observed, the graphical\nmodel contains an active path connecting hi and hj. all of the hidden units thus\nparticipate in one massive clique in p(h v|\n). if the model were gaussian then\nthese interactions could be modeled efficiently via the covariance matrix, but the\nsparse prior makes these interactions non-gaussian.\n\nbecause p(h v|\n\n) is intractable, so is the computation of the log-likelihood and\nits gradient. we thus cannot use exact maximum likelihood learning. instead, we\nuse map inference and learn the parameters by maximizing the elbo defined by\nthe dirac distribution around the map estimate of\n\n.h\n\nif we concatenate all of the h vectors in the training set into a matrix h, then\n\nthe sparse coding learning process consists of minimizing\n\nj\n\n,\n\n(h w ) =xi,j\n\n|hi,j| +xi,j (cid:29)x hw\u2212\n\n>(cid:30)2\n\ni,j\n\n.\n\n(19.16)\n\nmost applications of sparse coding also involve weight decay or a constraint on\nthe norms of the columns of w, in order to prevent the pathological solution with\nextremely small\n\nand large\n\nw\n\nh\n\n.\n\nwe can minimize j by alternating between minimization with respect to h\nand minimization with respect to w . both sub-problems are convex. in fact,\nthe minimization with respect to w is just a linear regression problem. however,\nminimization of j with respect to both arguments is usually not a convex problem.\n\nminimization with respect to h requires specialized algorithms such as the\n\nfeature-sign search algorithm (\n\nlee et al. 2007\n\n,\n\n).\n\n640\n\n "}, {"Page_number": 656, "text": "chapter 19. approximate inference\n\n19.4 variational inference and learning\n\nwe\u00a0have\u00a0seen\u00a0how\u00a0the\u00a0evidence\u00a0lower\u00a0bound l(v \u03b8,\n, q) is\u00a0a\u00a0lower\u00a0bound\u00a0on\nlog p(v; \u03b8), how inference can be viewed as maximizing l with respect to q, and\nhow learning can be viewed as maximizing l with respect to \u03b8. we have seen\nthat the em algorithm allows us to make large learning steps with a fixed q and\nthat learning algorithms based on map inference allow us to learn using a point\nestimate of p(h v|\n) rather than inferring the entire distribution. now we develop\nthe more general approach to variational learning.\n\nthe core idea behind variational learning is that we can maximize l over a\nrestricted family of distributions q. this family should be chosen so that it is easy\nto compute eq log p(h v,\n).\u00a0a typical way to do this is to introduce assumptions\nabout how factorizes.\n\nq\n\na common approach to variational learning is to impose the restriction that q\n\nis a factorial distribution:\n\nq(\n\n) =\n\nh v| yi\n\nq h( i | v).\n\n(19.17)\n\nthis is called the mean field approach. more generally, we can impose any graphical\nmodel structure we choose on q, to flexibly determine how many interactions we\nwant our approximation to capture. this fully general graphical model approach\nis called structured variational inference (\n\nsaul and jordan 1996\n\n).\n\n,\n\nthe beauty of the variational approach is that we do not need to specify a\nspecific parametric form for q. we specify how it should factorize, but then the\noptimization problem determines the optimal probability distribution within those\nfactorization constraints. for discrete latent variables, this just means that we\nuse traditional optimization techniques to optimize a finite number of variables\ndescribing the q distribution. for continuous latent variables, this means that we\nuse a branch of mathematics called calculus of variations to perform optimization\nover a space of functions, and actually determine which function should be used\nto represent q . calculus of\u00a0variations\u00a0is the origin\u00a0of the names\u00a0\u201cvariational\nlearning\u201d and \u201cvariational inference,\u201d though these names apply even when the\nlatent variables are discrete and calculus of variations is not needed. in the case\nof continuous latent variables, calculus of variations is a powerful technique that\nremoves much of the responsibility from the human designer of the model, who\nnow must specify only how q factorizes, rather than needing to guess how to design\na specific\n\nthat can accurately approximate the posterior.\n\nq\n\nbecause l(v \u03b8,\n\n, q ) is defined to be log p(v;\u03b8 ) \u2212 dkl(q(h v|\n\n)kp(h v|\n\ncan think of maximizing l with respect to q as minimizing dkl (q(h v|\n\n; \u03b8)), we\n)).\n\n)kp(h v|\n\n641\n\n "}, {"Page_number": 657, "text": "chapter 19. approximate inference\n\n3.6\n\nin this sense, we are fitting q to p .\u00a0however, we are doing so with the opposite\ndirection of the kl divergence than we are used to using for fitting an approximation.\nwhen we use maximum likelihood learning to fit a model to data, we minimize\nd kl(pdatakpmodel). as illustrated in fig.\n, this means that maximum likelihood\nencourages the model to have high probability everywhere that the data has high\nprobability,\u00a0while our optimization-based inference procedure encourages q to\nhave low probability everywhere the true posterior has low probability. both\ndirections of the kl divergence can have desirable and undesirable properties. the\nchoice of which to use depends on which properties are the highest priority for\neach application. in the case of the inference optimization problem, we choose\nto use d kl(q (h v|\n)) for computational reasons. specifically, computing\nd kl(q(h v| )kp(h v|\n)) involves evaluating expectations with respect to q, so by\ndesigning q to be simple, we can simplify the required expectations. the opposite\ndirection of the kl divergence would require computing expectations with respect\nto the true posterior. because the form of the true posterior is determined by\nthe choice of model, we cannot design a reduced-cost approach to computing\nd kl( (\n\n)kp(h v|\n\nexactly.\n\n))\n\np h v|\n\n(\n\nkq h v|\n)\n\n19.4.1 discrete latent variables\n\nvariational inference with discrete latent variables is relatively straightforward.\nwe define a distribution q, typically one where each factor of q is just defined\nby a lookup table over discrete states.\u00a0in the simplest case, h is binary and we\nmake the mean field assumption that\nhi. in this\ncase we can parametrize q with a vector \u02c6h whose entries are probabilities. then\nq h( i = 1 \n\nfactorizes over each individual\n\n\u02c6hi.\n\nq\n\nafter determining how to represent q, we simply optimize its parameters. in\nthe case of discrete latent variables, this is just a standard optimization problem.\nin principle the selection of q could be done with any optimization algorithm, such\nas gradient descent.\n\n) =| v\n\nbecause this optimization must occur in the inner loop of a learning algorithm,\nit must be very fast. to achieve this speed, we typically use special optimization\nalgorithms that are designed to solve comparatively small and simple problems in\nvery few iterations. a popular choice is to iterate fixed point equations, in other\nwords, to solve\n\n\u2202\n\u2202\u02c6hil = 0\n\n(19.18)\n\nfor \u02c6hi . we repeatedly update different elements of \u02c6h until we satisfy a convergence\n\n642\n\n "}, {"Page_number": 658, "text": "chapter 19. approximate inference\n\ncriterion.\n\nto make this more concrete, we show how to apply variational inference to the\nbinary sparse coding model (we present here the model developed by henniges et al.\n(\n) but demonstrate traditional, generic mean field applied to the model, while\n2010\nthey introduce a specialized algorithm). this derivation goes into considerable\nmathematical detail and is intended for the reader who wishes to fully resolve\nany ambiguity in the high-level conceptual description of variational inference and\nlearning we have presented so far. readers who do not plan to derive or implement\nvariational learning algorithms may safely skip to the next section without missing\nany new high-level concepts. readers who proceed with the binary sparse coding\nexample are encouraged to review the list of useful properties of functions that\ncommonly arise in probabilistic models in sec.\n. we use these properties\nliberally throughout the following derivations without highlighting exactly where\nwe use each one.\n\n3.10\n\nin the binary sparse coding model, the input v \u2208 r n is generated from the\nmodel by adding gaussian noise to the sum of m different components which\ncan each be present or absent. each component is switched on or off by the\ncorresponding hidden unit in h \u2208 {\n\n}0 1, m:\n\np h( i = 1) =  (\u03c3 bi )\n\nv h|\n(\np\n\n) = \n\n( ;\n\nn v w h \u03b2\u22121 )\n\n,\n\n(19.19)\n\n(19.20)\n\nwhere b is a learnable set of biases, w is a learnable weight matrix, and \u03b2 is a\nlearnable, diagonal precision matrix.\n\ntraining this model with maximum likelihood requires taking the derivative\nwith respect to the parameters. consider the derivative with respect to one of the\nbiases:\n\n\u2202\n\u2202bi\n\u2202\n\u2202bi\n\nlog ( )p v\n\np( )v\n\n=\n\n=\n\n=\n\np( )v\n\u2202\n\n\u2202bi ph p\n\u2202bi ph p\n\n\u2202\n\np( )v\n\n643\n\n,(h v)\n\n( )h (\np\n\nv h|\n)\n\np( )v\n\n(19.21)\n\n(19.22)\n\n(19.23)\n\n(19.24)\n\n "}, {"Page_number": 659, "text": "chapter 19. approximate inference\n\nh1h1\n\nh2h2\n\nh3h3\n\nh4h4\n\nv1v1\n\nv2v2\n\nv3v3\n\nh1h1\n\nh2h2\n\nh3h3\n\nh4h4\n\nfigure 19.2: the graph structure of a binary sparse coding model with four hidden units.\n(left) the graph structure of p(h v, ). note that the edges are directed, and that every two\nhidden units are co-parents of every visible unit.\n).\np(h v|\nin order to account for the active paths between co-parents, the posterior distribution\nneeds an edge between all of the hidden units.\n\nthe graph structure of\n\n(right)\n\n=ph p(\n=xh\n\np(\n\nv h|\n)\np( )v\n\n)\n\nh v|\n\u2202\n\u2202bi\n\n|\n\n=e\n\np(h v)\n\nh\u223c\n\n\u2202\n\u2202bi\n\np( )h\n\n\u2202\n\u2202bi\n\np( )h\n\np( )h\n\nlog ( )\np h .\n\n(19.25)\n\n(19.26)\n\n(19.27)\n\n) is a complicated distribution. see fig.\n\nthis requires computing expectations with respect to p(h v|\n\n). unfortunately,\np(h v|\nfor the graph structure of\np(h v, ) and p(h v|\n). the posterior distribution corresponds to the complete graph\nover the hidden units, so variable elimination algorithms do not help us to compute\nthe required expectations any faster than brute force.\n\n19.2\n\nwe can resolve this difficulty by using variational inference and variational\n\nlearning instead.\n\nwe can make a mean field approximation:\n\nq(\n\n) =\n\nh v| yi\n\nq h( i | v).\n\n(19.28)\n\nthe latent variables of the binary sparse coding model are binary, so to represent\na factorial q we simply need to model m bernoulli distributions q(hi | v). a natural\nway to represent the means of the bernoulli distributions is with a vector \u02c6h of\nprobabilities, with q(hi = 1 | v) = \u02c6hi. we impose a restriction that \u02c6h i is never\nequal to 0 or to 1, in order to avoid errors when computing, for example, log \u02c6hi.\n\u02c6hi\n\nwe will see that the variational inference equations never assign\n\nto\n\nor\n\n0\n\n1\n\n644\n\n "}, {"Page_number": 660, "text": "chapter 19. approximate inference\n\n1\n\n0\n\nor\n\nanalytically. however, in a software implementation, machine rounding error could\nresult in\nvalues. in software, we may wish to implement binary sparse\ncoding using an unrestricted vector of variational parameters z and obtain \u02c6h via\nthe relation \u02c6h = \u03c3 (z ). we can thus safely compute log \u02c6hi on a computer by using\nthe identity log (\u03c3 zi) = \n\n(\u2212\u03b6 \u2212zi) relating the sigmoid and the softplus.\n\nto begin our derivation of variational learning in the binary sparse coding\nmodel, we show that the use of this mean field approximation makes learning\ntractable.\n\nthe evidence lower bound is given by\n\nlog (q hi | v)#\n\n\u2212\n\nlog(1\n\n\u2212 \u02c6hi))i\n\n, q\n\nv \u03b8,\n\nl(\n)\nh q\n=eh\u223cq[log (\n)] + ( )\np v h|\np h\n=eh\u223cq[log ( ) + log (\n)\n\np h v,\n\n)]\n\n=\n\n\u2212\n\nlog (\n\nlog\u2212\n\nlog (p hi) +\n\nq h v|\nlog (p vi | \u2212h)\n\n=eh\u223cq\" mxi=1\nmxi=1h \u02c6h i(log (\u03c3 bi )\n+ eh\u223cq\" nxi=1\nmxi=1h \u02c6h i(log (\u03c3 bi )\n(cid:36)(cid:37) log\nnxi=1\n\nmxi=1\nnxi=1\n\u02c6hi) + (1 \u2212 \u02c6h i )(log (\u03c3 \u2212bi)\nexp(cid:31)\u2212\n\u02c6h i) + (1 \u2212 \u02c6h i )(log (\u03c3 \u2212bi)\n2\u03c0 \u2212 \u03b2i(cid:33)(cid:35) v2\ni \u2212 2viw i,:\u02c6h +xj\n\n(vi \u2212 w i,:h)2(cid:32)#\n(cid:36)(cid:37)w 2\n\nlogr \u03b2i\n\nlog\u2212\n\n\u03b2i\n2\n\n1\n2\n\n2\u03c0\n\n\u03b2i\n\n=\n\n+\n\nlog(1\n\n\u2212\n\ni,j\n\n\u02c6hj +xk j6=\n\n(19.35)\n\n\u2212 \u02c6hi))i\nw i,j wi,k \u02c6hj\u02c6hk(cid:41)(cid:42)(cid:38)(cid:40)(cid:41)(cid:42) .\n\n(19.36)\n\n(19.29)\n\n(19.30)\n\n(19.31)\n\n(19.32)\n\n(19.33)\n\n(19.34)\n\nwhile these equations are somewhat unappealing aesthetically,\u00a0they show\nthat l can be expressed in a small number of simple arithmetic operations. the\nevidence lower bound l is therefore tractable. we can use l as a replacement for\nthe intractable log-likelihood.\n\nin principle, we could simply run gradient ascent on both v and h and this\nwould make a perfectly acceptable combined inference and training algorithm.\nusually, however, we do not do this, for two reasons. first, this would require\nstoring \u02c6h for each v. we typically prefer algorithms that do not require per-\nexample memory. it is difficult to scale learning algorithms to billions of examples\nif we must remember a dynamically updated vector associated with each example.\n\n645\n\n "}, {"Page_number": 661, "text": "chapter 19. approximate inference\n\nsecond, we would like to be able to extract the features \u02c6h very quickly, in order to\nrecognize the content of v.\u00a0in a realistic deployed setting, we would need to be\nable to compute \u02c6h in real time.\n\nfor both these reasons, we typically do not use gradient descent to compute\nthe mean field parameters \u02c6h. instead, we rapidly estimate them with fixed point\nequations.\n\nthe idea behind fixed point equations is that we are seeking a local maximum\n, \u02c6h) = 0. we cannot efficiently solve this\nwith respect to \u02c6h,\u00a0where \u2207hl(v \u03b8,\nequation with respect to all of \u02c6h simultaneously. however, we can solve for a single\nvariable:\n\n\u2202\n\u2202\u02c6hi l(v \u03b8,\n\n, \u02c6h) = 0.\n\n(19.37)\n\nwe can then iteratively apply the solution to the equation for i = 1, . . . , m ,\nand repeat the cycle until we satisfy a converge criterion. common convergence\ncriteria include stopping when a full cycle of updates does not improve l by more\nthan some tolerance amount, or when the cycle does not change \u02c6h by more than\nsome amount.\n\niterating mean field fixed point equations is a general technique that can\nprovide fast variational inference in a broad variety of models. to make this more\nconcrete, we show how to derive the updates for the binary sparse coding model in\nparticular.\n\nfirst, we must write an expression for the derivatives with respect to \u02c6hi . to\n\ndo so, we substitute eq.\n\n19.36\n\ninto the left side of eq.\n\n19.37\n:\n\n, \u02c6h)\n\n(19.38)\n\n=\n\nlog\u2212\n\n\u2202\n\u2202 \u02c6hil(v \u03b8,\n\u2202\n\n+\n\n\u03b2j\n\n1\n2\n\n\u2202 \u02c6hi(cid:36)(cid:37)\nmxj=1h \u02c6hj(log (\u03c3 bj)\n(cid:36)(cid:37)log\n2\u03c0 \u2212 \u03b2j(cid:33)(cid:35)v2\nnxj=1\n(cid:36)(cid:37)\u03b2j(cid:33)(cid:35)vj wj,i \u2212\n\nnxj=1\n\nlog\u2212\n\n\u02c6hi \u2212\n\n1\n2\n\n+\n\nw 2\n\n= log (\u03c3 bi )\n\n1 + log(1\n\nj,k\n\n\u02c6hj ) + (1 \u2212 \u02c6hj )(log (\u03c3 \u2212bj)\n(cid:36)(cid:37)w 2\nj \u2212 2vjwj,:\u02c6h \u2212xk\nwj,kwj,i\u02c6hk(cid:38)(cid:40)(cid:41)(cid:42)\n\nj,i \u2212xk i\n\n\u03c3 \u2212bi )\n\n\u02c6hi) + 1\n\nlog (\n\n\u2212\n\n\u2212\n\n6=\n\n646\n\nlog(1\n\n\u2212\n\n\u2212 \u02c6h j))i (19.39)\n\u02c6hl(cid:41)(cid:42)(cid:38)(cid:40)(cid:41)(cid:42)(cid:41)(cid:42)\n\n(19.40)\n\n\u02c6hk\n\nw j,kwj,l\n\n\u02c6hk +xl k6=\n\n(19.41)\n\n(19.42)\n\n "}, {"Page_number": 662, "text": "chapter 19. approximate inference\n\n=bi \u2212 log \u02c6hi + log(1 \u2212 \u02c6hi) + v >\u03b2w:,i \u2212\n\n1\n2\n\nw:,i\u03b2w :,i \u2212xj\n\n6=\ni\n\nw >:,j \u03b2w:,i\u02c6hj. (19.43)\n\nto apply the fixed point update inference rule, we solve for the \u02c6hi that sets eq.\n\n19.43 to 0:\n\n\u02c6h i = \u03c3(cid:33)(cid:35)bi + v> \u03b2w:,i \u2212\n\n1\n2\n\nw:,i \u03b2w:,i \u2212xj\n\n6=\ni\n\nw >:,j \u03b2w:,i\u02c6hj(cid:38)(cid:40) .\n\n(19.44)\n\nat this point, we can see that there is a close connection between recurrent\nneural networks and inference in graphical models. specifically, the mean field\nfixed point equations defined a recurrent neural network. the task of this network\nis to perform inference. we have described how to derive this network from a\nmodel description, but it is also possible to train the inference network directly.\nseveral ideas based on this theme are described in chapter\n\n.20\n\n19.44\n\nin the case of binary sparse coding, we can see that the recurrent network\nconnection specified by eq.\nconsists of repeatedly updating the hidden\nunits based on the changing values of the neighboring hidden units. the input\nalways sends a fixed message of v >\u03b2w to the hidden units, but the hidden units\nconstantly update the message they send to each other. specifically, two units \u02c6hi\nand \u02c6hj inhibit each other when their weight vectors are aligned. this is a form of\ncompetition\u2014between two hidden units that both explain the input, only the one\nthat explains the input best will be allowed to remain active. this competition is\nthe mean field approximation\u2019s attempt to capture the explaining away interactions\nin the binary sparse coding posterior. the explaining away effect actually should\ncause a multi-modal posterior, so that if we draw samples from the posterior,\nsome samples will have one unit active, other samples will have the other unit\nactive, but very few samples have both active. unfortunately, explaining away\ninteractions cannot be modeled by the factorial q used for mean field, so the mean\nfield approximation is forced to choose one mode to model. this is an instance of\nthe behavior illustrated in fig.\n\n.3.6\n\nwe can rewrite eq.\u00a0\n\n19.44\n\ninto an equivalent form that reveals some further\n\ninsights:\n\n>\n\n\u02c6h i = \u03c3(cid:33)(cid:34)(cid:35)bi +(cid:33)(cid:35)v \u2212xj\n\nw:,i\u03b2w :,i(cid:38)(cid:39)(cid:40) .\nin this reformulation, we see the input at each step as consisting of v\u2212pj\n\nw:,j\u02c6hj(cid:38)(cid:40)\n\n\u03b2w:,i \u2212\n\n1\n2\n\n6=\ni\n\nrather than v. we can thus think of unit i as attempting to encode the residual\n\n6= w:,j \u02c6hj\n\ni\n\n(19.45)\n\n647\n\n "}, {"Page_number": 663, "text": "chapter 19. approximate inference\n\nerror in v given the code of the other units. we can thus think of sparse coding as\nan iterative autoencoder, that repeatedly encodes and decodes its input, attempting\nto fix mistakes in the reconstruction after each iteration.\n\nin this example, we have derived an update rule that updates a single unit at\na time. it would be advantageous to be able to update more units simultaneously.\nsome graphical models, such as deep boltzmann machines, are structured in such a\nway that we can solve for many entries of \u02c6h simultaneously. unfortunately, binary\nsparse coding does not admit such block updates. instead, we can use a heuristic\ntechnique called\nto perform block updates. in the damping approach, we\nsolve for the individually optimal values of every element of \u02c6h, then move all of\nthe values in a small step in that direction. this approach is no longer guaranteed\nto increase l at each step, but works well in practice for many models. see koller\n) for more information about choosing the degree of synchrony\nand friedman 2009\nand damping strategies in message passing algorithms.\n\ndamping\n\n(\n\n19.4.2 calculus of variations\n\nbefore continuing with our presentation of variational learning, we must briefly\nintroduce an important set of mathematical tools used in variational learning:\ncalculus of variations.\n\nmany machine learning techniques are based on minimizing a function j(\u03b8 ) by\nfinding the input vector \u03b8 \u2208 rn for which it takes on its minimal value. this can\nbe accomplished with multivariate calculus and linear algebra, by solving for the\ncritical points where \u2207\u03b8j (\u03b8) = 0. in some cases, we actually want to solve for a\nfunction f (x), such as when we want to find the probability density function over\nsome random variable. this is what calculus of variations enables us to do.\n\na function of a function f is known as a functional j[f ]. much as we can\ntake partial derivatives of a function with respect to elements of its vector-valued\nargument, we can take\nvariational derivatives\n,\nof a functional j[f ] with respect to individual values of the function f( x) at any\nspecific value of x. the functional derivative of the functional j with respect to\nthe value of the function\n\nfunctional derivatives\n\n, also known as\n\nis denoted\n\nat point\n\nx\n\nf\n\n\u03b4\n\n\u03b4f x( ) j .\n\na complete formal development of functional derivatives is beyond the scope of\nthis book. for our purposes, it is sufficient to state that for differentiable functions\nf\n\n( x) with continuous derivatives, that\n\n( )x and differentiable functions\n\ng y,\n\n\u03b4\n\n\u03b4f ( )x z g f\n\n( ( )x x) x =\n\nd\n\n,\n\n648\n\n\u2202\n\u2202y\n\ng f\n\n( ( )x x)\n.\n\n,\n\n(19.46)\n\n "}, {"Page_number": 664, "text": "chapter 19. approximate inference\n\nto gain some intuition for this identity, one can think of f (x) as being a vector\nwith uncountably many elements, indexed by a real vector x. in this (somewhat\nincomplete view), the identity providing the functional derivatives is the same as\nwe would obtain for a vector \u03b8 \u2208 r n indexed by positive integers:\n\n\u2202\n\n\u2202\u03b8ixj\n\ng \u03b8( j , j) =\n\n\u2202\n\u2202\u03b8i\n\ng \u03b8( i , i .)\n\n(19.47)\n\nmany results in other machine learning publications are presented using the more\ngeneral euler-lagrange equation which allows g to depend on the derivatives of f\nas well as the value of f , but we do not need this fully general form for the results\npresented in this book.\n\nto optimize a function with respect to a vector, we take the gradient of the\nfunction with respect to the vector and solve for the point where every element of\nthe gradient is equal to zero. likewise, we can optimize a functional by solving for\nthe function where the functional derivative at every point is equal to zero.\n\nas an example of how this process works, consider the problem of finding the\nprobability distribution function over x \u2208 r that has maximal differential entropy.\nrecall that the entropy of a probability distribution\n\nis defined as\n\np x( )\n\nfor continuous values, the expectation is an integral:\n\nh p[ ] = \u2212ex log ( )p x .\n\nh p[ ] = \u2212z p x\n\n( ) log ( )\n\np x dx.\n\n(19.48)\n\n(19.49)\n\nwe cannot simply maximize h(x) with respect to the function p(x), because the\nresult might not be a probability distribution. instead, we need to use lagrange\nmultipliers,\u00a0to add\u00a0a constraint that p(x) integrates to 1. also,\u00a0the entropy\nincreases without bound as the variance increases. this makes the question of\nwhich distribution has the greatest entropy uninteresting. instead, we ask which\ndistribution has maximal entropy for fixed variance \u03c32. finally, the problem\nis underdetermined because the distribution can be shifted arbitrarily without\nchanging the entropy. to impose a unique solution, we add a constraint that the\nmean of the distribution be \u00b5.\u00a0the lagrangian functional for this optimization\nproblem is\n\np\n\nl[ ] = \n\n\u03bb1(cid:31)z p x dx\n\n( ) \u2212 1(cid:32) + \u03bb2 ( [\n\n]\n\ne x \u2212 \u00b5\n\n)+\n\nx\n\n\u03bb3(cid:25)e[(\n\n)\n\n\u00b5\u2212 2] \u2212 \u03c32(cid:26)+ [ ]h p (19.50)\n\n649\n\n "}, {"Page_number": 665, "text": "chapter 19. approximate inference\n\n=z (cid:25) \u03bb1p x\n\n( ) + 2 p x x\n\n( ) + 3p x x\n\n\u03bb\n\n\u03bb\n\n( )( \u2212 )2 \u2212 p x\n\n\u00b5\n\np x\n\n( ) log ( )(cid:26) dx\n\n\u03bb\u2212 1 \u2212 \u00b5\u03bb2 \u2212 \u03c32 \u03bb3.\n(19.51)\n\nto minimize the lagrangian with respect to p, we set the functional derivatives\n\nequal to 0:\n\n\u03b4\n\u03b4p x( )l = \u03bb 1 + \u03bb2 x\n\n\u2200x,\n\n\u03bb+ 3(\n\nx\n\n\u00b5\u2212 2 \u2212 \u22121\n\n)\n\nlog ( ) = 0\n.\n\np x\n\n(19.52)\n\nthis condition now tells us the functional form of p(x). by algebraically\n\nre-arranging the equation, we obtain\n\np x( ) = exp(cid:25)\u2212\u03bb1 \u2212 \u03bb 2x\n\n\u03bb+ 3(\n\nx\n\n)\n\n\u00b5\u2212 2 + 1(cid:26) .\n\n(19.53)\n\nwe never assumed directly that p(x) would take this functional form; we\nobtained the expression itself by analytically minimizing a functional. to finish\nthe minimization problem, we must choose the \u03bb values to ensure that all of our\nconstraints are satisfied. we are free to choose any \u03bb values, because the gradient\nof the lagrangian with respect to the \u03bb variables is zero so long as the constraints\nare satisfied. to satisfy all of the constraints, we may set \u03bb1 = log \u03c3\u221a2\u03c0 , \u03bb 2 = 0,\nand \u03bb3 = \u2212 1\n\n2\u03c32 to obtain\n\n(19.54)\n\np x\n\n( ) = \n\nx \u00b5, \u03c3\n\n(n ;\n\n2 ).\n\nthis is one reason for using the normal distribution when we do not know the\ntrue distribution. because the normal distribution has the maximum entropy, we\nimpose the least possible amount of structure by making this assumption.\n\nwhile examining the critical points of the lagrangian functional for the entropy,\nwe found only one critical point, corresponding to maximizing the entropy for\nfixed variance. what about the probability distribution function that minimizes\nthe entropy? why did we not find a second critical point corresponding to the\nminimum? the reason is that there is no specific function that achieves minimal\nentropy. as functions place more probability density on the two points x = \u00b5 + \u03c3\nand x = \u00b5 \u03c3\u2212 , and place less probability density on all other values of x, they lose\nentropy while maintaining the desired variance. however, any function placing\nexactly zero mass on all but two points does not integrate to one, and is not a\nvalid probability distribution. there thus is no single minimal entropy probability\ndistribution function, much as there is no single minimal positive real number.\ninstead, we can say that there is a sequence of probability distributions converging\ntoward putting mass only on these two points. this degenerate scenario may be\ndescribed as a mixture of dirac distributions. because dirac distributions are\nnot described by a single probability distribution function, no dirac or mixture of\n\n650\n\n "}, {"Page_number": 666, "text": "chapter 19. approximate inference\n\ndirac distribution corresponds to a single specific point in function space. these\ndistributions are thus invisible to our method of solving for a specific point where\nthe functional derivatives are zero. this is a limitation of the method. distributions\nsuch as the dirac must be found by other methods, such as guessing the solution\nand then proving that it is correct.\n\n19.4.3 continuous latent variables\n\nwhen our graphical model contains continuous latent variables,\u00a0we may still\nperform variational inference and learning by maximizing l. however, we must\nnow use calculus of variations when maximizing with respect to\n\n.\n)\n\nl\n\nh v|\nq(\n\nin most cases, practitioners need not solve any calculus of variations problems\nthemselves. instead, there is a general equation for the mean field fixed point\nupdates. if we make the mean field approximation\n\nq(\n\n) =\n\nh v| yi\n\nq h( i | v),\n\n(19.55)\n\nand fix q (hj | v) for all j 6= i, then the optimal q (h i | v ) may be obtained by\nnormalizing the unnormalized distribution\n\n\u02dcq h( i | v) = exp(cid:25)eh\u2212i\u223cq(h\u2212i|v) log \u02dcp ,(v h)(cid:26)\n\n(19.56)\n\n0\n\nso long as p does not assign\nprobability to any joint configuration of variables.\ncarrying out the expectation inside the equation will yield the correct functional\nform of q (hi | v). it is only necessary to derive functional forms of q directly using\ncalculus of variations if one wishes to develop a new form of variational learning;\neq.\n\nyields the mean field approximation for any probabilistic model.\n\n19.56\n\neq.\n\n19.56\n\nis a fixed point equation, designed to be iteratively applied for each\nvalue of i repeatedly until convergence. however, it also tells us more than that. it\ntells us the functional form that the optimal solution will take, whether we arrive\nthere by fixed point equations or not. this means we can take the functional form\nfrom that equation but regard some of the values that appear in it as parameters,\nthat we can optimize with any optimization algorithm we like.\n\nas an example, consider a very simple probabilistic model, with latent variables\nh \u2208 r2 and just one visible variable, v. suppose that p(h ) = n(h; 0, i) and\np(v | h) = n (v; w> h; 1). we could actually simplify this model by integrating\nout h; the result is just a gaussian distribution over v.\u00a0the model itself is not\ninteresting; we have constructed it only to provide a simple demonstration of how\ncalculus of variations may be applied to probabilistic modeling.\n\n651\n\n "}, {"Page_number": 667, "text": "chapter 19. approximate inference\n\n(19.57)\n(19.58)\n\n(19.59)\n\n(19.60)\n\n(19.62)\n(19.63)\n\n(19.64)\n\n(19.65)\n\n)\n\nthe true posterior is given, up to a normalizing constant, by\nh v|\np(\n\u221dp\n,(h v)\n= (p h1 ) (p h2 ) (\n\np v h|\n)\n1 + h2\n\n2 + (v\n\nh\u2212 1w1 \u2212 h2 w2)2(cid:28)(cid:32)\n\n1 + h2\n\n2 + v 2 + h2\n\n1w2\n\n1 + h 2\n\n2w 2\n\n\u221d exp(cid:31)\u2212\n= exp(cid:31)\u2212\n\n1\n\n2(cid:27)h2\n2(cid:27)h2\n\n1\n\n2 \u2212 2vh1w1 \u2212 2vh 2w2 \u2212 2h 1 w1h2w2(cid:28)(cid:32) .\n\n(19.61)\n\ndue to the presence of the terms multiplying h1 and h2 together, we can see that\nthe true posterior does not factorize over h1 and h 2.\n\napplying eq.\n\n, we find that\n\n19.56\n\u02dcq h( 1 | v)\n\nh2\u223cq(h2|v) log \u02dcp ,(v h)(cid:26)\nh2\u223cq(h2|v)(cid:27) h2\n\n= exp(cid:25)e\n= exp(cid:31)\u2212\n\u22122vh1w1 \u2212 2vh2w2 \u2212 2h1w1h2 w2](cid:32).\n\n1 + h2\n\n2 + v2 + h2\n\n1\n2\n\ne\n\n1w2\n\n1 + h2\n\n2w2\n2\n\nfrom this, we can see that there are effectively only two values we need to obtain\n2 ].\u00a0writing these as hh2i and hh 2\n2i,\nfrom q(h2 | v): e\nwe obtain\n\nq(h v)[h2] and e\n\nq(h v)[h2\n\nh2\u223c |\n\nh2\u223c |\n\n\u02dcq h( 1 | v) = exp(cid:31)\u2212\n\n1\n\n2(cid:27)h2\n1 + hh2\n\n2\n\n1w 2\n\n(19.66)\n\n2i + v 2 + h2\n\n1 + hh 2\n2iw2\n\u22122vh1 w1 \u2212 h2v h2iw 2 \u2212 2h1 w1hh2iw 2](cid:32).\nfrom this, we can see that \u02dcq has the functional form of a gaussian. we can\n) = n (h; \u00b5 \u03b2, \u22121) where \u00b5 and diagonal \u03b2 are variational\nthus conclude q(h v|\nparameters that we can optimize using any technique we choose. it is important\nto recall that we did not ever assume that q would be gaussian; its gaussian\nform was derived automatically by using calculus of variations to maximize q with\nrespect to l. using the same approach on a different model could yield a different\nfunctional form of\n\n(19.67)\n\n.q\n\nthis was of course, just a small case constructed for demonstration purposes.\nfor examples of real applications of variational learning with continuous variables\nin the context of deep learning, see\n\ngoodfellow et al. 2013d\n\n).\n\n(\n\n652\n\n "}, {"Page_number": 668, "text": "chapter 19. approximate inference\n\n19.4.4\n\ninteractions between learning and inference\n\nusing approximate inference as part of a learning algorithm affects the learning\nprocess, and this in turn affects the accuracy of the inference algorithm.\n\nspecifically, the training algorithm tends to adapt the model in a way that makes\nthe approximating assumptions underlying the approximate inference algorithm\nbecome more true. when training the parameters, variational learning increases\n\neh\u223cq log (\n\n)\np v h,\n.\n\n(19.68)\n\nfor a specific v, this increases p(h v|\nunder q(h v|\n) and decreases p(h v|\nh v|\n.\nunder\n)\nq(\n\n) for values of h that have high probability\n) for values of h that have low probability\n\nthis behavior causes our approximating assumptions to become self-fulfilling\nprophecies. if we train the model with a unimodal approximate posterior, we will\nobtain a model with a true posterior that is far closer to unimodal than we would\nhave obtained by training the model with exact inference.\n\ncomputing the true amount of harm imposed on a model by a variational\napproximation is thus very difficult. there exist several methods for estimating\nlog p(v). we often estimate log p(v; \u03b8) after training the model, and find that\nthe gap with l(v \u03b8,\n, q ) is small. from this, we can conclude that our variational\napproximation is accurate for the specific value of \u03b8 that we obtained from the\nlearning process. we should not conclude that our variational approximation is\naccurate in general or that the variational approximation did little harm to the\nlearning process. to measure the true amount of harm induced by the variational\napproximation, we would need to know \u03b8\u2217 = max\u03b8 log p(v; \u03b8).\u00a0it is possible for\n, q) \u2248 log p(v; \u03b8 ) and log p(v;\u03b8 ) (cid:130) log p(v; \u03b8\u2217) to hold simultaneously. if\nl(v \u03b8,\nmaxq l(v \u03b8, \u2217 , q) (cid:130) log p(v ; \u03b8\u2217 ), because \u03b8\u2217 induces too complicated of a posterior\ndistribution for our q family to capture,\u00a0then the learning process will never\napproach \u03b8\u2217. such a problem is very difficult to detect, because we can only know\nfor sure that it happened if we have a superior learning algorithm that can find \u03b8\u2217\nfor comparison.\n\n19.5 learned approximate inference\n\nwe have seen that inference can be thought of as an optimization procedure\nthat increases the value of a function l. explicitly performing optimization via\niterative procedures such as fixed point equations or gradient-based optimization\nis often very expensive and time-consuming. many approaches to inference avoid\n\n653\n\n "}, {"Page_number": 669, "text": "chapter 19. approximate inference\n\nthis expense by learning to perform approximate inference.\u00a0specifically, we can\nthink of the optimization process as a function f that maps an input v to an\napproximate distribution q\u2217 = arg maxq l(v, q). once we think of the multi-step\niterative optimization process as just being a function, we can approximate it with\na neural network that implements an approximation \u02c6f ( ; )v \u03b8 .\n\n19.5.1 wake-sleep\n\net al.,\n\n1995b frey\n\none of the main difficulties with training a model to infer h from v is that we\ndo not have a supervised training set with which to train the model. given a v ,\nwe do not know the appropriate h. the mapping from v to h depends on the\nchoice of model family, and evolves throughout the learning process as \u03b8 changes.\nthe wake-sleep algorithm (hinton\n) resolves this\nproblem by drawing samples of both h and v from the model distribution.\u00a0for\nexample, in a directed model, this can be done cheaply by performing ancestral\nsampling beginning at h and ending at v. the inference network can then be\ntrained to perform the reverse mapping:\u00a0predicting which h caused the present\nv . the main drawback to this approach is that we will only be able to train the\ninference network on values of v that have high probability under the model. early\nin learning, the model distribution will not resemble the data distribution, so the\ninference network will not have an opportunity to learn on samples that resemble\ndata.\n\net al.,\n\n1996\n\n;\n\n18.2\n\nin sec.\n\nwe saw that one possible explanation for the role of dream sleep in\nhuman beings and animals is that dreams could provide the negative phase samples\nthat monte carlo training algorithms use to approximate the negative gradient of\nthe log partition function of undirected models. another possible explanation for\nbiological dreaming is that it is providing samples from p(h v,\n) which can be used\nto train an inference network to predict h given v. in some senses, this explanation\nis more satisfying than the partition function explanation. monte carlo algorithms\ngenerally do not perform well if they are run using only the positive phase of the\ngradient for several steps then with only the negative phase of the gradient for\nseveral steps. human beings and animals are usually awake for several consecutive\nhours then asleep for several consecutive hours. it is not readily apparent how this\nschedule could support monte carlo training of an undirected model.\u00a0learning\nalgorithms based on maximizing l can be run with prolonged periods of improving\nq and prolonged periods of improving \u03b8, however. if the role of biological dreaming\nis to train networks for predicting q, then this explains how animals are able to\nremain awake for several hours (the longer they are awake, the greater the gap\nbetween l and log p(v), but l will remain a lower bound) and to remain asleep\n\n654\n\n "}, {"Page_number": 670, "text": "chapter 19. approximate inference\n\nfor several hours (the generative model itself is not modified during sleep) without\ndamaging their internal models. of course, these ideas are purely speculative, and\nthere is no hard evidence to suggest that dreaming accomplishes either of these\ngoals. dreaming may also serve reinforcement learning rather than probabilistic\nmodeling, by sampling synthetic experiences from the animal\u2019s transition model,\non which to train the animal\u2019s policy. or sleep may serve some other purpose not\nyet anticipated by the machine learning community.\n\n19.5.2 other forms of learned inference\n\nthis strategy of learned approximate inference has also been applied to other\nmodels. salakhutdinov and larochelle 2010\n) showed that a single pass in a\nlearned inference network could yield faster inference than iterating the mean field\nfixed point equations in a dbm. the training procedure is based on running the\ninference network, then applying one step of mean field to improve its estimates,\nand training the inference network to output this refined estimate instead of its\noriginal estimate.\n\n(\n\n14.8\n\nwe have already seen in sec.\n\nthat the predictive sparse decomposition\nmodel trains a shallow encoder network to predict a sparse code for the input.\nthis can be seen as a hybrid between an autoencoder and sparse coding. it is\npossible to devise probabilistic semantics for the model, under which the encoder\nmay be viewed as performing learned approximate map inference. due to its\nshallow encoder, psd is not able to implement the kind of competition between\nunits that we have seen in mean field inference. however, that problem can be\nremedied by training a deep encoder to perform learned approximate inference, as\nin the ista technique (\n\ngregor and lecun 2010b\n\n).\n\n,\n\n,\n\n;\n\nlearned\u00a0approximate\u00a0inference\u00a0has recently\u00a0become\u00a0one\u00a0of\u00a0the\u00a0dominant\napproaches to generative modeling, in the form of the variational autoencoder\n(\n). in this elegant approach, there is no need to\nkingma 2013 rezende et al. 2014\nconstruct explicit targets for the inference network. instead, the inference network\nis simply used to define lelegant approach, there is no need the inference network\nare adapted to increase l. this model is described in depth later, in sec.\n20.10.3\n.\n\nusing approximate inference, it is possible to train and use a wide variety of\n\n,\n\nmodels. many of these models are described in the next chapter.\n\n655\n\n "}, {"Page_number": 671, "text": "chapter 20\n\ndeep generative models\n\nin this chapter, we present several of the specific kinds of generative models that\ncan be built and trained using the techniques presented in chapters\nand\n19. all of these models represent probability distributions over multiple variables\nin some way. some allow the probability distribution function to be evaluated\nexplicitly. others do not allow the evaluation of the probability distribution\nfunction, but support operations that implicitly require knowledge of it, such\nas drawing samples from the distribution. some of these models are structured\nprobabilistic models described in terms of graphs and factors, using the language\nof graphical models presented in chapter\n. others can not easily be described\nin terms of factors, but represent probability distributions nonetheless.\n\n16 17 18\n\n16\n\n,\n\n,\n\n20.1 boltzmann machines\n\n;\n\n1983 ackley\n\nboltzmann machines were originally introduced as a general \u201cconnectionist\u201d ap-\nproach to learning arbitrary probability distributions over binary vectors (fahlman\net al.,\n1984 hinton and sejnowski 1986\n).\nvariants of the boltzmann machine that include other kinds of variables have long\nago surpassed the popularity of the original. in this section we briefly introduce\nthe binary boltzmann machine and discuss the issues that come up when trying to\ntrain and perform inference in the model.\n\n1985 hinton\n\net al.,\n\net al.,\n\n;\n\n;\n\n,\n\nwe define the boltzmann machine over a d-dimensional binary random vector\n),\n\nx \u2208 {0, 1}d. the boltzmann machine is an energy-based model (sec.\n\n16.2.4\n\n656\n\n "}, {"Page_number": 672, "text": "chapter 20. deep generative models\n\nmeaning we define the joint probability distribution using an energy function:\n\np ( ) =x\n\nexp (\n\n\u2212e x\n( ))\nz\n\n,\n\n(20.1)\n\nwhere e(x) is the energy function and z is the partition function that ensures\n\n. the energy function of the boltzmann machine is given by\n\nthatpx p ( ) = 1\n\nx\n\ne( ) = \n\nx\n\n\u2212x>u x b\u2212 >x,\n\n(20.2)\n\nwhere u is the \u201cweight\u201d matrix of model parameters and b is the vector of bias\nparameters.\n\nin the general setting of the boltzmann machine, we are given a set of training\ndescribes the joint probability\nexamples, each of which are n-dimensional. eq.\ndistribution over the observed variables. while this scenario is certainly viable,\nit does limit the kinds of interactions between the observed variables to those\ndescribed by the weight matrix. specifically, it means that the probability of one\nunit being on is given by a linear model (logistic regression) from the values of the\nother units.\n\n20.1\n\nthe boltzmann machine becomes more powerful when not all the variables\nare observed. in this case, the non-observed variables, or\nvariables, can\nact similarly to hidden units in a multi-layer perceptron and model higher-order\ninteractions among the visible units. just as the addition of hidden units to\nconvert logistic regression into an mlp results in the mlp being a universal\napproximator of functions, a boltzmann machine with hidden units is no longer\nlimited to modeling linear relationships between variables. instead, the boltzmann\nmachine becomes a universal approximator of probability mass functions over\ndiscrete variables (\n\nle roux and bengio 2008\n\nlatent\n\n).\n\n,\n\nformally, we decompose the units x into two subsets: the visible units v and\n\nthe latent (or hidden) units\n\nh\n\ne ,(v h\n\n) = \u2212 >rv\n\nv\n\n. the energy function becomes\nv\u2212 > w h h\u2212 > sh b\u2212 >v\n\nc\u2212 >h.\n\n(20.3)\n\nboltzmann machine learning learning algorithms for boltzmann machines\nare usually based on maximum likelihood. all boltzmann machines have an\nintractable partition function, so the maximum likelihood gradient must be ap-\nproximated using the techniques described in chapter\n\n.18\n\none interesting property of boltzmann machines when trained with learning\nrules based on maximum likelihood is that the update for a particular weight\nconnecting two units depends only the statistics of those two units,\u00a0collected\n\n657\n\n "}, {"Page_number": 673, "text": "chapter 20. deep generative models\n\nunder different distributions: pmodel(v) and \u02c6pdata(v)pmodel(h v|\n). the rest of the\nnetwork participates in shaping those statistics, but the weight can be updated\nwithout knowing anything about the rest of the network or how those statistics were\nproduced. this means that the learning rule is \u201clocal,\u201d which makes boltzmann\nmachine learning somewhat biologically plausible.\u00a0it is conceivable that if each\nneuron were a random variable in a boltzmann machine, then the axons and\ndendrites connecting two random variables could learn only by observing the firing\npattern of the cells that they actually physically touch.\nin particular, in the\npositive phase, two units that frequently activate together have their connection\nstrengthened. this is an example of a hebbian learning rule (\n) often\nsummarized with the mnemonic \u201cfire together, wire together.\u201d\u00a0hebbian learning\nrules are among the oldest hypothesized explanations for learning in biological\nsystems and remain relevant today (\n\ngiudice et al. 2009\n\nhebb 1949\n\n).\n\n,\n\n,\n\nother learning algorithms that use more information than local statistics seem\nto require us to hypothesize the existence of more machinery than this. for\nexample, for the brain to implement back-propagation in a multilayer perceptron,\nit seems necessary for the brain to maintain a secondary communication network for\ntransmitting gradient information backwards through the network. proposals for\nbiologically plausible implementations (and approximations) of back-propagation\nhave been made (\n) but remain to be validated, and\nbengio 2015\n) links back-propagation of gradients to inference in energy-based\nmodels similar to the boltzmann machine (but with continuous latent variables).\n\nhinton 2007a bengio 2015\n\n(\n\n,\n\n;\n\n,\n\nthe negative phase of boltzmann machine learning is somewhat harder to\n, dream sleep may\n\nexplain from a biological point of view. as argued in sec.\nbe a form of negative phase sampling. this idea is more speculative though.\n\n18.2\n\n20.2 restricted boltzmann machines\n\n,\n\n(\n\nharmonium smolensky 1986\n\ninvented under the name\n), restricted boltzmann\nmachines are some of the most common building blocks of deep probabilistic models.\nwe have briefly described rbms previously, in sec.\u00a0\n.\u00a0here we review the\nprevious information and go into more detail. rbms are undirected probabilistic\ngraphical models containing a layer of observable variables and a single layer of\nlatent variables. rbms may be stacked (one on top of the other) to form deeper\nmodels. see fig.\na shows the graph\nstructure of the rbm itself. it is a bipartite graph, with no connections permitted\nbetween any variables in the observed layer or between any units in the latent\nlayer.\n\nfor some examples. in particular, fig.\n\n16.7.1\n\n20.1\n\n20.1\n\n658\n\n "}, {"Page_number": 674, "text": "chapter 20. deep generative models\n\nh(2)\n1h(2)\n1\n\nh(2)\n2h(2)\n2\n\nh(2)\n3h(2)\n3\n\nh1h1\n\nh2h2\n\nh3h3\n\nh4h4\n\nh(1)\n1h(1)\n1\n\nh(1)\n2h(1)\n2\n\nh(1)\n3h(1)\n3\n\nh(1)\n4h(1)\n4\n\nv1v1\n\nv2v2\n\na\n\nv3v3\n\nv1v1\n\nv2v2\n\nv3v3\n\nb\n\nh(2)\n1h(2)\n1\n\nh(2)\n2h(2)\n2\n\nh(2)\n3h(2)\n3\n\nh(1)\n1h(1)\n1\n\nh(1)\n2h(1)\n2\n\nh(1)\n3h(1)\n3\n\nh(1)\n4h(1)\n4\n\nv1v1\n\nv2v2\n\nv3v3\n\nc\n\nfigure 20.1: examples of models that may be built with restricted boltzmann machines.\n(a) the restricted boltzmann machine itself is an undirected graphical model based on\na bipartite graph, with visible units in one part of the graph and hidden units in the\nother part. there are no connections among the visible units, nor any connections among\nthe hidden units.\u00a0typically every visible unit is connected to every hidden unit but it\nis possible to construct sparsely connected rbms such as convolutional rbms.\na(b)\ndeep belief network is a hybrid graphical model involving both directed and undirected\nconnections. like an rbm, it has no intra-layer connections. however, a dbn has\nmultiple hidden layers, and thus there are connections between hidden units that are in\nseparate layers. all of the local conditional probability distributions needed by the deep\nbelief network are copied directly from the local conditional probability distributions of\nits constituent rbms. alternatively, we could also represent the deep belief network with\na completely undirected graph, but it would need intra-layer connections to capture the\ndependencies between parents.\na deep boltzmann machine is an undirected graphical\nmodel with several layers of latent variables. like rbms and dbns, dbms lack intra-layer\nconnections.\u00a0dbms are less closely tied to rbms than dbns are.\u00a0when initializing a\ndbm from a stack of rbms, it is necessary to modify the rbm parameters slightly. some\nkinds of dbms may be trained without first training a set of rbms.\n\n(c)\n\n659\n\n "}, {"Page_number": 675, "text": "chapter 20. deep generative models\n\nwe begin with the binary version of the restricted boltzmann machine, but as\n\nwe see later there are extensions to other types of visible and hidden units.\n\nmore formally, let the observed layer consist of a set of n v binary random\nvariables which we refer to collectively with the vector v. we refer to the latent or\nhidden layer of nh binary random variables as\n\n.h\n\nlike the general boltzmann machine, the restricted boltzmann machine is an\nenergy-based model with the joint probability distribution specified by its energy\nfunction:\n\np\n\n( = v\n\nv h =  ) =h\n\n,\n\nexp (\n\n(\n\n\u2212e v h,\n))\n\n.\n\n(20.4)\n\n1\nz\n\nthe energy function for an rbm is given by\n\nand\n\nz\n\nis the normalizing constant known as the partition function:\n\ne ,(v h\n\nb\n\n) = \u2212 >v\nz =xv xh\n\nc\u2212 > h v\u2212 >w h,\n\nexp\n\n} .\n{\u2212e v h,\n)\n\n(\n\n(20.5)\n\n(20.6)\n\nit is apparent from the definition of the partition function z that the naive method\nof computing z (exhaustively summing over all states) could be computationally\nintractable, unless a cleverly designed algorithm could exploit regularities in the\nprobability distribution to compute z faster. in the case of restricted boltzmann\nmachines,\nz\nis intractable. the intractable partition function z implies that the normalized\njoint probability distribution\n\n) formally proved that the partition function\n\nis also intractable to evaluate.\n\nlong and servedio 2010\n\np ( )v\n\n(\n\n20.2.1 conditional distributions\n\nthough p (v) is intractable, the bipartite graph structure of the rbm has the\nvery special property that its conditional distributions p(h v|\n) are\nfactorial and relatively simple to compute and to sample from.\n\n) and p (v h|\n\nderiving the conditional distributions from the joint distribution is straightfor-\n\nward:\n\np (\n\nh v|\n\n) =\n\n=\n\n=\n\np ,(h v)\n\np ( )v\n1\n1\nz\n\np ( )v\n1\nz 0\n\nexpnb>v\n\nc+ >h v+ >w ho\n\nexpnc>h v+ >w ho\n\n660\n\n(20.7)\n\n(20.8)\n\n(20.9)\n\n "}, {"Page_number": 676, "text": "chapter 20. deep generative models\n\n=\n\n=\n\n1\nz 0\n\n1\nz 0\n\ncj hj +\n\nnhxj=1\n\nv> w:,j hj(cid:46)(cid:47)(cid:48)\nexp(cid:41)(cid:42)(cid:43)\nnhxj=1\nexpnc jhj + v>w:,j hjo\nnhyj=1\n\n(20.10)\n\n(20.11)\n\nsince we are conditioning on the visible units v , we can treat these as constant\nwith respect to the distribution p (h v|\n). the factorial nature of the conditional\np (h v|\n) follows immediately from our ability to write the joint probability over\nthe vector h as the product of (unnormalized) distributions over the individual\nelements, hj. it is now a simple matter of normalizing the distributions over the\nindividual binary hj.\n\n)\n\n(20.12)\n\n(20.13)\n\n(20.14)\n\np h( j = 1 \n\n| v\n\n) =\n\n=\n\n\u02dcp h( j = 0 \n\n\u02dcp h( j = 1 \n) +| v\n\n)\n\n| v\n\u02dcp h( j = 1 \n\nexp(cid:30) cj + v>w:,j(cid:31)\n\n{cj + v>w:,j}\n\n{ }\n\n| v\n\nexp 0 + exp\n\n= \u03c3(cid:33)cj + v>w:,j(cid:34) .\n\u03c3(cid:33)(2\nnhyj=1\n\n1)\n\nh \u2212 (cid:130) c w >v)(cid:34)j\n\n( +\n\nwe can now express the full conditional over the hidden layer as the factorial\ndistribution:\n\np (\n\nh v|\n\n) =\n\n.\n\n(20.15)\n\na similar derivation will show that the other condition of interest to us, p (v h|\n),\n\nis also a factorial distribution:\n\np (\n\nv h|\n\n) =\n\nn vyi=1\n\n\u03c3 ((2\n\n1)\n\nv \u2212 (cid:130) b w h i .\n))\n\n( +\n\n(20.16)\n\n20.2.2 training restricted boltzmann machines\n\nbecause the rbm admits efficient evaluation and differentiation of \u02dcp (v) and\nefficient mcmc sampling in the form of block gibbs sampling, it can readily be\ntrained with any of the techniques described in chapter\nfor training models\nthat have intractable partition functions.\u00a0this includes cd, sml (pcd), ratio\nmatching and so on. compared to other undirected models used in deep learning,\nthe rbm is relatively straightforward to train because we can compute p(h | v)\n\n18\n\n661\n\n "}, {"Page_number": 677, "text": "chapter 20. deep generative models\n\nexactly in closed form. some other deep models, such as the deep boltzmann\nmachine, combine both the difficulty of an intractable partition function and the\ndifficulty of intractable inference.\n\n20.3 deep belief networks\n\net al.,\n\ndeep belief networks (dbns) were one of the first non-convolutional models to\nsuccessfully admit training of deep architectures (hinton\n2006 hinton\n,\n2007b). the introduction of deep belief networks in 2006 began the current deep\nlearning renaissance. prior to the introduction of deep belief networks, deep models\nwere considered too difficult to optimize. kernel machines with convex objective\nfunctions dominated the research landscape. deep belief networks demonstrated\nthat deep architectures can be successful, by outperforming kernelized support\nvector machines on the mnist dataset (\n). today, deep belief\nnetworks have mostly fallen out of favor and are rarely used, even compared to\nother unsupervised or generative learning algorithms, but they are still deservedly\nrecognized for their important role in deep learning history.\n\nhinton et al. 2006\n\n;\n\n,\n\ndeep belief networks are generative models with several layers of latent variables.\nthe latent variables are typically binary, while the visible units may be binary\nor real. there are no intra-layer connections. usually, every unit in each layer is\nconnected to every unit in each neighboring layer, though it is possible to construct\nmore sparsely connected dbns. the connections between the top two layers are\nundirected. the connections between all other layers are directed, with the arrows\npointed toward the layer that is closest to the data. see fig.\nb for an example.\na dbn with l hidden layers contains l weight matrices: w (1), . . . , w( )l . it\nalso contains l + 1 bias vectors: b (0), . . . , b( )l , with b(0) providing the biases for the\nvisible layer. the probability distribution represented by the dbn is given by\n\n20.1\n\np (h( )l , h(\n\n1)\n\nl\u2212 ) \n\nl\u2212 >h(\n\n1)\n\nl\u2212 + h(\n\n(20.17)\n\n1)\n\nexp\u221d\n\n(cid:33)b ( )l >h ( )l + b(\n) = \u03c3(cid:33)b( )k\np v( i = 1 | h(1)) = \u03c3(cid:33)b(0)\n\n>\ni + w ( +1)\n\nk\n:,i\n\nk\n\ni, k\n\nh( +1)\n\n(cid:34)\u2200 \u2200 \u2208\ni + w (1)>:,i h(1)(cid:34)\u2200i.\n\n1)\n\nl\u2212 >w ( )l h( )l(cid:34) ,\n\n1, . . . , l\n\np h( ( )k\n\ni = 1 | h( +1)\n\nk\n\nin the case of real-valued visible units, substitute\n\n2,\n\n\u2212\n\n(20.18)\n\n(20.19)\n\n(20.20)\n\nv \u223c n (cid:33)v b; (0) + w (1)> h(1), \u03b2 \u22121(cid:34)\n\n662\n\n "}, {"Page_number": 678, "text": "chapter 20. deep generative models\n\nwith \u03b2 diagonal for tractability. generalizations to other exponential family visible\nunits are straightforward, at least in theory. a dbn with only one hidden layer is\njust an rbm.\n\nto generate a sample from a dbn, we first run several steps of gibbs sampling\non the top two hidden layers. this stage is essentially drawing a sample from\nthe rbm defined by the top two hidden layers. we can then use a single pass of\nancestral sampling through the rest of the model to draw a sample from the visible\nunits.\n\ndeep belief networks incur many of the problems associated with both directed\n\nmodels and undirected models.\n\ninference in a deep belief network is intractable due to the explaining away\neffect within each directed layer, and due to the interaction between the two hidden\nlayers that have undirected connections. evaluating or maximizing the standard\nevidence lower bound on the log-likelihood is also intractable, because the evidence\nlower bound takes the expectation of cliques whose size is equal to the network\nwidth.\n\nevaluating or maximizing the log-likelihood requires not just confronting the\nproblem of intractable inference to marginalize out the latent variables, but also\nthe problem of an intractable partition function within the undirected model of\nthe top two layers.\n\nto train a deep belief network, one begins by training an rbm to maximize\nev\u223cpdata log p(v) using contrastive divergence or stochastic maximum likelihood.\nthe parameters of the rbm then define the parameters of the first layer of the\ndbn. next, a second rbm is trained to approximately maximize\n\nev\u223cpdata\n\ne\n\nh(1)\u223cp(1) (h(1)|v) log p(2)(h(1) )\n\n(20.21)\n\nwhere p(1) is the probability distribution represented by the first rbm and p(2)\nis the probability distribution represented by the second rbm. in other words,\nthe second rbm is trained to model the distribution defined by sampling the\nhidden units of the first rbm, when the first rbm is driven by the data. this\nprocedure can be repeated indefinitely, to add as many layers to the dbn as\ndesired, with each new rbm modeling the samples of the previous one. each rbm\ndefines another layer of the dbn. this procedure can be justified as increasing a\nvariational lower bound on the log-likelihood of the data under the dbn (hinton\net al.,\n\n2006\n\n).\n\nin most applications, no effort is made to jointly train the dbn after the greedy\nlayer-wise procedure is complete. however, it is possible to perform generative\nfine-tuning using the wake-sleep algorithm.\n\n663\n\n "}, {"Page_number": 679, "text": "chapter 20. deep generative models\n\nthe trained dbn may be used directly as a generative model, but most of the\ninterest in dbns arose from their ability to improve classification models. we can\ntake the weights from the dbn and use them to define an mlp:\n\nh(1) = \u03c3(cid:33)b(1) + v >w (1)(cid:34).\nl\u2212 >w ( )l (cid:34)\u2200 \u2208l\n\ni + h(\n\n1)\n\nh( )l = \u03c3(cid:33)b( )l\n\n2, . . . , m,\n\n(20.22)\n\n(20.23)\n\nafter initializing this mlp with the weights and biases learned via generative\ntraining of the dbn, we may train the mlp to perform a classification task. this\nadditional training of the mlp is an example of discriminative fine-tuning.\n\n19\n\ntight\n\nthis specific choice of mlp is somewhat arbitrary, compared to many of the\nthat are derived from first principles. this mlp\ninference equations in chapter\nis a heuristic choice that seems to work well in practice and is used consistently\nin the literature. many approximate inference techniques are motivated by their\nability to find a maximally\nvariational lower bound on the log-likelihood\nunder some set of constraints. one can construct a variational lower bound on the\nlog-likelihood using the hidden unit expectations defined by the dbn\u2019s mlp, but\nthis is true of\nprobability distribution over the hidden units, and there is no\nreason to believe that this mlp provides a particularly tight bound.\u00a0in particular,\nthe mlp ignores many important interactions in the dbn graphical model. the\nmlp propagates information upward from the visible units to the deepest hidden\nunits, but does not propagate any information downward or sideways. the dbn\ngraphical model has explaining away interactions between all of the hidden units\nwithin the same layer as well as top-down interactions between layers.\n\nany\n\nwhile the log-likelihood of a dbn is intractable, it may be approximated with\n). this permits evaluating its quality as a\n\n,\n\nais (salakhutdinov and murray 2008\ngenerative model.\n\nthe term \u201cdeep belief network\u201d is commonly used incorrectly to refer to any\nkind of deep neural network, even networks without latent variable semantics.\nthe term \u201cdeep belief network\u201d should refer specifically to models with undirected\nconnections in the deepest layer and directed connections pointing downward\nbetween all other pairs of consecutive layers.\n\nthe term \u201cdeep belief network\u201d may also cause some confusion because the\nterm \u201cbelief network\u201d is sometimes used to refer to purely directed models, while\ndeep belief networks contain an undirected layer. deep belief networks also share\nthe acronym dbn with dynamic bayesian networks (dean and kanazawa 1989\n),\nwhich are bayesian networks for representing markov chains.\n\n,\n\n664\n\n "}, {"Page_number": 680, "text": "chapter 20. deep generative models\n\nh(2)\n1h(2)\n1\n\nh(2)\n2h(2)\n2\n\nh(2)\n3h(2)\n3\n\nh(1)\n1h(1)\n1\n\nh(1)\n2h(1)\n2\n\nh(1)\n3h(1)\n3\n\nh(1)\n4h(1)\n4\n\nv1v1\n\nv2v2\n\nv3v3\n\nfigure 20.2: the graphical model for a deep boltzmann machine with one visible layer\n(bottom) and two hidden layers. connections are only between units in neighboring layers.\nthere are no intra-layer layer connections.\n\n20.4 deep boltzmann machines\n\nor\n\ndbm\n\n(salakhutdinov and hinton 2009a\n\na deep boltzmann machine\n) is another\nkind of deep, generative model.\u00a0unlike the deep belief network (dbn), it is an\nentirely undirected model. unlike the rbm, the dbm has several layers of latent\nvariables (rbms have just one). but like the rbm, within each layer, each of the\nvariables are mutually independent, conditioned on the variables in the neighboring\nlayers. see fig.\nfor the graph structure. deep boltzmann machines have been\n).\n2013\napplied to a variety of tasks including document modeling (srivastava\n\net al.,\n\n20.2\n\n,\n\nlike rbms and dbns,\u00a0dbms typically contain only binary units\u2014as we\nassume for simplicity of our presentation of the model\u2014but it is straightforward\nto include real-valued visible units.\n\na dbm is an energy-based model, meaning that the the joint probability\ndistribution over the model variables is parametrized by an energy function e. in\nthe case of a deep boltzmann machine with one visible layer, v, and three hidden\nlayers, h(1), h(2) and h(3) , the joint probability is given by:\n\np(cid:33)v h,\n\n(1), h(2), h(3)(cid:34) =\n\n1\n\nz( )\u03b8\n\nexp(cid:33)\u2212e ,(v h (1), h(2) , h(3); )\u03b8 (cid:34) .\n\n(20.24)\n\nto simplify our presentation, we omit the bias parameters below. the dbm energy\nfunction is then defined as follows:\n\ne ,(v h(1), h(2), h(3); ) = \n\n\u03b8\n\n\u2212v> w(1)h(1) \u2212 h(1)>w (2)h(2) \u2212 h(2)>w (3)h(3).\n\n(20.25)\n\n665\n\n "}, {"Page_number": 681, "text": "chapter 20. deep generative models\n\nh(3)\n1h(3)\n1\n\nh(3)\n2h(3)\n2\n\nh(2)\n1h(2)\n1\n\nh(2)\n2h(2)\n2\n\nh(1)\n1h(1)\n1\n\nh(1)\n2h(1)\n2\n\nh(2)\n3h(2)\n3\n\nh(1)\n3h(1)\n3\n\nv1v1\n\nv2v2\n\nh(2)\n1h(2)\n1\n\nh(2)\n2h(2)\n2\n\nh(2)\n3h(2)\n3\n\nv\n\n1\n\nv\n\n2\n\nh(3)\n1h(3)\n1\n\nh(3)\n2h(3)\n2\n\nh(1)\n1h(1)\n1\n\nh(1)\n2h(1)\n2\n\nh(1)\n3h(1)\n3\n\nfigure 20.3: a deep boltzmann machine, re-arranged to reveal its bipartite graph structure.\n\nin comparison to the rbm energy function (eq.\n\n), the dbm energy\nfunction includes connections between the hidden units (latent variables) in the\nform of the weight matrices (w (2) and w (3)). as we will see, these connections\nhave significant consequences for both the model behavior as well as how we go\nabout performing inference in the model.\n\n20.5\n\nin comparison to fully connected boltzmann machines (with every unit con-\nnected to every other unit), the dbm offers some advantages that are similar to\nthose offered by the rbm. specifically, as illustrated in fig.\n, the dbm layers\ncan be organized into a bipartite graph, with odd layers on one side and even layers\non the other. this immediately implies that when we condition on the variables in\nthe even layer, the variables in the odd layers become conditionally independent.\nof course, when we condition on the variables in the odd layers, the variables in\nthe even layers also become conditionally independent.\n\n20.3\n\nthe bipartite structure of the dbm means that we can apply the same equa-\ntions we have previously used for the conditional distributions of an rbm to\ndetermine the conditional distributions in a dbm. the units within a layer are\nconditionally independent from each other given the values of the neighboring\nlayers, so the distributions over binary variables can be fully described by the\nbernoulli parameters giving the probability of each unit being active.\nin our\nexample with two hidden layers, the activation probabilities are given by:\n\np v( i = 1 | h(1)) = \u03c3(cid:33)w (1)\n\ni,: h(1)(cid:34) ,\n\n666\n\n(20.26)\n\n "}, {"Page_number": 682, "text": "chapter 20. deep generative models\n\nand\n\np h( (1)\n\ni = 1 | v h,\n\n:,i + w (2)\n\n(2)) = \u03c3(cid:33)v >w (1)\n\ni,: h(2)(cid:34)\nk = 1 | h(1)) = \u03c3(cid:33)h (1)>w (2)\n:,k(cid:34) .\n\np h( (2)\n\n(20.27)\n\n(20.28)\n\nthe bipartite structure makes gibbs sampling in a deep boltzmann machine\nefficient.\u00a0the naive approach to gibbs sampling is to update only one variable\nat a time. rbms allow all of the visible units to be updated in one block and all\nof the hidden units to be updated in a second block. one might naively assume\nthat a dbm with l layers requires l + 1 updates, with each iteration updating a\nblock consisting of one layer of units. instead, it is possible to update all of the\nunits in only two iterations. gibbs sampling can be divided into two blocks of\nupdates, one including all even layers (including the visible layer) and the other\nincluding all odd layers. due to the bipartite dbm connection pattern, given\nthe even layers, the distribution over the odd layers is factorial and thus can be\nsampled simultaneously and independently as a block.\u00a0likewise, given the odd\nlayers, the even layers can be sampled simultaneously and independently as a\nblock. efficient sampling is especially important for training with the stochastic\nmaximum likelihood algorithm.\n\n20.4.1\n\ninteresting properties\n\ndeep boltzmann machines have many interesting properties.\n\ndbms were developed after dbns. compared to dbns, the posterior distribu-\ntion p(h v|\n) is simpler for dbms. somewhat counterintuitively, the simplicity of\nthis posterior distribution allows richer approximations of the posterior. in the case\nof the dbn, we perform classification using a heuristically motivated approximate\ninference procedure, in which we guess that a reasonable value for the mean field\nexpectation of the hidden units can be provided by an upward pass through the\nnetwork in an mlp that uses sigmoid activation functions and the same weights\nas the original dbn. any distribution q(h) may be used to obtain a variational\nlower bound on the log-likelihood. this heuristic procedure therefore allows us to\nobtain such a bound. however, the bound is not explicitly optimized in any way, so\nthe bound may be far from tight. in particular, the heuristic estimate of q ignores\ninteractions between hidden units within the same layer as well as the top-down\nfeedback influence of hidden units in deeper layers on hidden units that are closer\nto the input. because the heuristic mlp-based inference procedure in the dbn\nis not able to account for these interactions, the resulting q is presumably far\n\n667\n\n "}, {"Page_number": 683, "text": "chapter 20. deep generative models\n\nfrom optimal. in dbms, all of the hidden units within a layer are conditionally\nindependent given the other layers. this lack of intra-layer interaction makes it\npossible to use fixed point equations to actually optimize the variational lower\nbound and find the true optimal mean field expectations (to within some numerical\ntolerance).\n\nthe use of proper mean field allows the approximate inference procedure for\ndbms to capture the influence of top-down feedback interactions. this makes\ndbms interesting from the point of view of neuroscience, because the human brain\nis known to use many top-down feedback connections. because of this property,\ndbms have been used as computational models of real neuroscientific phenomena\n(\nseries et al. 2010 reichert et al. 2011\n\n).\n\n,\n\n;\n\n,\n\none unfortunate property of dbms is that sampling from them is relatively\ndifficult. dbns only need to use mcmc sampling in their top pair of layers. the\nother layers are used only at the end of the sampling process, in one efficient\nancestral sampling pass. to generate a sample from a dbm, it is necessary to\nuse mcmc across all layers, with every layer of the model participating in every\nmarkov chain transition.\n\n20.4.2 dbm mean field inference\n\n(1)), p (h(1) | v h,\n\nthe conditional distribution over one dbm layer given the neighboring layers is\nfactorial. in the example of the dbm with two hidden layers, these distributions\n(2)) and p(h(2) | h(1) ). the distribution over all\nare p (v h|\nhidden layers generally does not factorize because of interactions between layers.\nin the example with two hidden layers, p(h(1), h(2) | v) does not factorize due due\nto the interaction weights w (2) between h (1) and h(2) which render these variables\nmutually dependent.\n\nvariational\n\nas was the case with the dbn, we are left to seek out methods to approximate\nthe dbm posterior distribution.\u00a0however, unlike the dbn, the dbm posterior\ndistribution over their hidden units\u2014while complicated\u2014is easy to approximate\nwith a\n), specifically a mean\nfield approximation. the mean field approximation is a simple form of variational\ninference, where we restrict the approximating distribution to fully factorial distri-\nbutions. in the context of dbms, the mean field equations capture the bidirectional\ninteractions between layers.\u00a0in this section we derive the iterative approximate\ninference procedure originally introduced in salakhutdinov and hinton 2009a\n\napproximation (as discussed in sec.\n\n19.4\n\n).\n\n(\n\nin variational approximations to inference, we approach the task of approxi-\nmating a particular target distribution\u2014in our case, the posterior distribution over\n\n668\n\n "}, {"Page_number": 684, "text": "chapter 20. deep generative models\n\nthe hidden units given the visible units\u2014by some reasonably simple family of dis-\ntributions. in the case of the mean field approximation, the approximating family\nis the set of distributions where the hidden units are conditionally independent.\n\nwe now develop the mean field approach for the example with two hidden\nlayers. let q(h(1) , h(2) | v) be the approximation of p (h(1), h(2) | v). the mean\nfield assumption implies that\n\nq(h(1) , h(2) | v) =yj\n\nq h( (1)\nj\n\n| v)yk\n\nq h( (2)\nk\n\n| v).\n\n(20.29)\n\nthe mean field approximation attempts to find a member of this family of\ndistributions that best fits the true posterior p (h (1), h(2) | v).\u00a0importantly, the\ninference process must be run again to find a different distribution q every time\nwe use a new value of\n\n.v\n\none can conceive of many ways of measuring how well q(h v|\n\nthe mean field approach is to minimize\n\n) fits p (h v|\n).\n\nkl(\n\n) =\n\nq pk xh\n\nq(h(1), h(2) | v) log\u00a0 q(h(1), h(2) | v)\np (h(1), h(2) | v)! .\n\n(20.30)\n\nin general, we do not have to provide a parametric form of the approximating\ndistribution beyond enforcing the independence assumptions. the variational\napproximation procedure is generally able to recover a functional form of the\napproximate distribution. however, in the case of a mean field assumption on\nbinary hidden units (the case we are developing here) there is no loss of generality\nresulting from fixing a parametrization of the model in advance.\n\nk \u2208 [0 1]\n\n,\n\nwe parametrize q as a product of bernoulli distributions, that is we associate\nthe probability of each element of h(1) with a parameter. specifically, for each j,\n\u02c6h (1)\nj = q(h (1)\nk = 1 | v),\nwhere \u02c6h(2)\n\n. thus we have the following approximation to the posterior:\n\nk = q(h(2)\n\nj = 1 | v), where \u02c6h(1)\nj \u2208 [0 , 1] and for each k , \u02c6h (2)\n| v)yk\nj (1 \u2212 \u02c6h (1)\n\n| v)\nj )(1\u2212h(1)\n\nj ) h(1)\n\nk )h(2)\n\nq h( (2)\nk\n\nq h( (1)\n\n( \u02c6h (1)\n\n(\u02c6h (2)\n\nj\n\nj\n\nq(h(1), h (2) | v) = yj\n= yj\n\nk (1 \u2212 \u02c6h(2)\n\nk ) (1\u2212h(2)\nk ).\n\n) \u00d7yk\n\n(20.31)\n\n(20.32)\n\nof course, for dbms with more layers the approximate posterior parametrization\ncan be extended in the obvious way, exploiting the bipartite structure of the graph\n\n669\n\n "}, {"Page_number": 685, "text": "chapter 20. deep generative models\n\nto update all of the even layers simultaneously and then to update all of the odd\nlayers simultaneously, following the same schedule as gibbs sampling.\n\nnow that we have specified our family of approximating distributions q, it\nremains to specify a procedure for choosing the member of this family that best\nfits p . the most straightforward way to do this is to use the mean field equations\nspecified by eq.\n. these equations were derived by solving for where the\nderivatives of the variational lower bound are zero. they describe in an abstract\nmanner how to optimize the variational lower bound for any model, simply by\ntaking expectations with respect to\n\n19.56\n\n.q\n\napplying these general equations, we obtain the update rules (again, ignoring\n\nbias terms):\n\n\u02c6h(1)\n\nj = \u03c3\u00a0 xi\nk = \u03c3(cid:39)(cid:40)xj 0\n\n\u02c6h(2)\n\nvi w(1)\n\nw(2)\nj0 ,k\n\ni,j +xk0\nj0 (cid:44)(cid:45) ,\n\n\u02c6h(1)\n\nk0! ,\n\n\u02c6h (2)\n\nw (2)\nj,k0\n\nj\u2200\n\nk.\u2200\n\n(20.33)\n\n(20.34)\n\nat a fixed point of this system of equations, we have a local maximum of the\nvariational lower bound l(q). thus these fixed point update equations define\nan iterative algorithm where we alternate updates of \u02c6h(1)\n) and\nupdates of \u02c6h(2)\n). on small problems such as mnist, as few\nk\nas ten iterations can be sufficient to find an approximate positive phase gradient\nfor learning, and fifty usually suffice to obtain a high quality representation of\na single specific example to be used for high-accuracy classification. extending\napproximate variational inference to deeper dbms is straightforward.\n\n(using eq.\n\n(using eq.\n\n20.33\n\n20.34\n\nj\n\n20.4.3 dbm parameter learning\n\n.\n\nlearning in the dbm must confront both the challenge of\u00a0an intractable\n, and the challenge of an\n\npartition function, using the techniques from chapter\nintractable posterior distribution, using the techniques from chapter\n\n.19\n\n18\n\nas described in sec.\n\ndistribution q(h v|\nproceeds by maximizing l (v\n.\nlog-likelihood,\n)\n\nlog ( ;\n\np v \u03b8\n\n20.4.2\n\n) that approximates the intractable p(h v|\n\n, variational inference allows the construction of a\n). learning then\n), the variational lower bound on the intractable\n\n, q,\n\n\u03b8\n\n670\n\n "}, {"Page_number": 686, "text": "chapter 20. deep generative models\n\nfor a deep boltzmann machine with two hidden layers,\n\nl(\n\n) =\n\nq, \u03b8 xi xj0\n\nvi w(1)\ni,j0\n\n\u02c6h(1)\n\nj 0 +xj0 xk0\n\n\u02c6h(1)\nj 0 w (2)\nj0 ,k0\n\n\u02c6h(2)\nk0 \u2212\n\nis given by\n\nl\nz \u03b8\n\nlog ( ) +\n\n)q . (20.35)\n(\n\nh\n\nthis expression still contains the log partition function, log z(\u03b8). because a deep\nboltzmann machine contains restricted boltzmann machines as components, the\nhardness results for computing the partition function and sampling that apply to\nrestricted boltzmann machines also apply to deep boltzmann machines. this means\nthat evaluating the probability mass function of a boltzmann machine requires\napproximate methods such as annealed importance sampling. likewise, training\nthe model requires approximations to the gradient of the log partition function. see\nchapter\nfor a general description of these methods. dbms are typically trained\nusing stochastic maximum likelihood. many of the other techniques described in\nchapter\nare not applicable. techniques such as pseudolikelihood require the\nability to evaluate the unnormalized probabilities, rather than merely obtain a\nvariational lower bound on them. contrastive divergence is slow for deep boltzmann\nmachines because they do not allow efficient sampling of the hidden units given the\nvisible units\u2014instead, contrastive divergence would require burning in a markov\nchain every time a new negative phase sample is needed.\n\n18\n\n18\n\nthe non-variational version of stochastic maximum likelihood algorithm was\n. variational stochastic maximum likelihood as applied\n. recall that we describe a simplified varient\n\ndiscussed earlier, in sec.\nto the dbm is given in algorithm\nof the dbm that lacks bias parameters; including them is trivial.\n\n18.2\n\n20.1\n\n20.4.4 layer-wise pretraining\n\nunfortunately, training a dbm using stochastic maximum likelihood (as described\nabove) from a random initialization usually results in failure. in some cases, the\nmodel fails to learn to represent the distribution adequately. in other cases, the\ndbm may represent the distribution well, but with no higher likelihood than could\nbe obtained with just an rbm. a dbm with very small weights in all but the first\nlayer represents approximately the same distribution as an rbm.\n\n20.4.5\n\nvarious techniques that permit joint training have been developed and are\ndescribed in sec.\n. however, the original and most popular method for\novercoming the joint training problem of dbms is greedy layer-wise pretraining.\nin this method, each layer of the dbm is trained in isolation as an rbm. the\nfirst layer is trained to model the input data. each subsequent rbm is trained to\nmodel samples from the previous rbm\u2019s posterior distribution.\u00a0after all of the\n\n671\n\n "}, {"Page_number": 687, "text": "chapter 20. deep generative models\n\nalgorithm 20.1 the variational stochastic maximum likelihood algorithm for\ntraining a dbm with two hidden layers.\n\n(cid:115)\n\n(1), h(2) ;\u03b8 + (cid:115)\u2206\u03b8 ) to burn in, starting from samples from p(v h,\n\nset , the step size, to a small positive number\nset k , the number of gibbs steps, high enough to allow a markov chain of\n(1), h(2) ;\u03b8).\np(v h,\ninitialize three matrices, \u02dcv , \u02dch (1) and \u02dch(2) each with m columns set to random\nvalues (e.g., from bernoulli distributions, possibly with marginals matched to\nthe model\u2019s marginals).\nwhile not converged (learning loop) do\n\nl\n\n.v\n\ngibbs block 1:\n\nm v > \u02c6h (1)\n\n(gibbs sampling)\n\n\u02c6h (1) > \u02c6h (2)\nk\n\nsample a minibatch of m examples from the training data and arrange them\nas the rows of a design matrix\ninitialize matrices \u02c6h (1) and \u02c6h(2), possibly to the model\u2019s marginals.\nwhile not converged (mean field inference loop) do\n\n\u02c6h (1) \u2190 \u03c3(cid:33)v w (1) + \u02c6h(2)w (2)>(cid:34).\n\u02c6h (2) \u2190 \u03c3(cid:33) \u02c6h (1)w (2)(cid:34).\nend while\n\u2206w (1) \u2190 1\n\u2206w (2) \u2190 1\nm\n= 1 to\nfor\ni,: (cid:34) .\n\u2200i, j, \u02dcvi,j sampled from p ( \u02dcvi,j = 1) = \u03c3(cid:33)w (1)\n:,j (cid:34).\ni,j = 1) = \u03c3(cid:33) \u02dch(1)\n\u2200i, j, \u02dch(2)\ni,j = 1) = \u03c3(cid:33) \u02dcvi,:w (1)\n\u2200i, j, \u02dch(1)\nend for\n\u2206w (1) \u2190 \u2206w (1) \u2212 1\n\u2206w (2) \u2190 \u2206w (2) \u2212 1\nw (1) \u2190 w (1) + (cid:115)\u2206w (1) (this is a cartoon illustration, in practice use a more\neffective algorithm, such as momentum with a decaying learning rate)\nw (2) \u2190 w (2) + \u2206(cid:115) w (2)\nend while\n\ni,j sampled from p ( \u02dch (2)\n\ni,j sampled from p ( \u02dch (1)\n\n\u02dch (1)\ni,: w (2)\n\n:,j + \u02dch (2)\n\nm v > \u02dch (1)\n\n\u02dch(1)> \u02dch (2)\n\ngibbs block 2:\n\ni,: w (2)>j,:\n\n(cid:34).\n\ndo\n\nj,:\n\nm\n\n672\n\n "}, {"Page_number": 688, "text": "chapter 20. deep generative models\n\nrbms have been trained in this way, they can be combined to form a dbm. the\ndbm may then be trained with pcd. typically pcd training will make only a\nsmall change in the model\u2019s parameters and its performance as measured by the\nlog-likelihood it assigns to the data, or its ability to classify inputs. see fig. 20.4\nfor an illustration of the training procedure.\n\nthis greedy layer-wise training procedure is not just coordinate ascent. it bears\nsome passing resemblance to coordinate ascent because we optimize one subset of\nthe parameters at each step. however, in the case of the greedy layer-wise training\nprocedure, we actually use a different objective function at each step.\n\ngreedy layer-wise pretraining of a dbm differs from greedy layer-wise pre-\ntraining of a dbn. the parameters of each individual rbm may be copied to\nthe corresponding dbn directly. in the case of the dbm, the rbm parameters\nmust be modified before inclusion in the dbm. a layer in the middle of the stack\nof rbms is trained with only bottom-up input, but after the stack is combined\nto form the dbm, the layer will have both bottom-up and top-down input.\u00a0to\naccount for this effect, salakhutdinov and hinton 2009a\n) advocate dividing the\nweights of all but the top and bottom rbm in half before inserting them into the\ndbm. additionally, the bottom rbm must be trained using two \u201ccopies\u201d of each\nvisible unit and the weights tied to be equal between the two copies. this means\nthat the weights are effectively doubled during the upward pass. similarly, the top\nrbm should be trained with two copies of the topmost layer.\n\n(\n\n,\n\nobtaining the state of the art results with the deep boltzmann machine requires\na modification of the standard sml algorithm, which is to use a small amount of\nmean field during the negative phase of the joint pcd training step (salakhutdinov\nand hinton 2009a\n).\u00a0specifically, the expectation of the energy gradient should\nbe computed with respect to the mean field distribution in which all of the units\nare independent from each other. the parameters of this mean field distribution\nshould be obtained by running the mean field fixed point equations for just one\nstep. see goodfellow\n) for a comparison of the performance of centered\ndbms with and without the use of partial mean field in the negative phase.\n\net al. (\n\n2013b\n\n20.4.5 jointly training deep boltzmann machines\n\nclassic dbms require greedy unsupervised pretraining, and to perform classification\nwell, require a separate mlp-based classifier on top of the hidden features they\nextract. this has some undesirable properties. it is hard to track performance\nduring training because we cannot evaluate properties of the full dbm while\ntraining the first rbm. thus, it is hard to tell how well our hyperparameters\n\n673\n\n "}, {"Page_number": 689, "text": "chapter 20. deep generative models\n\na)\n\nb)\n\nc)\n\nd)\n\n;\n\n,\n\n).\n\n(a)\n\n2014\n\nfigure 20.4: the deep boltzmann machine training procedure used to classify the mnist\net al.,\ndataset (salakhutdinov and hinton 2009a srivastava\ntrain an rbm\nby using cd to approximately maximize logp (v ).\ntrain a second rbm that models\n(b)\nh(1) and target class y by using cd-k to approximately maximize log p (h(1), y ) where\nh(1) is drawn from the first rbm\u2019s posterior conditioned on the data. increase k from 1\ncombine the two rbms into a dbm. train it to approximately\nto 20 during learning.\nmaximize logp(v, y) using stochastic maximum likelihood with k = 5.\ny from\nthe model. define a new set of features h(1) and h(2) that are obtained by running mean\nfield inference in the model lacking y. use these features as input to an mlp whose\nstructure is the same as an additional pass of mean field, with an additional output layer\nfor the estimate of y. initialize the mlp\u2019s weights to be the same as the dbm\u2019s weights.\ntrain the mlp to approximately maximize log p (y | v) using stochastic gradient descent\nand dropout. figure reprinted from (goodfellow\n\ndelete\n\n2013b\n\net al.,\n\n(d)\n\n(c)\n\n).\n\n674\n\n "}, {"Page_number": 690, "text": "chapter 20. deep generative models\n\nare working until quite late in the training process. software implementations\nof dbms need to have many different components for cd training of individual\nrbms, pcd training of the full dbm, and training based on back-propagation\nthrough the mlp. finally, the mlp on top of the boltzmann machine loses many\nof the advantages of the boltzmann machine probabilistic model, such as being\nable to perform inference when some input values are missing.\n\n,\n\nthere are two main ways to resolve the joint training problem of the deep\nboltzmann machine. the first is the centered deep boltzmann machine (montavon\nand muller 2012\n), which reparametrizes the model in order to make the hessian of\nthe cost function better-conditioned at the beginning of the learning process. this\nyields a model that can be trained without a greedy layer-wise pretraining stage.\nthe resulting model obtains excellent test set log-likelihood and produces high\nquality samples. unfortunately, it remains unable to compete with appropriately\nregularized mlps as a classifier. the second way to jointly train a deep boltzmann\nmachine is to use a multi-prediction deep boltzmann machine (goodfellow et al.,\n2013b). this model uses an alternative training criterion that allows the use\nof the back-propagation algorithm in order to avoid the problems with mcmc\nestimates of the gradient. unfortunately, the new criterion does not lead to good\nlikelihood or samples, but, compared to the mcmc approach, it does lead to\nsuperior classification performance and ability to reason well about missing inputs.\n\nthe centering trick for the boltzmann machine is easiest to describe if we\nreturn to the general view of a boltzmann machine as consisting of a set of units x\nwith a weight matrix u and biases b. recall from eq.\nthat he energy function\nis given by\n\n20.2\n\ne( ) = \n\nx\n\n\u2212x>u x b\u2212 >x.\n\n(20.36)\n\nusing\u00a0different\u00a0sparsity\u00a0patterns in\u00a0the weight matrix u ,\u00a0we can\u00a0implement\nstructures of boltzmann machines, such as rbms, or dbms with different numbers\nof layers. this is accomplished by partitioning x into visible and hidden units and\nzeroing out elements of u for units that do not interact. the centered boltzmann\nmachine introduces a vector\n\nthat is subtracted from all of the states:\n\n\u00b5\n\ne0( ;\n\nx u b,\n\n) =  (\n\n\u2212 x \u00b5\u2212 > u x \u00b5\n( \u2212 )\n\n)\n\nx \u00b5\n\n(\u2212 \u2212 )> b.\n\n(20.37)\n\ntypically \u00b5 is a hyperparameter fixed at the beginning of training. it is usu-\nally chosen to make sure that x \u00b5\u2212 \u2248 0 when the model is initialized. this\nreparametrization does not change the set of probability distributions that the\nmodel can represent, but it does change the dynamics of stochastic gradient descent\napplied to the likelihood. specifically, in many cases, this reparametrization results\n) experimentally\nin a hessian matrix that is better conditioned. melchior\n\net al. (\n\n2013\n\n675\n\n "}, {"Page_number": 691, "text": "chapter 20. deep generative models\n\nconfirmed that the conditioning of the hessian matrix improves, and observed\nthat the centering trick is equivalent to another boltzmann machine learning\ntechnique, the enhanced gradient (\n). the improved conditioning of\nthe hessian matrix allows learning to succeed, even in difficult cases like training a\ndeep boltzmann machine with multiple layers.\n\ncho et al. 2011\n\n,\n\nthe other approach to jointly training deep boltzmann machines is the multi-\nprediction deep boltzmann machine (mp-dbm) which works by viewing the mean\nfield equations as defining a family of recurrent networks for approximately solving\nevery possible inference problem (\n). rather than training\nthe model to maximize the likelihood, the model is trained to make each recurrent\nnetwork obtain an accurate answer to the corresponding inference problem. the\ntraining process is illustrated in fig.\n. it consists of randomly sampling a\ntraining example, randomly sampling a subset of inputs to the inference network,\nand then training the inference network to predict the values of the remaining\nunits.\n\ngoodfellow et al. 2013b\n\n20.5\n\n,\n\n2013\n\net al.,\n\nthis general principle of back-propagating through the computational graph\n2011\n;\nfor approximate inference has been applied to other models (stoyanov\nbrakel\n). in these models and in the mp-dbm, the final loss is not\nthe lower bound on the likelihood. instead, the final loss is typically based on\nthe approximate conditional distribution that the approximate inference network\nimposes over the missing values. this means that the training of these models\nis somewhat heuristically motivated. if we inspect the p(v ) represented by the\nboltzmann machine learned by the mp-dbm, it tends to be somewhat defective,\nin the sense that gibbs sampling yields poor samples.\n\net al.,\n\nback-propagation through the inference graph has two main advantages. first,\nit trains the model as it is really used\u2014with approximate inference. this means\nthat approximate inference, for example, to fill in missing inputs, or to perform\nclassification despite the presence of missing inputs, is more accurate in the mp-\ndbm than in the original dbm. the original dbm does not make an accurate\nclassifier on its own; the best classification results with the original dbm were\nbased on training a separate classifier to use features extracted by the dbm,\nrather than by using inference in the dbm to compute the distribution over the\nclass labels. mean field inference in the mp-dbm performs well as a classifier\nwithout special modifications. the other advantage of back-propagating through\napproximate inference is that back-propagation computes the exact gradient of\nthe loss. this is better for optimization than the approximate gradients of sml\ntraining, which suffer from both bias and variance. this probably explains why mp-\ndbms may be trained jointly while dbms require a greedy layer-wise pretraining.\n\n676\n\n "}, {"Page_number": 692, "text": "chapter 20. deep generative models\n\nfigure 20.5: an illustration of the multi-prediction training process for a deep boltzmann\nmachine. each row indicates a different example within a minibatch for the same training\nstep.\u00a0each column represents a time step within the mean field inference process.\u00a0for\neach example, we sample a subset of the data variables to serve as inputs to the inference\nprocess. these variables are shaded black to indicate conditioning. we then run the\nmean field inference process, with arrows indicating which variables influence which other\nvariables in the process. in practical applications, we unroll mean field for several steps.\nin this illustration, we unroll for only two steps. dashed arrows indicate how the process\ncould be unrolled for more steps. the data variables that were not used as inputs to the\ninference process become targets, shaded in gray. we can view the inference process for\neach example as a recurrent network. we use gradient descent and back-propagation to\ntrain these recurrent networks to produce the correct targets given their inputs. this\ntrains the mean field process for the mp-dbm to produce accurate estimates. figure\nadapted from\n\ngoodfellow et al. 2013b\n\n).\n\n(\n\n677\n\n "}, {"Page_number": 693, "text": "chapter 20. deep generative models\n\nthe disadvantage of back-propagating through the approximate inference graph is\nthat it does not provide a way to optimize the log-likelihood, but rather a heuristic\napproximation of the generalized pseudolikelihood.\n\nthe mp-dbm inspired the nade-k (raiko\n\net al.,\n\n2014\n\n) extension to the\n\nnade framework, which is described in sec.\n\n20.10.10\n.\n\nthe mp-dbm has some connections to dropout. dropout shares the same pa-\nrameters among many different computational graphs, with the difference between\neach graph being whether it includes or excludes each unit. the mp-dbm also\nshares parameters across many computational graphs. in the case of the mp-dbm,\nthe difference between the graphs is whether each input unit is observed or not.\nwhen a unit is not observed, the mp-dbm does not delete it entirely as in the\ncase of dropout. instead, the mp-dbm treats it as a latent variable to be inferred.\none could imagine applying dropout to the mp-dbm by additionally removing\nsome units rather than making them latent.\n\n20.5 boltzmann machines for real-valued data\n\nwhile boltzmann machines were originally developed for use with binary data,\nmany applications such as image and audio modeling seem to require the ability\nto represent probability distributions over real values. in some cases, it is possible\nto treat real-valued data in the interval [0, 1] as representing the expectation of a\nbinary variable. for example, hinton 2000\n) treats grayscale images in the training\nset as defining [0,1] probability values. each pixel defines the probability of a\nbinary value being 1, and the binary pixels are all sampled independently from\neach other. this is a common procedure for evaluating binary models on grayscale\nimage datasets. however, it is not a particularly theoretically satisfying approach,\nand binary images sampled independently in this way have a noisy appearance. in\nthis section, we present boltzmann machines that define a probability density over\nreal-valued data.\n\n(\n\n20.5.1 gaussian-bernoulli rbms\n\nrestricted boltzmann machines may be developed for many exponential family\nconditional distributions (welling\n). of these, the most common is the\nrbm with binary hidden units and real-valued visible units, with the conditional\ndistribution over the visible units being a gaussian distribution whose mean is a\nfunction of the hidden units.\n\net al.,\n\n2005\n\nthere are many ways of parametrizing gaussian-bernoulli rbms. first, we may\n\n678\n\n "}, {"Page_number": 694, "text": "chapter 20. deep generative models\n\nchoose whether to use a covariance matrix or a precision matrix for the gaussian\ndistribution. here we present the precision formulation. the modification to obtain\nthe covariance formulation is straightforward.\u00a0we wish to have the conditional\ndistribution\n\nv h|\n(\np\n\n) = \n\nn v w h \u03b2\u22121).\n( ;\n\n,\n\n(20.38)\n\nwe can find the terms we need to add to the energy function by expanding the\nunnormalized log conditional distribution:\n\nlog\n\n( ;n v w h \u03b2, \u22121 ) = \u2212\n\n1\n2\n\nv w h\u2212\n(\n)\n\n> \u03b2 v w h\n\n( \u2212\n\n) + (f\n\n\u03b2\n\n).\n\n(20.39)\n\nhere f encapsulates all the terms that are a function only of the parameters\nand not the random variables in the model. we can discard f because its only\nrole is to normalize the distribution, and the partition function of whatever energy\nfunction we choose will carry out that role.\n\nif we include all of the terms (with their sign flipped) involving v from eq. 20.39\nin our energy function and do not add any other terms involving v, then our energy\nfunction will represent the desired conditional\n\np(\n\nwe have some freedom regarding the other conditional distribution, p(h v|\n\n).\n\nnote that eq.\n\n20.39\n\ncontains a term\n\nv h|\n.\n)\n\n1\n2\n\nh>w >\u03b2w h.\n\n(20.40)\n\nthis term cannot be included in its entirety because it includes h ihj terms. these\ncorrespond to edges between the hidden units. if we included these terms, we\nwould have a\u00a0linear factor model instead of\u00a0a restricted boltzmann machine.\nwhen designing our boltzmann machine, we simply omit these hih j cross terms.\nomitting them does not change the conditional p(v h|\nis still\nrespected. however, we still have a choice about whether to include the terms\ninvolving only a single hi. if we assume a diagonal precision matrix, we find that\nfor each hidden unit hi we have a term\n\n) so eq.\n\n20.39\n\n1\n2\n\nhixj\n\n\u03b2jw 2\nj,i.\n\n(20.41)\n\nin the above, we used the fact that h2\ni = hi because hi \u2208 {0, 1}. if we include this\nterm (with its sign flipped) in the energy function, then it will naturally bias h i\nto be turned off when the weights for that unit are large and connected to visible\nunits with high precision. the choice of whether or not to include this bias term\ndoes not affect the family of distributions the model can represent (assuming that\n\n679\n\n "}, {"Page_number": 695, "text": "chapter 20. deep generative models\n\nwe include bias parameters for the hidden units) but it does affect the learning\ndynamics of the model. including the term may help the hidden unit activations\nremain reasonable even when the weights rapidly increase in magnitude.\n\none way to define the energy function on a gaussian-bernoulli rbm is thus\n\ne ,(v h) =\n\n1\n2\n\nv> (\n\n\u03b2 v(cid:130) \u2212 v \u03b2(cid:130) > w h b\u2212 >h\n\n)\n\n(\n\n)\n\n(20.42)\n\nbut we may also add extra terms or parametrize the energy in terms of the variance\nrather than precision if we choose.\n\nin this derivation, we have not included a bias term on the visible units, but one\ncould easily be added. one final source of variability in the parametrization of a\ngaussian-bernoulli rbm is the choice of how to treat the precision matrix. it may\neither be fixed to a constant (perhaps estimated based on the marginal precision\nof the data) or learned.\u00a0it may also be a scalar times the identity matrix, or it\nmay be a diagonal matrix. typically we do not allow the precision matrix to be\nnon-diagonal in this context, because some operations would then require inverting\nthe matrix. in the sections ahead, we will see that other forms of boltzmann\nmachines permit modeling the covariance structure, using various techniques to\navoid inverting the precision matrix.\n\n20.5.2 undirected models of conditional covariance\n\n(\n\nranzato et al. 2010a\n\nwhile the gaussian rbm has been the canonical energy model for real-valued\ndata,\n) argue that the gaussian rbm inductive bias is not\nwell suited to the statistical variations present in some types of real-valued data,\nespecially natural images. the problem is that much of the information content\npresent in natural images is embedded in the covariance between pixels rather than\nin the raw pixel values. in other words, it is the relationships between pixels and\nnot their absolute values where most of the useful information in images resides.\nsince the gaussian rbm only models the conditional mean of the input given the\nhidden units, it cannot capture conditional covariance information. in response\nto these criticisms, alternative models have been proposed that attempt to better\naccount for the covariance of real-valued data. these models include the mean and\ncovariance rbm (mcrbm1), the mean-product of t-distribution (mpot) model\nand the spike and slab rbm (ssrbm).\n\n1the term \u201cmcrbm\u201d is pronounced by saying the name of the letters m-c-r-b-m; the \u201cmc\u201d\n\nis not pronounced like the \u201cmc\u201d in \u201cmcdonald\u2019s.\u201d\n\n680\n\n "}, {"Page_number": 696, "text": "chapter 20. deep generative models\n\nmean and covariance rbm the mcrbm uses its hidden units to indepen-\ndently encode the conditional mean and covariance of all observed units. the\nmcrbm hidden layer is divided into two groups of units: mean units and covariance\nunits. the group that models the conditional mean is simply a gaussian rbm.\nthe other half is a covariance rbm (\n), also called a crbm,\nwhose components model the conditional covariance structure, as described below.\n)m and binary covariance units h( )c , the\n\nspecifically, with binary mean units h(\n\nranzato et al. 2010a\n\n,\n\nmcrbm model is defined as the combination of two energy functions:\n\nemc(x h,\n\n(\n)m , h( )c ) = em(x h,\n\n(\n)m ) + e c(x h,\n\n( )c ),\n\n(20.43)\n\nwhere e m is the standard gaussian-bernoulli rbm energy function:2\n\nem(x h,\n\n(\n\n)m ) = \u2212\n\n1\n2\n\nx>x \u2212xj\n\nx> w:,jh(\n\n)m\n\nj \u2212xj\n\n)m\n\nb (\nj h(\n\n)m\n,\nj\n\n(20.44)\n\nand ec is the\u00a0crbm energy function that\u00a0models\u00a0the conditional covariance\ninformation:\n\nec (x h,\n\n( )c ) = \u2212\n\n1\n\n2 xj\n\nh( )c\n\nj (cid:33)x> r( )j (cid:34)2\n\n\u2212xj\n\nb( )c\nj h( )c\n\nj\n\n.\n\n(20.45)\n\nthe parameter r( )j corresponds to the covariance weight vector associated with\nh( )c\nj and b( )c is a vector of covariance offsets. the combined energy function defines\na joint distribution:\n\nand a corresponding conditional distribution over the observations given h(\nh( )c as a multivariate gaussian distribution:\n\n)m and\n\npmc(x h,\n\n(\n\n1\nz\n\n)m , h( )c ) =\n(\n\nexpn\u2212emc (x h,\n)m , h( )c )o ,\n)m , h( )c ) = n(cid:39)(cid:40)c mc\nx h| (cid:39)(cid:40)xj\nj (cid:44)(cid:45) , cmc\nx h| (cid:44)(cid:45) .\n=(cid:33)pj hj r( )j r ( )j > + i(cid:34)\u22121\n\nw:,jh(\n\n)m\n\n(\n\npmc (x h|\n\nnote that the covariance matrix cmc\nis non-diagonal\nx h|\nand that w is the weight matrix associated with the gaussian rbm modeling the\n\n2this version of the gaussian-bernoulli rbm energy function assumes the image data has\nzero mean, per pixel. pixel offsets can easily be added to the model to account for nonzero pixel\nmeans.\n\n681\n\n(20.46)\n\n(20.47)\n\n "}, {"Page_number": 697, "text": "chapter 20. deep generative models\n\nconditional means. it is difficult to train the mcrbm via contrastive divergence or\npersistent contrastive divergence because of its non-diagonal conditional covariance\n)m , h( )c\n(\nstructure. cd and pcd require sampling from the joint distribution of x h,\nwhich, in a standard rbm, is accomplished by gibbs sampling over the conditionals.\n(\n)m , h ( )c ) requires computing\nhowever, in the mcrbm, sampling from pmc(x h|\n(cmc)\u22121 at every iteration of learning. this can be an impractical computational\nburden for larger observations.\nranzato and hinton 2010\n) avoid direct sampling\n)m , h( )c ) by sampling directly from the marginal\n(\nfrom the conditional pmc(x h|\np(x) using hamiltonian (hybrid) monte carlo (\n) on the mcrbm free\nenergy.\n\nneal 1993\n\n(\n\n,\n\n,\n\nt\n\n2003a\n\nranzato et al. 2010b\n\nthe mean-product of student\u2019s\nmean-product of student\u2019s -distributions\n) extends the pot model (\nwelling\nt-distribution (mpot) model (\net al.,\n) in a manner similar to how the mcrbm extends the crbm. this\nis achieved by including nonzero gaussian means by the addition of gaussian\nrbm-like hidden units. like the mcrbm, the pot conditional distribution over the\nobservation is a multivariate gaussian (with non-diagonal covariance) distribution;\nhowever, unlike the mcrbm, the complementary conditional distribution over the\nhidden variables is given by conditionally independent gamma distributions. the\ngamma distribution g(k, \u03b8) is a probability distribution over positive real numbers,\nwith mean k\u03b8. it is not necessary to have a more detailed understanding of the\ngamma distribution to understand the basic ideas underlying the mpot model.\n\nthe mpot energy function is:\n\ne mpot(x h,\n\n(\n)m , h( )c )\n\n(\n= e m(x h,\n\n)m ) +xj (cid:35)h( )c\n\nj (cid:35)1 +\n\n1\n\n2(cid:33)r ( )j >x(cid:34)2(cid:36) + (1 \u2212 \u03b3j) log h( )c\nj (cid:36)\n\n(20.48)\n\n(20.49)\n\nwhere r ( )j\nis as defined in eq.\n\n20.44\n.\n\nis the covariance weight vector associated with unit h( )c\n\n(\n)m )\nj and em(x h,\n\njust as with the mcrbm, the mpot model energy function specifies a mul-\ntivariate gaussian, with a conditional distribution over x that has non-diagonal\ncovariance. the covariance units h ( )c are conditionally gamma-distributed:\n\npmpot(h( )c\nj\n\n|\n\nx) =  (cid:35)\u03b3j, 1 +\n\ng\n\n1\n\n2(cid:33)v ( )j >x(cid:34) 2(cid:36)\n\n(20.50)\n\nlearning in the mpot model\u2014again, like the mcrbm\u2014is complicated by the in-\n(\n)m , h ( )c ),\nability to sample from the non-diagonal gaussian conditional pmpot(x h|\n\n682\n\n "}, {"Page_number": 698, "text": "chapter 20. deep generative models\n\nranzato et al. 2010b\n\nso\n(hybrid) monte carlo.\n\n(\n\n) also advocate direct sampling of\n\np(x) via hamiltonian\n\n,\n\ncourville et al. 2011\n\nspike and slab restricted boltzmann machines spike and slab restricted\nboltzmann machines (\n) or ssrbms provide another means\nof modeling the covariance structure of real-valued data. compared to mcrbms,\nssrbms have the advantage of requiring neither matrix inversion nor hamiltonian\nmonte carlo methods.\u00a0as a model of natural images, the ssrbm is interesting\nin that, like the mcrbm and the mpot model, its binary hidden units encode\nthe conditional covariance across pixels through the use of auxiliary real-valued\nvariables.\n\nslab\n\nspike\n\nunits\n\nunits\n\nthe spike and slab rbm has two sets of hidden units: binary\n\nh,\nand real-valued\ns . the mean of the visible units conditioned on the\nhidden units is given by (h s(cid:130) )w >. in other words, each column w:,i defines a\ncomponent that can appear in the input when hi = 1. the corresponding spike\nvariable hi determines whether that component is present at all. the corresponding\nslab variable si determines the intensity of that component, if it is present. when\na spike variable is active, the corresponding slab variable adds variance to the\ninput along the axis defined by w:,i. this allows us to model the covariance of the\ninputs. fortunately, contrastive divergence and persistent contrastive divergence\nwith gibbs sampling are still applicable. there is no need to invert any matrix.\n\nformally, the ssrbm model is defined via its energy function:\n\ne ss(\n\nx s h,\n\n,\n\n) =\n\nx>w :,isi hi +\n\n\u2212xi\n2xi\n\n+\n\n1\n\n\u03b1is2\n\ni \u2212xi\n\n1\n2\n\nx>\u00a0 \u03bb +xi\n\u03b1i \u00b5isihi \u2212xi\n\n\u03c6 ih i! x\nb ihi +xi\n\n(20.51)\n\n\u03b1i \u00b52\n\ni hi,\n\n(20.52)\n\nwhere bi is the offset of the spike h i and \u03bb is a diagonal precision matrix on the\nobservations x. the parameter \u03b1 i > 0 is a scalar precision parameter for the\nreal-valued slab variable si. the parameter \u03c6i is a non-negative diagonal matrix\nthat defines an h-modulated quadratic penalty on x. each \u00b5i is a mean parameter\nfor the slab variable si.\n\nwith the joint distribution defined via the energy function, it is relatively\nstraightforward to derive the\u00a0ssrbm conditional\u00a0distributions. for\u00a0example,\nby marginalizing out the slab variables s, the conditional distribution over the\nobservations given the binary spike variables\n\nis given by:\n\nh\n\npss (\n\nx h|\n\n) =\n\n1\n\np ( )h\n\n1\n\nz z exp\n\n683\n\n{\u2212e x s h,\n} ds\n)\n,\n\n(\n\n(20.53)\n\n "}, {"Page_number": 699, "text": "chapter 20. deep generative models\n\nw:,i \u00b5ihi , css\n\nx h| !\n\n(20.54)\n\n= n\u00a0 css\n=(cid:28) \u03bb +p i \u03c6 ih i\u2212pi \u03b1\u22121\n\nx h| xi\n\nx h|\n\nwhere c ss\nthe covariance matrix c ss\nx h|\n\ni hiw :,iw>:,i(cid:29)\u22121. the last equality holds only if\ngating by the spike variables means that the true marginal distribution over\nh s(cid:130) is sparse. this is different from sparse coding, where samples from the model\n\u201calmost never\u201d (in the measure theoretic sense) contain zeros in the code, and map\ninference is required to impose sparsity.\n\nis positive definite.\n\ncomparing the ssrbm to the mcrbm and the mpot models, the ssrbm\nparametrizes the conditional covariance of the observation in a significantly different\nway. the mcrbm and mpot both model the covariance structure of the observation\n\nas (cid:33)pj h ( )c\n\nj r( )j r( )j > + i(cid:34)\u22121\n\n, using the activation of the hidden units hj > 0 to\nenforce constraints on the conditional covariance in the direction r ( )j . in contrast,\nthe ssrbm specifies the conditional covariance of the observations using the hidden\nspike activations hi = 1 to pinch the precision matrix along the direction specified\nby the corresponding weight vector.\u00a0the ssrbm conditional covariance is very\nsimilar to that given by a different model: the product of probabilistic principal\ncomponents analysis (poppca) (williams and agakov 2002\n). in the overcomplete\nsetting, sparse activations with the ssrbm parametrization permit significant\nvariance (above the nominal variance given by \u03bb\u22121) only in the selected directions\nof the sparsely activated hi.\u00a0in the mcrbm or mpot models, an overcomplete\nrepresentation would mean that to capture variation in a particular direction in\nthe observation space requires removing potentially all constraints with positive\nprojection in that direction.\u00a0this would suggest that these models are less well\nsuited to the overcomplete setting.\n\n,\n\nthe primary disadvantage of the spike and slab restricted boltzmann machine\nis that some settings of the parameters can correspond to a covariance matrix\nthat is not positive definite. such a covariance matrix places more unnormalized\nprobability on values that are farther from the mean, causing the integral over\nall possible outcomes to diverge. generally this issue can be avoided with simple\nheuristic tricks. there is not yet any theoretically satisfying solution. using\nconstrained optimization to explicitly avoid the regions where the probability is\nundefined is difficult to do without being overly conservative and also preventing\nthe model from accessing high-performing regions of parameter space.\n\nqualitatively, convolutional variants of the ssrbm produce excellent samples\n\nof natural images. some examples are shown in fig.\n\n.\n16.1\n\n684\n\n "}, {"Page_number": 700, "text": "chapter 20. deep generative models\n\nthe ssrbm allows for several extensions. including higher-order interactions\nand average-pooling of the slab variables (\n) enables the model\nto learn excellent features for a classifier when labeled data is scarce. adding a\nterm to the energy function that prevents the partition function from becoming\nundefined results in a sparse coding model, spike and slab sparse coding (goodfellow\net al.,\n\n), also known as s3c.\n\ncourville et al. 2014\n\n2013d\n\n,\n\n20.6 convolutional boltzmann machines\n\n9\n\nas seen in chapter\n, extremely high dimensional inputs such as images place\ngreat strain on the computation, memory and statistical requirements of machine\nlearning models. replacing matrix multiplication by discrete convolution with a\nsmall kernel is the standard way of solving these problems for inputs that have\ntranslation invariant spatial or temporal structure.\ndesjardins and bengio 2008\n)\nshowed that this approach works well when applied to rbms.\n\n(\n\nlee\n\ndeep convolutional networks usually require a pooling operation so that the\nspatial size of each successive layer decreases. feedforward convolutional networks\noften use a pooling function such as the maximum of the elements to be pooled.\nit is unclear how to generalize this to the setting of energy-based models. we\ncould introduce a binary pooling unit p over n binary detector units d and enforce\np = maxi di by setting the energy function to be \u221e whenever that constraint is\nviolated. this does not scale well though, as it requires evaluating 2 n different\nenergy configurations to compute the normalization constant. for a small 3 \u00d7 3\npooling region this requires 29 = 512 energy function evaluations per pooling unit!\n) developed a solution to this problem called probabilistic max\npooling (not to be confused with \u201cstochastic pooling,\u201d\u00a0which is a technique for\nimplicitly constructing ensembles of convolutional feedforward networks). the\nstrategy behind probabilistic max pooling is to constrain the detector units so\nat most one may be active at a time. this means there are only n + 1 total\nstates (one state for each of the n detector units being on, and an additional state\ncorresponding to all of the detector units being off). the pooling unit is on if\nand only if one of the detector units is on. the state with all units off is assigned\nenergy zero. we can think of this as describing a model with a single variable that\nhas n + 1 states, or equivalently as a model that has n + 1 variables that assigns\nenergy\n\njoint assignments of variables.\n\nto all but\n\net al. (\n\nn + 1\n\n2009\n\nwhile efficient, probabilistic max pooling does force the detector units to be\nmutually exclusive, which may be a useful regularizing constraint in some contexts\nor a harmful limit on model capacity in other contexts. it also does not support\n\n\u221e\n\n685\n\n "}, {"Page_number": 701, "text": "chapter 20. deep generative models\n\noverlapping pooling regions. overlapping pooling regions are usually required\nto obtain the best performance from feedforward convolutional networks, so this\nconstraint probably greatly reduces the performance of convolutional boltzmann\nmachines.\n\nlee\n\n2009\n\net al. (\n\n) demonstrated that probabilistic max pooling could be used\nto build convolutional deep boltzmann machines.3 this model is able to perform\noperations such as filling in missing portions of its input. while intellectually\nappealing, this model is challenging to make work in practice, and usually does\nnot perform as well as a classifier as traditional convolutional networks trained\nwith supervised learning.\n\nmany convolutional models work equally well with inputs of many different\nspatial sizes. for boltzmann machines, it is difficult to change the input size\nfor a variety of reasons.\u00a0the partition function changes as the size of the input\nchanges. moreover, many convolutional networks achieve size invariance by scaling\nup the size of their pooling regions proportional to the size of the input, but scaling\nboltzmann machine pooling regions is awkward. traditional convolutional neural\nnetworks can use a fixed number of pooling units and dynamically increase the\nsize of their pooling regions in order to obtain a fixed-size representation of a\nvariable-sized input. for boltzmann machines, large pooling regions become too\nexpensive for the naive approach.\u00a0the approach of\n) of making\neach of the detector units in the same pooling region mutually exclusive solves\nthe computational problems, but still does not allow variable-size pooling regions.\nfor example, suppose we learn a model with 2 \u00d7 2 probabilistic max pooling over\ndetector units that learn edge detectors.\u00a0this enforces the constraint that only\none of these edges may appear in each 2\u00d7 2 region. if we then increase the size of\nthe input image by 50% in each direction, we would expect the number of edges to\nincrease correspondingly. instead, if we increase the size of the pooling regions by\n50% in each direction to 3\u00d7 3, then the mutual exclusivity constraint now specifies\nthat each of these edges may only appear once in a 3 \u00d7 3 region. as we grow\na model\u2019s input image in this way, the model generates edges with less density.\nof course, these issues only arise when the model must use variable amounts of\npooling in order to emit a fixed-size output vector. models that use probabilistic\nmax pooling may still accept variable-sized input images so long as the output of\nthe model is a feature map that can scale in size proportional to the input image.\n\nlee et al. 2009\n\n(\n\npixels at the boundary of the image also pose some difficulty, which is exac-\n\n3the publication describes the model as a \u201cdeep belief network\u201d but because it can be described\nas a purely undirected model with tractable layer-wise mean field fixed point updates, it best fits\nthe definition of a deep boltzmann machine.\n\n686\n\n "}, {"Page_number": 702, "text": "chapter 20. deep generative models\n\nerbated by the fact that connections in a boltzmann machine are symmetric. if\nwe do not implicitly zero-pad the input, then there are fewer hidden units than\nvisible units, and the visible units at the boundary of the image are not modeled\nwell because they lie in the receptive field of fewer hidden units. however, if we do\nimplicitly zero-pad the input, then the hidden units at the boundary are driven by\nfewer input pixels, and may fail to activate when needed.\n\n20.7 boltzmann machines for structured or sequential\n\noutputs\n\nin the structured output scenario, we wish to train a model that can map from\nsome input x to some output y, and the different entries of y are related to each\nother and must obey some constraints. for example, in the speech synthesis task,\ny is a waveform, and the entire waveform must sound like a coherent utterance.\n\na natural way to represent the relationships between the entries in y is to\nuse a probability distribution p(y | x). boltzmann machines, extended to model\nconditional distributions, can supply this probabilistic model.\n\nthe same tool of conditional modeling with a boltzmann machine can be used\nnot just for structured output tasks, but also for sequence modeling. in the latter\ncase, rather than mapping an input x to an output y, the model must estimate a\nprobability distribution over a sequence of variables, p(x(1), . . . , x( )\u03c4 ). conditional\nt\u2212 ) in\nboltzmann machines can represent factors of the form p(x( )t\norder to accomplish this task.\n\n| x(1), . . . , x(\n\n1)\n\nan important sequence modeling task for the video game and film industry\nis modeling sequences of joint angles of skeletons used to render 3-d characters.\nthese sequences are often collected using motion capture systems to record the\nmovements of actors. a probabilistic model of a character\u2019s movement allows\nthe generation\u00a0of new,\u00a0previously unseen,\u00a0but realistic\u00a0animations. to solve\nthis sequence modeling task, taylor\n) introduced a conditional rbm\nt m\u2212 ) for small m. the model is an rbm over\nmodeling p (x( )t\n)\np(x( )t ) whose bias parameters are a linear function of the preceding m values of x.\nt\u2212 and earlier variables, we get a new\nwhen we condition on different values of x(\nrbm over x. the weights in the rbm over x never change, but by conditioning on\ndifferent past values, we can change the probability of different hidden units in the\nrbm being active. by activating and deactivating different subsets of hidden units,\nwe can make large changes to the probability distribution induced on x .\u00a0other\n) and other variants of sequence\nvariants of conditional rbm (\n\nmnih et al. 2011\n\nt\u2212 , . . . , x(\n\n| x(\n\net al. (\n\n2007\n\n1)\n\n1)\n\n,\n\n687\n\n "}, {"Page_number": 703, "text": "chapter 20. deep generative models\n\nmodeling using conditional rbms are possible (taylor and hinton 2009 sutskever\net al.,\n\n2009 boulanger-lewandowski\n\net al.,\n\n2012\n\n).\n\n;\n\n,\n\n;\n\net al. (\n\nrnn-rbm\n\nanother sequence modeling task is to model the distribution over sequences\n2012\n)\nof musical notes used to compose songs. boulanger-lewandowski\nintroduced the\nsequence model and applied it to this task. the rnn-\nrbm is a generative model of a sequence of frames x( )t consisting of an rnn\nthat emits the rbm parameters for each time step. unlike the model described\nabove, the rnn emits all of the parameters of the rbm, including the weights.\nto train the model, we need to be able to back-propagate the gradient of the\nloss function through the rnn. the loss function is not applied directly to the\nrnn outputs.\ninstead,\u00a0it is applied to the rbm. this means that we must\napproximately differentiate the loss with respect to the rbm parameters using\ncontrastive divergence or a related algorithm. this approximate gradient may then\nbe back-propagated through the rnn using the usual back-propagation through\ntime algorithm.\n\n20.8 other boltzmann machines\n\nmany other variants of boltzmann machines are possible.\n\nboltzmann machines may be extended with different training criteria. we have\nfocused on boltzmann machines trained to approximately maximize the generative\ncriterion log p(v). it is also possible to train discriminative rbms that aim to\nmaximize log p( y | v) instead (\n). this approach often\nperforms the best when using a linear combination of both the generative and\nthe discriminative criteria. unfortunately, rbms do not seem to be as powerful\nsupervised learners as mlps, at least using existing methodology.\n\nlarochelle and bengio 2008\n\n,\n\nmost boltzmann machines used in practice have only second-order interactions\nin their energy functions, meaning that their energy functions are the sum of many\nterms and each individual term only includes the product between two random\nvariables. an example of such a term is viwi,jhj. it is also possible to train\nhigher-order boltzmann machines (\n) whose energy function terms\ninvolve the products between many variables. three-way interactions between a\nhidden unit and two different images can model spatial transformations from one\nframe of video to the next (\n). multiplication by a\none-hot class variable can change the relationship between visible and hidden units\ndepending on which class is present (\n). one recent example\nof the use of higher-order interactions is a boltzmann machine with two groups of\nhidden units, with one group of hidden units that interact with both the visible\n\nmemisevic and hinton 2007 2010\n\nnair and hinton 2009\n\nsejnowski 1987\n\n,\n\n,\n\n,\n\n,\n\n688\n\n "}, {"Page_number": 704, "text": "chapter 20. deep generative models\n\n,\n\nluo et al. 2011\n\nunits v and the class label y, and another group of hidden units that interact only\nwith the v input values (\n). this can be interpreted as encouraging\nsome hidden units to learn to model the input using features that are relevant to\nthe class but also to learn extra hidden units that explain nuisance details that\nare necessary for the samples of v to be realistic but do not determine the class\nof the example. another use of higher-order interactions is to gate some features.\nsohn\n) introduced a boltzmann machine with third-order interactions\nwith binary mask variables associated with each visible unit. when these masking\nvariables are set to zero, they remove the influence of a visible unit on the hidden\nunits. this allows visible units that are not relevant to the classification problem\nto be removed from the inference pathway that estimates the class.\n\net al. (\n\n2013\n\nmore generally, the boltzmann machine framework is a rich space of models\npermitting many more model structures than have been explored so far. developing\na new form of boltzmann machine requires some more care and creativity than\ndeveloping a new neural network layer, because it is often difficult to find an energy\nfunction that maintains tractability of all of the different conditional distributions\nneeded to use the boltzmann machine, but despite this required effort the field\nremains open to innovation.\n\n20.9 back-propagation through random operations\n\ntraditional neural networks implement a deterministic transformation of some\ninput variables x. when developing generative models, we often wish to extend\nneural networks to implement stochastic transformations of x. one straightforward\nway to do this is to augment the neural network with extra inputs z that are\nsampled from some simple probability distribution, such as a uniform or gaussian\ndistribution. the neural network can then continue to perform deterministic\ncomputation internally,\u00a0but the\u00a0function f(x z,\n) will appear stochastic to an\nobserver who does not have access\u00a0to z. provided that f is continuous and\ndifferentiable, we can then compute the gradients necessary for training using\nback-propagation as usual.\n\nas an example, let us consider the operation consisting of drawing samples y\n\nfrom a gaussian distribution with mean\n\n\u00b5\n\nand variance\n\n\u03c32:\n\ny \u223c n (\u00b5, \u03c32).\n\n(20.55)\n\nbecause an individual sample of y is not produced by a function, but rather by\na sampling process whose output changes every time we query it, it may seem\ncounterintuitive to take the derivatives of y with respect to the parameters of\n\n689\n\n "}, {"Page_number": 705, "text": "chapter 20. deep generative models\n\nits distribution, \u00b5 and \u03c32. however,\u00a0we can rewrite the sampling process as\ntransforming an underlying random value z \u223c n(z; 0, 1) to obtain a sample from\nthe desired distribution:\n(20.56)\n\n=  +\n\n\u03c3z\n\n\u00b5\n\ny\n\nwe are now able to back-propagate through the sampling operation, by regard-\ning it as a deterministic operation with an extra input z. crucially, the extra input\nis a random variable whose distribution is not a function of any of the variables\nwhose derivatives we want to calculate.\u00a0the result tells us how an infinitesimal\nchange in \u00b5 or \u03c3 would change the output if we could repeat the sampling operation\nagain with the same value of z.\n\nbeing able to back-propagate through this sampling operation allows us to\nincorporate it into a larger graph. we can build elements of the graph on top of the\noutput of the sampling distribution. for example, we can compute the derivatives\nof some loss function j(y). we can also build elements of the graph whose outputs\nare the inputs or the parameters of the sampling operation. for example, we could\nbuild a larger graph with \u00b5 = f( x; \u03b8) and \u03c3 = g(x; \u03b8). in this augmented graph,\nwe can use back-propagation through these functions to derive \u2207 \u03b8j y( ).\n\nthe principle used in this gaussian sampling example is more generally appli-\ncable. we can express any probability distribution of the form p(y; \u03b8) or p(y | x;\u03b8 )\nas p(y | \u03c9 ), where \u03c9 is a variable containing both parameters \u03b8 , and if applicable,\nthe inputs x . given a value y sampled from distribution p(y | \u03c9), where \u03c9 may in\nturn be a function of other variables, we can rewrite\n\nas\n\ny\n\ny\n\ny \n\n\u223c p(\n\n| \u03c9)\n\n=  (f\n\nz \u03c9\n\n;\n\n),\n\n(20.57)\n\n(20.58)\n\nwhere z is a source of randomness. we may then compute the derivatives of y with\nrespect to \u03c9 using traditional tools such as the back-propagation algorithm applied\nto f, so long as f is continuous and differentiable almost everywhere. crucially, \u03c9\nmust not be a function of z, and z must not be a function of \u03c9. this technique is\noften called the reparametrization trick stochastic back-propagation\nperturbation\nanalysis.\n\nor\n\n,\n\nthe requirement that f be continuous and differentiable of course requires y\nto be continuous. if we wish to back-propagate through a sampling process that\nproduces discrete-valued samples, it may still be possible to estimate a gradient on\n\u03c9, using reinforcement learning algorithms such as variants of the reinforce\nalgorithm (\n\n), discussed in sec.\n\nwilliams 1992\n\n20.9.1\n.\n\n,\n\n690\n\n "}, {"Page_number": 706, "text": "chapter 20. deep generative models\n\nin neural network applications, we typically choose z to be drawn from some\nsimple distribution, such as a unit uniform or unit gaussian distribution, and\nachieve more complex distributions by allowing the deterministic portion of the\nnetwork to reshape its input.\n\n;\n\n,\n\nprice 1958 bonnet 1964\n\nthe idea of propagating gradients or optimizing through stochastic operations\ndates back to the mid-twentieth century (\n) and was\nfirst used for machine learning in the context of reinforcement learning (\nwilliams\n,\n1992).\u00a0more recently, it has been applied to variational approximations (opper\n) and stochastic or generative neural networks (bengio\nand archambeau 2009\net al.,\n2014\n;\ngoodfellow\n). many networks, such as denoising autoencoders or\nnetworks regularized\u00a0with dropout, are also\u00a0naturally designed\u00a0to take noise\nas an input without requiring any special reparametrization to make the noise\nindependent from the model.\n\n2013b kingma 2013 kingma and welling 2014b a rezende\n\n;\net al.,\n\net al.,\n\n2014c\n\n, ;\n\n,\n\n,\n\n,\n\n;\n\n,\n\n20.9.1 back-propagating through discrete stochastic operations\n\nwhen a model emits a discrete variable y, the reparametrization trick is not\napplicable. suppose that\u00a0the model\u00a0takes inputs x and parameters \u03b8,\u00a0both\nencapsulated in the vector \u03c9, and combines them with random noise z to produce\ny:\n\ny\n\n=  (f\n\nz \u03c9\n\n;\n\n).\n\n(20.59)\n\nbecause y is discrete, f must be a step function. the derivatives of a step function\nare not useful at any point. right at each step boundary, the derivatives are\nundefined, but that is a small problem. the large problem is that the derivatives\nare zero almost everywhere, on the regions between step boundaries. the derivatives\nof any cost function j (y ) therefore do not give any information for how to update\nthe model parameters\n\n.\u03b8\n\nwilliams 1992\n\nthe reinforce algorithm (reward increment = non-negative factor \u00d7\noffset reinforcement \u00d7 characteristic eligibility) provides a framework defining a\n). the core idea is that\nfamily of simple but powerful solutions (\neven though j ( f(z;\u03c9)) is a step function with useless derivatives, the expected\ncost ez\nz \u03c9 is often a smooth function amenable to gradient descent.\nalthough that expectation is typically not tractable when y is high-dimensional\n(or is the result of the composition of many discrete stochastic decisions), it can be\nestimated without bias using a monte carlo average. the stochastic estimate of\nthe gradient can be used with sgd or other stochastic gradient-based optimization\ntechniques.\n\nz\u223cp( )j f( ( ;\n\n))\n\n,\n\n691\n\n "}, {"Page_number": 707, "text": "chapter 20. deep generative models\n\nthe simplest version of reinforce can be derived by simply differentiating\n\nthe expected cost:\n\n\u2202 je[ ( )]y\n\nez [ ( )] =\n\nj y xy\n=xy\n=xy\n\n\u2202\u03c9\n\nj\n\np( )y ( )y\n\nj ( )y\n\n\u2202p( )y\n\n\u2202\u03c9\n\nj\n\np( )y ( )y\n\n\u2202\n\nlog ( )y\n\np\n\u2202\u03c9\n\n1\nm\n\n\u2248\n\nmxy( )i \u223cp\n\n( )y =1\n\n, i\n\n\u2202\n\nj (y ( )i )\n\nlog (y ( )i )\n\np\n\u2202\u03c9\n\n.\n\n(20.60)\n\n(20.61)\n\n(20.62)\n\n(20.63)\n\n20.61\n\nrelies on the assumption that\n\neq.\nto extend the approach to relax this assumption. eq.\nrule for the logarithm, \u2202\ncarlo estimator of the gradient.\n\n\u2202\u03c9 = 1\n\n\u2202p( )y\n\u2202\u03c9 . eq.\n\nj does not reference \u03c9 directly. it is trivial\nexploits the derivative\ngives an unbiased monte\n\n20.63\n\n20.62\n\nlog ( )y\n\np( )y\n\np\n\nanywhere we write p(y ) in this section, one could equally write p( y x|\n\n). this\nis because p(y) is parametrized by \u03c9 , and \u03c9 contains both \u03b8 and x, if x is present.\n\none issue with the above simple reinforce estimator is that it has a very\nhigh variance, so that many samples of y need to be drawn to obtain a good\nestimator of the gradient, or equivalently, if only one sample is drawn, sgd will\nconverge very slowly and will require a smaller learning rate. it is possible to\nconsiderably reduce the variance of that estimator by using variance reduction\nmethods (\n). the idea is to modify the estimator so\nthat its expected value remains unchanged but its variance get reduced.\u00a0in the\ncontext of reinforce, the proposed variance reduction methods involve the\ncomputation of a baseline that is used to offset j (y).\u00a0note that any offset b(\u03c9)\nthat does not depend on y would not change the expectation of the estimated\ngradient because\n\nwilson 1984 l\u2019ecuyer 1994\n\n,\n\n;\n\n,\n\nep( )y (cid:37)\u2202\n\nlog ( )y\n\np\n\u2202\u03c9\n\n\u2202\n\nlog ( )y\n\np\n\u2202\u03c9\n\n\u2202p( )y\n\np( )y\n\n(cid:38) =xy\n=xy\n\u2202\u03c9xy\n\n=\n\n\u2202\n\n\u2202\u03c9\n\n692\n\np( ) =y\n\n\u2202\n\u2202\u03c9\n\n1 = 0,\n\n(20.64)\n\n(20.65)\n\n(20.66)\n\n "}, {"Page_number": 708, "text": "chapter 20. deep generative models\n\nwhich means that\n\nep( )y (cid:37)( ( )\n\nj y \u2212 b \u03c9\n(\n\n\u2202\n\n))\n\nlog ( )y\n\np\n\u2202\u03c9\n\n(cid:38) = ep( )y (cid:37)j ( )y\n= ep( )y (cid:37)j ( )y\n\n\u2202\n\nlog ( )y\n\np\n\u2202\u03c9\n\n\u2202\n\nlog ( )y\n\np\n\u2202\u03c9\n\ne( )\u03c9 p( )y (cid:37)\u2202\n\n(cid:38)\u2212 b\n(cid:38).\n\n(cid:38)\n\nlog ( )y\n\np\n\u2202\u03c9\n(20.67)\n\n(20.68)\n\nfurthermore, we can obtain the optimal b(\u03c9) by computing the variance of ( j(y) \u2212\nb(\u03c9)) \u2202\nunder p(y ) and minimizing with respect to b( \u03c9). what we find is\nthat this optimal baseline b\u2217 ( )\u03c9 i is different for each element \u03c9i of the vector\n:\u03c9\n\nlog ( )y\n\np\n\u2202\u03c9\n\nb\u2217( )\u03c9 i =\n\nep( )y hj ( )y \u2202\ne p( )y h \u2202\n\nlog ( )y\n\np\n\u2202\u03c9i\n\nlog ( )y\n\np\n\u2202\u03c9i\n\n2i\n\n2i\n\n.\n\nthe gradient estimator with respect to \u03c9 i then becomes\n\nj y \u2212 b \u03c9 i)\n( ( )\n)\n\n(\n\n\u2202\n\np\nlog ( )y\n\u2202\u03c9i\n\n(20.69)\n\n(20.70)\n\n2\n\n2\n\n2\n\n(\n\nlog ( )y\n\np\n\u2202\u03c9i\n\nand \u2202\n\np\nlog ( )y\n\u2202\u03c9i\n\nlog ( )y\np\n\u2202\u03c9 i\n\nlog ( )y\np\n\u2202\u03c9 i\n\nmnih and gregor 2014\n\n] and ep( )y h \u2202\n\n2i for each element of \u03c9. these extra\n\nwhere b(\u03c9 )i estimates the above b\u2217(\u03c9)i. the estimate b is usually obtained by\nadding extra outputs to the neural network and training the new outputs to estimate\nep( )y [j(y) \u2202\noutputs can be trained with the mean squared error objective, using respectively\nj (y )\u2202\nas targets when y is sampled from p(y), for a given\n\u03c9. the estimate b may then be recovered by substituting these estimates into eq.\n20.69.\n) preferred to use a single shared output (across all\nelements i of \u03c9) trained with the target j(y), using as baseline b(\u03c9 ) \u2248 ep( )y [ j(y)].\nvariance reduction methods have been introduced in the reinforcement learning\n), generalizing previous work\nsutton et al. 2000 weaver and tao 2001\ncontext (\n,\n2013b mnih\nbengio\n).\u00a0see\non the case of binary reward by dayan 1990\n), or\n) for\nand gregor 2014 ba\net al. (\nexamples of modern uses of the reinforce algorithm with reduced variance in\nthe context of deep learning. in addition to the use of an input-dependent baseline\nj(y) \u2212 b( \u03c9)) could be\n,\nb(\u03c9)\n(\nadjusted during training by dividing it by its standard deviation estimated by a\nmoving average during training, as a kind of adaptive learning rate, to counter\nthe effect of important variations that occur during the course of training in the\nmagnitude of this quantity.\nvariance\nnormalization.\n\n) found that the scale of\n\nmnih and gregor 2014\n\nmnih and gregor 2014\n\n) called this heuristic\n\net al. (\nxu\n\n2014 mnih\n\net al. (\n\net al. (\n\n2014\n\n2015\n\n),\n\n),\n\n),\n\n(\n\n(\n\n(\n\n(\n\n,\n\n;\n\n693\n\n "}, {"Page_number": 709, "text": "chapter 20. deep generative models\n\nreinforce-based estimators can be understood as estimating the gradient\nby correlating choices of y with corresponding values of j (y). if a good value of y\nis unlikely under the current parametrization, it might take a long time to obtain it\nby chance, and get the required signal that this configuration should be reinforced.\n\n20.10 directed generative nets\n\n16\n\nas discussed in chapter\n, directed graphical models make up a prominent class\nof graphical models. while directed graphical models have been very popular\nwithin the greater machine learning community, within the smaller deep learning\ncommunity they have until roughly 2013 been overshadowed by undirected models\nsuch as the rbm.\n\nin this section we review some of the standard directed graphical models that\n\nhave traditionally been associated with the deep learning community.\n\nwe have already described deep belief networks, which are a partially directed\nmodel. we have also already described sparse coding models, which can be thought\nof as shallow directed generative models. they are often used as feature learners\nin the context of deep learning, though they tend to perform poorly at sample\ngeneration and density estimation. we now describe a variety of deep, fully directed\nmodels.\n\n20.10.1 sigmoid belief nets\n\nneal 1990\n\n) are a simple form of directed graphical model\nsigmoid belief networks (\nwith a specific kind of conditional probability distribution. in general, we can\nthink of a sigmoid belief network as having a vector of binary states s, with each\nelement of the state influenced by its ancestors:\n\n,\n\np s( i ) = \u03c3(cid:39)(cid:40)xj<i\n\nwj,isj + bi(cid:44)(cid:45) .\n\n(20.71)\n\nthe most common structure of sigmoid belief network is one that is divided\ninto many layers, with ancestral sampling proceeding through a series of many\nhidden layers and then ultimately generating the visible layer. this structure is\nvery similar to the deep belief network, except that the units at the beginning of\nthe sampling process are independent from each other, rather than sampled from\na restricted boltzmann machine.\u00a0such a structure is interesting for a variety of\n\n694\n\n "}, {"Page_number": 710, "text": "chapter 20. deep generative models\n\nreasons. one reason is that the structure is a universal approximator of probability\ndistributions over the visible units,\u00a0in the sense that it can approximate any\nprobability distribution over binary variables arbitrarily well, given enough depth,\neven if the width of the individual layers is restricted to the dimensionality of the\nvisible layer (sutskever and hinton 2008\n\n).\n\n,\n\nwhile generating a sample of the visible units is very efficient in a sigmoid\nbelief network, most other operations are not. inference over the hidden units given\nthe visible units is intractable. mean field inference is also intractable because the\nvariational lower bound involves taking expectations of cliques that encompass\nentire layers. this problem has remained difficult enough to restrict the popularity\nof directed discrete networks.\n\n,\n\n;\n\n;\n\n19.5\n\net al.,\n\n1995 dayan and hinton 1996\n\none approach for performing inference in a sigmoid belief network is to construct\na different lower bound that is specialized for sigmoid belief networks (\nsaul et al.\n,\n1996). this approach has only been applied to very small networks. another\napproach is to use learned inference mechanisms as described in sec.\u00a0\n. the\nhelmholtz machine (dayan\n) is a sigmoid belief\nnetwork combined with an inference network that predicts the parameters of the\nmean field distribution over the hidden units. modern approaches (\ngregor et al.\n,\n) to sigmoid belief networks still use this inference\n2014 mnih and gregor 2014\nnetwork approach. these techniques remain difficult due to the discrete nature of\nthe latent variables. one cannot simply back-propagate through the output of the\ninference network, but instead must use the relatively unreliable machinery for back-\npropagating through discrete sampling processes, described in sec.\n. recent\napproaches based on importance sampling, reweighted wake-sleep (bornschein\nand bengio 2015\n2015\n)\nmake it possible to quickly train sigmoid belief networks and reach state-of-the-art\nperformance on benchmark tasks.\n\n) and bidirectional helmholtz machines (bornschein\n\n20.9.1\n\net al.,\n\n,\n\n,\n\na special case of sigmoid belief networks is the case where there are no latent\nvariables. learning in this case is efficient, because there is no need to marginalize\nlatent variables out of the likelihood.\u00a0a family of models called auto-regressive\nnetworks generalize this fully visible belief network to other kinds of variables\nbesides binary variables and other structures of conditional distributions besides log-\nlinear relationships. auto-regressive networks are described later, in sec.\n20.10.7\n.\n\n20.10.2 differentiable generator nets\n\nmany generative models are based on the idea of using a differentiable generator\nnetwork. the model transforms samples of latent variables z to samples x or\n\n695\n\n "}, {"Page_number": 711, "text": "chapter 20. deep generative models\n\nto distributions over samples x using a differentiable function g(z; \u03b8( )g ) which is\ntypically represented by a neural network. this model class includes variational\nautoencoders,\u00a0which pair\u00a0the generator net with an inference net,\u00a0generative\nadversarial\u00a0networks,\u00a0which\u00a0pair the\u00a0generator\u00a0network\u00a0with a\u00a0discriminator\nnetwork, and techniques that train generator networks in isolation.\n\ngenerator networks are essentially just parametrized computational procedures\nfor generating samples, where the architecture provides the family of possible\ndistributions to sample from and the parameters select a distribution from within\nthat family.\n\nas an example, the standard procedure for drawing samples from a normal\ndistribution with mean \u00b5 and covariance \u03c3 is to feed samples z from a normal\ndistribution with zero mean and identity covariance into a very simple generator\nnetwork. this generator network contains just one affine layer:\n\nx\n\n=  (g\n\nz\n\n) =  +\u00b5\n\nlz\n\n(20.72)\n\nwhere\n\nl\n\nis given by the cholesky decomposition of\n\n.\n\u03c3\n\npseudorandom number generators can also use nonlinear transformations of\nsimple distributions. for example, inverse transform sampling (devroye 2013\n)\ndraws a scalar z from u (0,1) and applies a nonlinear transformation to a scalar\nx. in this case g(z ) is given by the inverse of the cumulative distribution function\np(v)dv . if we are able to specify p(x), integrate over x, and invert the\n\n,\n\nresulting function, we can sample from\n\np x( )\n\nwithout using machine learning.\n\nf (x) =r x\n\n\u2212\u221e\n\nto generate samples from more complicated distributions that are difficult\nto specify directly, difficult to integrate over, or whose resulting integrals are\ndifficult to invert, we use a feedforward network to represent a parametric family\nof nonlinear functions g , and use training data to infer the parameters selecting\nthe desired function.\n\nwe can think of g as providing a nonlinear change of variables that transforms\n\nthe distribution over\n\nz \n\ninto the desired distribution over\n\n.\nx\n\nrecall from eq.\n\n3.47\n\nthat, for invertible, differentiable, continuous\n\ng\n\n,\n\nthis implicitly imposes a probability distribution over\n\npz ( ) = \n\nz\n\npx( ( ))\n\n(20.73)\n\n:x\n\npx( ) =x\n\n.\n\n(20.74)\n\n\u2202g\n\u2202z\n\n)(cid:32)(cid:32)(cid:32)(cid:32) .\n\npz (g\u22121( ))x\n\ng z (cid:32)(cid:32)(cid:32)(cid:32)det(\n\u2202z )(cid:32)\n(cid:32)\n(cid:32)(cid:32)\n(cid:32)(cid:32)\n\ndet(\u2202g\n\n696\n\n "}, {"Page_number": 712, "text": "chapter 20. deep generative models\n\nof course, this formula may be difficult to evaluate, depending on the choice of\ng, so we often use indirect means of learning g, rather than trying to maximize\nlog ( )\n\np x directly.\n\nin some cases, rather than using g to provide a sample of x directly, we use g\nto define a conditional distribution over x. for example, we could use a generator\nnet whose final layer consists of sigmoid outputs to provide the mean parameters\nof bernoulli distributions:\n\np(x i = 1 \n\n| z\n\n) =  ( )\n\ng z i .\n\n(20.75)\n\nin this case, when we use g to define p(x z|\nmarginalizing :z\nx z|\n(\nezp\n\np( ) = \n\nx\n\n), we impose a distribution over x by\n\n.\n)\n\n(20.76)\n\nboth approaches define a distribution pg(x) and allow us to train various\n\ncriteria of pg using the reparametrization trick of sec.\n\n.\n20.9\n\nthe two different approaches to formulating generator nets\u2014emitting the\nparameters of a conditional distribution versus directly emitting samples\u2014have\ncomplementary strengths and weaknesses. when the generator net defines a\nconditional distribution over x, it is capable of generating discrete data as well\nas continuous data. when\u00a0the generator\u00a0net provides samples\u00a0directly,\u00a0it\u00a0is\ncapable of generating only continuous data (we could introduce discretization in\nthe forward propagation, but this would lose the ability to learn the model using\nback-propagation). the advantage to direct sampling is that we are no longer\nforced to use conditional distributions whose form can be easily written down and\nalgebraically manipulated by a human designer.\n\napproaches based on differentiable generator networks are motivated by the\nsuccess of gradient descent applied\u00a0to differentiable feedforward networks\u00a0for\nclassification.\u00a0in the context of supervised learning, deep feedforward networks\ntrained with gradient-based learning seem practically guaranteed to succeed given\nenough hidden units and enough training data. can this same recipe for success\ntransfer to generative modeling?\n\ngenerative modeling seems to be more difficult than classification or regression\nbecause the learning process requires optimizing intractable criteria. in the context\nof differentiable generator nets, the criteria are intractable because the data does\nnot specify both the inputs z and the outputs x of the generator net. in the case\nof supervised learning, both the inputs x and the outputs y were given, and the\noptimization procedure needs only to learn how to produce the specified mapping.\nin the case of generative modeling, the learning procedure needs to determine how\nto arrange\n\nspace in a useful way and additionally how to map from to .\nx\n\nz\n\nz\n\n697\n\n "}, {"Page_number": 713, "text": "chapter 20. deep generative models\n\n2015\n\net al. (\n\ndosovitskiy\n\n) studied a simplified problem, where the correspondence\nbetween z and x is given. specifically, the training data is computer-rendered\nimagery of chairs. the latent variables z are parameters given to the rendering\nengine describing the choice of which chair model to use, the position of the chair,\nand other configuration details that affect the rendering of the image. using this\nsynthetically generated data, a convolutional network is able to learn to map z\ndescriptions of the content of an image to x approximations of rendered images.\nthis suggests that contemporary differentiable generator networks have sufficient\nmodel capacity to be good generative models, and that contemporary optimization\nalgorithms have the ability to fit them. the difficulty lies in determining how to\ntrain generator networks when the value of z for each x is not fixed and known\nahead of each time.\n\nthe following sections describe several approaches to training differentiable\n\ngenerator nets given only training samples of\n\n.x\n\n20.10.3 variational autoencoders\n\nthe variational autoencoder\n) is a\ndirected model that uses learned approximate inference and can be trained purely\nwith gradient-based methods.\n\n(\nkingma 2013 rezende et al. 2014\n\nvae\n\nor\n\n,\n\n;\n\n,\n\nto generate a sample from the model, the vae first draws a sample z from\nthe code distribution pmodel( z). the sample is then run through a differentiable\ngenerator network g(z ). finally, x is sampled from a distribution p model(x; g(z)) =\npmodel(x z|\n).\u00a0however, during training, the approximate inference network (or\nencoder) q(z x|\n) is then viewed as a decoder\nnetwork.\n\n) is used to obtain z and pmodel (x z|\n\nthe key insight behind variational autoencoders is that they may be trained\n\nby maximizing the variational lower bound\n\nassociated with data point\n\n:\nx\n\nl( )q\nh q z | x\nz x,\n) + ( (\n))\nx z| \u2212 dkl ( (\n\n)\n\n)\n\nq\n\nz x\n\nz x\n\ne\nz\n\n(20.77)\n\nq z | x ||pmodel( ))z\n\n\u223cq( | ) log pmodel(\n\u223cq( | ) log pmodel(\n\nl( ) = \n= e\nz\n\u2264 log pmodel( )x .\nin eq.\n, we recognize the first term as the joint log-likelihood of the visible\nand hidden variables under the approximate posterior over the latent variables (just\nlike with em, except that we use an approximate rather than the exact posterior).\nwe recognize also a second term, the entropy of the approximate posterior. when\nq is chosen to be a gaussian distribution, with noise added to a predicted mean\nvalue, maximizing this entropy term encourages increasing the standard deviation\n\n(20.78)\n\n(20.79)\n\n20.77\n\n698\n\n "}, {"Page_number": 714, "text": "chapter 20. deep generative models\n\nl\n\nof this noise. more generally, this entropy term encourages the variational posterior\nto place high probability mass on many z values that could have generated x,\nrather than collapsing to a single point estimate of the most likely value. in eq.\n20.78, we recognize the first term as the reconstruction log-likelihood found in\nother autoencoders. the second term tries to make the approximate posterior\ndistribution\n\npmodel( )z approach each other.\n\nand the model prior\n\nz | x\nq(\n)\n\ntraditional approaches to variational inference and learning infer q via an\n). these\noptimization algorithm, typically iterated fixed point equations (sec.\napproaches are slow and often require the ability to compute ez\u223cq log pmodel (z x,\n)\nin closed form. the main idea behind the variational autoencoder is to train a\nparametric encoder (also sometimes called an inference network or recognition\nmodel) that produces the parameters of q . so long as z is a continuous variable, we\ncan then back-propagate through samples of z drawn from q(z x|\n) = q (z; f(x; \u03b8 ))\nin order to obtain a gradient with respect to \u03b8. learning then consists solely of\nmaximizing l with respect to the parameters of the encoder and decoder. all of\nthe expectations in may be approximated by monte carlo sampling.\n\n19.4\n\n3.6\n\nthe variational autoencoder approach is elegant, theoretically pleasing, and\nsimple to implement. it also obtains excellent results and is among the state of\nthe art approaches to generative modeling. its main drawback is that samples\nfrom variational autoencoders trained on images tend to be somewhat blurry. the\ncauses of this phenomenon are not yet known. one possibility is that the blurriness\nis an intrinsic effect of maximum likelihood, which minimizes dkl(pdatakpmodel ).\nas illustrated in fig.\n, this means that the model will assign high probability to\npoints that occur in the training set, but may also assign high probability to other\npoints. these other points may include blurry images. part of the reason that the\nmodel would choose to put probability mass on blurry images rather than some\nother part of the space is that the variational autoencoders used in practice usually\nhave a gaussian distribution for pmodel(x; g(z)).\u00a0maximizing a lower bound on\nthe likelihood of such a distribution is similar to training a traditional autoencoder\nwith mean squared error, in the sense that it has a tendency to ignore features\nof the input that occupy few pixels or that cause only a small change in the\nbrightness of the pixels that they occupy. this issue is not specific to vaes and\nis shared with generative models that optimize a log-likelihood, or equivalently,\ndkl(pdatakpmodel), as argued by\n). another\ntroubling issue with contemporary vae models is that they tend to use only a small\nsubset of the dimensions of z, as if the encoder was not able to transform enough\nof the local directions in input space to a space where the marginal distribution\nmatches the factorized prior.\n\ntheis et al. 2015\n\nhuszar 2015\n\n) and by\n\n(\n\n(\n\n699\n\n "}, {"Page_number": 715, "text": "chapter 20. deep generative models\n\nor\n\nthe vae framework is very straightforward to extend to a wide range of model\narchitectures. this is a key advantage over boltzmann machines, which require\nextremely careful model design to maintain tractability. vaes work very well\nwith a diverse family of differentiable operators.\u00a0one particularly sophisticated\nvae is the deep recurrent attention writer\ngregor et al. 2015\n).\ndraw uses a recurrent encoder and recurrent decoder combined with an attention\nmechanism. the generation process for the draw model consists of sequentially\nvisiting different small image patches and drawing the values of the pixels at those\npoints. vaes can also be extended to generate sequences by defining variational\nrnns (chung\n) by using a recurrent encoder and decoder within\nthe vae framework. generating a sample from a traditional rnn involves only\nnon-deterministic operations at the output space. variational rnns also have\nrandom variability at the potentially more abstract level captured by the vae\nlatent variables.\n\nmodel (\n\ndraw\n\net al.,\n\n2015b\n\n,\n\nthe vae framework has been extended to maximize not just the traditional\nvariational lower bound, but instead the importance weighted autoencoder (burda\net al.,\n\n) objective:\n\n2015\n\nx, q\n\nlk(\n\n) = \n\ne\nz(1) ,...,z( )k \u223c |\n\nq(z x)\"log\n\n1\nk\n\nkxi=1\n\npmodel(x z,\n\nq(z ( )i\n\n( )i )\n\n| x) # .\n\n(20.80)\n\nthis new objective is equivalent to the traditional lower bound l when k = 1.\nhowever, it may also be interpreted as forming an estimate of the true log pmodel (x)\nusing importance sampling of z from proposal distribution q(z x|\n). the importance\nweighted autoencoder objective is also a lower bound on log pmodel (x) and becomes\ntighter as\n\nincreases.\n\nk\n\n,\n\n;\n\n;\n\net al.,\n\n2011 brakel\n\ngoodfellow et al. 2013b stoyanov\n\nvariational autoencoders have some interesting connections to the mp-dbm\nand other approaches that involve back-propagation through the approximate\ninference graph (\n).\n2013\nthese previous approaches required an inference procedure such as mean field fixed\npoint equations to provide the computational graph. the variational autoencoder\nis defined for arbitrary computational graphs, which makes it applicable to a wider\nrange of probabilistic model families because there is no need to restrict the choice\nof models to those with tractable mean field fixed point equations. the variational\nautoencoder also has the advantage that it increases a bound on the log-likelihood\nof the model, while the criteria for the mp-dbm and related models are more\nheuristic and have little probabilistic interpretation beyond making the results of\napproximate inference accurate. one disadvantage of the variational autoencoder\nis that it learns an inference network for only one problem, inferring z given x.\n\net al.,\n\n700\n\n "}, {"Page_number": 716, "text": "chapter 20. deep generative models\n\nthe older methods are able to perform approximate inference over any subset of\nvariables given any other subset of variables, because the mean field fixed point\nequations specify how to share parameters between the computational graphs for\nall of these different problems.\n\none very nice property of the variational autoencoder is that simultaneously\ntraining a parametric encoder in combination with the generator network forces\nthe model to learn a predictable coordinate system that the encoder can capture.\nthis makes it an excellent manifold learning algorithm. see fig.\nfor examples\nof low-dimensional manifolds learned by the variational autoencoder. in one of the\ncases demonstrated in the figure, the algorithm discovered two independent factors\nof variation present in images of faces: angle of rotation and emotional expression.\n\n20.6\n\n,\n\nfigure 20.6: examples of two-dimensional coordinate systems for high-dimensional mani-\nfolds, learned by a variational autoencoder (kingma and welling 2014a\n). two dimensions\nmay be plotted directly on the page for visualization, so we can gain an understanding of\nhow the model works by training a model with a 2-d latent code, even if we believe the\nintrinsic dimensionality of the data manifold is much higher. the images shown are not\nexamples from the training set but images x actually generated by the model p (x z|\n),\nsimply by changing the 2-d \u201ccode\u201d z (each image corresponds to a different choice of \u201ccode\u201d\nz on a 2-d uniform grid).\u00a0(left) the two-dimensional map of the frey faces manifold.\none dimension that has been discovered (horizontal) mostly corresponds to a rotation of\nthe face, while the other (vertical) corresponds to the emotional expression.\nthe\ntwo-dimensional map of the mnist manifold.\n\n(right)\n\n701\n\n "}, {"Page_number": 717, "text": "chapter 20. deep generative models\n\n20.10.4 generative adversarial networks\n\ngenerative adversarial networks or gans (\ngenerative modeling approach based on differentiable generator networks.\n\ngoodfellow et al. 2014c\n\n,\n\n) are another\n\ngenerative adversarial networks are based on a game theoretic scenario in\nwhich the generator network must compete against an adversary. the generator\nnetwork directly produces samples x = g (z; \u03b8( )g ). its adversary, the discriminator\nnetwork, attempts to distinguish between samples drawn from the training data\nand samples drawn from the generator. the discriminator emits a probability value\ngiven by d(x; \u03b8( )d ), indicating the probability that x is a real training example\nrather than a fake sample drawn from the model.\n\nthe simplest way to formulate learning in generative adversarial networks is\nas a zero-sum game, in which a function v(\u03b8( )g , \u03b8( )d ) determines the payoff of the\ndiscriminator. the generator receives \u2212v(\u03b8( )g , \u03b8( )d ) as its own payoff. during\nlearning, each player attempts to maximize its own payoff, so that at convergence\n\ng\u2217 = arg min\n\ng\n\nmax\n\nd\n\nv g, d .\n)\n\n(\n\n(20.81)\n\nthe default choice for\n\nisv\n\nv(\u03b8 ( )g , \u03b8( )d ) = e x\u223cpdata log ( ) +\n\nd x\n\ne x\u223cpmodel log (1\n\n\u2212 d x .\n( ))\n\n(20.82)\n\nthis drives the discriminator to attempt to learn to correctly classify samples as real\nor fake. simultaneously, the generator attempts to fool the classifier into believing\nits samples are real. at convergence, the generator\u2019s samples are indistinguishable\nfrom real data, and the discriminator outputs 1\n2 everywhere. the discriminator\nmay then be discarded.\n\nthe main motivation for the design of gans is that the learning process\nrequires neither approximate inference nor approximation of a partition function\ngradient. in the case where maxd v(g, d) is convex in \u03b8 ( )g (such as the case where\noptimization is performed directly in the space of probability density functions)\nthen the procedure is guaranteed to converge and is asymptotically consistent.\n\nunfortunately, learning in gans can be difficult in practice when g and d\nare represented by neural networks and maxd v(g, d) is not convex. goodfellow\n(\n2014\n) identified non-convergence as an issue that may cause gans to underfit.\nin general, simultaneous gradient descent on two players\u2019 costs is not guaranteed\nto reach an equilibrium. consider for example the value function v(a, b) = ab,\nwhere one player controls a and incurs cost ab, while the other player controls b\nand receives a cost \u2212ab. if we model each player as making infinitesimally small\n\n702\n\n "}, {"Page_number": 718, "text": "chapter 20. deep generative models\n\ngradient steps, each player reducing their own cost at the expense of the other\nplayer, then a and b go into a stable, circular orbit, rather than arriving at the\nequilibrium point at the origin. note that the equilibria for a minimax game are\nnot local minima of v. instead, they are points that are simultaneously minima\nfor both players\u2019 costs. this means that they are saddle points of v that are local\nminima with respect to the first player\u2019s parameters and local maxima with respect\nto the second player\u2019s parameters. it is possible for the two players to take turns\nincreasing then decreasing v forever, rather than landing exactly on the saddle\npoint where neither player is capable of reducing their cost. it is not known to\nwhat extent this non-convergence problem affects gans.\n\n(\n\ngoodfellow\u00a0 2014\n\n) identified\u00a0an alternative\u00a0formulation of\u00a0the payoffs, in\nwhich\u00a0the game\u00a0is no longer\u00a0zero-sum,\u00a0that\u00a0has the\u00a0same expected gradient\nas maximum likelihood learning whenever the discriminator is optimal. because\nmaximum likelihood training converges, this reformulation of the gan game should\nalso converge, given enough samples. unfortunatley, this alternative formulation\ndoes not seem to perform well in practice, possibly due to suboptimality of the\ndiscriminator, or possibly due to high variance around the expected gradient.\n\n2014c\n\net al. (\n\nin practice, the best-performing formulation of the gan game is a different for-\nmulation that is neither zero-sum nor equivalent to maximum likelihood, introduced\nby goodfellow\n) with a heuristic motivation. in this best-performing\nformulation, the generator aims to increase the log probability that the discrimina-\ntor makes a mistake, rather than aiming to decrease the log probability that the\ndiscriminator makes the correct prediction. this reformulation is motivated solely\nby the observation that it causes the derivative of the generator\u2019s cost function\nwith respect to the discriminator\u2019s logits to remain large even in the situation\nwhere the discriminator confidently rejects all generator samples.\n\nstabilization of gan learning remains an open problem. fortunately, gan\nlearning performs well when the model architecture and hyperparameters are care-\nfully selected.\n) crafted a deep convolutional gan (dcgan)\nthat performs very well for image synthesis tasks, and showed that its latent\nrepresentation space captures important factors of variation, as shown in fig.\n.\n15.9\nsee fig.\n\nfor examples of images generated by a dcgan generator.\n\nradford et al. 2015\n\n20.7\n\n(\n\nthe gan learning problem can also be simplified by breaking the generation\nprocess into many levels of detail. it is possible to train conditional gans (mirza\n) rather\nand osindero 2014\nthan simply sampling from a marginal distribution p(x). denton\n2015\n)\nshowed that a series of conditional gans can be trained to first generate a very\nlow-resolution version of an image, then incrementally add details to the image.\n\n) that learn to sample from a distribution p(x y|\n\net al. (\n\n,\n\n703\n\n "}, {"Page_number": 719, "text": "chapter 20. deep generative models\n\nfigure 20.7:\u00a0images generated by gans trained on the lsun dataset. (left) images\nof bedrooms generated by a dcgan model, reproduced with permission from radford\net al. (\n(right) images of churches generated by a lapgan model, reproduced\nwith permission from\n\ndenton et al. 2015\n\n2015\n\n).\u00a0\n\n).\n\n(\n\nthis technique is called the lapgan model, due to the use of a laplacian pyramid\nto generate the images containing varying levels of detail. lapgan generators\nare able to fool not only discriminator networks but also human observers, with\nexperimental subjects identifying up to 40% of the outputs of the network as being\nreal data. see fig.\nfor examples of images generated by a lapgan generator.\n\n20.7\n\none unusual capability of the gan training procedure is that it can fit proba-\nbility distributions that assign zero probability to the training points. rather than\nmaximizing the log probability of specific points, the generator net learns to trace\nout a manifold whose points resemble training points in some way. somewhat para-\ndoxically, this means that the model may assign a log-likelihood of negative infinity\nto the test set, while still representing a manifold that a human observer judges\nto capture the essence of the generation task. this is not clearly an advantage or\na disadvantage, and one may also guarantee that the generator network assigns\nnon-zero probability to all points simply by making the last layer of the generator\nnetwork add gaussian noise to all of the generated values.\u00a0generator networks\nthat add gaussian noise in this manner sample from the same distribution that one\nobtains by using the generator network to parametrize the mean of a conditional\ngaussian distribution.\n\ndropout seems to be important in the discriminator network.\u00a0in particular,\nunits\u00a0should be\u00a0stochastically dropped\u00a0while computing\u00a0the\u00a0gradient\u00a0for\u00a0the\ngenerator network to follow. following the gradient of the deterministic version of\nthe discriminator with its weights divided by two does not seem to be as effective.\n\n704\n\n "}, {"Page_number": 720, "text": "chapter 20. deep generative models\n\nlikewise, never using dropout seems to yield poor results.\n\nwhile the gan framework is designed for differentiable generator networks,\nsimilar principles can be used to train other kinds of models. for example, self-\nsupervised boosting can be used to train an rbm generator to fool a logistic\nregression discriminator (welling\n\net al.,\n\n2002\n\n).\n\n20.10.5 generative moment matching networks\n\ngenerative moment matching networks (\nli et al. 2015 dziugaite et al. 2015\n)\nare another form of generative model based on differentiable generator networks.\nunlike vaes and gans, they do not need to pair the generator network with any\nother network\u2014neither an inference network as used with vaes nor a discriminator\nnetwork as used with gans.\n\n,\n\n;\n\n,\n\nmoment matching\n\nthese networks are trained with a technique called\n\n. the\nbasic idea behind moment matching is to train the generator in such a way that\nmany of the statistics of samples generated by the model are as similar as possible\nto those of the statistics of the examples in the training set. in this context, a\nmoment is an expectation of different powers of a random variable. for example,\nthe first moment is the mean, the second moment is the mean of the squared\nvalues, and so on. in multiple dimensions, each element of the random vector may\nbe raised to different powers, so that a moment may be any quantity of the form\n\nex\u03c0ix ni\ni\n\n(20.83)\n\nwhere n = [n 1, n2, . . . , n d]> is a vector of non-negative integers.\n\nupon first examination, this approach seems to be computationally infeasible.\nfor example, if we want to match all the moments of the form xi xj, then we need\nto minimize the difference between a number of values that is quadratic in the\ndimension of x. moreover, even matching all of the first and second moments\nwould only be sufficient to fit a multivariate gaussian distribution, which captures\nonly linear relationships between values. our ambitions for neural networks are to\ncapture complex nonlinear relationships, which would require far more moments.\ngans avoid this problem of exhaustively enumerating all moments by using a\ndynamically updated discriminator, that automatically focuses its attention on\nwhichever statistic the generator network is matching the least effectively.\n\ninstead, generative moment matching networks can be trained by minimizing\na cost function called maximum mean discrepancy (sch\u00f6lkopf and smola 2002\n;\ngretton\n) or mmd. this cost function measures the error in the first\nmoments in an infinite-dimensional space, using an implicit mapping to feature\n\net al.,\n\n2012\n\n,\n\n705\n\n "}, {"Page_number": 721, "text": "chapter 20. deep generative models\n\nspace defined by a kernel function in order to make computations on infinite-\ndimensional vectors tractable. the mmd cost is zero if and only if the two\ndistributions being compared are equal.\n\nvisually, the samples from generative moment matching networks are somewhat\ndisappointing. fortunately, they can be improved by combining the generator\nnetwork with an autoencoder. first, an autoencoder is trained to reconstruct the\ntraining set. next, the encoder of the autoencoder is used to transform the entire\ntraining set into code space. the generator network is then trained to generate\ncode samples, which may be mapped to visually pleasing samples via the decoder.\n\nunlike gans, the cost function is defined only with respect to a batch of\nexamples from both the training set and the generator network. it is not possible\nto make a training update as a function of only one training example or only\none sample from the generator network. this is because the moments must be\ncomputed as an empirical average across many samples. when the batch size is too\nsmall, mmd can underestimate the true amount of variation in the distributions\nbeing sampled. no finite batch size is sufficiently large to eliminate this problem\nentirely, but larger batches reduce the amount of underestimation. when the batch\nsize is too large, the training procedure becomes infeasibly slow, because many\nexamples must be processed in order to compute a single small gradient step.\n\nas with gans, it is possible to train a generator net using mmd even if that\n\ngenerator net assigns zero probability to the training points.\n\n20.10.6 convolutional generative networks\n\n2015\n\nwhen generating images, it is often useful to use a generator network that includes\na convolutional structure (see for example goodfellow\ndosovitskiy\n)). to do so, we use the \u201ctranspose\u201d of the convolution operator,\net al. (\ndescribed in sec.\n. this approach often yields more realistic images and does\nso using fewer parameters than using fully connected layers without parameter\nsharing.\n\net al. (\n\n2014c\n\n) or\n\n9.5\n\nconvolutional networks for recognition tasks have information flow from the\nimage to some summarization layer at the top of the network, often a class label.\nas this image flows upward through the network, information is discarded as the\nrepresentation of the image becomes more invariant to nuisance transformations.\nin a generator network,\u00a0the opposite is true. rich details must be added as\nthe representation of the image to be generated propagates through the network,\nculminating in the final representation of the image, which is of course the image\nitself, in all of its detailed glory, with object positions and poses and textures and\n\n706\n\n "}, {"Page_number": 722, "text": "chapter 20. deep generative models\n\nlighting.\u00a0the primary mechanism for discarding information in a convolutional\nrecognition network is the pooling layer. the generator network seems to need to\nadd information. we cannot put the inverse of a pooling layer into the generator\nnetwork because most pooling functions are not invertible. a simpler operation is\nto merely increase the spatial size of the representation. an approach that seems\nto perform acceptably is to use an \u201cun-pooling\u201d as introduced by dosovitskiy et al.\n(\n). this layer corresponds to the inverse of the max-pooling operation under\n2015\ncertain simplifying conditions.\u00a0first, the stride of the max-pooling operation is\nconstrained to be equal to the width of the pooling region. second, the maximum\ninput within each pooling region is assumed to be the input in the upper-left\ncorner. finally, all non-maximal inputs within each pooling region are assumed to\nbe zero. these are very strong and unrealistic assumptions, but they do allow the\nmax-pooling operator to be inverted. the inverse un-pooling operation allocates\na tensor of zeros, then copies each value from spatial coordinate i of the input\nk\u00d7 of the output. the integer value k defines the size\nto spatial coordinate i\nof the pooling region. even though the assumptions motivating the definition of\nthe un-pooling operator are unrealistic, the subsequent layers are able to learn to\ncompensate for its unusual output, so the samples generated by the model as a\nwhole are visually pleasing.\n\n20.10.7 auto-regressive networks\n\nauto-regressive networks are directed probabilistic models with no latent random\nvariables. the conditional probability distributions in these models are represented\nby neural networks (sometimes extremely simple neural networks such as logistic\nregression). the graph structure of these models is the complete graph. they\ndecompose a joint probability over the observed variables using the chain rule of\nprobability to obtain a product of conditionals of the form p (xd | xd\u22121, . . . , x1).\nsuch models have been called\n(fvbns) and used\nsuccessfully\u00a0in many forms, first\u00a0with logistic\u00a0regression for\u00a0each conditional\n) and then with neural networks with hidden units (bengio\ndistribution (frey 1998\nand bengio 2000b larochelle and\u00a0murray 2011\nin some\u00a0forms of auto-\nregressive networks, such as nade (\n), described in\nsec.\nbelow, we can introduce a form of parameter sharing that brings both\na statistical advantage (fewer unique parameters) and a computational advantage\n(less computation). this is one more instance of the recurring deep learning motif\nof reuse of features.\n\nlarochelle and murray 2011\n\nfully-visible bayes networks\n\n20.10.10\n\n,\u00a0\n\n).\n\n;\n\n,\n\n,\n\n,\n\n707\n\n "}, {"Page_number": 723, "text": "chapter 20. deep generative models\n\nx1x1\n\nx2x2\n\nx3x3\n\nx4x4\n\np x( 1)\np x( 1)\n\np x( 3 | x1, x2)\np x( 3 | x1, x2)\n\np x( 2 | x1)\np x( 2 | x1)\n\np x( 4 | x1, x2, x3)\np x( 4 | x1, x2, x3)\n\nx1x1\n\nx2x2\n\nx3x3\n\nx4x4\n\nfigure 20.8: a\u00a0fully visible belief\u00a0network predicts the i-th variable\u00a0from the i \u2212 1\nprevious ones. (top)\ncorresponding\ncomputational graph, in the case of the logistic fvbn, where each prediction is made by\na linear predictor.\n\nthe directed graphical model for an fvbn.\n\n(bottom)\n\n20.10.8 linear auto-regressive networks\n\nthe simplest form of auto-regressive network has no hidden units and no sharing\nof parameters or features. each p (x i | xi\u22121, . . . , x 1 ) is parametrized as a linear\nmodel (linear regression for real-valued data, logistic regression for binary data,\nsoftmax regression for discrete data). this model was introduced by frey 1998\n)\nand has o(d 2) parameters when there are d variables to model. it is illustrated in\nfig.\n\n20.8\n.\n\n(\n\nif the variables are continuous, a linear auto-regressive model is merely another\nway to formulate a multivariate gaussian distribution, capturing linear pairwise\ninteractions between the observed variables.\n\nlinear auto-regressive networks are essentially the generalization of linear\nclassification methods to generative modeling. they therefore have the same\nadvantages and disadvantages as linear classifiers. like linear classifiers, they may\nbe trained with convex loss functions, and sometimes admit closed form solutions\n(as in the gaussian case). like linear classifiers, the model itself does not offer\na way of increasing its capacity, so capacity must be raised using techniques like\nbasis expansions of the input or the kernel trick.\n\n708\n\n "}, {"Page_number": 724, "text": "chapter 20. deep generative models\n\np x( 1)\np x( 1)\n\np x( 3 | x1, x2)\np x( 3 | x1, x2)\n\np x( 2 | x1)\np x( 2 | x1)\n\np x( 4 | x1, x2, x3)\np x( 4 | x1, x2, x3)\n\nh1h1\n\nx1x1\n\nh2h2\n\nx2x2\n\nh3h3\n\nx3x3\n\nx4x4\n\nfigure 20.9: a neural auto-regressive network predicts the i-th variable xi from the i \u2212 1\nprevious ones, but is parametrized so that features (groups of hidden units denoted hi)\nthat are functions of x1 , . . . , xi can be reused in predicting all of the subsequent variables\nxi+1, xi+2 , . . . , xd.\n\n20.10.9 neural auto-regressive networks\n\n,\n\n20.8\n\nbengio and bengio 2000a b\n\n, ) have the same\nneural auto-regressive networks (\nleft-to-right graphical model as logistic auto-regressive networks (fig.\n) but\nemploy a different parametrization of the conditional distributions within that\ngraphical model structure. the new parametrization is more powerful in the sense\nthat its capacity can be increased as much as needed, allowing approximation of\nany joint distribution. the new parametrization can also improve generalization\nby introducing a parameter sharing and feature sharing principle common to deep\nlearning in general. the models were motivated by the objective of avoiding the\ncurse of dimensionality arising out of traditional tabular graphical models, sharing\nthe same structure as fig.\n. in tabular discrete probabilistic models, each\nconditional distribution is represented by a table of probabilities, with one entry\nand one parameter for each possible configuration of the variables involved. by\nusing a neural network instead, two advantages are obtained:\n\n20.8\n\n1. the parametrization of each p(xi | xi\u22121, . . . , x1) by a neural network with\n(i \u2212 1) \u00d7 k inputs and k outputs (if the variables are discrete and take k\nvalues, encoded one-hot) allows one to estimate the conditional probability\nwithout requiring an exponential number of parameters (and examples), yet\nstill is able to capture high-order dependencies between the random variables.\n\n2. instead of having a different neural network for the prediction of each xi,\n\n709\n\n "}, {"Page_number": 725, "text": "chapter 20. deep generative models\n\n20.9\n\nleft-to-right\n\nconnectivity illustrated in fig.\n\na\nallows one to merge all\nthe neural networks into one. equivalently, it means that the hidden layer\nfeatures computed for predicting xi can be reused for predicting xi k+ (k > 0).\nthe hidden units are thus organized in groups that have the particularity\nthat all the units in the i-th group only depend on the input values x1, . . . , xi.\nthe parameters used to compute these hidden units are jointly optimized\nto\u00a0improve the\u00a0prediction\u00a0of all\u00a0the variables in the\u00a0sequence. this\u00a0is\nan instance of the reuse principle that recurs throughout deep learning in\nscenarios ranging from recurrent and convolutional network architectures to\nmulti-task and transfer learning.\n\neach p (xi | xi\u22121, . . . , x 1) can represent a conditional distribution by having\noutputs of the neural network predict parameters of the conditional distribution\nof xi, as discussed in sec.\n. although the original neural auto-regressive\nnetworks were initially evaluated in the context of purely discrete multivariate\ndata (with a sigmoid output for a bernoulli variable or softmax output for a\nmultinoulli variable) it is natural to extend such models to continuous variables or\njoint distributions involving both discrete and continuous variables.\n\n6.2.1.1\n\n20.10.10 nade\n\nthe neural autoregressive density estimator (nade) is a very successful recent form\nof neural auto-regressive network (larochelle and murray 2011\n). the connectivity\nis the\u00a0same as\u00a0for the\u00a0original neural\u00a0auto-regressive network of bengio and\n) but nade introduces an additional parameter sharing scheme, as\nbengio 2000b\nillustrated in fig.\nj\nare shared.\n\n. the parameters of the hidden units of different groups\n\n20.10\n\n(\n\n,\n\nthe weights w 0j,k,i from the i-th input xi to the k -th element of the j-th group\n\nof hidden unit h( )j\nk\n\n(\nj\n\n) are shared among the groups:\n\ni\u2265\n\nw0j,k,i = wk,i.\n\n(20.84)\n\nthe remaining weights, where\n\nj < i\n\n, are zero.\n\n(\n\nlarochelle and murray 2011\n\n) chose this sharing scheme so that forward\npropagation in a nade model loosely resembles the computations performed in\nmean field inference to fill in missing inputs in an rbm. this mean field inference\ncorresponds to running a recurrent network with shared weights and the first step\nof that inference is the same as in nade. the only difference is that with nade,\nthe output weights connecting the hidden units to the output are parametrized\n\n710\n\n "}, {"Page_number": 726, "text": "chapter 20. deep generative models\n\np x( 1)\np x( 1)\n\np x( 3 | x1, x2)\np x( 3 | x1, x2)\n\np x( 2 | x1)\np x( 2 | x1)\n\np x( 4 | x1, x2, x3)\np x( 4 | x1, x2, x3)\n\nh1h1\n\nw: 1,\n\nw: 1,\n\nw: 1,\n\nx1x1\n\nh2h2\n\nx2x2\n\nh3h3\n\nw: 2,\n\nw: 2,\n\nw: 3,\n\nx3x3\n\nx4x4\n\nfigure 20.10: an illustration of the neural autoregressive density estimator (nade). the\nhidden units are organized in groups h( )j\nso that only the inputs x1, . . . , x i participate\nin computing h( )i and predicting p (xj | xj\u22121 , . . . , x1), for j > i. nade is differentiated\nfrom earlier neural auto-regressive networks by the use of a particular weight sharing\npattern: w 0j,k,i = wk,i is shared (indicated in the figure by the use of the same line pattern\nfor every instance of a replicated weight) for all the weights going out from xi to the k-th\nunit of any group\n\n(w1,i, w2,i, . . . , wn,i) is denoted w:,i.\n\n. recall that the vector\n\nj\n\ni\u2265\n\nindependently from the weights connecting the input units to the hidden units. in\nthe rbm, the hidden-to-output weights are the transpose of the input-to-hidden\nweights. the nade architecture can be extended to mimic not just one time step\nof the mean field recurrent inference but to mimic k steps. this approach is called\nnade-\n\n(k raiko\n\net al.,\n\n2014\n\n).\n\n3.9.6\n\nuria et al. 2013\n\nas mentioned previously, auto-regressive networks may be extend to process\ncontinuous-valued data. a particularly powerful and generic way of parametrizing a\ncontinuous density is as a gaussian mixture (introduced in sec.\n) with mixture\nweights \u03b1i (the coefficient or prior probability for component i), per-component\nconditional mean \u00b5i and per-component conditional variance \u03c3 2\ni . a model called\nrnade (\n) uses this parametrization to extend nade to real\nvalues. as with other mixture density networks, the parameters of this distribution\nare outputs of the network, with the mixture weight probabilities produced by a\nsoftmax unit, and the variances parametrized so that they are positive. stochastic\ngradient descent can be numerically ill-behaved due to the interactions between the\nconditional means \u00b5i and the conditional variances \u03c32\ni . to reduce this difficulty,\n) use a pseudo-gradient that replaces the gradient on the mean, in\nuria\nthe back-propagation phase.\n\net al. (\n\n2013\n\n,\n\n711\n\n "}, {"Page_number": 727, "text": "chapter 20. deep generative models\n\n,\n\nanother very interesting extension of the neural auto-regressive architectures\ngets rid of the need to choose an arbitrary order for the observed variables (murray\nand larochelle 2014\n). in auto-regressive networks, the idea is to train the network\nto be able to cope with any order by randomly sampling orders and providing the\ninformation to hidden units specifying which of the inputs are observed (on the\nright side of the conditioning bar) and which are to be predicted and are thus\nconsidered missing (on the left side of the conditioning bar). this is nice because\nit allows one to use a trained auto-regressive network to perform any inference\nproblem (i.e. predict or sample from the probability distribution over any subset\nof variables given any subset) extremely efficiently. finally, since many orders of\nvariables are possible (n! for n variables) and each order o of variables yields a\ndifferent\n\n, we can form an ensemble of models for many values of\no\n)\n\n:\no\n\n(x |\np\n\np ensemble( ) =x\n\n1\nk\n\nkxi=1\n\n( )i ).\n\no\n\n(x |\np\n\n(20.85)\n\nthis ensemble model usually generalizes better and assigns higher probability to\nthe test set than does an individual model defined by a single ordering.\n\nbengio and bengio 2000b\n\nin the same paper, the authors propose deep versions of the architecture, but\nunfortunately that immediately makes computation as expensive as in the original\nneural auto-regressive neural network (\n). the first layer\nand the output layer can still be computed in o(nh) multiply-add operations,\nas in the regular nade, where h is the number of hidden units (the size of the\no(n2h) in bengio and bengio\ngroups hi , in fig.\no(n2h2) if every\n(\n2000b\n\u201cprevious\u201d group at layer l participates in predicting the \u201cnext\u201d group at layer l + 1,\nassuming n groups of h hidden units at each layer. making the i-th group at layer\nl + 1 only depend on the i-th group, as in murray and larochelle 2014\n) at layer l\nreduces it to o nh(\n\n). however, for the other hidden layers, the computation is\n\ntimes worse than the regular nade.\n\n2), which is still\n\n), whereas it is\n\nand fig.\n\n20.10\n\n20.9\n\nh\n\n(\n\n,\n\n20.11 drawing samples from autoencoders\n\n14\n\nin chapter\n, we saw that many kinds of autoencoders learn the data distribution.\nthere are close connections between score matching, denoising autoencoders, and\ncontractive autoencoders. these connections demonstrate that some kinds of\nautoencoders learn the data distribution in some way. we have not yet seen how\nto draw samples from such models.\n\nsome kinds of autoencoders, such as the variational autoencoder, explicitly\n\n712\n\n "}, {"Page_number": 728, "text": "chapter 20. deep generative models\n\nrepresent a probability distribution and admit straightforward ancestral sampling.\nmost other kinds of autoencoders require mcmc sampling.\n\ncontractive autoencoders are designed to recover an estimate of the tangent\nplane of the data manifold. this means that repeated encoding and decoding with\ninjected noise will induce a random walk along the surface of the manifold (rifai\net al.\n).\u00a0this manifold diffusion technique is a kind of\n,\nmarkov chain.\n\n2012 mesnil\n\net al.\n,\n\n2012\n\n;\n\nthere is also a more general markov chain that can sample from any denoising\n\nautoencoder.\n\n20.11.1 markov chain associated with any denoising autoen-\n\ncoder\n\nthe above discussion left open the question of what noise to inject and where, in\norder to obtain a markov chain that would generate from the distribution estimated\nby the autoencoder.\n) showed how to construct such a markov\nchain for generalized denoising autoencoders. generalized denoising autoencoders\nare specified by a denoising distribution for sampling an estimate of the clean input\ngiven the corrupted input.\n\nbengio et al. 2013c\n\n(\n\neach step of the markov chain that generates from the estimated distribution\n\nconsists of the following sub-steps, illustrated in fig.\n\n:\n20.11\n\n1. starting from the previous state x, inject corruption noise, sampling \u02dcx from\n\nc(\u02dcx x|\n).\n\n2.\u00a0encode \u02dcx into h =  (f \u02dcx).\n\n3.\u00a0decode\n\nh\n\nto obtain the parameters\n\n\u03c9\n\nh=  (g\n)\n\nof\n\n4.\u00a0sample the next state\n\nfromx\n\nx | \u03c9\n(\np\n\ng\n= \n\n( )) = \nh\n\n( )) = \nh\n\n(x | \u02dcx).\np\n\nx | \u03c9\n(\np\n= \ng\n(x | \u02dcx).\np\n\n2014\n\net al. (\n\n) showed that if the autoencoder p(x | \u02dcx) forms a consistent\nbengio\nestimator of the corresponding true conditional distribution, then the stationary\ndistribution of the above markov chain forms a consistent estimator (albeit an\nimplicit one) of the data generating distribution of\n\n.x\n\n20.11.2 clamping and conditional sampling\n\nsimilarly to boltzmann machines, denoising autoencoders and their generalizations\n(such as gsns, described below) can be used to sample from a conditional distri-\nbution p( xf | x o), simply by clamping the observed units xf and only resampling\n\n713\n\n "}, {"Page_number": 729, "text": "chapter 20. deep generative models\n\nhh\n\ng\n\nf\n\n\u02dcx\u02dcx\n\n!!\n\np(\n\nx | !\n\n)\n\n\u02c6x\u02c6x\n\nc(\u02dcx x|\n)\n\nxx\n\nfigure 20.11: each step of the markov chain associated with a trained denoising autoen-\ncoder, that generates the samples from the probabilistic model implicitly trained by the\ndenoising log-likelihood criterion. each step consists in (a) injecting noise via corruption\nprocess c in state x, yielding \u02dcx, (b) encoding it with function f, yielding h = f (\u02dcx),\n(c) decoding the result with function g, yielding parameters \u03c9 for the reconstruction\ndistribution, and (d) given \u03c9, sampling a new state from the reconstruction distribution\np(x | \u03c9 = g(f( \u02dcx ))). in the typical squared reconstruction error case, g (h) = \u02c6x, which\nestimates e[x | \u02dcx], corruption consists in adding gaussian noise and sampling from\np(x | \u03c9) consists in adding gaussian noise, a second time, to the reconstruction \u02c6x. the\nlatter noise level should correspond to the mean squared error of reconstructions, whereas\nthe injected noise is a hyperparameter that controls the mixing speed as well as the\nextent to which the estimator smooths the empirical distribution (vincent 2011\n). in the\nexample illustrated here, only the c and p conditionals are stochastic steps (f and g are\ndeterministic computations), although noise can also be injected inside the autoencoder,\nas in generative stochastic networks (\n\nbengio et al. 2014\n\n).\n\n,\n\n,\n\n714\n\n "}, {"Page_number": 730, "text": "chapter 20. deep generative models\n\nthe free units x o given xf and the sampled latent variables (if any). for example,\nmp-dbms can be interpreted as a form of denoising autoencoder, and are able\nto sample missing inputs. gsns later generalized some of the ideas present in\nmp-dbms to perform the same operation (\nbengio et al. 2014 alain et al. 2015\n)\nidentified a missing condition from proposition 1 of\n), which is\nthat the transition operator (defined by the stochastic mapping going from one\nstate of the chain to the next) should satisfy a property called detailed balance,\nwhich specifies that a markov chain at equilibrium will remain in equilibrium\nwhether the transition operator is run in forward or reverse.\n\nbengio et al. 2014\n\n).\n\n(\n\n(\n\n,\n\nan experiment in clamping half of the pixels (the right part of the image) and\n\nrunning the markov chain on the other half is shown in fig.\n\n.\n20.12\n\nfigure 20.12: illustration of clamping the right half of the image and running the markov\nchain by resampling only the left half at each step.\u00a0these samples come from a gsn\ntrained to reconstruct mnist digits at each time step using the walkback procedure.\n\n20.11.3 walk-back training procedure\n\nthe walk-back training procedure was proposed by\n) as a way\nto accelerate the convergence of generative training of denoising autoencoders.\ninstead of performing a one-step encode-decode reconstruction, this procedure\nconsists in alternative multiple stochastic encode-decode steps (as in the generative\n\nbengio et al. 2013c\n\n(\n\n715\n\n "}, {"Page_number": 731, "text": "chapter 20. deep generative models\n\nmarkov chain) initialized at a training example (just like with the contrastive\ndivergence algorithm, described in sec.\n) and penalizing the last probabilistic\nreconstructions (or all of the reconstructions along the way).\n\n18.2\n\ntraining with k steps is equivalent (in the sense of achieving the same stationary\ndistribution) as training with one step, but practically has the advantage that\nspurious modes farther from the data can be removed more efficiently.\n\n20.12 generative stochastic networks\n\ngenerative stochastic networks gsns\n) are generalizations of\ndenoising autoencoders that include latent variables h in the generative markov\nchain, in addition to the visible variables (usually denoted ).x\n\n(\nbengio et al. 2014\n\nor\n\n,\n\na gsn is parametrized by two conditional probability distributions which\n\nspecify one step of the markov chain:\n\n1.\u00a0p(x( )k | h( )k ) tells how to generate the next visible variable given the current\nlatent state. such a \u201creconstruction distribution\u201d is also found in denoising\nautoencoders, rbms, dbns and dbms.\n\n2.\u00a0p(h ( )k | h(\n\n1)\n\nk\u2212 , x(\n\n1)\n\nk\u2212 ) tells how to update the latent state variable, given\n\nthe previous latent state and visible variable.\n\ndenoising autoencoders and gsns differ from classical probabilistic models\n(directed or undirected) in that they parametrize the generative process itself rather\nthan the mathematical specification of the joint distribution of visible and latent\nvariables. instead, the latter is defined implicitly,\n, as the stationary\ndistribution of the generative markov chain. the conditions for existence of the\nstationary distribution are mild and are the same conditions required by standard\nmcmc methods (see sec.\n). these conditions are necessary to guarantee\nthat the chain mixes, but they can be violated by some choices of the transition\ndistributions (for example, if they were deterministic).\n\nif it exists\n\n17.3\n\n(\n\nbengio et al. 2014\n\none could imagine different training criteria for gsns. the one proposed and\nevaluated by\n) is simply reconstruction log-probability on the\nvisible units, just like for denoising autoencoders. this is achieved by clamping\nx(0) = x to the observed example and maximizing the probability of generating x\nat some subsequent time steps, i.e., maximizing log p(x( )k = x | h( )k ), where h( )k\nis sampled from the chain, given x(0) = x.\u00a0in order to estimate the gradient of\nlog p(x( )k = x | h( )k ) with respect to the other pieces of the model, bengio et al.\n(\n2014\n\n) use the reparametrization trick, introduced in sec.\n\n20.9\n.\n\n716\n\n "}, {"Page_number": 732, "text": "chapter 20. deep generative models\n\nthe walk-back training protocol (described in sec.\n) to improve training convergence of gsns.\n\n2014\n\net al.,\n\n20.12.1 discriminant gsns\n\n20.11.3\n\n) was used (\n\nbengio\n\ny | x\n.\n)\n(\n\nthe original formulation of gsns (\n) was meant for unsupervised\nlearning and implicitly modeling p(x) for observed data x, but it is possible to\nmodify the framework to optimize\n\nbengio et al. 2014\n\np(\n\n,\n\nfor example, zhou and troyanskaya 2014\n\n) generalize gsns in this way, by only\nback-propagating the reconstruction log-probability over the output variables, keep-\ning the input variables fixed. they applied this successfully to model sequences\n(protein secondary structure) and introduced a (one-dimensional) convolutional\nstructure in the transition operator of the markov chain. it is important to re-\nmember that, for each step of the markov chain, one generates a new sequence for\neach layer, and that sequence is the input for computing other layer values (say\nthe one below and the one above) at the next time step.\n\nhence the markov chain is really over the output variable (and associated\nhigher-level hidden layers), and the input sequence only serves to condition that\nchain, with back-propagation allowing to learn how the input sequence can condition\nthe output distribution implicitly represented by the markov chain. it is therefore\na case of using the gsn in the context of structured outputs, where p(y  x|\n)\ndoes not have a simple parametric form but instead the components of y are\nstatistically dependent of each other, given , in complicated ways.\n\nx\n\n(\n\nz\u00f6hrer and pernkopf 2014\n\n) introduced a hybrid model that combines a super-\nvised objective (as in the above work) and an unsupervised objective (as in the\noriginal gsn work), by simply adding (with a different weight) the supervised and\nunsupervised costs i.e., the reconstruction log-probabilities of y and x respectively.\nsuch a hybrid criterion had previously been introduced for rbms by larochelle\nand bengio 2008\n). they show improved classification performance using this\nscheme.\n\n(\n\n20.13 other generation schemes\n\nthe methods we have described so far use either mcmc sampling, ancestral\nsampling, or some mixture of the two to generate samples.\u00a0while these are the\nmost popular approaches to generative modeling, they are by no means the only\napproaches.\n\n717\n\n "}, {"Page_number": 733, "text": "chapter 20. deep generative models\n\n2015\n\net al. (\n\n) developed a\n\nsohl-dickstein\n\ndiffusion inversion\n\ntraining scheme\nfor learning a generative model, based on non-equilibrium thermodynamics. the\napproach is based on the idea that the probability distributions we wish to sample\nfrom have structure. this structure can gradually be destroyed by a diffusion\nprocess that\u00a0incrementally changes the probability distribution\u00a0to have more\nentropy. to form a generative model, we can run the process in reverse, by training\na model that gradually restores the structure to an unstructured distribution. by\niteratively applying a process that brings a distribution closer to the target one, we\ncan gradually approach that target distribution. this approach resembles mcmc\nmethods in the sense that it involves many iterations to produce a sample. however,\nthe model is defined to be the probability distribution produced by the final step\nof the chain. in this sense, there is no approximation induced by the iterative\nprocedure. the approach introduced by sohl-dickstein\n) is also very\nclose to the generative interpretation of the denoising autoencoder (sec.\n20.11.1\n).\nlike with the denoising autoencoder, the training objective trains a transition\noperator which attempts to probabilistically undo the effect of adding some noise,\ntrying to undo one step of the diffusion process. if we compare with the walkback\ntraining procedure (sec.\n) for denoising autoencoders and gsns, the main\ndifference is that instead of reconstructing only towards the observed training point\nx, the objective function only tries to reconstruct towards the previous point in\nthe diffusion trajectory that started at x (which should be easier). this addresses\nthe following dilemma present with the ordinary reconstruction log-likelihood\nobjective of denoising autoencoders: with small levels of noise the learner only sees\nconfigurations near the data points, while with large levels of noise it is asked to do\nan almost impossible job (because the denoising distribution is going to be highly\ncomplex and multi-modal). with the diffusion inversion objective, the learner can\nlearn more precisely the shape of the density around the data points as well as\nremove spurious modes that could show up far from the data points.\n\n20.11.3\n\net al. (\n\n2015\n\n,\n\nrubin et al. 1984\n\nanother approach to sample generation is the approximate bayesian computa-\ntion (abc) framework (\n). in this approach, samples are rejected\nor modified in order to make the moments of selected functions of the samples\nmatch those of the desired distribution. while this idea uses the moments of the\nsamples like in moment matching, it is different from moment matching because it\nmodifies the samples themselves, rather than training the model to automatically\nemit samples with the correct moments. bachman and precup 2015\n) showed how\nto use ideas from abc in the context of deep learning, by using abc to shape the\nmcmc trajectories of gsns.\n\n(\n\nwe expect that many other possible approaches to generative modeling await\n\ndiscovery.\n\n718\n\n "}, {"Page_number": 734, "text": "chapter 20. deep generative models\n\n20.14 evaluating generative models\n\nresearchers studying generative models often need to compare one generative\nmodel to another, usually in order to demonstrate that a newly invented generative\nmodel is better at capturing some distribution than the pre-existing models.\n\nthis can be a difficult and subtle task. in many cases, we can not actually\nevaluate the log probability of the data under the model, but only an approximation.\nin these cases, it is important to think and communicate clearly about exactly what\nis being measured. for example, suppose we can evaluate a stochastic estimate of\nthe log-likelihood for model a, and a deterministic lower bound on the log-likelihood\nfor model b. if model a gets a higher score than model b, which is better? if we\ncare about determining which model has a better internal representation of the\ndistribution, we actually cannot tell, unless we have some way of determining how\nloose the bound for model b is. however, if we care about how well we can use\nthe model in practice, for example to perform anomaly detection, then it is fair to\nsay that a model is preferable based on a criterion specific to the practical task of\ninterest, e.g., based on ranking test examples and ranking criteria such as precision\nand recall.\n\nanother subtlety of evaluating generative models is that the evaluation metrics\nare often hard research problems in and of themselves. it can be very difficult\nto establish that models are being compared fairly. for example, suppose we use\nais to estimate log z in order to compute log \u02dcp(x) \u2212 log z for a new model we\nhave just invented. a computationally economical implementation of ais may fail\nto find several modes of the model distribution and underestimate z, which will\nresult in us overestimating log p(x). it can thus be difficult to tell whether a high\nlikelihood estimate is due to a good model or a bad ais implementation.\n\nother fields of machine learning usually allow for some variation in the pre-\nprocessing of the data. for example, when comparing the accuracy of object\nrecognition algorithms, it is usually acceptable to preprocess the input images\nslightly differently for each algorithm based on what kind of input requirements\nit has. generative modeling is different because changes in preprocessing, even\nvery small and subtle ones, are completely unacceptable. any change to the input\ndata changes the distribution to be captured and fundamentally alters the task.\nfor example, multiplying the input by 0.1 will artificially increase likelihood by a\nfactor of 10.\n\nissues with preprocessing commonly arise when benchmarking generative models\non the mnist dataset, one of the more popular generative modeling benchmarks.\nmnist consists of grayscale images. some models treat mnist images as points\n\n719\n\n "}, {"Page_number": 735, "text": "chapter 20. deep generative models\n\nin a real vector space, while others treat them as binary. yet others treat the\ngrayscale values as probabilities for a binary samples. it is essential to compare\nreal-valued models only to other real-valued models and binary-valued models only\nto other binary-valued models.\u00a0otherwise the likelihoods measured are not on the\nsame space. for binary-valued models, the log-likelihood can be at most zero, while\nfor real-valued models it can be arbitrarily high, since it is the measurement of a\ndensity. among binary models, it is important to compare models using exactly\nthe same kind of binarization. for example, we might binarize a gray pixel to 0 or 1\nby thresholding at 0.5, or by drawing a random sample whose probability of being\n1 is given by the gray pixel intensity. if we use the random binarization, we might\nbinarize the whole dataset once, or we might draw a different random example for\neach step of training and then draw multiple samples for evaluation. each of these\nthree schemes yields wildly different likelihood numbers, and when comparing\ndifferent models it is important that both models use the same binarization scheme\nfor training and for evaluation.\u00a0in fact, researchers who apply a single random\nbinarization step share a file containing the results of the random binarization, so\nthat there is no difference in results based on different outcomes of the binarization\nstep.\n\n2015\n\net al.,\n\nbecause being able to generate realistic samples from the data distribution\nis one of the goals of a generative model, practitioners often evaluate generative\nmodels by visually inspecting the samples. in the best case, this is done not by the\nresearchers themselves, but by experimental subjects who do not know the source\nof the samples (denton\n). unfortunately, it is possible for a very poor\nprobabilistic model to produce very good samples. a common practice to verify\nif the model only copies some of the training examples is illustrated in fig.\n16.1\n.\nthe idea is to show for some of the generated samples their nearest neighbor in\nthe training set, according to euclidean distance in the space of x.\u00a0this test is\nintended to detect the case where the model overfits the training set and just\nreproduces training instances. it is even possible to simultaneously underfit and\noverfit yet still produce samples that individually look good. imagine a generative\nmodel trained on images of dogs and cats that simply learns to reproduce the\ntraining images of dogs. such a model has clearly overfit, because it does not\nproduces images that were not in the training set, but it has also underfit, because\nit assigns no probability to the training images of cats. yet a human observer\nwould judge each individual image of a dog to be high quality. in this simple\nexample, it would be easy for a human observer who can inspect many samples to\ndetermine that the cats are absent. in more realistic settings, a generative model\ntrained on data with tens of thousands of modes may ignore a small number of\nmodes, and a human observer would not easily be able to inspect or remember\n\n720\n\n "}, {"Page_number": 736, "text": "chapter 20. deep generative models\n\nenough images to detect the missing variation.\n\nsince\u00a0the visual\u00a0quality\u00a0of samples\u00a0is\u00a0not\u00a0a reliable\u00a0guide,\u00a0we often\u00a0also\nevaluate the log-likelihood that the model assigns to the test data, when this is\ncomputationally feasible. unfortunately, in some cases the likelihood seems not\nto measure any attribute of the model that we really care about. for example,\nreal-valued models of mnist can obtain arbitrarily high likelihood by assigning\narbitrarily low variance to background pixels that never change. models and\nalgorithms that detect these constant features can reap unlimited rewards, even\nthough this is not a very useful thing to do. the potential to achieve a cost\napproaching\u00a0negative infinity\u00a0is present\u00a0for any\u00a0kind of\u00a0maximum\u00a0likelihood\nproblem with real values, but it is especially problematic for generative models of\nmnist because so many of the output values are trivial to predict. this strongly\nsuggests a need for developing other ways of evaluating generative models.\n\n2015\n\ntheis\n\net al. (\n\n) review many of the issues involved in evaluating generative\nmodels, including many of the ideas described above. they highlight the fact\nthat there are many different uses of generative models and that the choice of\nmetric must match the intended use of the model. for example, some generative\nmodels are better at assigning high probability to most realistic points while other\ngenerative models are better at rarely assigning high probability to unrealistic\npoints. these differences can result from whether a generative model is designed\nto minimize dkl(pdatakpmodel ) or d kl(pmodelkp data), as illustrated in fig.\n.3.6\nunfortunately, even when we restrict the use of each metric to the task it is most\nsuited for, all of the metrics currently in use continue to have serious weaknesses.\none of the most important research topics in generative modeling is therefore not\njust how to improve generative models, but in fact, designing new techniques to\nmeasure our progress.\n\n20.15 conclusion\n\ntraining generative models with hidden units is a powerful way to make models\nunderstand the world represented in the given training data. by learning a model\npmodel(x) and a representation pmodel (h x|\n), a generative model can provide\nanswers to many inference problems about the relationships between input variables\nin x and can provide many different ways of representing x by taking expectations\nof h at different layers of the hierarchy.\u00a0generative models hold the promise to\nprovide ai systems with a framework for all of the many different intuitive concepts\nthey need to understand, and the ability to reason about these concepts in the\nface of uncertainty. we hope that our readers will find new ways to make these\n\n721\n\n "}, {"Page_number": 737, "text": "chapter 20. deep generative models\n\napproaches more powerful and continue the journey to understanding the principles\nthat underlie learning and intelligence.\n\n722\n\n "}, {"Page_number": 738, "text": "bibliography\n\nabadi, m., agarwal, a., barham, p., brevdo, e., chen, z., citro, c., corrado, g. s., davis,\na., dean, j., devin, m., ghemawat, s., goodfellow, i., harp, a., irving, g., isard, m.,\njia, y., jozefowicz, r., kaiser, l., kudlur, m., levenberg, j., man\u00e9, d., monga, r.,\nmoore, s., murray, d., olah, c., schuster, m., shlens, j., steiner, b., sutskever, i.,\ntalwar, k., tucker, p., vanhoucke, v., vasudevan, v., vi\u00e9gas, f., vinyals, o., warden,\np., wattenberg, m., wicke, m., yu, y., and zheng, x. (2015). tensorflow: large-scale\nmachine learning on heterogeneous systems. software available from tensorflow.org.\n,25\n212 449\n\n,\n\nackley, d. h., hinton, g. e., and sejnowski, t. j. (1985). a learning algorithm for\n\nboltzmann machines. cognitive science,\n\n9\n\n, 147\u2013169.\n\n573 656\n\n,\n\nalain, g. and bengio, y. (2013).\u00a0what regularized auto-encoders learn from the data\n\ngenerating distribution. in\n\niclr\u20192013, arxiv:1211.4246 510 516 524\n\n.\n\n,\n\n,\n\nalain, g., bengio, y., yao, l., \u00e9ric thibodeau-laufer, yosinski, j., and vincent, p. (2015).\n\ngsns: generative stochastic networks. arxiv:1503.05571.\n\n,513 715\n\nanderson, e. (1935). the irises of the gasp\u00e9 peninsula. bulletin of the american iris\n\nsociety,\n\n59\n\n, 2\u20135.\n\n21\n\nba, j., mnih, v., and kavukcuoglu, k. (2014). multiple object recognition with visual\n\nattention.\n\narxiv:1412.7755 693\n\n.\n\nbachman, p. and precup, d. (2015). variational generative stochastic networks with\ncollaborative shaping. in proceedings of the 32nd international conference on machine\nlearning, icml 2015, lille, france, 6-11 july 2015 , pages 1964\u20131972. 718\n\nbacon, p.-l., bengio, e., pineau, j., and precup, d. (2015). conditional computation in\nneural networks using a decision-theoretic approach. in 2nd multidisciplinary conference\non reinforcement learning and decision making (rldm 2015). 453\n\nbagnell, j. a. and bradley, d. m. (2009). differentiable sparse coding. in d. koller,\nd. schuurmans, y. bengio, and l. bottou, editors, advances in neural information\nprocessing systems 21 (nips\u201908), pages 113\u2013120. 501\n\n723\n\n "}, {"Page_number": 739, "text": "bibliography\n\nbahdanau, d., cho, k., and bengio, y. (2015). neural machine translation by jointly\niclr\u20192015, arxiv:1409.0473 25 101 399 421 422\n,\n\n.\n\n,\n\n,\n\n,\n\n,\n\nlearning to align and translate. in\n468 478\n\n,\n\nbahl, l. r., brown, p., de souza, p. v., and mercer, r. l. (1987).\u00a0speech recognition\nwith continuous-parameter hidden markov models. computer, speech and language, 2,\n219\u2013234. 461\n\nbaldi, p. and hornik, k. (1989). neural networks and principal component analysis:\n\nlearning from examples without local minima. neural networks,\n\n2\n\n, 53\u201358.\n\n286\n\nbaldi, p., brunak, s., frasconi, p., soda, g., and pollastri, g. (1999). exploiting the\nbioinformatics 15(11),\n\npast and the future in protein secondary structure prediction.\n937\u2013946. 396\n\n,\n\nbaldi,\u00a0p.,\u00a0sadowski,\u00a0p.,\u00a0and whiteson,\u00a0d. (2014). searching for exotic particles in\n\nhigh-energy physics with deep learning. nature communications,\n\n.5 26\n\nballard, d. h., hinton, g. e., and sejnowski, t. j. (1983). parallel vision computation.\n\nnature. 455\n\nbarlow, h. b. (1989). unsupervised learning. neural computation,\n\n1\n\n, 295\u2013311.\n\n146\n\nbarron, a. e. (1993). universal approximation bounds for superpositions of a sigmoidal\n\nfunction. ieee trans. on information theory,\n\n39\n\n, 930\u2013945.\n\n198\n\nbartholomew, d. j. (1987). latent variable models and factor analysis. oxford university\n\npress. 493\n\nbasilevsky,\u00a0a. (1994). statistical factor analysis and related methods: theory and\n\napplications. wiley. 493\n\nbastien, f., lamblin, p.,\u00a0pascanu, r., bergstra, j.,\u00a0goodfellow, i. j., bergeron, a.,\nbouchard, n., and bengio, y. (2012). theano: new features and speed improvements.\ndeep learning and unsupervised feature learning nips 2012 workshop.\n25 82 212\n,\n222 449\n\n,\n\n,\n\n,\n\nbasu, s. and christensen, j. (2013).\u00a0teaching classification boundaries to humans.\u00a0in\n\naaai\u20192013 . 329\n\nbaxter, j. (1995). learning internal representations. in proceedings of the 8th international\nconference on computational learning theory (colt\u201995), pages 311\u2013320, santa cruz,\ncalifornia. acm press. 246\n\nbayer, j. and osendorfer, c. (2014). learning stochastic recurrent networks. arxiv\n\ne-prints. 264\n\nbecker, s. and hinton, g. (1992). a self-organizing neural network that discovers surfaces\n\nin random-dot stereograms. nature,\n\n355\n\n, 161\u2013163.\n\n544\n\n724\n\n "}, {"Page_number": 740, "text": "bibliography\n\nbehnke, s. (2001). learning iterative image reconstruction in the neural abstraction\n\npyramid. int. j. computational intelligence and applications,\n\n1\n\n(4), 427\u2013438.\n\n518\n\nbeiu, v., quintana, j. m., and avedillo, m. j. (2003). vlsi implementations of threshold\nlogic-a comprehensive survey. neural networks, ieee transactions on, 14(5), 1217\u2013\n1243. 454\n\nbelkin,\u00a0m. and\u00a0niyogi,\u00a0p. (2002). laplacian eigenmaps and spectral techniques for\nembedding and clustering.\u00a0in t. dietterich, s. becker, and z. ghahramani, editors,\nadvances in neural information processing systems 14 (nips\u201901), cambridge, ma.\nmit press. 244\n\nbelkin, m. and niyogi, p. (2003). laplacian eigenmaps for dimensionality reduction and\n\ndata representation. neural computation,\n\n15\n\n(6), 1373\u20131396.\n\n163 521\n\n,\n\nbengio, e., bacon, p.-l., pineau, j., and precup, d. (2015a). conditional computation in\n\nneural networks for faster models. arxiv:1511.06297. 453\n\nbengio,\u00a0s.\u00a0and bengio,\u00a0y.\u00a0(2000a). taking\u00a0on the curse of dimensionality in joint\ndistributions using neural networks. ieee transactions on neural networks, special\nissue on data mining and knowledge discovery,\n\n(3), 550\u2013557.\n\n709\n\n11\n\nbengio, s., vinyals, o., jaitly, n., and shazeer, n. (2015b). scheduled sampling for\nsequence prediction with recurrent neural networks. technical report, arxiv:1506.03099.\n385\n\nbengio, y. (1991). artificial neural networks and their application to sequence recognition.\n\nph.d. thesis, mcgill university, (computer science), montreal, canada. 409\n\nbengio, y. (2000). gradient-based optimization of hyperparameters. neural computation,\n\n12(8), 1889\u20131900. 438\n\nbengio, y. (2002). new distributed probabilistic language models. technical report 1215,\n\ndept. iro, universit\u00e9 de montr\u00e9al. 470\n\nbengio, y. (2009). learning deep architectures for ai . now publishers.\n\n,200 626\n\nbengio, y. (2013). deep learning of representations:\n\nlooking forward. in statistical\nlanguage and speech processing, volume 7978 of lecture notes in computer science,\npages 1\u201337. springer, also in arxiv at http://arxiv.org/abs/1305.0445. 451\n\nbengio, y. (2015). early inference in energy-based models approximates back-propagation.\n\ntechnical report arxiv:1510.02777, universite de montreal. 658\n\nbengio, y. and bengio, s. (2000b). modeling high-dimensional discrete data with multi-\n\nlayer neural networks. in\n\nnips 12\n\n, pages 400\u2013406. mit press.\n\n707 709 710 712\n\n,\n\n,\n\n,\n\nbengio, y. and delalleau, o. (2009). justifying and generalizing contrastive divergence.\n\nneural computation,\n\n21\n\n(6), 1601\u20131621.\n\n516 614\n\n,\n\n725\n\n "}, {"Page_number": 741, "text": "bibliography\n\nbengio, y. and grandvalet, y. (2004). no unbiased estimator of the variance of k-fold\ncross-validation. in s. thrun, l. saul, and b. sch\u00f6lkopf, editors, advances in neural\ninformation processing systems 16 (nips\u201903), cambridge, ma. mit press, cambridge.\n122\n\nbengio, y. and lecun, y. (2007). scaling learning algorithms towards ai. in large scale\n\nkernel machines. 19\n\nbengio, y. and monperrus, m. (2005). non-local manifold tangent learning. in l. saul,\ny. weiss, and l. bottou, editors, advances in neural information processing systems\n17 (nips\u201904), pages 129\u2013136. mit press.\n\n,159 522\n\nbengio, y. and s\u00e9n\u00e9cal, j.-s. (2003). quick training of probabilistic neural nets by\n\nimportance sampling. in proceedings of aistats 2003 . 473\n\nbengio, y. and s\u00e9n\u00e9cal, j.-s. (2008). adaptive importance sampling to accelerate training\nof a neural probabilistic language model. ieee trans. neural networks, 19(4), 713\u2013722.\n473\n\nbengio, y., de mori, r., flammia, g., and kompe, r. (1991). phonetically motivated\nacoustic parameters for continuous speech recognition using artificial neural networks.\nin proceedings of eurospeech\u201991.\n\n,27 462\n\nbengio, y., de mori, r., flammia, g., and kompe, r. (1992). neural network-gaussian\n, pages 175\u2013182.\n\nmixture hybrid for speech recognition or density estimation. in\nmorgan kaufmann. 462\n\nnips 4\n\nbengio, y., frasconi, p., and simard, p. (1993). the problem of learning long-term\nin ieee international conference on neural\n\ndependencies in recurrent networks.\nnetworks, pages 1183\u20131195, san francisco. ieee press. (invited paper). 405\n\nbengio, y., simard, p., and frasconi, p. (1994). learning long-term dependencies with\n\ngradient descent is difficult. ieee tr. neural nets.\n\n18 403 405 406 414\n\n,\n\n,\n\n,\n\n,\n\nbengio, y., latendresse, s., and dugas, c. (1999). gradient-based learning of hyper-\n\nparameters. learning conference, snowbird. 438\n\nbengio, y., ducharme, r., and vincent, p. (2001). a neural probabilistic language model.\n, pages 932\u2013938. mit\n\nin t. k. leen, t. g. dietterich, and v. tresp, editors,\npress.\n\n18 450 466 469 475 480 485\n\nnips\u20192000\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nbengio, y., ducharme, r., vincent, p., and jauvin, c. (2003). a neural probabilistic\n\nlanguage model.\n\njmlr 3\n\n,\n\n, 1137\u20131155.\n\n469 475\n\n,\n\nbengio, y., le roux, n., vincent, p., delalleau, o., and marcotte, p. (2006a). convex\n\nneural networks. in\n\nnips\u20192005\n\n, pages 123\u2013130.\n\n257\n\nbengio, y., delalleau, o., and le roux, n. (2006b). the curse of highly variable functions\n\nfor local kernel machines. in\n\nnips\u20192005 157\n\n.\n\n726\n\n "}, {"Page_number": 742, "text": "bibliography\n\nbengio, y., larochelle, h., and vincent, p. (2006c). non-local manifold parzen windows.\n\nin\n\nnips\u20192005\n\n. mit press.\n\n159 523\n\n,\n\nbengio, y., lamblin, p., popovici, d., and larochelle, h. (2007). greedy layer-wise\n\ntraining of deep networks. in\n\nnips\u20192006 14 19 200 324 325 531 533\n\n.\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nbengio, y., louradour, j., collobert, r., and weston, j. (2009). curriculum learning. in\n\nicml\u201909 . 329\n\nbengio, y., mesnil, g., dauphin, y., and rifai, s. (2013a). better mixing via deep\n\nrepresentations. in\n\nicml\u20192013 607\n\n.\n\nbengio, y., l\u00e9onard, n., and courville, a. (2013b). estimating or propagating gradients\n451 453\n,\n\nthrough stochastic neurons for conditional computation.\u00a0arxiv:1308.3432.\n691 693\n\n,\n\n,\n\nbengio, y., yao, l., alain, g., and vincent, p. (2013c). generalized denoising auto-\n\nencoders as generative models. in\n\nnips\u20192013 510 713 715\n\n.\n\n,\n\n,\n\nbengio, y., courville, a., and vincent, p. (2013d). representation learning: a review and\nnew perspectives. ieee trans. pattern analysis and machine intelligence (pami),\n35(8), 1798\u20131828. 558\n\nbengio, y., thibodeau-laufer, e., alain, g., and yosinski, j. (2014).\u00a0deep generative\n\nstochastic networks trainable by backprop. in\n\nicml\u20192014 713 714 715 716 717\n\n.\n\n,\n\n,\n\n,\n\n,\n\nbennett, c. (1976). efficient estimation of free energy differences from monte carlo data.\n\njournal of computational physics,\n\n22\n\n(2), 245\u2013268.\n\n632\n\nbennett, j. and lanning, s. (2007). the netflix prize. 482\n\nberger, a. l., della pietra, v. j., and della pietra, s. a. (1996). a maximum entropy\n476\n\napproach to natural language processing.\n\ncomputational linguistics 22\n\n, 39\u201371.\n\n,\n\nberglund, m. and raiko, t. (2013). stochastic gradient estimate variance in contrastive\n\ndivergence and persistent contrastive divergence.\n\ncorr abs/1312.6002 617\n\n,\n\n.\n\nbergstra,\u00a0j. (2011).\n\nincorporating complex cells into neural networks for\u00a0pattern\n\nclassification. ph.d. thesis, universit\u00e9 de montr\u00e9al. 254\n\nbergstra, j. and bengio, y. (2009). slow, decorrelated features for pretraining complex\n\ncell-like networks. in\n\nnips\u20192009 497\n\n.\n\nbergstra, j. and bengio, y. (2012). random search for hyper-parameter optimization. j.\n\nmachine learning res.,\n\n13\n\n, 281\u2013305.\n\n437 438\n\n,\n\nbergstra, j., breuleux, o., bastien, f., lamblin, p., pascanu, r., desjardins, g., turian,\nj., warde-farley, d., and bengio, y. (2010). theano: a cpu and gpu math expression\ncompiler. in proc. scipy.\n\n25 82 212 222 449\n\n,\n\n,\n\n,\n\n,\n\n727\n\n "}, {"Page_number": 743, "text": "bibliography\n\nbergstra, j., bardenet, r., bengio, y., and k\u00e9gl, b. (2011). algorithms for hyper-parameter\n\noptimization. in\n\nnips\u20192011 439\n\n.\n\nberkes, p. and wiskott, l. (2005). slow feature analysis yields a rich repertoire of complex\n\ncell properties.\n\njournal of vision 5\n\n,\n\n(6), 579\u2013602.\n\n498\n\nbertsekas, d. p. and tsitsiklis, j. (1996). neuro-dynamic programming. athena scientific.\n\n106\n\nbesag, j. (1975). statistical analysis of non-lattice data.\n\nthe statistician 24(3), 179\u2013195.\n\n,\n\n618\n\nbishop, c. m. (1994). mixture density networks. 188\n\nbishop, c. m. (1995a). regularization and complexity control in feed-forward networks.\nin proceedings international conference on artificial neural networks icann\u201995,\nvolume 1, page 141\u2013148.\n\n,242 249\n\nbishop, c. m. (1995b). training with noise is equivalent to tikhonov regularization.\n\nneural computation,\n\n7\n\n(1), 108\u2013116.\n\n242\n\nbishop, c. m. (2006). pattern recognition and machine learning. springer.\n\n,98 145\n\nblum, a. l. and rivest, r. l. (1992). training a 3-node neural network is np-complete.\n\n293\n\nblumer, a., ehrenfeucht, a., haussler, d., and warmuth, m. k. (1989). learnability and\n\nthe vapnik\u2013chervonenkis dimension.\n\njournal of the acm 36\n\n,\n\n(4), 929\u2013\u2013865.\n\n114\n\nbonnet, g. (1964). transformations des signaux al\u00e9atoires \u00e0 travers les syst\u00e8mes non\n\nlin\u00e9aires sans m\u00e9moire. annales des t\u00e9l\u00e9communications,\n\n19\n\n(9\u201310), 203\u2013220.\n\n691\n\nbordes,\u00a0a.,\u00a0weston,\u00a0j.,\u00a0collobert,\u00a0r.,\u00a0and bengio,\u00a0y. (2011). learning structured\n\nembeddings of knowledge bases. in\n\naaai 2011 487\n\n.\n\nbordes, a., glorot, x., weston, j., and bengio, y. (2012). joint learning of words and\n\nmeaning representations for open-text semantic parsing. aistats\u20192012 .\n\n,403 487\n\nbordes, a., glorot, x., weston, j., and bengio, y. (2013a). a semantic matching energy\nfunction for learning with multi-relational data. machine learning: special issue on\nlearning semantics. 486\n\nbordes, a., usunier,\u00a0n., garcia-duran, a., weston, j., and yakhnenko, o. (2013b).\ntranslating embeddings for modeling multi-relational data. in c. burges, l. bottou,\nm. welling, z. ghahramani, and k. weinberger, editors, advances in neural information\nprocessing systems 26 , pages 2787\u20132795. curran associates, inc. 487\n\nbornschein, j. and bengio, y. (2015). reweighted wake-sleep.\n\nin iclr\u20192015,\n\narxiv:1406.2751 . 695\n\n728\n\n "}, {"Page_number": 744, "text": "bibliography\n\nbornschein, j., shabanian, s., fischer, a., and bengio, y. (2015). training bidirectional\n\nhelmholtz machines. technical report, arxiv:1506.03877. 695\n\nboser, b. e., guyon, i. m., and vapnik, v. n. (1992). a training algorithm for opti-\nmal margin classifiers. in colt \u201992: proceedings of the fifth annual workshop on\ncomputational learning theory, pages 144\u2013152, new york, ny, usa. acm.\n\n,18 140\n\nbottou, l. (1998). online algorithms and stochastic approximations. in d. saad, editor,\nonline learning in neural networks. cambridge university press, cambridge, uk. 296\n\nbottou,\u00a0l. (2011). from machine learning\u00a0to machine reasoning. technical report,\n\narxiv.1102.1808.\n\n,401 403\n\nbottou, l. (2015). multilayer neural networks. deep learning summer school. 443\n\nbottou, l. and bousquet, o. (2008). the tradeoffs of large scale learning. in\n\nnips\u20192008\n\n.\n\n282 295\n\n,\n\nboulanger-lewandowski, n., bengio, y., and vincent, p. (2012). modeling temporal\ndependencies in high-dimensional sequences: application to polyphonic music generation\nand transcription. in\n\nicml\u201912 688\n\n.\n\nboureau, y., ponce, j., and lecun, y. (2010). a theoretical analysis of feature pooling in\nvision algorithms. in proc. international conference on machine learning (icml\u201910).\n346\n\nboureau, y., le roux, n., bach, f., ponce, j., and lecun, y. (2011).\u00a0ask the locals:\nmulti-way local pooling for image recognition. in proc. international conference on\ncomputer vision (iccv\u201911). ieee. 346\n\nbourlard, h. and kamp, y. (1988). auto-association by multilayer perceptrons and\n\nsingular value decomposition. biological cybernetics,\n\n59\n\n, 291\u2013294.\n\n505\n\nbourlard, h. and wellekens, c. (1989). speech pattern discrimination and multi-layered\n\nperceptrons. computer speech and language,\n\n3\n\n, 1\u201319.\n\n462\n\nboyd, s. and vandenberghe, l. (2004).\n\nconvex optimization\n\n. cambridge university\n\npress, new york, ny, usa. 93\n\nbrady, m. l., raghavan, r., and slawny, j. (1989). back-propagation fails to separate\nwhere perceptrons succeed. ieee transactions on circuits and systems, 36, 665\u2013674.\n284\n\nbrakel, p., stroobandt, d., and schrauwen, b. (2013). training energy-based models for\n,676\n\ntime-series imputation. journal of machine learning research, 14, 2771\u20132797.\n700\n\nbrand, m. (2003). charting a manifold. in\n\nnips\u20192002\n\n, pages 961\u2013968. mit press.\n\n,\n163\n\n521\n\n729\n\n "}, {"Page_number": 745, "text": "bibliography\n\nbreiman, l. (1994). bagging predictors. machine learning,\n\n24\n\n(2), 123\u2013140.\n\n255\n\nbreiman, l., friedman, j. h., olshen, r. a., and stone, c. j. (1984). classification and\n\nregression trees. wadsworth international group, belmont, ca. 145\n\nbridle, j. s. (1990). alphanets: a recurrent \u2018neural\u2019 network architecture with a hidden\n\nmarkov model interpretation. speech communication,\n\n9\n\n(1), 83\u201392.\n\n185\n\nbriggman, k., denk, w., seung, s., helmstaedter, m. n., and turaga, s. c. (2009).\n360\n\nmaximin affinity learning of image segmentation. in\n\n, pages 1865\u20131873.\n\nnips\u20192009\n\nbrown, p. f., cocke, j., pietra, s. a. d., pietra, v. j. d., jelinek, f., lafferty, j. d.,\nmercer, r. l., and roossin, p. s. (1990). a statistical approach to machine translation.\ncomputational linguistics,\n\n(2), 79\u201385.\n\n21\n\n16\n\nbrown, p. f., pietra, v. j. d., desouza, p. v., lai, j. c., and mercer, r. l. (1992). class-\ncomputational linguistics 18, 467\u2013479.\n\nn\n\n,\n\nbased -gram models of natural language.\n466\n\nbryson, a. and ho, y. (1969). applied optimal control:\u00a0optimization, estimation, and\n\ncontrol. blaisdell pub. co. 225\n\nbryson, jr., a. e. and denham, w. f. (1961). a steepest-ascent method for solving\noptimum programming problems. technical report br-1303, raytheon company,\nmissle and space division. 225\n\nbucilu\u02c7a,\u00a0c., caruana, r.,\u00a0and\u00a0niculescu-mizil,\u00a0a.\u00a0(2006). model\u00a0compression.\n\nin\nproceedings of the 12th acm sigkdd international conference on knowledge discovery\nand data mining, pages 535\u2013541. acm. 451\n\nburda, y., grosse, r., and salakhutdinov, r. (2015). importance weighted autoencoders.\n\narxiv preprint arxiv:1509.00519 . 700\n\ncai, m., shi, y., and liu, j. (2013). deep maxout neural networks for speech recognition.\nin automatic speech recognition and understanding (asru), 2013 ieee workshop\non, pages 291\u2013296. ieee. 193\n\ncarreira-perpi\u00f1an, m. a. and hinton, g. e. (2005). on contrastive divergence learning.\nin r. g. cowell and z. ghahramani, editors, proceedings of the tenth international\nworkshop on artificial intelligence and statistics (aistats\u201905), pages 33\u201340. society\nfor artificial intelligence and statistics. 614\n\ncaruana, r. (1993). multitask connectionist learning. in proc. 1993 connectionist models\n\nsummer school, pages 372\u2013379. 245\n\ncauchy, a. (1847). m\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution de syst\u00e8mes d\u2019\u00e9quations simul-\n,83\n\ntan\u00e9es. in compte rendu des s\u00e9ances de l\u2019acad\u00e9mie des sciences, pages 536\u2013538.\n224\n\n730\n\n "}, {"Page_number": 746, "text": "bibliography\n\ncayton, l. (2005). algorithms for manifold learning. technical report cs2008-0923,\n\nucsd. 163\n\nchandola, v., banerjee, a., and kumar, v. (2009). anomaly detection: a survey. acm\n\ncomputing surveys (csur),\n\n41\n\n(3), 15.\n\n102\n\nchapelle, o., weston, j., and sch\u00f6lkopf, b. (2003). cluster kernels for semi-supervised\nlearning. in s. becker, s. thrun, and k. obermayer, editors, advances in neural\ninformation processing systems 15 (nips\u201902), pages 585\u2013592, cambridge, ma. mit\npress. 244\n\nchapelle, o., sch\u00f6lkopf, b., and zien, a., editors (2006). semi-supervised learning. mit\n\npress, cambridge, ma.\n\n,244 544\n\nchellapilla, k., puri, s., and simard, p. (2006). high performance convolutional neural\nin\u00a0guy lorette,\u00a0editor, tenth\u00a0international\nnetworks\u00a0for document\u00a0processing.\nworkshop on frontiers in handwriting recognition, la baule (france). universit\u00e9 de\nrennes 1, suvisoft. http://www.suvisoft.com.\n\n24 27 448\n\n,\n\n,\n\nchen, b., ting, j.-a., marlin, b. m., and de freitas, n. (2010). deep learning of invariant\nspatio-temporal features from video. nips*2010 deep learning and unsupervised\nfeature learning workshop. 361\n\nchen, s. f. and goodman, j. t. (1999). an empirical study of smoothing techniques for\n\nlanguage modeling. computer, speech and language,\n\n13\n\n(4), 359\u2013393.\n\n465 476\n\n,\n\nchen, t., du, z., sun, n., wang, j., wu, c., chen, y., and temam, o. (2014a). diannao:\na small-footprint high-throughput accelerator for ubiquitous machine-learning. in pro-\nceedings of the 19th international conference on architectural support for programming\nlanguages and operating systems, pages 269\u2013284. acm. 454\n\nchen, t., li, m., li, y., lin, m., wang, n., wang, m., xiao, t., xu, b., zhang, c.,\nand zhang, z. (2015). mxnet:\u00a0a flexible and efficient machine learning library for\nheterogeneous distributed systems. arxiv preprint arxiv:1512.01274 . 25\n\n(2014b). dadiannao: a machine-learning supercomputer. in\n\nchen, y., luo, t., liu, s., zhang, s., he, l., wang, j., li, l., chen, t., xu, z., sun, n.,\net al.\nmicroarchitecture\n(micro), 2014 47th annual ieee/acm international symposium on, pages 609\u2013622.\nieee. 454\n\nchilimbi, t., suzue, y., apacible, j., and kalyanaraman, k. (2014). project adam:\nbuilding an efficient and scalable deep learning training system. in 11th usenix\nsymposium on operating systems design and implementation (osdi\u201914). 450\n\ncho, k., raiko, t., and ilin, a. (2010). parallel tempering is efficient for learning restricted\n\nboltzmann machines. in\n\nijcnn\u20192010 606 617\n\n.\n\n,\n\n731\n\n "}, {"Page_number": 747, "text": "bibliography\n\ncho, k., raiko, t., and ilin, a. (2011). enhanced gradient and adaptive learning rate for\n\ntraining restricted boltzmann machines. in\n\nicml\u20192011\n\n, pages 105\u2013112.\n\n676\n\ncho, k., van merri\u00ebnboer, b., gulcehre, c., bougares, f., schwenk, h., and bengio, y.\n(2014a). learning phrase representations using rnn encoder-decoder for statistical\nmachine translation.\u00a0in proceedings of the empiricial methods in natural language\nprocessing (emnlp 2014).\n\n397 477 478\n\n,\n\n,\n\ncho, k., van merri\u00ebnboer, b., bahdanau, d., and bengio, y. (2014b). on the prop-\n,\narxiv e-prints\n\nerties of neural machine translation: encoder-decoder approaches.\nabs/1409.1259. 414\n\nchoromanska, a., henaff, m., mathieu, m., arous, g. b., and lecun, y. (2014).\u00a0the\n\nloss surface of multilayer networks.\n\n,285 286\n\nchorowski, j., bahdanau, d., cho, k., and bengio, y. (2014).\u00a0end-to-end continuous\nspeech recognition using attention-based recurrent nn: first results. arxiv:1412.1602.\n463\n\nchristianson, b. (1992). automatic hessians by reverse accumulation. ima journal of\n\nnumerical analysis,\n\n12\n\n(2), 135\u2013150.\n\n224\n\nchrupala, g., kadar, a., and alishahi, a. (2015). learning language through pictures.\n\narxiv 1506.03694. 414\n\nchung, j., gulcehre, c., cho, k., and bengio, y. (2014). empirical evaluation of gated\nrecurrent neural networks on sequence modeling. nips\u20192014 deep learning workshop,\narxiv 1412.3555.\n\n,414 463\n\nchung, j., g\u00fcl\u00e7ehre, \u00e7., cho, k., and bengio, y. (2015a). gated feedback recurrent\n\nneural networks. in\n\nicml\u201915 414\n\n.\n\nchung, j., kastner, k., dinh, l., goel, k., courville, a., and bengio, y. (2015b). a\n\nrecurrent latent variable model for sequential data. in\n\nnips\u20192015 700\n\n.\n\nciresan, d., meier, u., masci, j., and schmidhuber, j. (2012). multi-column deep neural\n\nnetwork for traffic sign classification. neural networks,\n\n32\n\n, 333\u2013338.\n\n23 200\n\n,\n\nciresan, d. c., meier, u., gambardella, l. m., and schmidhuber, j. (2010).\u00a0deep big\nsimple neural nets for handwritten digit recognition. neural computation, 22, 1\u201314.\n24 27 449\n\n,\n\n,\n\ncoates, a. and ng, a. y. (2011). the importance of encoding versus training with sparse\n\ncoding and vector quantization. in\n\nicml\u20192011 27 254 501\n\n.\n\n,\n\n,\n\ncoates,\u00a0a., lee,\u00a0h., and ng, a. y.\u00a0(2011). an analysis of single-layer networks in\nunsupervised feature learning. in proceedings of the thirteenth international conference\non artificial intelligence and statistics (aistats 2011).\n\n364 365 458\n\n,\n\n,\n\n732\n\n "}, {"Page_number": 748, "text": "bibliography\n\ncoates, a., huval, b., wang,\u00a0t., wu,\u00a0d., catanzaro,\u00a0b., and\u00a0andrew, n.\u00a0(2013).\ndeep learning with cots hpc systems. in s. dasgupta and d. mcallester, editors,\nproceedings of the 30th international conference on machine learning (icml-13),\nvolume 28 (3), pages 1337\u20131345. jmlr workshop and conference proceedings.\n24 27\n,\n365 450\n\n,\n\n,\n\ncohen, n., sharir, o., and shashua, a. (2015). on the expressive power of deep learning:\n\na tensor analysis. arxiv:1509.05009. 557\n\ncollobert, r. (2004). large scale machine learning. ph.d. thesis, universit\u00e9 de paris vi,\n\nlip6. 196\n\ncollobert, r. (2011). deep learning for efficient discriminative parsing. in aistats\u20192011 .\n\n101 480\n\n,\n\ncollobert, r. and weston, j. (2008a). a unified architecture for natural language processing:\n\ndeep neural networks with multitask learning. in\n\nicml\u20192008 474 480\n\n.\n\n,\n\ncollobert,\u00a0r.\u00a0and\u00a0weston, j.\u00a0(2008b). a\u00a0unified\u00a0architecture\u00a0for natural\u00a0language\n\nprocessing: deep neural networks with multitask learning. in\n\nicml\u20192008 538\n\n.\n\ncollobert, r., bengio, s., and bengio, y. (2001).\u00a0a parallel mixture of svms for very\n\nlarge scale problems. technical report idiap-rr-01-12, idiap. 453\n\ncollobert, r., bengio, s., and bengio, y. (2002). parallel mixture of svms for very large\n\nscale problems. neural computation,\n\n14\n\n(5), 1105\u20131114.\n\n453\n\ncollobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p. (2011a).\nnatural language processing (almost) from scratch. the journal of machine learning\nresearch,\n\n329 480 538 539\n\n, 2493\u20132537.\n\n12\n\n,\n\n,\n\n,\n\ncollobert, r., kavukcuoglu, k., and farabet, c. (2011b). torch7: a matlab-like environ-\n\nment for machine learning. in biglearn, nips workshop.\n\n25 210 449\n\n,\n\n,\n\ncomon, p. (1994). independent component analysis - a new concept? signal processing,\n\n36, 287\u2013314. 494\n\ncortes, c. and vapnik, v. (1995). support vector networks. machine learning, 20,\n\n273\u2013297.\n\n,18 140\n\ncouprie, c., farabet, c., najman, l., and lecun, y. (2013). indoor semantic segmentation\nusing depth information. in international conference on learning representations\n(iclr2013).\n\n,23 200\n\ncourbariaux, m., bengio, y., and david, j.-p. (2015). low precision arithmetic for deep\n\nlearning. in arxiv:1412.7024, iclr\u20192015 workshop. 455\n\ncourville, a., bergstra, j., and bengio, y. (2011). unsupervised models of images by\n\nspike-and-slab rbms. in\n\nicml\u201911 564 683\n\n.\n\n,\n\n733\n\n "}, {"Page_number": 749, "text": "bibliography\n\ncourville, a., desjardins, g., bergstra, j., and bengio, y. (2014). the spike-and-slab\nrbm and extensions to discrete and sparse data distributions. pattern analysis and\nmachine intelligence, ieee transactions on,\n\n(9), 1874\u20131887.\n\n685\n\n36\n\ncover, t. m. and thomas, j. a. (2006). elements of information theory, 2nd edition.\n\nwiley-interscience. 73\n\ncox, d. and pinto, n. (2011). beyond simple features: a large-scale feature search\napproach to unconstrained face recognition. in automatic face & gesture recognition\nand workshops (fg 2011), 2011 ieee international conference on, pages 8\u201315. ieee.\n364\n\ncram\u00e9r, h. (1946). mathematical methods of statistics. princeton university press.\n\n,135\n\n295\n\ncrick, f. h. c. and mitchison, g. (1983). the function of dream sleep. nature, 304,\n\n111\u2013114. 612\n\ncybenko, g. (1989). approximation by superpositions of a sigmoidal function. mathematics\n\nof control, signals, and systems,\n\n2\n\n, 303\u2013314.\n\n197\n\ndahl, g. e., ranzato, m., mohamed, a., and hinton, g. e. (2010). phone recognition\n\nwith the mean-covariance restricted boltzmann machine. in\n\nnips\u20192010 23\n\n.\n\ndahl, g. e., yu, d., deng, l., and acero, a. (2012). context-dependent pre-trained deep\nneural networks for large vocabulary speech recognition. ieee transactions on audio,\nspeech, and language processing,\n\n(1), 33\u201342.\n\n462\n\n20\n\ndahl, g. e., sainath, t. n., and hinton, g. e. (2013). improving deep neural networks\n\nfor lvcsr using rectified linear units and dropout. in\n\nicassp\u20192013 462\n\n.\n\ndahl, g. e., jaitly, n., and salakhutdinov, r. (2014). multi-task neural networks for\n\nqsar predictions. arxiv:1406.1231. 26\n\ndauphin,\u00a0y. and bengio,\u00a0y. (2013). stochastic ratio matching of\u00a0rbms for sparse\n\nhigh-dimensional inputs. in\n\nnips26\n\n. nips foundation.\n\n622\n\ndauphin, y., glorot, x., and bengio, y. (2011). large-scale learning of embeddings with\n\nreconstruction sampling. in\n\nicml\u20192011 474\n\n.\n\ndauphin, y., pascanu, r., gulcehre, c., cho, k., ganguli, s., and bengio, y. (2014).\nidentifying and attacking the saddle point problem in high-dimensional non-convex\noptimization. in\n\nnips\u20192014 285 286 288\n\n.\n\n,\n\n,\n\ndavis, a., rubinstein, m., wadhwa, n., mysore, g., durand, f., and freeman, w. t.\n(2014). the visual microphone: passive recovery of sound from video. acm transactions\non graphics (proc. siggraph),\n\n(4), 79:1\u201379:10.\n\n455\n\n33\n\n734\n\n "}, {"Page_number": 750, "text": "bibliography\n\ndayan, p. (1990). reinforcement comparison. in connectionist models: proceedings of\n\nthe 1990 connectionist summer school, san mateo, ca. 693\n\ndayan, p. and hinton, g. e. (1996). varieties of helmholtz machine. neural networks,\n\n9(8), 1385\u20131403. 695\n\ndayan, p., hinton, g. e., neal, r. m., and zemel, r. s. (1995). the helmholtz machine.\n\nneural computation,\n\n7\n\n(5), 889\u2013904.\n\n695\n\ndean, j., corrado, g., monga, r., chen, k., devin, m., le, q., mao, m., ranzato, m.,\nsenior, a., tucker, p., yang, k., and ng, a. y. (2012). large scale distributed deep\nnetworks. in\n\nnips\u20192012 25 450\n\n.\n\n,\n\ndean, t. and kanazawa, k. (1989). a model for reasoning about persistence and causation.\n\ncomputational intelligence,\n\n5\n\n(3), 142\u2013150.\n\n664\n\ndeerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., and harshman, r. (1990).\nindexing by latent semantic analysis. journal of the american society for information\nscience,\n\n(6), 391\u2013407.\n\n479 485\n\n41\n\n,\n\ndelalleau, o. and bengio, y. (2011). shallow vs. deep sum-product networks. in\n\nnips\n\n.\n\n19 557\n\n,\n\ndeng, j., dong, w., socher, r., li, l.-j., li, k., and fei-fei, l. (2009). imagenet:\u00a0a\n\nlarge-scale hierarchical image database. in\n\ncvpr09 21\n\n.\n\ndeng, j., berg, a. c., li, k., and fei-fei, l. (2010a). what does classifying more than\n10,000 image categories tell us? in proceedings of the 11th european conference on\ncomputer vision: part v , eccv\u201910, pages 71\u201384, berlin, heidelberg. springer-verlag.\n21\n\ndeng, l. and yu, d. (2014). deep learning \u2013 methods and applications. foundations and\n\ntrends in signal processing. 463\n\ndeng, l., seltzer, m., yu, d., acero, a., mohamed, a., and hinton, g. (2010b). binary\ncoding of speech spectrograms using a deep auto-encoder. in interspeech 2010 , makuhari,\nchiba, japan. 23\n\ndenil, m., bazzani, l., larochelle, h., and de freitas, n. (2012). learning where to attend\nwith deep architectures for image tracking. neural computation, 24(8), 2151\u20132184. 368\n\ndenton, e., chintala, s., szlam, a., and fergus, r. (2015). deep generative image models\n\nusing a laplacian pyramid of adversarial networks.\n\nnips 703 704 720\n\n.\n\n,\n\n,\n\ndesjardins, g. and bengio, y. (2008). empirical evaluation of convolutional rbms for\nvision.\u00a0technical report 1327, d\u00e9partement d\u2019informatique et de recherche op\u00e9ra-\ntionnelle, universit\u00e9 de montr\u00e9al. 685\n\n735\n\n "}, {"Page_number": 751, "text": "bibliography\n\ndesjardins,\u00a0g.,\u00a0courville,\u00a0a. c.,\u00a0bengio,\u00a0y.,\u00a0vincent,\u00a0p.,\u00a0and delalleau,\u00a0o. (2010).\ntempered markov chain monte carlo for training of restricted boltzmann machines. in\ninternational conference on artificial intelligence and statistics, pages 145\u2013152.\n,606\n617\n\ndesjardins, g., courville, a., and bengio, y. (2011). on tracking the partition function.\n\nin\n\nnips\u20192011 633\n\n.\n\ndesjardins, g., simonyan, k., pascanu, r.,\n\net al.\n\n(2015).\u00a0natural neural networks.\u00a0in\n\nadvances in neural information processing systems, pages 2062\u20132070. 321\n\ndevlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j. (2014). fast\nand robust neural network joint models for statistical machine translation.\u00a0in proc.\nacl\u20192014 . 476\n\ndevroye, l. (2013). non-uniform random variate generation. springerlink : b\u00fccher.\n\nspringer new york. 696\n\ndicarlo, j. j. (2013). mechanisms underlying visual object recognition: humans vs.\n\nneurons vs. machines. nips tutorial.\n\n,26 367\n\ndinh, l., krueger, d., and bengio, y. (2014). nice: non-linear independent components\n\nestimation. arxiv:1410.8516. 496\n\ndonahue, j., hendricks, l. a., guadarrama, s., rohrbach, m., venugopalan, s., saenko,\nk., and darrell, t. (2014). long-term recurrent convolutional networks for visual\nrecognition and description. arxiv:1411.4389. 102\n\ndonoho, d. l. and grimes, c. (2003). hessian eigenmaps: new locally linear embedding\ntechniques for high-dimensional\u00a0data. technical report 2003-08,\u00a0dept. statistics,\nstanford university.\n\n,163 522\n\ndosovitskiy, a., springenberg, j. t., and brox, t. (2015). learning to generate chairs with\nconvolutional neural networks. in proceedings of the ieee conference on computer\nvision and pattern recognition, pages 1538\u20131546.\n\n697 706 707\n\n,\n\n,\n\ndoya, k. (1993). bifurcations of recurrent neural networks in gradient descent learning.\n\nieee transactions on neural networks,\n\n1\n\n, 75\u201380.\n\n403 406\n\n,\n\ndreyfus,\u00a0s.\u00a0e. (1962). the numerical solution\u00a0of variational\u00a0problems. journal\u00a0of\n\nmathematical analysis and applications,\n\n5(1)\n\n, 30\u201345.\n\n225\n\ndreyfus, s. e. (1973). the computational solution of optimal control problems with time\n\nlag. ieee transactions on automatic control,\n\n18(4)\n\n, 383\u2013385.\n\n225\n\ndrucker, h. and lecun, y. (1992). improving generalisation performance using double\n\nback-propagation. ieee transactions on neural networks,\n\n3\n\n(6), 991\u2013997.\n\n270\n\n736\n\n "}, {"Page_number": 752, "text": "bibliography\n\nduchi, j., hazan, e., and singer, y. (2011).\u00a0adaptive subgradient methods for online\n\nlearning and stochastic optimization. journal of machine learning research. 307\n\ndudik, m., langford, j., and li, l. (2011). doubly robust policy evaluation and learning.\nin proceedings of the 28th international conference on machine learning, icml \u201911.\n485\n\ndugas, c., bengio, y., b\u00e9lisle, f., and nadeau, c. (2001).\u00a0incorporating second-order\nfunctional knowledge for better option pricing. in t. leen, t. dietterich, and v. tresp,\neditors,\u00a0advances in neural\u00a0information processing systems\u00a013 (nips\u201900),\u00a0pages\n472\u2013478. mit press.\n\n,68 196\n\ndziugaite, g. k., roy, d. m., and ghahramani, z. (2015). training generative neural net-\nworks via maximum mean discrepancy optimization. arxiv preprint arxiv:1505.03906 .\n705\n\nel hihi, s. and bengio, y. (1996). hierarchical recurrent neural networks for long-term\n\ndependencies. in\n\nnips\u20191995 401 410\n\n.\n\n,\n\nelkahky, a. m., song, y., and he, x. (2015). a multi-view deep learning approach for\ncross domain user modeling in recommendation systems.\u00a0in proceedings of the 24th\ninternational conference on world wide web, pages 278\u2013288. 483\n\nelman, j. l. (1993). learning and development in neural networks: the importance of\n\nstarting small. cognition,\n\n48\n\n, 781\u2013799.\n\n329\n\nerhan, d., manzagol, p.-a., bengio, y., bengio, s., and vincent, p. (2009). the difficulty\nof training deep architectures and the effect of unsupervised pre-training. in proceedings\nof aistats\u20192009 . 200\n\nerhan, d., bengio, y., courville, a., manzagol, p., vincent, p., and bengio, s. (2010).\nwhy does unsupervised pre-training help deep learning? j. machine learning res.\n532 536 537\n\n,\n\n,\n\nfahlman, s. e., hinton, g. e., and sejnowski, t. j. (1983). massively parallel architectures\nin proceedings\u00a0of\u00a0the national\n\nfor\u00a0ai: netl, thistle,\u00a0and\u00a0boltzmann machines.\nconference on artificial intelligence aaai-83 .\n\n,573 656\n\nfang, h., gupta, s., iandola, f., srivastava, r., deng, l., doll\u00e1r, p., gao, j., he, x.,\nmitchell, m., platt, j. c., zitnick, c. l., and zweig, g. (2015). from captions to visual\nconcepts and back. arxiv:1411.4952. 102\n\nfarabet, c., lecun, y., kavukcuoglu, k., culurciello, e., martini, b., akselrod, p., and\ntalay, s. (2011). large-scale fpga-based convolutional networks. in r. bekkerman,\nm. bilenko, and j. langford,\u00a0editors, scaling up machine learning: parallel and\ndistributed approaches. cambridge university press. 526\n\n737\n\n "}, {"Page_number": 753, "text": "bibliography\n\nfarabet, c., couprie, c., najman, l., and lecun, y. (2013). learning hierarchical features\nfor scene labeling. ieee transactions on pattern analysis and machine intelligence,\n35(8), 1915\u20131929.\n\n23 200 360\n\n,\n\n,\n\nfei-fei, l., fergus, r., and perona, p. (2006). one-shot learning of object categories.\nieee transactions on pattern analysis and machine intelligence, 28(4), 594\u2013611. 541\n\nfinn, c., tan, x. y., duan, y., darrell, t., levine, s., and abbeel, p. (2015). learning\nvisual feature spaces for robotic manipulation with deep spatial autoencoders. arxiv\npreprint arxiv:1509.06113 . 25\n\nfisher, r. a. (1936). the use of multiple measurements in taxonomic problems. annals\n\nof eugenics,\n\n7\n\n, 179\u2013188.\n\n21 105\n\n,\n\nf\u00f6ldi\u00e1k, p. (1989). adaptive network for optimal linear feature extraction. in international\njoint conference on neural networks (ijcnn), volume 1, pages 401\u2013405, washington\n1989. ieee, new york. 497\n\nfranzius, m., sprekeler, h., and wiskott, l. (2007). slowness and sparseness lead to place,\n\nhead-direction, and spatial-view cells. 498\n\nfranzius, m., wilbert, n., and wiskott, l. (2008). invariant object recognition with slow\nfeature analysis. in artificial neural networks-icann 2008 , pages 961\u2013970. springer.\n499\n\nfrasconi, p., gori, m., and sperduti, a. (1997). on the efficient classification of data\n,401\n\nstructures by neural networks. in proc. int. joint conf. on artificial intelligence.\n403\n\nfrasconi,\u00a0p.,\u00a0gori,\u00a0m.,\u00a0and sperduti,\u00a0a. (1998). a general framework for adaptive\nprocessing of data structures. ieee transactions on neural networks, 9(5), 768\u2013786.\n401 403\n\n,\n\nfreund, y. and schapire, r. e. (1996a). experiments with a new boosting algorithm. in\nmachine learning:\u00a0proceedings of thirteenth international conference, pages 148\u2013156,\nusa. acm. 257\n\nfreund, y. and schapire, r. e. (1996b). game theory, on-line prediction and boosting. in\nproceedings of the ninth annual conference on computational learning theory, pages\n325\u2013332. 257\n\nfrey, b. j. (1998). graphical models for machine learning and digital communication.\n\nmit press.\n\n,707 708\n\nfrey, b. j., hinton, g. e., and dayan, p. (1996). does the wake-sleep algorithm learn good\ndensity estimators? in d. touretzky, m. mozer, and m. hasselmo, editors, advances\nin neural information processing systems 8 (nips\u201995), pages 661\u2013670. mit press,\ncambridge, ma. 654\n\n738\n\n "}, {"Page_number": 754, "text": "bibliography\n\nfrobenius, g. (1908). \u00fcber matrizen aus positiven elementen, s. b. preuss. akad. wiss.\n\nberlin, germany. 600\n\nfukushima, k. (1975). cognitron: a self-organizing multilayered neural network. biological\n\ncybernetics,\n\n20\n\n, 121\u2013136.\n\n16 226 531\n\n,\n\n,\n\nfukushima,\u00a0k. (1980). neocognitron: a self-organizing neural network model for a\nmechanism of pattern recognition unaffected by shift in position. biological cybernetics,\n36, 193\u2013202.\n\n16 24 27 226 368\n\n,\n\n,\n\n,\n\n,\n\ngal, y. and ghahramani, z. (2015). bayesian convolutional neural networks with bernoulli\n\napproximate variational inference. arxiv preprint arxiv:1506.02158 . 263\n\ngallinari, p., lecun, y., thiria, s., and fogelman-soulie, f. (1987). memoires associatives\n\ndistribuees. in proceedings of cognitiva 87 , paris, la villette. 518\n\ngarcia-duran, a., bordes, a., usunier, n., and grandvalet, y. (2015). combining two\nand three-way embeddings models for link prediction in knowledge bases. arxiv preprint\narxiv:1506.00999 . 487\n\ngarofolo, j. s., lamel, l. f., fisher, w. m., fiscus, j. g., and pallett, d. s. (1993).\ndarpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1.\nnasa sti/recon technical report n ,\n\n, 27403.\n\n462\n\n93\n\ngarson, j. (1900). the metric system of identification of criminals, as used in great\nbritain and ireland. the journal of the anthropological institute of great britain and\nireland, (2), 177\u2013227. 21\n\ngers, f. a., schmidhuber, j., and cummins, f. (2000).\u00a0learning to forget: continual\n\nprediction with lstm. neural computation,\n\n12\n\n(10), 2451\u20132471.\n\n411 415\n\n,\n\nghahramani, z. and hinton, g. e. (1996). the em algorithm for mixtures of factor\nanalyzers. technical report crg-tr-96-1, dpt. of comp. sci., univ. of toronto. 492\n\ngillick, d., brunk, c., vinyals, o., and subramanya, a. (2015). multilingual language\n\nprocessing from bytes. arxiv preprint arxiv:1512.00103 . 480\n\ngirshick, r., donahue, j., darrell, t., and malik, j. (2015). region-based convolutional\n\nnetworks for accurate object detection and segmentation. 429\n\ngiudice, m. d., manera, v., and keysers, c. (2009). programmed to learn? the ontogeny\n\nof mirror neurons.\n\ndev. sci. 12\n\n,\n\n(2), 350\u2013\u2013363.\n\n658\n\nglorot, x. and bengio, y. (2010). understanding the difficulty of training deep feedforward\n\nneural networks. in aistats\u20192010 . 303\n\nglorot, x., bordes, a., and bengio, y. (2011a). deep sparse rectifier neural networks. in\n\naistats\u20192011 .\n\n16 173 196 226\n\n,\n\n,\n\n,\n\n739\n\n "}, {"Page_number": 755, "text": "bibliography\n\nglorot,\u00a0x., bordes,\u00a0a., and bengio,\u00a0y. (2011b). domain adaptation for large-scale\n\nsentiment classification: a deep learning approach. in\n\nicml\u20192011 510 540\n\n.\n\n,\n\ngoldberger, j., roweis, s., hinton, g. e., and salakhutdinov, r. (2005). neighbourhood\ncomponents analysis. in l. saul, y. weiss, and l. bottou, editors, advances in neural\ninformation processing systems 17 (nips\u201904). mit press. 115\n\ngong, s., mckenna, s., and psarrou, a. (2000). dynamic vision: from images to face\n\nrecognition. imperial college press.\n\n,164 522\n\ngoodfellow, i., le, q., saxe, a.,\u00a0and ng, a. (2009). measuring invariances in deep\n\nnetworks. in\n\nnips\u20192009\n\n, pages 646\u2013654.\n\n254\n\ngoodfellow, i., koenig, n., muja, m., pantofaru, c., sorokin, a., and takayama, l. (2010).\nhelp me help you: interfaces for personal robots. in proc. of human robot interaction\n(hri), osaka, japan. acm press, acm press. 100\n\ngoodfellow, i. j. (2010). technical report: multidimensional, downsampled convolution\n\nfor autoencoders. technical report, universit\u00e9 de montr\u00e9al. 358\n\ngoodfellow, i. j. (2014). on distinguishability criteria for estimating generative models.\n625 702\n,\n\nin international conference on learning representations, workshops track.\n703\n\n,\n\ngoodfellow, i. j., courville, a., and bengio, y. (2011). spike-and-slab sparse coding\nin nips workshop on challenges in learning\n\nfor unsupervised feature discovery.\nhierarchical models.\n\n,535 541\n\ngoodfellow, i. j., warde-farley, d., mirza, m., courville, a., and bengio, y. (2013a).\n, pages 1319\u2013\n\nmaxout networks. in s. dasgupta and d. mcallester, editors,\n1327.\n\n192 263 345 366 458\n\nicml\u201913\n\n,\n\n,\n\n,\n\n,\n\ngoodfellow, i. j., mirza, m., courville, a., and bengio, y. (2013b). multi-prediction deep\n100 620 673 674 675 676 677\n,\n\n. nips foundation.\n\nnips26\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nboltzmann machines. in\n700\n\ngoodfellow, i. j., warde-farley, d., lamblin, p., dumoulin, v., mirza, m., pascanu, r.,\nbergstra, j., bastien, f., and bengio, y. (2013c). pylearn2: a machine learning research\nlibrary. arxiv preprint arxiv:1308.4214 .\n\n,25 449\n\ngoodfellow, i. j., courville, a., and bengio, y. (2013d). scaling up spike-and-slab models\nfor unsupervised feature learning. ieee transactions on pattern analysis and machine\nintelligence,\n\n500 501 502 652 685\n\n(8), 1902\u20131914.\n\n35\n\n,\n\n,\n\n,\n\n,\n\ngoodfellow, i. j., mirza, m., xiao, d., courville, a., and bengio, y. (2014a). an empirical\n.\niclr\u20192014\n\ninvestigation of catastrophic forgeting in gradient-based neural networks. in\n193\n\n740\n\n "}, {"Page_number": 756, "text": "bibliography\n\ngoodfellow, i. j., shlens, j., and szegedy, c. (2014b). explaining and harnessing adver-\n\nsarial examples.\n\ncorr abs/1412.6572 267 268 270 558 559\n\n,\n\n.\n\n,\n\n,\n\n,\n\n,\n\ngoodfellow, i. j., pouget-abadie, j., mirza, m., xu, b., warde-farley, d., ozair, s.,\n.\nnips\u20192014\n\ncourville, a., and bengio, y. (2014c). generative adversarial networks. in\n547 691 702 703 706\n\n,\n\n,\n\n,\n\n,\n\ngoodfellow, i. j., bulatov, y., ibarz, j., arnoud, s., and shet, v. (2014d). multi-digit\nnumber recognition from street view imagery using deep convolutional neural networks.\nin international conference on learning representations.\n25 101 200 201 202 391\n,\n425 452\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\ngoodfellow, i. j., vinyals, o., and saxe, a. m. (2015). qualitatively characterizing neural\nnetwork optimization problems. in international conference on learning representa-\ntions.\n\n285 286 287 291\n\n,\n\n,\n\n,\n\ngoodman, j. (2001). classes\u00a0for fast maximum entropy training.\n\nin\u00a0international\n\nconference on acoustics, speech and signal processing (icassp), utah. 470\n\ngori, m. and tesi, a. (1992). on the problem of local minima in backpropagation. ieee\ntransactions on pattern analysis and machine intelligence, pami-14(1), 76\u201386. 284\n\ngosset, w. s. (1908). the probable error of a mean.\n\nbiometrika 6(1), 1\u201325. originally\n\n,\n\npublished under the pseudonym \u201cstudent\u201d. 21\n\ngouws, s., bengio, y., and corrado, g. (2014). bilbowa: fast bilingual distributed\n,479 542\n\nrepresentations without word alignments. technical report, arxiv:1410.2455.\n\ngraf, h. p. and jackel, l. d. (1989). analog electronic neural network circuits. circuits\n\nand devices magazine, ieee,\n\n5\n\n(4), 44\u201349.\n\n454\n\ngraves, a. (2011). practical variational inference for neural networks. in\n\nnips\u20192011 242\n\n.\n\ngraves, a. (2012). supervised sequence labelling with recurrent neural networks. studies\n\nin computational intelligence. springer.\n\n375 396 414 463\n\n,\n\n,\n\n,\n\ngraves, a. (2013). generating sequences with recurrent neural networks. technical report,\n\narxiv:1308.0850.\n\n189 411 418 422\n\n,\n\n,\n\n,\n\ngraves, a. and jaitly, n. (2014). towards end-to-end speech recognition with recurrent\n\nneural networks. in\n\nicml\u20192014 411\n\n.\n\ngraves, a. and schmidhuber, j. (2005). framewise phoneme classification with bidirec-\ntional lstm and other neural network architectures. neural networks, 18(5), 602\u2013610.\n396\n\ngraves, a. and schmidhuber, j. (2009). offline handwriting recognition with multidi-\nmensional recurrent neural networks. in d. koller, d. schuurmans, y. bengio, and\nl. bottou, editors,\n\n, pages 545\u2013552.\n\nnips\u20192008\n\n396\n\n741\n\n "}, {"Page_number": 757, "text": "bibliography\n\ngraves, a., fern\u00e1ndez, s., gomez, f., and schmidhuber, j. (2006). connectionist temporal\nclassification: labelling unsegmented sequence data with recurrent neural networks. in\nicml\u20192006 , pages 369\u2013376, pittsburgh, usa. 463\n\ngraves, a., liwicki, m., bunke, h., schmidhuber, j., and fern\u00e1ndez, s. (2008). uncon-\nstrained on-line handwriting recognition with recurrent neural networks. in j. platt,\nd. koller, y. singer, and s. roweis, editors,\n\n, pages 577\u2013584.\n\nnips\u20192007\n\n396\n\ngraves, a., liwicki, m., fern\u00e1ndez, s., bertolami, r., bunke, h., and schmidhuber, j.\n(2009). a novel connectionist system for unconstrained handwriting recognition. pattern\nanalysis and machine intelligence, ieee transactions on,\n\n(5), 855\u2013868.\n\n411\n\n31\n\ngraves, a., mohamed, a., and hinton, g. (2013). speech recognition with deep recurrent\n396 399 401 411 413 414 463\n\nneural networks. in\n\n, pages 6645\u20136649.\n\nicassp\u20192013\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\ngraves, a., wayne, g., and danihelka,\n\ni. (2014a).\n\nneural turing machines.\n\narxiv:1410.5401. 25\n\ngraves, a., wayne, g., and danihelka, i. (2014b). neural turing machines. arxiv preprint\n\narxiv:1410.5401 .\n\n,419 421\n\ngrefenstette, e., hermann, k. m., suleyman, m., and blunsom, p. (2015). learning to\n\ntransduce with unbounded memory. in\n\nnips\u20192015 421\n\n.\n\ngreff, k., srivastava, r. k., koutn\u00edk, j., steunebrink, b. r., and schmidhuber, j. (2015).\n\nlstm: a search space odyssey. arxiv preprint arxiv:1503.04069 . 415\n\ngregor, k. and lecun, y. (2010a). emergence of complex-like cells in a temporal product\n\nnetwork with local receptive fields. technical report, arxiv:1006.0448. 353\n\ngregor, k. and lecun, y. (2010b). learning fast approximations of sparse coding. in\nl. bottou and m. littman, editors, proceedings of the twenty-seventh international\nconference on machine learning (icml-10). acm. 655\n\ngregor,\u00a0k., danihelka,\u00a0i., mnih, a.,\u00a0blundell, c., and\u00a0wierstra,\u00a0d.\u00a0(2014). deep\nautoregressive networks. in international conference on machine learning (icml\u20192014).\n695\n\ngregor, k., danihelka, i., graves, a., and wierstra, d. (2015). draw: a recurrent neural\n\nnetwork for image generation. arxiv preprint arxiv:1502.04623 . 700\n\ngretton, a., borgwardt, k. m., rasch, m. j., sch\u00f6lkopf, b., and smola, a. (2012). a\nkernel two-sample test. the journal of machine learning research, 13(1), 723\u2013773.\n705\n\ng\u00fcl\u00e7ehre, \u00e7. and bengio, y. (2013). knowledge matters: importance of prior information\nfor optimization. in international conference on learning representations (iclr\u20192013).\n25\n\n742\n\n "}, {"Page_number": 758, "text": "bibliography\n\nguo, h. and gelfand, s. b. (1992). classification trees with neural network feature\n\nextraction. neural networks, ieee transactions on,\n\n3\n\n(6), 923\u2013933.\n\n453\n\ngupta, s., agrawal, a., gopalakrishnan, k., and narayanan, p. (2015). deep learning\n\nwith limited numerical precision.\n\ncorr abs/1502.02551 455\n\n,\n\n.\n\ngutmann, m. and hyvarinen, a. (2010). noise-contrastive estimation:\u00a0a new estima-\ntion principle for unnormalized statistical models.\u00a0in proceedings of the thirteenth\ninternational conference on artificial intelligence and statistics (aistats\u201910). 623\n\nhadsell,\u00a0r.,\u00a0sermanet,\u00a0p.,\u00a0ben,\u00a0j., erkan, a.,\u00a0han,\u00a0j.,\u00a0muller, u.,\u00a0and lecun,\u00a0y.\n(2007). online learning for offroad robots: spatial label propagation to learn long-range\ntraversability. in proceedings of robotics: science and systems, atlanta, ga, usa. 456\n\nhajnal, a., maass, w., pudlak, p., szegedy, m., and turan, g. (1993). threshold circuits\n\nof bounded depth.\n\nj. comput. system. sci. 46\n\n,\n\n, 129\u2013154.\n\n198\n\nh\u00e5stad, j. (1986). almost optimal lower bounds for small depth circuits. in proceedings\nof the 18th annual acm symposium on theory of computing, pages 6\u201320, berkeley,\ncalifornia. acm press. 198\n\nh\u00e5stad, j. and goldmann, m. (1991). on the power of small-depth threshold circuits.\n\ncomputational complexity,\n\n1\n\n, 113\u2013129.\n\n198\n\nhastie, t., tibshirani, r., and friedman, j. (2001). the elements of statistical learning:\ndata mining, inference and prediction.\u00a0springer series in statistics. springer verlag.\n145\n\nhe, k., zhang, x., ren, s., and sun, j. (2015). delving deep into rectifiers: surpassing\nhuman-level performance on imagenet classification. arxiv preprint arxiv:1502.01852 .\n28 192\n\n,\n\nhebb, d. o. (1949). the organization of behavior. wiley, new york.\n\n14 17 658\n\n,\n\n,\n\nhenaff, m., jarrett, k., kavukcuoglu, k., and lecun, y. (2011). unsupervised learning\n\nof sparse features for scalable audio classification. in\n\nismir\u201911 526\n\n.\n\nhenderson, j. (2003). inducing history representations for broad coverage statistical\n\nparsing. in hlt-naacl, pages 103\u2013110. 480\n\nhenderson, j. (2004). discriminative training of a neural network statistical parser. in\nproceedings of the 42nd annual meeting on association for computational linguistics,\npage 95. 480\n\nhenniges, m., puertas, g., bornschein, j., eggert, j., and l\u00fccke, j. (2010). binary sparse\ncoding. in latent variable analysis and signal separation, pages 450\u2013457. springer.\n643\n\n743\n\n "}, {"Page_number": 759, "text": "bibliography\n\nherault, j. and ans, b. (1984). circuits neuronaux \u00e0 synapses modifiables: d\u00e9codage de\nmessages composites par apprentissage non supervis\u00e9. comptes rendus de l\u2019acad\u00e9mie\ndes sciences,\n\n299(iii-13)\n\n, 525\u2013\u2013528.\n\n494\n\nhinton, g. (2012). neural networks for machine learning. coursera, video lectures. 307\n\nhinton, g., deng, l., dahl, g. e., mohamed, a., jaitly, n., senior, a., vanhoucke, v.,\nnguyen, p., sainath, t., and kingsbury, b. (2012a). deep neural networks for acoustic\nmodeling in speech recognition. ieee signal processing magazine, 29(6), 82\u201397.\n,23\n463\n\nhinton, g., vinyals, o., and dean, j. (2015). distilling the knowledge in a neural network.\n\narxiv preprint arxiv:1503.02531 . 451\n\nhinton, g. e. (1989). connectionist learning procedures. artificial intelligence, 40,\n\n185\u2013234. 497\n\nhinton, g. e. (1990). mapping part-whole hierarchies into connectionist networks. artificial\n\nintelligence,\n\n46\n\n(1), 47\u201375.\n\n421\n\nhinton, g. e. (1999). products of experts. in\n\nicann\u20191999 573\n\n.\n\nhinton, g. e. (2000). training products of experts by minimizing contrastive divergence.\n,613\n\ntechnical report gcnu tr 2000-004, gatsby unit, university college london.\n678\n\nhinton, g. e. (2006). to recognize shapes, first learn to generate images. technical report\n\nutml tr 2006-003, university of toronto.\n\n,531 598\n\nhinton, g. e. (2007a). how to do backpropagation in a brain.\n\ninvited talk at the\n\nnips\u20192007 deep learning workshop. 658\n\nhinton, g. e. (2007b).\u00a0learning multiple layers of representation.\u00a0trends in cognitive\n\nsciences,\n\n11\n\n(10), 428\u2013434.\n\n662\n\nhinton,\u00a0g. e. (2010). a practical guide to training restricted boltzmann machines.\ntechnical report utml tr 2010-003, department of computer science, university of\ntoronto. 613\n\nhinton, g. e. and ghahramani, z. (1997). generative models for discovering sparse\ndistributed representations. philosophical transactions of the royal society of london.\n146\n\nhinton, g. e. and mcclelland, j. l. (1988). learning representations by recirculation. in\n\nnips\u20191987 , pages 358\u2013366. 505\n\nhinton, g. e. and roweis, s. (2003). stochastic neighbor embedding. in\n\nnips\u20192002 522\n\n.\n\n744\n\n "}, {"Page_number": 760, "text": "bibliography\n\nhinton, g. e. and salakhutdinov, r. (2006). reducing the dimensionality of data with\n\nneural networks. science,\n\n313\n\n(5786), 504\u2013507.\n\n512 527 531 532 537\n\n,\n\n,\n\n,\n\n,\n\nhinton, g. e. and sejnowski, t. j. (1986). learning and relearning in boltzmann machines.\nin d. e. rumelhart and j. l. mcclelland, editors, parallel distributed processing,\nvolume 1, chapter 7, pages 282\u2013317. mit press, cambridge.\n\n,573 656\n\nhinton, g. e. and sejnowski, t. j. (1999). unsupervised learning: foundations of neural\n\ncomputation. mit press. 544\n\nhinton, g. e. and shallice, t. (1991). lesioning an attractor network: investigations of\n\nacquired dyslexia. psychological review,\n\n98\n\n(1), 74.\n\n13\n\nhinton, g. e. and zemel, r. s. (1994). autoencoders, minimum description length, and\n\nhelmholtz free energy. in\n\nnips\u20191993 505\n\n.\n\nhinton, g. e., sejnowski, t. j., and ackley, d. h. (1984). boltzmann machines: constraint\nsatisfaction networks that learn. technical report tr-cmu-cs-84-119, carnegie-mellon\nuniversity, dept. of computer science.\n\n,573 656\n\nhinton, g. e., mcclelland, j., and rumelhart, d. (1986).\u00a0distributed representations.\nin d. e. rumelhart and j. l. mcclelland, editors, parallel distributed processing:\nexplorations in the microstructure of cognition, volume 1, pages 77\u2013109. mit press,\ncambridge.\n\n17 225 529\n\n,\n\n,\n\nhinton, g. e., revow, m., and dayan, p. (1995a). recognizing handwritten digits using\nmixtures of linear models. in g. tesauro, d. touretzky, and t. leen, editors, advances\nin neural information processing systems 7 (nips\u201994), pages 1015\u20131022. mit press,\ncambridge, ma. 492\n\nhinton, g. e., dayan, p., frey, b. j., and neal, r. m. (1995b). the wake-sleep algorithm\n\nfor unsupervised neural networks. science,\n\n268\n\n, 1558\u20131161.\n\n507 654\n\n,\n\nhinton, g. e., dayan, p., and revow, m. (1997). modelling the manifolds of images of\n\nhandwritten digits. ieee transactions on neural networks,\n\n8\n\n, 65\u201374.\n\n502\n\nhinton, g. e., welling, m., teh, y. w., and osindero, s. (2001). a new view of ica. in\nproceedings of 3rd international conference on independent component analysis and\nblind signal separation (ica\u201901), pages 746\u2013751, san diego, ca. 494\n\nhinton, g. e., osindero, s., and teh, y. (2006). a fast learning algorithm for deep belief\n\nnets. neural computation,\n\n18\n\n, 1527\u20131554.\n\n14 19 27 142 531 532 662 663\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nhinton, g. e.,\u00a0deng, l., yu,\u00a0d., dahl, g. e., mohamed,\u00a0a., jaitly,\u00a0n., senior, a.,\nvanhoucke, v., nguyen, p., sainath, t. n., and kingsbury, b. (2012b). deep neural\nnetworks for acoustic modeling in speech recognition: the shared views of four research\ngroups. ieee signal process. mag.,\n\n(6), 82\u201397.\n\n101\n\n29\n\n745\n\n "}, {"Page_number": 761, "text": "bibliography\n\nhinton, g. e., srivastava, n., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2012c).\nimproving neural networks by preventing co-adaptation of feature detectors. technical\nreport, arxiv:1207.0580.\n\n239 261 266\n\n,\n\n,\n\nhinton, g. e., vinyals, o., and dean, j. (2014). dark knowledge.\u00a0invited talk at the\n\nbaylearn bay area machine learning symposium. 451\n\nhochreiter, s. (1991). untersuchungen zu dynamischen neuronalen netzen. diploma\n\nthesis, t.u. m\u00fcnich.\n\n18 403 405\n\n,\n\n,\n\nhochreiter, s. and schmidhuber, j. (1995).\u00a0simplifying neural nets by discovering flat\nminima. in advances in neural information processing systems 7 , pages 529\u2013536. mit\npress. 243\n\nhochreiter, s. and schmidhuber, j. (1997). long short-term memory. neural computation,\n\n9(8), 1735\u20131780.\n\n18 411 414\n\n,\n\n,\n\nhochreiter, s., informatik, f. f., bengio, y., frasconi, p., and schmidhuber, j. (2000).\ngradient flow in recurrent nets: the difficulty of learning long-term dependencies. in\nj. kolen and s. kremer, editors, field guide to dynamical recurrent networks. ieee\npress. 414\n\nholi, j. l. and hwang, j.-n. (1993). finite precision error analysis of neural network\n\nhardware implementations. computers, ieee transactions on,\n\n42\n\n(3), 281\u2013290.\n\n454\n\nholt, j. l. and baker, t. e. (1991).\u00a0back propagation simulations using limited preci-\nsion calculations. in neural networks, 1991., ijcnn-91-seattle international joint\nconference on, volume 2, pages 121\u2013126. ieee. 454\n\nhornik, k., stinchcombe, m., and white, h. (1989). multilayer feedforward networks are\n\nuniversal approximators. neural networks,\n\n2\n\n, 359\u2013366.\n\n197\n\nhornik, k., stinchcombe, m., and white, h. (1990). universal approximation of an\nunknown mapping and its derivatives using multilayer feedforward networks. neural\nnetworks,\n\n(5), 551\u2013560.\n\n197\n\n3\n\nhsu, f.-h. (2002). behind deep blue: building the computer that defeated the world\n\nchess champion. princeton university press, princeton, nj, usa. 2\n\nhuang, f. and ogata, y. (2002). generalized pseudo-likelihood estimates for markov\nrandom fields on lattice. annals of the institute of statistical mathematics, 54(1), 1\u201318.\n619\n\nhuang, p.-s., he, x., gao, j., deng, l., acero, a., and heck, l. (2013). learning deep\nstructured semantic models for web search using clickthrough data. in proceedings of\nthe 22nd acm international conference on conference on information & knowledge\nmanagement, pages 2333\u20132338. acm. 483\n\n746\n\n "}, {"Page_number": 762, "text": "bibliography\n\nhubel, d. and wiesel, t. (1968). receptive fields and functional architecture of monkey\n\nstriate cortex. journal of physiology (london),\n\n195\n\n, 215\u2013243.\n\n365\n\nhubel, d. h. and wiesel, t. n. (1959). receptive fields of single neurons in the cat\u2019s\n\nstriate cortex. journal of physiology,\n\n148\n\n, 574\u2013591.\n\n365\n\nhubel, d. h. and wiesel,\u00a0t. n. (1962). receptive fields, binocular interaction, and\nfunctional architecture in the cat\u2019s visual cortex. journal of physiology (london), 160,\n106\u2013154. 365\n\nhuszar, f. (2015). how (not) to train your generative model: schedule sampling, likelihood,\n\nadversary?\n\narxiv:1511.05101 699\n\n.\n\nhutter, f., hoos, h., and leyton-brown, k. (2011).\u00a0sequential model-based optimization\n. extended version as ubc tech report\n\nlion-5\n\nfor general algorithm configuration. in\ntr-2010-10. 439\n\nhyotyniemi, h. (1996). turing machines are recurrent neural networks. in step\u201996 , pages\n\n13\u201324. 380\n\nhyv\u00e4rinen, a. (1999).\u00a0survey on independent component analysis.\u00a0neural computing\n\nsurveys,\n\n2\n\n, 94\u2013128.\n\n494\n\nhyv\u00e4rinen, a. (2005). estimation of non-normalized statistical models using score matching.\n\njournal of machine learning research,\n\n6\n\n, 695\u2013709.\n\n516 620\n\n,\n\nhyv\u00e4rinen, a. (2007a). connections between score matching, contrastive divergence,\nand pseudolikelihood for continuous-valued variables. ieee transactions on neural\nnetworks,\n\n, 1529\u20131531.\n\n621\n\n18\n\nhyv\u00e4rinen, a. (2007b). some extensions of score matching. computational statistics and\n\ndata analysis,\n\n51\n\n, 2499\u20132512.\n\n621\n\nhyv\u00e4rinen, a. and hoyer, p. o. (1999). emergence of topography and complex cell\n496\n\nproperties from natural images using extensions of ica. in\n\n, pages 827\u2013833.\n\nnips\n\nhyv\u00e4rinen,\u00a0a. and pajunen,\u00a0p. (1999). nonlinear independent component analysis:\n\nexistence and uniqueness results. neural networks,\n\n12\n\n(3), 429\u2013439.\n\n496\n\nhyv\u00e4rinen, a., karhunen, j., and oja, e. (2001a). independent component analysis.\n\nwiley-interscience. 494\n\nhyv\u00e4rinen, a., hoyer, p. o., and inki, m. o. (2001b). topographic independent component\n\nanalysis. neural computation,\n\n13\n\n(7), 1527\u20131558.\n\n496\n\nhyv\u00e4rinen, a., hurri, j., and hoyer, p. o. (2009). natural image statistics: a probabilistic\n\napproach to early computational vision. springer-verlag. 371\n\n747\n\n "}, {"Page_number": 763, "text": "bibliography\n\niba, y. (2001). extended ensemble monte carlo. international journal of modern physics,\n\nc12, 623\u2013656. 606\n\ninayoshi,\u00a0h.\u00a0and\u00a0kurita,\u00a0t.\u00a0(2005).\n\nimproved\u00a0generalization by adding\u00a0both\u00a0auto-\nassociation and hidden-layer noise to neural-network-based-classifiers. ieee workshop\non machine learning for signal processing, pages 141\u2014-146. 518\n\nioffe, s. and szegedy, c. (2015). batch normalization: accelerating deep network training\n\nby reducing internal covariate shift.\n\n100 318 321\n\n,\n\n,\n\njacobs, r. a. (1988).\u00a0increased rates of convergence through learning rate adaptation.\n\nneural networks,\n\n1\n\n(4), 295\u2013307.\n\n307\n\njacobs, r. a., jordan, m. i., nowlan, s. j., and hinton, g. e. (1991). adaptive mixtures\n\nof local experts. neural computation,\n\n3\n\n, 79\u201387.\n\n188 453\n\n,\n\njaeger, h. (2003). adaptive nonlinear system identification with echo state networks. in\n\nadvances in neural information processing systems 15 . 406\n\njaeger, h. (2007a). discovering multiscale dynamical features with hierarchical echo state\n\nnetworks. technical report, jacobs university. 401\n\njaeger, h. (2007b). echo state network. scholarpedia,\n\n2\n\n(9), 2330.\n\n406\n\njaeger, h. (2012). long short-term memory in echo state networks: details of a simulation\n\nstudy. technical report, technical report, jacobs university bremen. 407\n\njaeger, h. and haas, h. (2004). harnessing nonlinearity: predicting chaotic systems and\n\nsaving energy in wireless communication. science,\n\n304\n\n(5667), 78\u201380.\n\n27 406\n\n,\n\njaeger, h., lukosevicius, m., popovici, d., and siewert, u. (2007). optimization and\napplications of echo state networks with leaky- integrator neurons. neural networks,\n20(3), 335\u2013352. 410\n\njain, v., murray, j. f., roth, f., turaga, s., zhigulin, v., briggman, k. l., helmstaedter,\nm. n., denk, w., and seung, h. s. (2007). supervised learning of image restoration\nwith convolutional networks.\nin computer\u00a0vision, 2007. iccv 2007. ieee 11th\ninternational conference on, pages 1\u20138. ieee. 360\n\njaitly, n. and hinton, g. (2011). learning a better representation of speech soundwaves\nin acoustics,\u00a0speech and signal processing\n\nusing restricted boltzmann machines.\n(icassp), 2011 ieee international conference on, pages 5884\u20135887. ieee. 461\n\njaitly, n. and hinton, g. e. (2013). vocal tract length perturbation (vtlp) improves\n\nspeech recognition. in\n\nicml\u20192013 241\n\n.\n\njarrett, k., kavukcuoglu, k., ranzato, m., and lecun, y. (2009). what is the best\niccv\u201909 16 24 27 173 192 226\n,\n\nmulti-stage architecture for object recognition? in\n364 365 526\n\n,\n\n,\n\n.\n\n,\n\n,\n\n,\n\n,\n\n,\n\n748\n\n "}, {"Page_number": 764, "text": "bibliography\n\njarzynski, c. (1997). nonequilibrium equality for free energy differences. phys. rev. lett.,\n\n78, 2690\u20132693.\n\n,628 631\n\njaynes, e. t. (2003). probability theory: the logic of science. cambridge university\n\npress. 53\n\njean, s., cho, k., memisevic, r., and bengio, y. (2014). on using very large target\n\nvocabulary for neural machine translation. arxiv:1412.2007. 477\n\njelinek, f. and mercer, r. l. (1980). interpolated estimation of markov source parameters\nfrom sparse data. in e. s. gelsema and l. n. kanal, editors, pattern recognition in\npractice. north-holland, amsterdam.\n\n,465 476\n\njia, y. (2013). caffe: an open source convolutional architecture for fast feature embedding.\n\nhttp://caffe.berkeleyvision.org/.\n\n,25 210\n\njia, y., huang, c., and darrell, t. (2012). beyond spatial pyramids: receptive field\nin computer vision and pattern recognition\n\nlearning for pooled image features.\n(cvpr), 2012 ieee conference on, pages 3370\u20133377. ieee. 346\n\njim, k.-c., giles, c. l., and horne, b. g. (1996). an analysis of noise in recurrent neural\nnetworks:\u00a0convergence and generalization. ieee transactions on neural networks,\n7(6), 1424\u20131438. 242\n\njordan, m. i. (1998). learning in graphical models. kluwer, dordrecht, netherlands. 18\n\njoulin, a. and mikolov, t. (2015). inferring algorithmic patterns with stack-augmented\n\nrecurrent nets. arxiv preprint arxiv:1503.01007 . 421\n\njozefowicz, r., zaremba, w., and sutskever, i. (2015). an empirical evaluation of recurrent\n\nnetwork architectures. in\n\nicml\u20192015 306 414 415\n\n.\n\n,\n\n,\n\njudd, j. s. (1989). neural network design and the complexity of learning. mit press.\n\n293\n\njutten,\u00a0c. and herault,\u00a0j. (1991). blind separation of sources,\u00a0part i: an adaptive\n\nalgorithm based on neuromimetic architecture. signal processing,\n\n24\n\n, 1\u201310.\n\n494\n\nkahou, s. e., pal, c., bouthillier, x., froumenty, p., g\u00fcl\u00e7ehre, c., memisevic, r., vincent,\np., courville, a., bengio, y., ferrari, r. c., mirza, m., jean, s., carrier, p. l., dauphin,\ny., boulanger-lewandowski, n., aggarwal, a., zumer,\u00a0j., lamblin, p., raymond,\nj.-p., desjardins, g., pascanu, r., warde-farley, d., torabi, a., sharma, a., bengio,\ne., c\u00f4t\u00e9, m., konda, k. r., and wu, z. (2013). combining modality specific deep\nneural networks for emotion recognition in video. in proceedings of the 15th acm on\ninternational conference on multimodal interaction. 200\n\nkalchbrenner, n. and blunsom, p. (2013). recurrent continuous translation models. in\n\nemnlp\u20192013 . 477\n\n749\n\n "}, {"Page_number": 765, "text": "bibliography\n\nkalchbrenner, n., danihelka, i., and graves, a. (2015).\u00a0grid long short-term memory.\n\narxiv preprint arxiv:1507.01526 . 397\n\nkamyshanska, h. and memisevic, r. (2015). the potential energy of an autoencoder.\n\nieee transactions on pattern analysis and machine intelligence. 518\n\nkarpathy, a. and li, f.-f. (2015). deep visual-semantic alignments for generating image\n\ndescriptions. in\n\ncvpr\u20192015\n\n. arxiv:1412.2306.\n\n102\n\nkarpathy, a., toderici, g., shetty, s., leung, t., sukthankar, r., and fei-fei, l. (2014).\n\nlarge-scale video classification with convolutional neural networks. in\n\ncvpr 21\n\n.\n\nkarush, w. (1939). minima of functions of several variables with inequalities as side\n\nconstraints. master\u2019s thesis, dept. of mathematics, univ. of chicago. 95\n\nkatz, s. m. (1987). estimation of probabilities from sparse data for the language model\ncomponent of a speech recognizer. ieee transactions on acoustics, speech, and signal\nprocessing,\n\n(3), 400\u2013401.\n\nassp-35\n\n465 476\n\n,\n\nkavukcuoglu, k., ranzato, m., and lecun, y. (2008).\u00a0fast inference in sparse coding\nalgorithms with applications to object recognition. technical report, computational and\nbiological learning lab, courant institute, nyu. tech report cbll-tr-2008-12-01.\n526\n\nkavukcuoglu, k., ranzato, m.-a., fergus, r., and lecun, y. (2009). learning invariant\n\nfeatures through topographic filter maps. in\n\ncvpr\u20192009 526\n\n.\n\nkavukcuoglu, k., sermanet, p., boureau, y.-l., gregor, k., mathieu, m., and lecun, y.\n.\nnips\u20192010\n\n(2010). learning convolutional feature hierarchies for visual recognition. in\n365 526\n\n,\n\nkelley, h. j. (1960). gradient theory of optimal flight paths.\n\nars journal 30(10),\n\n,\n\n947\u2013954. 225\n\nkhan, f., zhu, x., and mutlu, b. (2011). how do humans teach: on curriculum learning\nand teaching dimension. in advances in neural information processing systems 24\n(nips\u201911), pages 1449\u20131457. 329\n\nkim, s. k., mcafee, l. c., mcmahon, p. l., and olukotun, k. (2009). a highly scalable\nrestricted boltzmann machine fpga implementation. in field programmable logic\nand applications, 2009. fpl 2009. international conference on, pages 367\u2013372. ieee.\n454\n\nkindermann, r. (1980). markov random fields and their applications (contemporary\n\nmathematics ; v. 1). american mathematical society. 569\n\nkingma, d. and ba, j. (2014). adam: a method for stochastic optimization. arxiv\n\npreprint arxiv:1412.6980 . 308\n\n750\n\n "}, {"Page_number": 766, "text": "bibliography\n\nkingma, d. and lecun, y. (2010). regularized estimation of image statistics by score\n\nmatching. in\n\nnips\u20192010 516 623\n\n.\n\n,\n\nkingma, d., rezende, d., mohamed, s., and welling, m. (2014). semi-supervised learning\n\nwith deep generative models. in\n\nnips\u20192014 429\n\n.\n\nkingma, d. p. (2013). fast gradient-based inference with continuous latent variable\n\nmodels in auxiliary form. technical report, arxiv:1306.0733.\n\n655 691 698\n\n,\n\n,\n\nkingma, d. p. and welling, m. (2014a). auto-encoding variational bayes. in proceedings\n\nof the international conference on learning representations (iclr).\n\n,691 701\n\nkingma,\u00a0d. p. and welling,\u00a0m. (2014b). efficient gradient-based inference through\ntransformations between bayes nets and neural nets. technical report, arxiv:1402.0480.\n691\n\nkirkpatrick, s., jr., c. d. g., , and vecchi, m. p. (1983). optimization by simulated\n\nannealing. science,\n\n220\n\n, 671\u2013680.\n\n328\n\nkiros, r., salakhutdinov, r., and zemel, r. (2014a). multimodal neural language models.\n\nin\n\nicml\u20192014 102\n\n.\n\nkiros, r., salakhutdinov, r., and zemel, r. (2014b). unifying visual-semantic embeddings\n\nwith multimodal neural language models.\n\narxiv:1411.2539 [cs.lg] 102 411\n\n.\n\n,\n\nklementiev, a., titov, i., and bhattarai, b. (2012). inducing crosslingual distributed\n\nrepresentations of words. in proceedings of coling 2012 .\n\n,479 542\n\nknowles-barley, s., jones, t. r., morgan, j., lee, d., kasthuri, n., lichtman, j. w., and\npfister, h. (2014). deep learning for the connectome. gpu technology conference. 26\n\nkoller, d. and friedman, n. (2009). probabilistic graphical models: principles and\n\ntechniques. mit press.\n\n585 598 648\n\n,\n\n,\n\nkonig, y., bourlard, h., and morgan, n. (1996). remap: recursive estimation and\nmaximization of a posteriori probabilities \u2013 application to transition-based connectionist\nspeech recognition. in d. touretzky, m. mozer, and m. hasselmo, editors, advances in\nneural information processing systems 8 (nips\u201995). mit press, cambridge, ma. 462\n\nkoren, y. (2009). the bellkor solution to the netflix grand prize.\n\n,256 482\n\nkotzias, d., denil, m., de freitas, n., and smyth, p. (2015). from group to individual\n\nlabels using deep features. in\n\nacm sigkdd 106\n\n.\n\nkoutnik, j., greff, k., gomez, f., and schmidhuber, j. (2014). a clockwork rnn. in\n\nicml\u20192014 . 410\n\nko\u010disk\u00fd, t., hermann, k. m., and blunsom, p. (2014). learning bilingual word repre-\n\nsentations by marginalizing alignments. in proceedings of acl. 479\n\n751\n\n "}, {"Page_number": 767, "text": "bibliography\n\nkrause, o., fischer, a., glasmachers, t., and igel, c. (2013). approximation properties\n\nof dbns with binary hidden units and real-valued visible units. in\n\nicml\u20192013 556\n\n.\n\nkrizhevsky, a. (2010). convolutional deep belief networks on cifar-10. technical report,\nuniversity of toronto. unpublished manuscript: http://www.cs.utoronto.ca/ kriz/conv-\ncifar10-aug2010.pdf. 449\n\nkrizhevsky, a. and hinton, g. (2009). learning multiple layers of features from tiny\n\nimages. technical report, university of toronto.\n\n,21 564\n\nkrizhevsky, a. and hinton, g. e. (2011). using very deep autoencoders for content-based\n\nimage retrieval. in\n\nesann 528\n\n.\n\nkrizhevsky, a., sutskever, i., and hinton, g. (2012). imagenet classification with deep\n\nconvolutional neural networks. in\n\nnips\u20192012 23 24 27 100 200 372 457 461\n\n.\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nkrueger, k. a. and dayan, p. (2009). flexible shaping: how learning in small steps helps.\n\ncognition,\n\n110\n\n, 380\u2013394.\n\n329\n\nkuhn, h. w. and tucker, a. w. (1951). nonlinear programming. in proceedings of the\nsecond berkeley symposium on mathematical statistics and probability, pages 481\u2013492,\nberkeley, calif. university of california press. 95\n\nkumar, a., irsoy, o., su, j., bradbury, j., english, r., pierce, b., ondruska, p., iyyer,\nm., gulrajani, i., and socher, r. (2015). ask me anything: dynamic memory networks\nfor natural language processing.\n\narxiv:1506.07285 421 488\n\n.\n\n,\n\nkumar, m. p., packer, b., and koller, d. (2010). self-paced learning for latent variable\n\nmodels. in\n\nnips\u20192010 329\n\n.\n\nlang, k. j. and hinton, g. e. (1988). the development of the time-delay neural network\narchitecture for speech recognition. technical report cmu-cs-88-152, carnegie-mellon\nuniversity.\n\n368 375 409\n\n,\n\n,\n\nlang, k. j., waibel, a. h., and hinton, g. e. (1990). a time-delay neural network\n\narchitecture for isolated word recognition. neural networks,\n\n3\n\n(1), 23\u201343.\n\n375\n\nlangford, j. and zhang, t. (2008). the epoch-greedy algorithm for contextual multi-armed\n\nbandits. in\n\nnips\u20192008\n\n, pages 1096\u2013\u20131103.\n\n483\n\nlappalainen, h., giannakopoulos, x., honkela, a., and karhunen, j. (2000). nonlinear\nindependent component analysis using ensemble learning: experiments and discussion.\nin proc. ica. citeseer. 496\n\nlarochelle,\u00a0h.\u00a0and\u00a0bengio,\u00a0y. (2008). classification\u00a0using discriminative\u00a0restricted\n\nboltzmann machines. in\n\nicml\u20192008 244 254 533 688 717\n\n.\n\n,\n\n,\n\n,\n\n,\n\n752\n\n "}, {"Page_number": 768, "text": "bibliography\n\nlarochelle, h. and hinton, g. e. (2010). learning to combine foveal glimpses with a\nthird-order boltzmann machine. in advances in neural information processing systems\n23 , pages 1243\u20131251. 368\n\nlarochelle, h. and murray, i. (2011). the neural autoregressive distribution estimator.\n\nin aistats\u20192011 .\n\n,707 710\n\nlarochelle, h., erhan, d., and bengio, y. (2008).\u00a0zero-data learning of new tasks. in\n\naaai conference on artificial intelligence. 542\n\nlarochelle, h., bengio, y., louradour, j., and lamblin, p. (2009). exploring strategies for\n538\n\ntraining deep neural networks. journal of machine learning research,\n\n, 1\u201340.\n\n10\n\nlasserre, j. a., bishop, c. m., and minka, t. p. (2006). principled hybrids of generative and\ndiscriminative models. in proceedings of the computer vision and pattern recognition\nconference (cvpr\u201906), pages 87\u201394, washington, dc, usa. ieee computer society.\n244 252\n\n,\n\nle, q., ngiam, j., chen, z., hao chia, d. j., koh, p. w., and ng, a. (2010). tiled\nin j. lafferty, c. k. i. williams,\u00a0j. shawe-taylor,\nconvolutional neural networks.\nr. zemel, and a. culotta, editors, advances in neural information processing systems\n23 (nips\u201910), pages 1279\u20131287. 353\n\nle, q., ngiam, j., coates, a., lahiri, a., prochnow, b., and ng, a. (2011). on optimization\n\nmethods for deep learning. in proc. icml\u20192011. acm. 316\n\nle, q., ranzato, m., monga, r., devin, m., corrado, g., chen, k., dean, j., and ng,\na. (2012). building high-level features using large scale unsupervised learning. in\nicml\u20192012 .\n\n,24 27\n\nle roux, n. and bengio, y. (2008). representational power of restricted boltzmann\n556 657\n\nmachines and deep belief networks. neural computation,\n\n(6), 1631\u20131649.\n\n20\n\n,\n\nle roux, n. and bengio, y. (2010). deep belief networks are compact universal approxi-\n\nmators. neural computation,\n\n22\n\n(8), 2192\u20132207.\n\n556\n\nlecun, y. (1985). une proc\u00e9dure d\u2019apprentissage pour r\u00e9seau \u00e0 seuil assym\u00e9trique. in\ncognitiva 85: a la fronti\u00e8re de l\u2019intelligence artificielle, des sciences de la connaissance\net des neurosciences, pages 599\u2013604, paris 1985. cesta, paris. 225\n\nlecun, y. (1986). learning processes in an asymmetric threshold network. in f. fogelman-\nsouli\u00e9, e. bienenstock, and g. weisbuch, editors, disordered systems and biological\norganization, pages 233\u2013240. springer-verlag, les houches, france. 351\n\nlecun, y. (1987). mod\u00e8les connexionistes de l\u2019apprentissage. ph.d. thesis, universit\u00e9 de\n\nparis vi.\n\n18 505 518\n\n,\n\n,\n\nlecun,\u00a0y. (1989). generalization and network design strategies. technical report\n\ncrg-tr-89-4, university of toronto.\n\n,331 351\n\n753\n\n "}, {"Page_number": 769, "text": "bibliography\n\nlecun, y., jackel, l. d., boser, b., denker, j. s., graf, h. p., guyon, i., henderson, d.,\nhoward, r. e., and hubbard, w. (1989). handwritten digit recognition: applications\nof neural network chips and automatic learning. ieee communications magazine,\n27(11), 41\u201346. 369\n\nlecun, y., bottou, l., orr, g. b., and m\u00fcller, k.-r. (1998a). efficient backprop. in\nneural networks, tricks of the trade, lecture notes in computer science lncs 1524.\nspringer verlag.\n\n,310 432\n\nlecun, y., bottou, l., bengio, y., and haffner, p. (1998b). gradient based learning\n\napplied to document recognition. proc. ieee.\n\n16 18 21 27 372 461 463\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nlecun,\u00a0y.,\u00a0kavukcuoglu,\u00a0k.,\u00a0and farabet,\u00a0c. (2010). convolutional networks and\napplications in vision.\u00a0in circuits and systems (iscas), proceedings of 2010 ieee\ninternational symposium on, pages 253\u2013256. ieee. 372\n\nl\u2019ecuyer, p. (1994). efficiency improvement and variance reduction. in proceedings of\n\nthe 1994 winter simulation conference, pages 122\u2013\u2013132. 692\n\nlee, c.-y., xie, s., gallagher, p., zhang, z., and tu, z. (2014). deeply-supervised nets.\n\narxiv preprint arxiv:1409.5185 . 327\n\nlee, h., battle, a., raina, r., and ng, a. (2007). efficient sparse coding algorithms.\nin b. sch\u00f6lkopf, j. platt, and t. hoffman, editors, advances in neural information\nprocessing systems 19 (nips\u201906), pages 801\u2013808. mit press. 640\n\nlee, h., ekanadham, c., and ng, a. (2008). sparse deep belief net model for visual area\n\nv2. in\n\nnips\u201907 254\n\n.\n\nlee, h., grosse, r., ranganath, r., and ng, a. y. (2009). convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. in l. bottou\nand m. littman, editors, proceedings of the twenty-sixth international conference on\nmachine learning (icml\u201909). acm, montreal, canada.\n\n364 685 686\n\n,\n\n,\n\nlee, y. j. and grauman, k. (2011). learning the easy things first: self-paced visual\n\ncategory discovery. in\n\ncvpr\u20192011 329\n\n.\n\nleibniz, g. w. (1676). memoir using the chain rule. (cited in tmme 7:2&3 p 321-332,\n\n2010). 224\n\nlenat, d. b. and guha, r. v. (1989). building large knowledge-based systems; representa-\ntion and inference in the cyc project. addison-wesley longman publishing co., inc.\n2\n\nleshno, m., lin, v. y., pinkus, a., and schocken, s. (1993). multilayer feedforward\nnetworks with a nonpolynomial activation function can approximate any function.\nneural networks,\n\n, 861\u2013\u2013867.\n\n197 198\n\n6\n\n,\n\n754\n\n "}, {"Page_number": 770, "text": "bibliography\n\nlevenberg, k. (1944). a method for the solution of certain non-linear problems in least\n\nsquares. quarterly journal of applied mathematics,\n\ni\u00a0i\n\n(2), 164\u2013168.\n\n312\n\nl\u2019h\u00f4pital, g. f. a. (1696). analyse des infiniment petits, pour l\u2019intelligence des lignes\n\ncourbes. paris: l\u2019imprimerie royale. 224\n\nli, y., swersky, k., and zemel, r. s. (2015). generative moment matching networks.\n\ncorr,\n\nabs/1502.02761 705\n\n.\n\nlin, t., horne, b. g., tino, p., and giles, c. l. (1996). learning long-term dependencies\nis not as difficult with narx recurrent neural networks. ieee transactions on neural\nnetworks,\n\n(6), 1329\u20131338.\n\n409\n\n7\n\nlin, y., liu, z., sun, m., liu, y., and zhu, x. (2015). learning entity and relation\n\nembeddings for knowledge graph completion. in proc. aaai\u201915 . 487\n\nlinde, n. (1992). the machine that changed the world, episode 3. documentary miniseries.\n\n2\n\nlindsey, c. and lindblad, t. (1994). review of hardware neural networks: a user\u2019s\nperspective. in proc. third workshop on neural networks: from biology to high\nenergy physics, pages 195\u2013\u2013202, isola d\u2019elba, italy. 454\n\nlinnainmaa,\u00a0s.\u00a0(1976). taylor expansion of\u00a0the accumulated rounding error. bit\n\nnumerical mathematics,\n\n16\n\n(2), 146\u2013160.\n\n225\n\nlisa (2008). deep learning tutorials: restricted boltzmann machines. technical report,\n\nlisa lab, universit\u00e9 de montr\u00e9al. 591\n\nlong, p. m. and servedio, r. a. (2010). restricted boltzmann machines are hard to\napproximately evaluate or simulate. in proceedings of the 27th international conference\non machine learning (icml\u201910). 660\n\nlotter, w., kreiman, g., and cox, d. (2015). unsupervised learning of visual structure\n\nusing predictive generative networks. arxiv preprint arxiv:1511.06380 .\n\n,547 548\n\nlovelace, a. (1842). notes upon l. f. menabrea\u2019s \u201csketch of the analytical engine\n\ninvented by charles babbage\u201d. 1\n\nlu, l., zhang, x., cho, k., and renals, s. (2015). a study of the recurrent neural network\n\nencoder-decoder for large vocabulary speech recognition. in proc. interspeech. 463\n\nlu, t., p\u00e1l, d., and p\u00e1l, m. (2010). contextual multi-armed bandits. in international\n\nconference on artificial intelligence and statistics, pages 485\u2013492. 483\n\nluenberger, d. g. (1984). linear and nonlinear programming. addison wesley. 317\n\nluko\u0161evi\u010dius, m. and jaeger, h. (2009). reservoir computing approaches to recurrent\n\nneural network training. computer science review,\n\n3\n\n(3), 127\u2013149.\n\n406\n\n755\n\n "}, {"Page_number": 771, "text": "bibliography\n\nluo, h., shen, r., niu, c., and ullrich, c. (2011). learning class-relevant features and\nclass-irrelevant features via a hybrid third-order rbm. in international conference on\nartificial intelligence and statistics, pages 470\u2013478. 689\n\nluo, h., carrier, p. l., courville, a., and bengio, y. (2013). texture modeling with\n\nconvolutional spike-and-slab rbms and deep extensions. in aistats\u20192013 . 102\n\nlyu, s. (2009). interpretation and generalization of score matching. in proceedings of the\n\ntwenty-fifth conference in uncertainty in artificial intelligence (uai\u201909). 621\n\nma, j., sheridan, r. p., liaw, a., dahl, g. e., and svetnik, v. (2015). deep neural nets\nas a method for quantitative structure \u2013 activity relationships. j. chemical information\nand modeling. 533\n\nmaas, a. l., hannun, a. y., and ng, a. y. (2013). rectifier nonlinearities improve neural\nnetwork acoustic models. in icml workshop on deep learning for audio, speech, and\nlanguage processing. 192\n\nmaass, w. (1992). bounds for the computational power and learning complexity of analog\nneural nets (extended abstract). in proc. of the 25th acm symp. theory of computing,\npages 335\u2013344. 198\n\nmaass, w., schnitger, g., and sontag, e. d. (1994). a comparison of the computational\npower of sigmoid and boolean threshold circuits. theoretical advances in neural\ncomputation and learning, pages 127\u2013151. 198\n\nmaass, w., natschlaeger, t., and markram, h. (2002). real-time computing without\nstable states: a new framework for neural computation based on perturbations. neural\ncomputation,\n\n(11), 2531\u20132560.\n\n406\n\n14\n\nmackay, d. (2003).\u00a0information theory, inference and learning algorithms. cambridge\n\nuniversity press. 73\n\nmaclaurin, d., duvenaud, d., and adams, r. p. (2015). gradient-based hyperparameter\n\noptimization through reversible learning. arxiv preprint arxiv:1502.03492 . 438\n\nmao, j., xu, w., yang, y., wang, j., huang, z., and yuille, a. l. (2015). deep captioning\n\nwith multimodal recurrent neural networks. in\n\niclr\u20192015\n\n. arxiv:1410.1090.\n\n102\n\nmarcotte, p. and savard, g. (1992). novel approaches to the discrimination problem.\n\nzeitschrift f\u00fcr operations research (theory),\n\n36\n\n, 517\u2013545.\n\n276\n\nmarlin, b. and de freitas, n. (2011). asymptotic efficiency of deterministic estimators for\nuai\u20192011 620\n,\n\ndiscrete energy-based models: ratio matching and pseudolikelihood. in\n622\n\n.\n\n756\n\n "}, {"Page_number": 772, "text": "bibliography\n\nmarlin, b., swersky, k., chen, b., and de freitas, n. (2010). inductive principles for\nrestricted boltzmann machine learning. in proceedings of the thirteenth international\nconference on artificial intelligence and statistics (aistats\u201910), volume 9, pages\n509\u2013516.\n\n616 621 622\n\n,\n\n,\n\nmarquardt, d. w. (1963). an algorithm for least-squares estimation of non-linear param-\neters. journal of the society of industrial and applied mathematics, 11(2), 431\u2013441.\n312\n\nmarr, d. and poggio, t. (1976). cooperative computation of stereo disparity. science,\n\n194. 368\n\nmartens,\u00a0j. (2010). deep learning via hessian-free optimization.\n\nin l. bottou and\nm. littman, editors, proceedings of the twenty-seventh international conference on\nmachine learning (icml-10), pages 735\u2013742. acm. 304\n\nmartens, j. and medabalimi, v. (2014). on the expressive efficiency of sum product\n\nnetworks.\n\narxiv:1411.7717 557\n\n.\n\nmartens, j. and sutskever, i. (2011). learning recurrent neural networks with hessian-free\n\noptimization. in proc. icml\u20192011 . acm. 415\n\nmase, s. (1995). consistency of the maximum pseudo-likelihood estimator of continuous\nstate space gibbsian processes. the annals of applied probability, 5(3), pp. 603\u2013612.\n619\n\nmcclelland, j., rumelhart, d., and hinton, g. (1995). the appeal of parallel distributed\nprocessing. in computation & intelligence, pages 305\u2013341. american association for\nartificial intelligence. 17\n\nmcculloch, w. s. and pitts, w. (1943). a logical calculus of ideas immanent in nervous\n\nactivity. bulletin of mathematical biophysics,\n\n5\n\n, 115\u2013133.\n\n14 15\n\n,\n\nmead, c. and ismail, m. (2012). analog vlsi implementation of neural systems, volume 80.\n\nspringer science & business media. 454\n\nmelchior, j., fischer, a., and wiskott, l. (2013). how to center binary deep boltzmann\n\nmachines. arxiv preprint arxiv:1311.1354 . 675\n\nmemisevic, r. and hinton, g. e. (2007). unsupervised learning of image transformations.\nin proceedings of the computer vision and pattern recognition conference (cvpr\u201907).\n688\n\nmemisevic, r. and hinton, g. e. (2010). learning to represent spatial transformations\nwith factored higher-order boltzmann machines. neural computation, 22(6), 1473\u20131492.\n688\n\n757\n\n "}, {"Page_number": 773, "text": "bibliography\n\nmesnil, g., dauphin, y., glorot, x., rifai, s., bengio, y., goodfellow, i., lavoie, e.,\nmuller, x., desjardins, g., warde-farley, d., vincent, p., courville, a., and bergstra,\nj. (2011). unsupervised and transfer learning challenge: a deep learning approach. in\njmlr w&cp: proc. unsupervised and transfer learning, volume 7.\n\n200 535 541\n\n,\n\n,\n\nmesnil, g., rifai, s., dauphin, y., bengio, y., and vincent, p. (2012). surfing on the\n\nmanifold. learning workshop, snowbird. 713\n\nmiikkulainen, r. and dyer, m. g. (1991). natural language processing with modular\n\npdp networks and distributed lexicon. cognitive science,\n\n15\n\n, 343\u2013399.\n\n480\n\nmikolov, t. (2012). statistical language models based on neural networks. ph.d. thesis,\n\nbrno university of technology. 417\n\nmikolov, t., deoras, a., kombrink, s., burget, l., and cernocky, j. (2011a). empirical\nevaluation and combination of advanced language modeling techniques. in proc. 12th an-\nnual conference of the international speech communication association (interspeech\n2011). 475\n\nmikolov, t., deoras, a., povey, d., burget, l., and cernocky, j. (2011b). strategies for\n\ntraining large scale neural network language models. in proc. asru\u20192011.\n\n,329 475\n\nmikolov, t., chen, k., corrado, g., and dean, j. (2013a). efficient estimation of word rep-\nresentations in vector space. in international conference on learning representations:\nworkshops track. 539\n\nmikolov, t., le, q. v., and sutskever, i. (2013b). exploiting similarities among languages\n\nfor machine translation. technical report, arxiv:1309.4168. 542\n\nminka, t. (2005). divergence measures and message passing. microsoft research cambridge\n\nuk tech rep msrtr2005173 ,\n\n72\n\n(tr-2005-173).\n\n628\n\nminsky, m. l. and papert, s. a. (1969). perceptrons. mit press, cambridge. 15\n\nmirza, m. and osindero, s. (2014). conditional generative adversarial nets. arxiv preprint\n\narxiv:1411.1784 . 703\n\nmishkin, d. and\u00a0matas, j. (2015). all you need is a good init.\n\narxiv preprint\n\narxiv:1511.06422 . 305\n\nmisra, j. and saha, i. (2010).\u00a0artificial neural networks in hardware: a survey of two\n\ndecades of progress. neurocomputing,\n\n74\n\n(1), 239\u2013255.\n\n454\n\nmitchell, t. m. (1997). machine learning. mcgraw-hill, new york. 99\n\nmiyato, t., maeda, s., koyama, m., nakae, k., and ishii, s. (2015). distributional\n268\n\nsmoothing with virtual adversarial training. in\n\n. preprint: arxiv:1507.00677.\n\niclr\n\n758\n\n "}, {"Page_number": 774, "text": "bibliography\n\nmnih, a. and gregor,\u00a0k. (2014). neural variational inference and learning in belief\n\nnetworks. in\n\nicml\u20192014 693 695\n\n.\n\n,\n\nmnih, a. and hinton, g. e. (2007). three new graphical models for statistical language\nmodelling. in z. ghahramani, editor, proceedings of the twenty-fourth international\nconference on machine learning (icml\u201907), pages 641\u2013648. acm. 467\n\nmnih, a. and hinton, g. e. (2009). a scalable hierarchical distributed language model.\nin d. koller, d. schuurmans, y. bengio, and l. bottou, editors, advances in neural\ninformation processing systems 21 (nips\u201908), pages 1081\u20131088. 470\n\nmnih, a. and kavukcuoglu, k. (2013). learning word embeddings efficiently with noise-\ncontrastive estimation. in c. burges, l. bottou, m. welling, z. ghahramani, and\nk. weinberger, editors, advances in neural information processing systems 26 , pages\n2265\u20132273. curran associates, inc.\n\n,475 625\n\nmnih,\u00a0a. and teh,\u00a0y.\u00a0w. (2012). a fast and simple\u00a0algorithm for training neural\n\nprobabilistic language models. in\n\nicml\u20192012\n\n, pages 1751\u20131758.\n\n475\n\nmnih, v. and hinton, g. (2010). learning to detect roads in high-resolution aerial images.\n\nin proceedings of the 11th european conference on computer vision (eccv). 102\n\nmnih, v., larochelle, h.,\u00a0and hinton, g. (2011). conditional restricted boltzmann\nmachines for structure output prediction. in proc. conf. on uncertainty in artificial\nintelligence (uai). 687\n\nmnih, v., kavukcuoglo, k., silver, d., graves, a., antonoglou, i., and wierstra, d. (2013).\nplaying atari with deep reinforcement learning. technical report, arxiv:1312.5602. 106\n\nmnih, v., heess, n., graves, a., and kavukcuoglu, k. (2014). recurrent models of visual\nattention. in z. ghahramani, m. welling, c. cortes, n. lawrence, and k. weinberger,\neditors,\n\n, pages 2204\u20132212.\n\nnips\u20192014\n\n693\n\nmnih, v., kavukcuoglo, k., silver, d., rusu, a. a., veness, j., bellemare, m. g., graves,\na., riedmiller, m., fidgeland, a. k., ostrovski, g., petersen, s., beattie, c., sadik, a.,\nantonoglou, i., king, h., kumaran, d., wierstra, d., legg, s., and hassabis, d. (2015).\nhuman-level control through deep reinforcement learning. nature,\n\n, 529\u2013533.\n\n518\n\n25\n\nmobahi, h. and fisher,\u00a0iii, j. w. (2015). a theoretical analysis of optimization by\n\ngaussian continuation. in\n\naaai\u20192015 328\n\n.\n\nmobahi, h., collobert, r., and weston, j. (2009). deep learning from temporal coherence\nin video. in l. bottou and m. littman, editors, proceedings of the 26th international\nconference on machine learning, pages 737\u2013744, montreal. omnipress. 497\n\nmohamed, a., dahl, g., and hinton, g. (2009). deep belief networks for phone recognition.\n\n462\n\n759\n\n "}, {"Page_number": 775, "text": "bibliography\n\nmohamed, a., sainath, t. n., dahl, g., ramabhadran, b., hinton, g. e., and picheny,\nm. a. (2011). deep belief networks using discriminative features for phone recognition. in\nacoustics, speech and signal processing (icassp), 2011 ieee international conference\non, pages 5060\u20135063. ieee. 462\n\nmohamed, a., dahl, g., and hinton, g. (2012a).\u00a0acoustic modeling using deep belief\nnetworks. ieee trans. on audio, speech and language processing, 20 (1), 14\u201322. 462\n\nmohamed, a., hinton, g., and penn, g. (2012b). understanding how deep belief networks\nperform acoustic modelling. in acoustics, speech and signal processing (icassp),\n2012 ieee international conference on, pages 4273\u20134276. ieee. 462\n\nmoller, m. f. (1993). a scaled conjugate gradient algorithm for fast supervised learning.\n\nneural networks,\n\n6\n\n, 525\u2013533.\n\n316\n\nmontavon, g. and muller, k.-r. (2012).\u00a0deep boltzmann machines and the centering\ntrick. in g. montavon, g. orr, and k.-r. m\u00fcller, editors, neural networks: tricks of\n, pages 621\u2013637. preprint:\nthe trade\nhttp://arxiv.org/abs/1203.3783. 675\n\nlecture notes in computer science\n\n, volume 7700 of\n\nmont\u00fafar, g. (2014). universal approximation depth and errors of narrow belief networks\n\nwith discrete units. neural computation,\n\n.26 556\n\nmont\u00fafar, g. and ay, n. (2011). refinements of universal approximation results for\ndeep belief networks and restricted boltzmann machines. neural computation, 23(5),\n1306\u20131319. 556\n\nmontufar, g. f., pascanu, r., cho, k., and bengio, y. (2014). on the number of linear\n\nregions of deep neural networks. in\n\nnips\u20192014 19 199\n\n.\n\n,\n\nmor-yosef, s., samueloff, a., modan, b., navot, d., and schenker, j. g. (1990). ranking\nthe risk factors for cesarean: logistic regression analysis of a nationwide study. obstet\ngynecol,\n\n(6), 944\u20137.\n\n75\n\n3\n\nmorin, f. and bengio, y. (2005). hierarchical probabilistic neural network language\n\nmodel. in aistats\u20192005 .\n\n,470 472\n\nmozer, m. c. (1992). the induction of multiscale temporal structure. in j. m. s. hanson\nand r. lippmann,\u00a0editors,\u00a0advances\u00a0in neural information processing systems 4\n(nips\u201991), pages 275\u2013282, san mateo, ca. morgan kaufmann. 410\n\nmurphy, k.\u00a0p. (2012). machine learning: a\u00a0probabilistic\u00a0perspective. mit\u00a0press,\n\ncambridge, ma, usa.\n\n62 98 145\n\n,\n\n,\n\nmurray, b. u. i. and larochelle, h. (2014). a deep and tractable density estimator. in\n\nicml\u20192014 .\n\n,189 712\n\nnair, v. and hinton, g. (2010). rectified linear units improve restricted boltzmann\n\nmachines. in\n\nicml\u20192010 16 173 196\n\n.\n\n,\n\n,\n\n760\n\n "}, {"Page_number": 776, "text": "bibliography\n\nnair, v. and hinton, g. e. (2009). 3d object recognition with deep belief nets. in y. bengio,\nd. schuurmans, j. d. lafferty, c. k. i. williams, and a. culotta, editors, advances in\nneural information processing systems 22 , pages 1339\u20131347. curran associates, inc.\n688\n\nnarayanan, h. and mitter, s. (2010). sample complexity of testing the manifold hypothesis.\n\nin\n\nnips\u20192010 163\n\n.\n\nnaumann, u. (2008). optimal jacobian accumulation is np-complete. mathematical\n\nprogramming,\n\n112\n\n(2), 427\u2013441.\n\n221\n\nnavigli, r. and velardi, p. (2005).\u00a0structural semantic interconnections: a knowledge-\nbased approach to word sense disambiguation. ieee trans. pattern analysis and\nmachine intelligence,\n\n(7), 1075\u2013\u20131086.\n\n487\n\n27\n\nneal, r. and hinton, g. (1999). a view of the em algorithm that justifies incremental,\nsparse, and other variants. in m. i. jordan, editor, learning in graphical models. mit\npress, cambridge, ma. 637\n\nneal, r. m. (1990). learning stochastic feedforward networks. technical report. 694\n\nneal, r. m. (1993). probabilistic inference using markov chain monte-carlo methods.\ntechnical report crg-tr-93-1, dept. of computer science, university of toronto. 682\n\nneal, r. m. (1994). sampling from multimodal distributions using tempered transitions.\n\ntechnical report 9421, dept. of statistics, university of toronto. 606\n\nneal, r. m. (1996). bayesian learning for neural networks. lecture notes in statistics.\n\nspringer. 264\n\nneal, r. m. (2001). annealed importance sampling.\n\nstatistics and computing 11(2),\n\n,\n\n125\u2013139.\n\n628 630 631 632\n\n,\n\n,\n\n,\n\nneal, r. m. (2005). estimating ratios of normalizing constants using linked importance\n\nsampling. 632\n\nnesterov, y. (1983). a method of solving a convex programming problem with convergence\n\nrate o /k(1\n\n2).\n\nsoviet mathematics doklady 27\n\n,\n\n, 372\u2013376.\n\n300\n\nnesterov, y. (2004). introductory lectures on convex optimization : a basic course. applied\n\noptimization. kluwer academic publ., boston, dordrecht, london. 300\n\nnetzer, y., wang, t., coates, a., bissacco, a., wu, b., and ng, a. y. (2011). reading\ndigits\u00a0in\u00a0natural images\u00a0with unsupervised\u00a0feature\u00a0learning. deep\u00a0learning\u00a0and\nunsupervised feature learning workshop, nips. 21\n\nney, h. and kneser, r. (1993). improved clustering techniques for class-based statistical\nlanguage modelling. in european conference on speech communication and technology\n(eurospeech), pages 973\u2013976, berlin. 466\n\n761\n\n "}, {"Page_number": 777, "text": "bibliography\n\nng,\n\na.\n\n(2015).\n\nadvice\n\nfor\n\napplying\n\nmachine\n\nlearning.\n\nhttps://see.stanford.edu/materials/aimlcs229/ml-advice.pdf. 424\n\nniesler, t. r., whittaker, e. w. d., and woodland, p. c. (1998). comparison of part-of-\nspeech and automatically derived category-based language models for speech recognition.\nin international conference on acoustics, speech and signal processing (icassp),\npages 177\u2013180. 466\n\nning, f., delhomme, d., lecun, y., piano, f., bottou, l., and barbano, p. e. (2005).\ntoward automatic phenotyping of developing embryos from videos. image processing,\nieee transactions on,\n\n(9), 1360\u20131371.\n\n361\n\n14\n\nnocedal, j. and wright, s. (2006). numerical optimization. springer.\n\n,92 95\n\nnorouzi, m. and fleet, d. j. (2011). minimal loss hashing for compact binary codes. in\n\nicml\u20192011 . 528\n\nnowlan, s. j. (1990). competing experts: an experimental investigation of associative\n\nmixture models. technical report crg-tr-90-5, university of toronto. 453\n\nnowlan, s. j. and hinton, g. e. (1992). simplifying neural networks by soft weight-sharing.\n\nneural computation,\n\n4\n\n(4), 473\u2013493.\n\n139\n\nolshausen, b. and field, d. j. (2005). how close are we to understanding v1? neural\n\ncomputation,\n\n17\n\n, 1665\u20131699.\n\n16\n\nolshausen, b. a. and field, d. j. (1996). emergence of simple-cell receptive field properties\n146 254 371 499\n\nby learning a sparse code for natural images.\u00a0nature, 381, 607\u2013609.\n\n,\n\n,\n\n,\n\nolshausen, b. a., anderson, c. h., and van essen, d. c. (1993). a neurobiological\nmodel of visual attention and invariant pattern recognition based on dynamic routing\nof information. j. neurosci.,\n\n(11), 4700\u20134719.\n\n453\n\n13\n\nopper, m. and archambeau, c. (2009). the variational gaussian approximation revisited.\n\nneural computation,\n\n21\n\n(3), 786\u2013792.\n\n691\n\noquab, m., bottou, l., laptev, i., and sivic, j. (2014). learning and transferring mid-level\nimage representations using convolutional neural networks. in computer vision and\npattern recognition (cvpr), 2014 ieee conference on, pages 1717\u20131724. ieee. 539\n\nosindero, s. and hinton, g. e. (2008). modeling image patches with a directed hierarchy\nof markov random fields. in j. platt, d. koller, y. singer, and s. roweis, editors,\nadvances in neural information processing systems 20 (nips\u201907), pages 1121\u20131128,\ncambridge, ma. mit press. 635\n\novid and martin, c. (2004).\n\nmetamorphoses\n\n. w.w. norton.\n\n1\n\n762\n\n "}, {"Page_number": 778, "text": "bibliography\n\npaccanaro, a. and hinton, g. e. (2000). extracting distributed representations of concepts\nand relations from positive and negative propositions. in international joint conference\non neural networks (ijcnn), como, italy. ieee, new york. 487\n\npaine, t. l., khorrami, p., han, w., and huang, t. s. (2014). an analysis of unsupervised\n\npre-training in light of recent advances. arxiv preprint arxiv:1412.6597 . 535\n\npalatucci, m., pomerleau, d., hinton, g. e., and mitchell, t. m. (2009). zero-shot\nlearning with semantic output codes. in y. bengio, d. schuurmans, j. d. lafferty,\nc. k. i. williams, and a. culotta, editors, advances in neural information processing\nsystems 22 , pages 1410\u20131418. curran associates, inc. 542\n\nparker, d. b. (1985). learning-logic. technical report tr-47, center for comp. research\n\nin economics and management sci., mit. 225\n\npascanu, r., mikolov, t., and bengio, y. (2013a). on the difficulty of training recurrent\n\nneural networks. in\n\nicml\u20192013 289 403 406 410 417 419\n\n.\n\n,\n\n,\n\n,\n\n,\n\n,\n\npascanu, r., montufar, g., and bengio, y. (2013b). on the number of inference regions\nof deep feed forward networks with piece-wise linear activations. technical report, u.\nmontreal, arxiv:1312.6098. 198\n\npascanu, r., g\u00fcl\u00e7ehre, \u00e7., cho, k., and bengio, y. (2014a). how to construct deep\n\nrecurrent neural networks. in\n\niclr\u20192014 19 199 264 399 400 401 413 463\n\n.\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\npascanu, r., montufar, g., and bengio, y. (2014b). on the number of inference regions\niclr\u20192014 553\n\nof deep feed forward networks with piece-wise linear activations. in\n\n.\n\npati, y., rezaiifar, r., and krishnaprasad, p. (1993). orthogonal matching pursuit:\nrecursive function approximation with applications to wavelet decomposition. in pro-\nceedings of the 27 th annual asilomar conference on signals, systems, and computers,\npages 40\u201344. 254\n\npearl, j. (1985). bayesian networks: a model of self-activated memory for evidential\nin proceedings of the 7th conference of the\u00a0cognitive science\u00a0society,\n\nreasoning.\nuniversity of california, irvine, pages 329\u2013334. 566\n\npearl, j. (1988).\u00a0probabilistic reasoning in intelligent systems:\u00a0networks of plausible\n\ninference. morgan kaufmann. 54\n\nperron, o. (1907). zur theorie der matrices.\n\nmathematische annalen 64(2), 248\u2013263. 600\n\n,\n\npetersen, k. b. and pedersen, m. s. (2006). the matrix cookbook. version 20051003. 31\n\npeterson, g. b. (2004). a day of great illumination: b. f. skinner\u2019s discovery of shaping.\n\njournal of the experimental analysis of behavior,\n\n82\n\n(3), 317\u2013328.\n\n329\n\npham, d.-t., garat, p., and jutten, c. (1992). separation of a mixture of independent\n\nsources through a maximum likelihood approach. in\n\neusipco\n\n, pages 771\u2013774.\n\n494\n\n763\n\n "}, {"Page_number": 779, "text": "bibliography\n\npham, p.-h., jelaca, d., farabet, c., martini, b., lecun, y., and culurciello, e. (2012).\nneuflow: dataflow vision processing system-on-a-chip. in circuits and systems (mws-\ncas), 2012 ieee 55th international midwest symposium on, pages 1044\u20131047. ieee.\n454\n\npinheiro, p. h. o. and collobert, r. (2014). recurrent convolutional neural networks for\n\nscene labeling. in\n\nicml\u20192014 360\n\n.\n\npinheiro, p. h. o. and collobert, r. (2015). from image-level to pixel-level labeling with\nconvolutional networks. in conference on computer vision and pattern recognition\n(cvpr). 360\n\npinto, n., cox, d. d., and dicarlo, j. j. (2008). why is real-world visual object recognition\n\nhard? plos comput biol,\n\n.4 459\n\npinto, n., stone, z., zickler, t., and cox, d. (2011). scaling up biologically-inspired\ncomputer vision: a case study in unconstrained face recognition on facebook.\nin\ncomputer vision and pattern recognition workshops (cvprw), 2011 ieee computer\nsociety conference on, pages 35\u201342. ieee. 364\n\npollack, j. b. (1990). recursive distributed representations. artificial intelligence, 46(1),\n\n77\u2013105. 401\n\npolyak, b. and juditsky, a. (1992). acceleration of stochastic approximation by averaging.\n\nsiam j. control and optimization,\n\n30(4)\n\n, 838\u2013855.\n\n323\n\npolyak, b. t. (1964). some methods of speeding up the convergence of iteration methods.\n\nussr computational mathematics and mathematical physics,\n\n4\n\n(5), 1\u201317.\n\n296\n\npoole, b., sohl-dickstein, j., and ganguli, s. (2014).\u00a0analyzing noise in autoencoders\n\nand deep networks.\n\ncorr abs/1406.1831 241\n\n,\n\n.\n\npoon, h. and domingos, p. (2011). sum-product networks: a new deep architecture. in\nproceedings of the twenty-seventh conference in uncertainty in artificial intelligence\n(uai), barcelona, spain. 557\n\npresley, r. k. and haggard, r. l. (1994). a fixed point implementation of the backpropa-\ngation learning algorithm. in southeastcon\u201994. creative technology transfer-a global\naffair., proceedings of the 1994 ieee , pages 136\u2013138. ieee. 454\n\nprice, r. (1958). a useful theorem for nonlinear devices having gaussian inputs. ieee\n\ntransactions on information theory,\n\n4\n\n(2), 69\u201372.\n\n691\n\nquiroga, r. q., reddy, l., kreiman, g., koch, c., and fried, i. (2005). invariant visual\nrepresentation by single neurons in the human brain. nature, 435(7045), 1102\u20131107.\n367\n\n764\n\n "}, {"Page_number": 780, "text": "bibliography\n\nradford, a., metz, l., and chintala, s. (2015). unsupervised representation learning with\ndeep convolutional generative adversarial networks. arxiv preprint arxiv:1511.06434 .\n555 703 704\n\n,\n\n,\n\nraiko, t., yao, l., cho, k., and bengio,\u00a0y. (2014).\n\niterative neural autoregressive\n\ndistribution estimator (nade-k). technical report, arxiv:1406.1485.\n\n,678 711\n\nraina, r., madhavan, a., and ng, a. y. (2009). large-scale deep unsupervised learning\nusing graphics processors.\u00a0in l. bottou and m. littman, editors, proceedings of the\ntwenty-sixth international conference on machine learning (icml\u201909), pages 873\u2013880,\nnew york, ny, usa. acm.\n\n,27 449\n\nramsey, f. p. (1926). truth and probability. in r. b. braithwaite, editor, the foundations\nof mathematics and other logical essays, chapter 7, pages 156\u2013198. mcmaster university\narchive for the history of economic thought. 56\n\nranzato, m. and hinton, g. h. (2010). modeling pixel means and covariances using\n\nfactorized third-order boltzmann machines. in\n\ncvpr\u20192010\n\n, pages 2551\u20132558.\n\n682\n\nranzato, m., poultney, c., chopra, s., and lecun, y. (2007a). efficient learning of sparse\n\nrepresentations with an energy-based model. in\n\nnips\u20192006 14 19 510 531 533\n\n.\n\n,\n\n,\n\n,\n\n,\n\nranzato, m., huang, f., boureau, y., and lecun, y. (2007b). unsupervised learning of\ninvariant feature hierarchies with applications to object recognition. in proceedings of\nthe computer vision and pattern recognition conference (cvpr\u201907). ieee press. 365\n\nranzato, m., boureau, y., and lecun, y. (2008). sparse feature learning for deep belief\n\nnetworks. in\n\nnips\u20192007 510\n\n.\n\nranzato, m., krizhevsky, a., and hinton, g. e. (2010a). factored 3-way restricted\nboltzmann machines for modeling natural images. in proceedings of aistats 2010 .\n680 681\n\n,\n\nranzato, m., mnih, v., and hinton, g. (2010b). generating more realistic images using\n\ngated mrfs. in\n\nnips\u20192010 682 683\n\n.\n\n,\n\nrao, c. (1945). information and the accuracy attainable in the estimation of statistical\n\nparameters. bulletin of the calcutta mathematical society,\n\n37\n\n, 81\u201389.\n\n135 295\n\n,\n\nrasmus, a., valpola, h., honkala, m., berglund, m., and raiko, t. (2015). semi-supervised\n\nlearning with ladder network. arxiv preprint arxiv:1507.02672 .\n\n,429 533\n\nrecht, b., re, c., wright, s., and niu, f. (2011). hogwild: a lock-free approach to\n\nparallelizing stochastic gradient descent. in\n\nnips\u20192011 450\n\n.\n\nreichert, d. p., seri\u00e8s, p., and storkey, a. j. (2011). neuronal adaptation for sampling-\nbased probabilistic inference in perceptual bistability. in advances in neural information\nprocessing systems, pages 2357\u20132365. 668\n\n765\n\n "}, {"Page_number": 781, "text": "bibliography\n\nrezende, d. j., mohamed, s., and wierstra, d. (2014). stochastic backpropagation\n. preprint:\n\nand\u00a0approximate inference in\u00a0deep generative\u00a0models.\narxiv:1401.4082.\n\n655 691 698\n\nicml\u20192014\n\nin\u00a0\n\n,\n\n,\n\nrifai,\u00a0s.,\u00a0vincent,\u00a0p.,\u00a0muller, x.,\u00a0glorot,\u00a0x., and bengio, y. (2011a). contractive\nicml\u20192011 524 525\n,\n\nauto-encoders: explicit invariance during feature extraction. in\n526\n\n.\n\n,\n\nrifai, s., mesnil, g., vincent, p., muller, x., bengio, y., dauphin, y., and glorot, x.\n\n(2011b). higher order contractive auto-encoder. in\n\necml pkdd 524 525\n\n.\n\n,\n\nrifai, s., dauphin, y., vincent, p., bengio, y., and muller, x. (2011c).\u00a0the manifold\n\ntangent classifier. in\n\nnips\u20192011 270 271\n\n.\n\n,\n\nrifai, s., bengio, y., dauphin, y., and vincent, p. (2012). a generative process for\n\nsampling contractive auto-encoders. in\n\nicml\u20192012 713\n\n.\n\nringach, d. and shapley, r. (2004). reverse correlation in neurophysiology. cognitive\n\nscience,\n\n28\n\n(2), 147\u2013166.\n\n369\n\nroberts, s. and everson, r. (2001). independent component analysis: principles and\n\npractice. cambridge university press. 496\n\nrobinson, a. j. and fallside, f. (1991). a recurrent error propagation network speech\n\nrecognition system. computer speech and language,\n\n5\n\n(3), 259\u2013274.\n\n27 462\n\n,\n\nrockafellar, r. t. (1997). convex analysis. princeton landmarks in mathematics. 93\n\nromero, a., ballas, n., ebrahimi kahou, s., chassang, a., gatta, c., and bengio, y.\n\n(2015). fitnets: hints for thin deep nets. in\n\niclr\u20192015, arxiv:1412.6550 326\n\n.\n\nrosen, j. b. (1960). the gradient projection method for nonlinear programming. part i.\nlinear constraints. journal of the society for industrial and applied mathematics, 8(1),\npp. 181\u2013217. 93\n\nrosenblatt, f. (1958). the perceptron: a probabilistic model for information storage and\n\norganization in the brain. psychological review,\n\n65\n\n, 386\u2013408.\n\n14 15 27\n\n,\n\n,\n\nrosenblatt, f. (1962). principles of neurodynamics. spartan, new york.\n\n,15 27\n\nroweis, s. and saul, l. k. (2000). nonlinear dimensionality reduction by locally linear\n\nembedding. science,\n\n290\n\n(5500).\n\n163 521\n\n,\n\nroweis, s., saul, l., and hinton, g. (2002). global coordination of local linear models. in\nt. dietterich, s. becker, and z. ghahramani, editors, advances in neural information\nprocessing systems 14 (nips\u201901), cambridge, ma. mit press. 492\n\nrubin, d. b.\n\net al.\n\n(1984). bayesianly justifiable and relevant frequency calculations for\n\nthe applied statistician.\n\nthe annals of statistics 12\n\n,\n\n(4), 1151\u20131172.\n\n718\n\n766\n\n "}, {"Page_number": 782, "text": "bibliography\n\nrumelhart,\u00a0d.,\u00a0hinton,\u00a0g.,\u00a0and williams,\u00a0r. (1986a). learning representations by\n\nback-propagating errors. nature,\n\n323\n\n, 533\u2013536.\n\n14 18 23 203 225 374 479 485\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nrumelhart, d. e., hinton, g. e., and williams, r. j. (1986b). learning internal represen-\ntations by error propagation. in d. e. rumelhart and j. l. mcclelland, editors, parallel\ndistributed processing, volume 1, chapter 8, pages 318\u2013362. mit press, cambridge.\n,21\n27 225\n\n,\n\nrumelhart, d. e., mcclelland, j. l., and the pdp research group (1986c). parallel\ndistributed processing: explorations in the microstructure of cognition. mit press,\ncambridge. 17\n\nrussakovsky, o., deng, j., su, h., krause, j., satheesh, s., ma, s., huang, z., karpathy,\na., khosla, a., bernstein, m., berg, a. c., and fei-fei, l. (2014a). imagenet large\nscale visual recognition challenge. 21\n\nrussakovsky, o., deng, j., su, h., krause, j., satheesh, s., ma, s., huang, z., karpathy,\n(2014b). imagenet large scale visual recognition\n\net al.\n\na., khosla, a., bernstein, m.,\nchallenge. arxiv preprint arxiv:1409.0575 . 28\n\nrussel, s. j. and norvig, p. (2003). artificial intelligence: a modern approach. prentice\n\nhall. 86\n\nrust, n., schwartz, o., movshon, j. a., and simoncelli, e. (2005). spatiotemporal\n\nelements of macaque v1 receptive fields. neuron,\n\n46\n\n(6), 945\u2013956.\n\n368\n\nsainath, t., mohamed, a., kingsbury, b., and ramabhadran, b. (2013). deep convolu-\n\ntional neural networks for lvcsr. in\n\nicassp 2013 463\n\n.\n\nsalakhutdinov, r. (2010). learning in markov random fields using tempered transitions. in\ny. bengio, d. schuurmans, c. williams, j. lafferty, and a. culotta, editors, advances\nin neural information processing systems 22 (nips\u201909). 606\n\nsalakhutdinov, r. and hinton, g. (2009a). deep boltzmann machines. in proceedings of\nthe international conference on artificial intelligence and statistics, volume 5, pages\n448\u2013455.\n\n24 27 532 665 668 673 674\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nsalakhutdinov, r. and hinton, g. (2009b). semantic hashing. in international journal of\n\napproximate reasoning. 528\n\nsalakhutdinov,\u00a0r.\u00a0and hinton,\u00a0g.\u00a0e. (2007a). learning\u00a0a\u00a0nonlinear embedding\u00a0by\npreserving class neighbourhood structure. in proceedings of the eleventh international\nconference on artificial intelligence and statistics (aistats\u201907), san juan, porto\nrico. omnipress. 530\n\nsalakhutdinov, r. and hinton, g. e. (2007b). semantic hashing. in\n\nsigir\u20192007 528\n\n.\n\n767\n\n "}, {"Page_number": 783, "text": "bibliography\n\nsalakhutdinov, r. and hinton, g. e. (2008). using deep belief nets to learn covariance\nkernels for gaussian processes. in j. platt, d. koller, y. singer, and s. roweis, editors,\nadvances in neural information processing systems 20 (nips\u201907), pages 1249\u20131256,\ncambridge, ma. mit press. 244\n\nsalakhutdinov, r. and larochelle, h. (2010). efficient learning of deep boltzmann machines.\nin proceedings of the thirteenth international conference on artificial intelligence and\nstatistics (aistats 2010), jmlr w&cp, volume 9, pages 693\u2013700. 655\n\nsalakhutdinov, r. and mnih, a. (2008). probabilistic matrix factorization. in\n\nnips\u20192008\n\n.\n\n482\n\nsalakhutdinov, r. and murray, i. (2008). on the quantitative analysis of deep belief\nnetworks. in w. w. cohen, a. mccallum, and s. t. roweis, editors, proceedings of\nthe twenty-fifth international conference on machine learning (icml\u201908), volume 25,\npages 872\u2013879. acm.\n\n,631 664\n\nsalakhutdinov, r., mnih, a., and hinton, g. (2007). restricted boltzmann machines for\n\ncollaborative filtering. in\n\nicml 482\n\n.\n\nsanger,\u00a0t. d.\u00a0(1994). neural network learning control of robot manipulators using\ngradually increasing task difficulty. ieee transactions on robotics and automation,\n10(3). 329\n\nsaul, l. k. and jordan, m. i. (1996). exploiting tractable substructures in intractable\nnetworks. in d. touretzky, m. mozer, and m. hasselmo, editors, advances in neural\ninformation processing systems 8 (nips\u201995). mit press, cambridge, ma. 641\n\nsaul, l. k., jaakkola, t., and jordan, m. i. (1996). mean field theory for sigmoid belief\n\nnetworks. journal of artificial intelligence research,\n\n4\n\n, 61\u201376.\n\n27 695\n\n,\n\nsavich, a. w., moussa, m., and areibi, s. (2007). the impact of arithmetic representation\non implementing mlp-bp on fpgas: a study. neural networks, ieee transactions on,\n18(1), 240\u2013252. 454\n\nsaxe, a. m., koh, p. w., chen, z., bhand, m., suresh, b., and ng, a. (2011). on random\n\nweights and unsupervised feature learning. in proc. icml\u20192011 . acm. 364\n\nsaxe, a. m., mcclelland, j. l., and ganguli, s. (2013). exact solutions to the nonlinear\n\ndynamics of learning in deep linear neural networks. in\n\niclr 285 286 303\n\n.\n\n,\n\n,\n\nschaul, t., antonoglou, i., and silver, d. (2014). unit tests for stochastic optimization.\n\nin international conference on learning representations. 309\n\nschmidhuber, j. (1992). learning complex, extended sequences using the principle of\n\nhistory compression. neural computation,\n\n4\n\n(2), 234\u2013242.\n\n401\n\nschmidhuber, j. (1996). sequential neural text compression. ieee transactions on neural\n\nnetworks,\n\n7\n\n(1), 142\u2013146.\n\n480\n\n768\n\n "}, {"Page_number": 784, "text": "bibliography\n\nschmidhuber, j. (2012). self-delimiting neural networks. arxiv preprint arxiv:1210.0118 .\n\n391\n\nsch\u00f6lkopf, b. and smola, a. j. (2002). learning with kernels: support vector machines,\n\nregularization, optimization, and beyond. mit press. 705\n\nsch\u00f6lkopf, b., smola, a., and m\u00fcller, k.-r. (1998). nonlinear component analysis as a\n\nkernel eigenvalue problem. neural computation,\n\n10\n\n, 1299\u20131319.\n\n163 521\n\n,\n\nsch\u00f6lkopf, b., burges, c. j. c., and smola, a. j. (1999). advances in kernel methods \u2014\n\nsupport vector learning. mit press, cambridge, ma.\n\n,18 142\n\nsch\u00f6lkopf, b., janzing, d., peters, j., sgouritsa, e., zhang, k., and mooij, j. (2012). on\n\ncausal and anticausal learning. in\n\nicml\u20192012\n\n, pages 1255\u20131262.\n\n548\n\nschuster, m. (1999). on supervised learning from sequential data with applications for\n\nspeech recognition. 189\n\nschuster, m. and paliwal, k. (1997). bidirectional recurrent neural networks. ieee\n\ntransactions on signal processing,\n\n45\n\n(11), 2673\u20132681.\n\n396\n\nschwenk, h. (2007). continuous space language models. computer speech and language,\n\n21, 492\u2013518. 469\n\nschwenk, h. (2010). continuous space language models for statistical machine translation.\n\nthe prague bulletin of mathematical linguistics,\n\n93\n\n, 137\u2013146.\n\n476\n\nschwenk, h. (2014). cleaned subset of wmt \u201914 dataset. 21\n\nschwenk, h. and bengio, y. (1998). training methods for adaptive boosting of neural net-\nworks. in m. jordan, m. kearns, and s. solla, editors, advances in neural information\nprocessing systems 10 (nips\u201997), pages 647\u2013653. mit press. 257\n\nschwenk,\u00a0h. and gauvain,\u00a0j.-l. (2002). connectionist language modeling\u00a0for large\nvocabulary continuous speech recognition. in international conference on acoustics,\nspeech and signal processing (icassp), pages 765\u2013768, orlando, florida. 469\n\nschwenk, h., costa-juss\u00e0, m. r., and fonollosa, j. a. r. (2006). continuous space\nin international workshop on spoken\n\nlanguage models for the iwslt 2006 task.\nlanguage translation, pages 166\u2013173. 476\n\nseide, f., li, g., and yu, d. (2011). conversational speech transcription using context-\n\ndependent deep neural networks. in interspeech 2011 , pages 437\u2013440. 23\n\nsejnowski, t. (1987). higher-order boltzmann machines. in aip conference proceedings\n151 on neural networks for computing, pages 398\u2013403. american institute of physics\ninc. 688\n\n769\n\n "}, {"Page_number": 785, "text": "bibliography\n\nseries, p., reichert, d. p., and storkey, a. j. (2010). hallucinations in charles bonnet\nsyndrome induced by homeostasis: a deep boltzmann machine model. in advances in\nneural information processing systems, pages 2020\u20132028. 668\n\nsermanet, p., chintala, s., and lecun, y. (2012). convolutional neural networks applied\n\nto house numbers digit classification.\n\ncorr abs/1204.3968 459\n\n,\n\n.\n\nsermanet, p., kavukcuoglu, k., chintala, s., and lecun, y. (2013). pedestrian detection\nwith unsupervised multi-stage feature learning. in proc. international conference on\ncomputer vision and pattern recognition (cvpr\u201913). ieee.\n\n,23 200\n\nshilov, g. (1977). linear algebra. dover books on mathematics series. dover publications.\n\n31\n\nsiegelmann, h. (1995). computation beyond the turing limit. science, 268(5210),\n\n545\u2013548. 380\n\nsiegelmann, h. and sontag, e. (1991). turing computability with neural nets. applied\n\nmathematics letters,\n\n4\n\n(6), 77\u201380.\n\n380\n\nsiegelmann, h. t. and sontag, e. d. (1995). on the computational power of neural nets.\n\njournal of computer and systems sciences,\n\n50\n\n(1), 132\u2013150.\n\n380 406\n\n,\n\nsietsma, j. and dow, r. (1991). creating artificial neural networks that generalize. neural\n\nnetworks,\n\n4\n\n(1), 67\u201379.\n\n241\n\nsimard, d., steinkraus, p. y., and platt, j. c. (2003). best practices for convolutional\n\nneural networks. in\n\nicdar\u20192003 372\n\n.\n\nsimard, p. and graf, h. p. (1994). backpropagation without multiplication. in advances\n\nin neural information processing systems, pages 232\u2013239. 454\n\nsimard, p., victorri, b., lecun, y., and denker, j. (1992). tangent prop - a formalism\nnips\u20191991 269 271 357\n\nfor specifying selected invariances in an adaptive network. in\n\n.\n\n,\n\n,\n\nsimard, p. y., lecun, y., and denker, j. (1993). efficient pattern recognition using a\n\nnew transformation distance. in\n\nnips\u201992 269\n\n.\n\nsimard, p. y., lecun, y. a., denker, j. s., and victorri, b. (1998). transformation\ninvariance in pattern recognition \u2014 tangent distance and tangent propagation. lecture\nnotes in computer science,\n\n1524 269\n\n.\n\nsimons, d. j. and levin, d. t. (1998). failure to detect changes to people during a\n\nreal-world interaction. psychonomic bulletin & review,\n\n5\n\n(4), 644\u2013649.\n\n546\n\nsimonyan, k. and zisserman, a. (2015). very deep convolutional networks for large-scale\n\nimage recognition. in\n\niclr 324\n\n.\n\n770\n\n "}, {"Page_number": 786, "text": "bibliography\n\nsj\u00f6berg, j. and ljung, l. (1995). overtraining, regularization and searching for a minimum,\nwith application to neural networks. international journal of control, 62(6), 1391\u20131407.\n249\n\nskinner, b. f. (1958). reinforcement today. american psychologist,\n\n13\n\n, 94\u201399.\n\n329\n\nsmolensky, p. (1986). information processing in dynamical systems: foundations of\nharmony theory. in d. e. rumelhart and j. l. mcclelland, editors, parallel distributed\n574 590 658\nprocessing, volume 1, chapter 6, pages 194\u2013281. mit press, cambridge.\n\n,\n\n,\n\nsnoek, j., larochelle, h., and adams, r. p. (2012). practical bayesian optimization of\n\nmachine learning algorithms. in\n\nnips\u20192012 439\n\n.\n\nsocher, r., huang, e. h., pennington, j., ng, a. y., and manning, c. d. (2011a). dynamic\n.\nnips\u20192011\n\npooling and unfolding recursive autoencoders for paraphrase detection. in\n401 403\n\n,\n\nsocher, r., manning, c., and ng, a. y. (2011b). parsing natural scenes and natural lan-\nguage with recursive neural networks. in proceedings of the twenty-eighth international\nconference on machine learning (icml\u20192011). 401\n\nsocher,\u00a0r.,\u00a0pennington,\u00a0j.,\u00a0huang,\u00a0e. h.,\u00a0ng,\u00a0a. y., and manning,\u00a0c. d. (2011c).\nin\n\nsemi-supervised recursive autoencoders\u00a0for predicting sentiment distributions.\nemnlp\u20192011 . 401\n\nsocher, r., perelygin, a., wu, j. y., chuang, j., manning, c. d., ng, a. y., and potts,\nc. (2013a). recursive deep models for semantic compositionality over a sentiment\ntreebank. in\n\nemnlp\u20192013 401 403\n\n.\n\n,\n\nsocher, r., ganjoo, m., manning, c. d., and ng, a. y. (2013b). zero-shot learning through\ncross-modal transfer. in 27th annual conference on neural information processing\nsystems (nips 2013). 542\n\nsohl-dickstein, j., weiss, e. a., maheswaranathan, n., and ganguli, s. (2015). deep\n\nunsupervised learning using nonequilibrium thermodynamics.\n\n,717 718\n\nsohn, k., zhou, g., and lee, h. (2013). learning and selecting features jointly with\n\npoint-wise gated boltzmann machines. in\n\nicml\u20192013 689\n\n.\n\nsolomonoff, r. j. (1989). a system for incremental learning based on algorithmic proba-\n\nbility. 329\n\nsontag, e. d. (1998). vc dimension of neural networks. nato asi series f computer\n\nand systems sciences,\n\n168\n\n, 69\u201396.\n\n550 554\n\n,\n\nsontag, e. d. and sussman, h. j. (1989). backpropagation can give rise to spurious local\n\nminima even for networks without hidden layers.\n\n771\n\ncomplex systems 3\n\n,\n\n, 91\u2013106.\n\n284\n\n "}, {"Page_number": 787, "text": "bibliography\n\nsparkes, b. (1996). the red and the black: studies in greek pottery. routledge. 1\n\nspitkovsky, v. i., alshawi, h., and jurafsky, d. (2010). from baby steps to leapfrog: how\n\n\u201cless is more\u201d in unsupervised dependency parsing. in hlt\u201910 . 329\n\nsquire, w. and trapp, g. (1998).\u00a0using complex variables to estimate derivatives of real\n\nfunctions. siam rev.,\n\n40\n\n(1), 110\u2013\u2013112.\n\n442\n\nsrebro, n. and shraibman, a. (2005). rank, trace-norm and max-norm. in proceedings of\nthe 18th annual conference on learning theory, pages 545\u2013560. springer-verlag. 239\n\nsrivastava, n. (2013). improving neural networks with dropout. master\u2019s thesis, u.\n\ntoronto. 538\n\nsrivastava, n. and salakhutdinov, r. (2012). multimodal learning with deep boltzmann\n\nmachines. in\n\nnips\u20192012 544\n\n.\n\nsrivastava, n., salakhutdinov, r. r., and hinton, g. e. (2013). modeling documents with\n\ndeep boltzmann machines. arxiv preprint arxiv:1309.6865 . 665\n\nsrivastava, n., hinton, g., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2014).\ndropout: a simple way to prevent neural networks from overfitting. journal of machine\nlearning research,\n\n257 263 264 265 674\n\n, 1929\u20131958.\n\n15\n\n,\n\n,\n\n,\n\n,\n\nsrivastava, r. k., greff, k., and schmidhuber, j. (2015). highway networks.\n\narxiv:1505.00387 . 327\n\nsteinkrau, d., simard, p. y., and buck, i. (2005). using gpus for machine learning\nalgorithms. 2013 12th international conference on document analysis and recognition,\n0, 1115\u20131119. 448\n\nstoyanov, v., ropson, a., and eisner, j. (2011). empirical risk minimization of graphical\nmodel parameters given approximate inference, decoding, and model structure. in\nproceedings of the 14th international conference on artificial intelligence and statistics\n(aistats), volume 15 of jmlr workshop and conference proceedings, pages 725\u2013733,\nfort lauderdale. supplementary material (4 pages) also available.\n\n,676 700\n\nsukhbaatar, s., szlam, a., weston, j., and fergus, r. (2015). weakly supervised memory\n\nnetworks. arxiv preprint arxiv:1503.08895 . 421\n\nsupancic, j. and ramanan, d. (2013). self-paced learning for long-term tracking. in\n\ncvpr\u20192013 . 329\n\nsussillo, d. (2014). random walks: training very deep nonlinear feed-forward networks\n\nwith smart initialization.\n\ncorr abs/1412.6558 290 303 305 405\n\n,\n\n.\n\n,\n\n,\n\n,\n\nsutskever, i. (2012). training recurrent neural networks. ph.d. thesis, department of\n\ncomputer science, university of toronto.\n\n,408 415\n\n772\n\n "}, {"Page_number": 788, "text": "bibliography\n\nsutskever, i. and hinton, g. e. (2008). deep narrow sigmoid belief networks are universal\n\napproximators. neural computation,\n\n20\n\n(11), 2629\u20132636.\n\n695\n\nsutskever, i. and tieleman, t. (2010). on the convergence properties of contrastive\ndivergence. in y. w. teh and m. titterington, editors, proc. of the international\nconference on artificial intelligence and statistics (aistats), volume 9, pages 789\u2013795.\n615\n\nsutskever, i., hinton, g., and taylor, g. (2009). the recurrent temporal restricted\n\nboltzmann machine. in\n\nnips\u20192008 688\n\n.\n\nsutskever, i., martens, j., and hinton, g. e. (2011). generating text with recurrent\n\nneural networks. in\n\nicml\u20192011\n\n, pages 1017\u20131024.\n\n480\n\nsutskever,\u00a0i., martens, j., dahl,\u00a0g., and hinton, g. (2013). on the importance of\n\ninitialization and momentum in deep learning. in\n\nicml 300 408 415\n\n.\n\n,\n\n,\n\nsutskever, i., vinyals, o., and le, q. v. (2014). sequence to sequence learning with\n\nneural networks. in\n\nnips\u20192014, arxiv:1409.3215 25 101 397 411 414 477 478\n\n.\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nsutton, r. and barto, a. (1998). reinforcement learning: an introduction. mit press.\n\n106\n\nsutton, r. s., mcallester, d., singh, s., and mansour, y. (2000). policy gradient methods\n, pages 1057\u2013\n\nfor reinforcement learning with function approximation. in\n\u20131063. mit press. 693\n\nnips\u20191999\n\nswersky, k., ranzato, m., buchman, d., marlin, b., and de freitas, n. (2011). on\n516\n\nautoencoders and score matching for energy based models. in\n\nicml\u20192011\n\n. acm.\n\nswersky, k., snoek, j., and adams, r. p. (2014). freeze-thaw bayesian optimization.\n\narxiv preprint arxiv:1406.3896 . 439\n\nszegedy, c., liu, w., jia, y., sermanet, p., reed, s., anguelov, d., erhan, d., vanhoucke,\nv., and rabinovich, a. (2014a). going deeper with convolutions. technical report,\narxiv:1409.4842.\n\n24 27 200 257 267 327 348\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nszegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d., goodfellow, i. j., and\niclr abs/1312.6199.\n\nfergus, r. (2014b). intriguing properties of neural networks.\n267 268 270\n\n,\n\n,\n\n,\n\nszegedy, c., vanhoucke, v., ioffe, s., shlens, j., and wojna, z. (2015). rethinking the\n\ninception architecture for computer vision.\n\narxiv e-prints 244 323\n\n.\n\n,\n\ntaigman, y., yang, m., ranzato, m., and wolf, l. (2014). deepface: closing the gap to\n\nhuman-level performance in face verification. in\n\ncvpr\u20192014 100\n\n.\n\ntandy, d. w. (1997). works and days: a translation and commentary for the social\n\nsciences. university of california press. 1\n\n773\n\n "}, {"Page_number": 789, "text": "bibliography\n\ntang, y. and eliasmith, c. (2010). deep networks for robust visual recognition. in\nproceedings of the 27th international conference on machine learning, june 21-24,\n2010, haifa, israel. 241\n\ntang, y., salakhutdinov, r., and hinton, g. (2012). deep mixtures of factor analysers.\n\narxiv preprint arxiv:1206.4635 . 492\n\ntaylor, g. and hinton, g. (2009). factored conditional restricted boltzmann machines\nfor modeling motion style.\nin l. bottou and m. littman,\u00a0editors, proceedings of\nthe twenty-sixth international conference on machine learning (icml\u201909), pages\n1025\u20131032, montreal, quebec, canada. acm. 688\n\ntaylor, g., hinton, g. e., and roweis, s. (2007). modeling human motion using binary\nlatent variables. in b. sch\u00f6lkopf, j. platt, and t. hoffman, editors, advances in neural\ninformation processing systems 19 (nips\u201906), pages 1345\u20131352. mit press, cambridge,\nma. 687\n\nteh, y., welling, m., osindero, s., and hinton, g. e. (2003). energy-based models\nfor sparse overcomplete representations. journal of machine learning research, 4,\n1235\u20131260. 494\n\ntenenbaum, j., de silva, v., and langford, j. c. (2000). a global geometric framework\n163 521 536\n\nfor nonlinear dimensionality reduction. science,\n\n(5500), 2319\u20132323.\n\n290\n\n,\n\n,\n\ntheis, l., van den oord, a., and bethge, m. (2015). a note on the evaluation of generative\n\nmodels. arxiv:1511.01844.\n\n,699 721\n\nthompson, j., jain, a., lecun, y., and bregler, c. (2014). joint training of a convolutional\n\nnetwork and a graphical model for human pose estimation. in\n\nnips\u20192014 361\n\n.\n\nthrun, s. (1995). learning to play the game of chess. in\n\nnips\u20191994 269\n\n.\n\ntibshirani, r. j. (1995). regression shrinkage and selection via the lasso. journal of the\n\nroyal statistical society b,\n\n58\n\n, 267\u2013288.\n\n236\n\ntieleman, t. (2008). training restricted boltzmann machines using approximations to\nthe likelihood gradient. in w. w. cohen, a. mccallum, and s. t. roweis, editors, pro-\nceedings of the twenty-fifth international conference on machine learning (icml\u201908),\npages 1064\u20131071. acm. 615\n\ntieleman, t. and hinton, g. (2009). using fast weights to improve persistent contrastive\ndivergence. in l. bottou and m. littman, editors, proceedings of the twenty-sixth\ninternational conference on machine learning (icml\u201909), pages 1033\u20131040. acm.\n617\n\ntipping, m. e. and bishop, c. m. (1999). probabilistic principal components analysis.\n\njournal of the royal statistical society b,\n\n61\n\n(3), 611\u2013622.\n\n494\n\n774\n\n "}, {"Page_number": 790, "text": "bibliography\n\ntorralba, a., fergus, r., and weiss, y. (2008). small codes and large databases for\nrecognition. in proceedings of the computer vision and pattern recognition conference\n(cvpr\u201908), pages 1\u20138. 528\n\ntouretzky, d. s. and minton, g. e. (1985). symbols among the neurons: details of\na connectionist inference architecture.\u00a0in proceedings of the 9th international joint\nconference on artificial intelligence - volume 1 , ijcai\u201985, pages 238\u2013243, san francisco,\nca, usa. morgan kaufmann publishers inc. 17\n\ntu, k. and honavar, v. (2011).\u00a0on the utility of curricula in unsupervised learning of\n\nprobabilistic grammars. in\n\nijcai\u20192011 329\n\n.\n\nturaga, s. c., murray, j. f., jain, v., roth, f., helmstaedter, m., briggman, k., denk,\nw., and seung, h. s. (2010). convolutional networks can learn to generate affinity\ngraphs for image segmentation. neural computation,\n\n(2), 511\u2013538.\n\n360\n\n22\n\nturian, j., ratinov, l., and bengio, y. (2010). word representations: a simple and\ngeneral method for semi-supervised learning. in proc. acl\u20192010, pages 384\u2013394. 538\n\nt\u00f6scher, a., jahrer, m., and bell, r. m. (2009).\u00a0the bigchaos solution to the netflix\n\ngrand prize. 482\n\nuria, b., murray, i., and larochelle, h. (2013). rnade: the real-valued neural autoregres-\n\nsive density-estimator. in\n\nnips\u20192013 711\n\n.\n\nvan den o\u00f6rd, a., dieleman, s., and schrauwen, b. (2013). deep content-based music\n\nrecommendation. in\n\nnips\u20192013 483\n\n.\n\nvan der maaten, l. and hinton, g. e. (2008). visualizing data using t-sne. j. machine\n\nlearning res.,\n\n9 480 522\n\n.\n\n,\n\nvanhoucke, v., senior, a., and mao, m. z. (2011). improving the speed of neural networks\non cpus. in proc. deep learning and unsupervised feature learning nips workshop.\n447 455\n\n,\n\nvapnik, v. n. (1982). estimation of dependences based on empirical data. springer-\n\nverlag, berlin. 114\n\nvapnik, v. n. (1995). the nature of statistical learning theory. springer, new york.\n\n114\n\nvapnik, v. n. and chervonenkis, a. y. (1971). on the uniform convergence of relative\nfrequencies of events to their probabilities. theory of probability and its applications,\n16, 264\u2013280. 114\n\nvincent, p. (2011).\u00a0a connection between score matching and denoising autoencoders.\n\nneural computation,\n\n23\n\n(7).\n\n516 518 714\n\n,\n\n,\n\n775\n\n "}, {"Page_number": 791, "text": "bibliography\n\nvincent, p. and bengio, y. (2003). manifold parzen windows. in\n\nnips\u20192002\n\n. mit press.\n\n523\n\nvincent, p., larochelle, h., bengio, y., and manzagol, p.-a. (2008). extracting and\n\ncomposing robust features with denoising autoencoders. in\n\nicml 2008 241 518\n\n.\n\n,\n\nvincent, p., larochelle, h., lajoie, i., bengio, y., and manzagol, p.-a. (2010). stacked\ndenoising autoencoders: learning useful representations in a deep network with a local\ndenoising criterion. j. machine learning res.,\n\n.11 518\n\nvincent, p., de br\u00e9bisson, a., and bouthillier, x. (2015). efficient exact gradient update\nfor training deep networks with very large sparse targets. in c. cortes, n. d. lawrence,\nd. d. lee, m. sugiyama, and r. garnett, editors, advances in neural information\nprocessing systems 28 , pages 1108\u20131116. curran associates, inc. 468\n\nvinyals,\u00a0o.,\u00a0kaiser,\u00a0l.,\u00a0koo,\u00a0t.,\u00a0petrov,\u00a0s.,\u00a0sutskever,\u00a0i.,\u00a0and hinton,\u00a0g. (2014a).\n\ngrammar as a foreign language. technical report, arxiv:1412.7449. 411\n\nvinyals, o., toshev, a., bengio, s., and erhan, d. (2014b). show and tell: a neural image\n\ncaption generator. arxiv 1411.4555. 411\n\nvinyals, o., fortunato, m., and jaitly, n. (2015a). pointer networks. arxiv preprint\n\narxiv:1506.03134 . 421\n\nvinyals, o., toshev, a., bengio, s., and erhan, d. (2015b). show and tell: a neural image\n\ncaption generator. in\n\ncvpr\u20192015\n\n. arxiv:1411.4555.\n\n102\n\nviola, p. and jones, m. (2001). robust real-time object detection. in international\n\njournal of computer vision. 452\n\nvisin, f., kastner, k., cho, k., matteucci, m., courville, a., and bengio, y. (2015).\nrenet: a recurrent neural network based alternative to convolutional networks. arxiv\npreprint arxiv:1505.00393 . 397\n\nvon melchner, l., pallas, s. l., and sur, m. (2000). visual behaviour mediated by retinal\n\nprojections directed to the auditory pathway. nature,\n\n404\n\n(6780), 871\u2013876.\n\n16\n\nwager, s., wang, s., and liang, p. (2013). dropout training as adaptive regularization.\n\nin advances in neural information processing systems 26 , pages 351\u2013359. 264\n\nwaibel, a., hanazawa, t., hinton, g. e., shikano, k., and lang, k. (1989). phoneme\nrecognition using time-delay neural networks. ieee transactions on acoustics, speech,\nand signal processing,\n\n375 456 462\n\n, 328\u2013339.\n\n37\n\n,\n\n,\n\nwan, l., zeiler, m., zhang, s., lecun, y., and fergus, r. (2013). regularization of neural\n\nnetworks using dropconnect. in\n\nicml\u20192013 265\n\n.\n\nwang, s. and manning, c. (2013). fast dropout training. in\n\nicml\u20192013 264\n\n.\n\n776\n\n "}, {"Page_number": 792, "text": "bibliography\n\nwang, z., zhang, j., feng, j., and chen, z. (2014a). knowledge graph and text jointly\n\nembedding. in proc. emnlp\u20192014 . 487\n\nwang, z., zhang, j., feng, j., and chen, z. (2014b).\u00a0knowledge graph embedding by\n\ntranslating on hyperplanes. in proc. aaai\u20192014 . 487\n\nwarde-farley, d., goodfellow, i. j., courville, a., and bengio, y. (2014). an empirical\n\nanalysis of dropout in piecewise linear networks. in\n\niclr\u20192014 261 265 266\n\n.\n\n,\n\n,\n\nwawrzynek, j., asanovic, k., kingsbury, b., johnson, d., beck, j., and morgan, n.\n\n(1996). spert-ii: a vector microprocessor system.\n\ncomputer 29\n\n,\n\n(3), 79\u201386.\n\n454\n\nweaver, l. and tao, n. (2001). the optimal reward baseline for gradient-based reinforce-\n\nment learning. in proc. uai\u20192001 , pages 538\u2013545. 693\n\nweinberger, k. q. and saul, l. k. (2004). unsupervised learning of image manifolds by\n\nsemidefinite programming. in\n\ncvpr\u20192004\n\n, pages 988\u2013995.\n\n163 522\n\n,\n\nweiss,\u00a0y.,\u00a0torralba,\u00a0a.,\u00a0and fergus,\u00a0r. (2008). spectral\u00a0hashing.\n\nin\n\nnips\n\n, pages\n\n1753\u20131760. 528\n\nwelling, m., zemel, r. s., and hinton, g. e. (2002). self supervised boosting. in advances\n\nin neural information processing systems, pages 665\u2013672. 705\n\nwelling,\u00a0m., hinton, g. e.,\u00a0and osindero,\u00a0s. (2003a). learning sparse topographic\n\nrepresentations with products of student-t distributions. in\n\nnips\u20192002 682\n\n.\n\nwelling, m., zemel, r., and hinton, g. e. (2003b). self-supervised boosting. in s. becker,\ns. thrun, and k. obermayer, editors, advances in neural information processing\nsystems 15 (nips\u201902), pages 665\u2013672. mit press. 626\n\nwelling, m., rosen-zvi, m., and hinton, g. e. (2005). exponential family harmoniums\nwith an application to information retrieval. in l. saul, y. weiss, and l. bottou,\neditors, advances in neural information processing systems 17 (nips\u201904), volume 17,\ncambridge, ma. mit press. 678\n\nwerbos,\u00a0p. j. (1981). applications of advances in nonlinear sensitivity analysis.\n\nin\n\nproceedings of the 10th ifip conference, 31.8 - 4.9, nyc , pages 762\u2013770. 225\n\nweston, j., bengio, s., and usunier, n. (2010). large scale image annotation: learning to\n\nrank with joint word-image embeddings. machine learning,\n\n81\n\n(1), 21\u201335.\n\n403\n\nweston,\u00a0j.,\u00a0chopra,\u00a0s.,\u00a0and bordes,\u00a0a. (2014). memory networks. arxiv preprint\n\narxiv:1410.3916 .\n\n,421 488\n\nwidrow, b. and hoff, m. e. (1960). adaptive switching circuits. in 1960 ire wescon\n\nconvention record, volume 4, pages 96\u2013104. ire, new york.\n\n777\n\n15 21 24 27\n\n,\n\n,\n\n,\n\n "}, {"Page_number": 793, "text": "bibliography\n\nwikipedia (2015). list of animals by number of neurons \u2014 wikipedia, the free encyclopedia.\n\n[online; accessed 4-march-2015].\n\n,24 27\n\nwilliams, c. k. i. and agakov, f. v. (2002). products of gaussians and probabilistic\n\nminor component analysis. neural computation,\n\n14(5)\n\n, 1169\u20131182.\n\n684\n\nwilliams, c. k. i. and rasmussen, c. e. (1996). gaussian processes for regression. in\nd. touretzky, m. mozer, and m. hasselmo, editors, advances in neural information\nprocessing systems 8 (nips\u201995), pages 514\u2013520. mit press, cambridge, ma. 142\n\nwilliams, r. j. (1992). simple statistical gradient-following algorithms connectionist\n\nreinforcement learning. machine learning,\n\n8\n\n, 229\u2013256.\n\n690 691\n\n,\n\nwilliams, r. j. and zipser, d. (1989). a learning algorithm for continually running fully\n\nrecurrent neural networks. neural computation,\n\n1\n\n, 270\u2013280.\n\n222\n\nwilson, d. r. and martinez, t. r. (2003). the general inefficiency of batch training for\n\ngradient descent learning. neural networks,\n\n16\n\n(10), 1429\u20131451.\n\n279\n\nwilson, j. r. (1984). variance reduction techniques for digital simulation. american\n\njournal of mathematical and management sciences,\n\n4\n\n(3), 277\u2013\u2013312.\n\n692\n\nwiskott, l. and sejnowski, t. j. (2002). slow feature analysis: unsupervised learning of\n\ninvariances. neural computation,\n\n14\n\n(4), 715\u2013770.\n\n497\n\nwolpert, d. and macready, w. (1997). no free lunch theorems for optimization. ieee\n\ntransactions on evolutionary computation,\n\n1\n\n, 67\u201382.\n\n293\n\nwolpert, d. h. (1996). the lack of a priori distinction between learning algorithms. neural\n\ncomputation,\n\n8\n\n(7), 1341\u20131390.\n\n116\n\nwu, r., yan, s., shan, y., dang, q., and sun, g. (2015). deep image: scaling up image\n\nrecognition. arxiv:1501.02876. 450\n\nwu, z. (1997). global continuation for distance geometry problems. siam journal of\n\noptimization,\n\n7\n\n, 814\u2013836.\n\n328\n\nxiong, h. y., barash, y., and frey, b. j. (2011). bayesian prediction of tissue-regulated\nbioinformatics 27(18), 2554\u20132562.\n\nsplicing using rna sequence and cellular context.\n264\n\n,\n\nxu, k., ba, j. l., kiros, r., cho, k., courville, a., salakhutdinov, r., zemel, r. s., and\nbengio, y. (2015). show, attend and tell: neural image caption generation with visual\nattention. in\n\nicml\u20192015, arxiv:1502.03044 102 411 693\n\n.\n\n,\n\n,\n\nyildiz, i. b., jaeger, h., and kiebel, s. j. (2012). re-visiting the echo state property.\n\nneural networks,\n\n35\n\n, 1\u20139.\n\n407\n\n778\n\n "}, {"Page_number": 794, "text": "bibliography\n\nyosinski, j., clune, j., bengio, y., and lipson, h. (2014). how transferable are features\n\nin deep neural networks? in\n\nnips\u20192014 324 539\n\n.\n\n,\n\nyounes, l. (1998). on the convergence of markovian stochastic algorithms with rapidly\ndecreasing ergodicity rates. in stochastics and stochastics models, pages 177\u2013228. 615\n\nyu, d., wang, s., and\u00a0deng,\u00a0l.\u00a0(2010). sequential\u00a0labeling\u00a0using\u00a0deep-structured\nconditional random fields. ieee journal of selected topics in signal processing. 324\n\nzaremba, w. and sutskever, i. (2014). learning to execute. arxiv 1410.4615. 330\n\nzaremba, w. and sutskever, i. (2015). reinforcement learning neural turing machines.\n\narxiv:1505.00521 . 422\n\nzaslavsky, t. (1975). facing up to arrangements: face-count formulas for partitions\nof space by hyperplanes. number no. 154 in memoirs of the american mathematical\nsociety. american mathematical society. 553\n\nzeiler, m. d. and fergus, r. (2014). visualizing and understanding convolutional networks.\n\nin\n\neccv\u201914 6\n\n.\n\nzeiler, m. d., ranzato, m., monga, r., mao, m., yang, k., le, q., nguyen, p., senior,\na., vanhoucke, v., dean, j., and hinton, g. e. (2013).\u00a0on rectified linear units for\nspeech processing. in\n\nicassp 2013 462\n\n.\n\nzhou, b., khosla, a., lapedriza, a., oliva, a., and torralba, a. (2015).\u00a0object detectors\n\nemerge in deep scene cnns. iclr\u20192015, arxiv:1412.6856. 554\n\nzhou, j. and troyanskaya, o. g. (2014). deep supervised and convolutional generative\n\nstochastic network for protein secondary structure prediction. in\n\nicml\u20192014 717\n\n.\n\nzhou, y. and chellappa, r. (1988). computation of optical flow using a neural network.\nin neural networks, 1988., ieee international conference on, pages 71\u201378. ieee. 340\n\nz\u00f6hrer, m. and pernkopf, f. (2014). general stochastic networks for classification. in\n\nnips\u20192014 . 717\n\n779\n\n "}, {"Page_number": 795, "text": "index\n\n0-1 loss,\n\n,104 276\n\n,308 428\n\nabsolute value rectification, 192\naccuracy, 426\nactivation function, 170\nactive constraint, 95\nadagrad, 307\nadaline, see adaptive linear element\nadam,\nadaptive linear element,\nadversarial example, 267\nadversarial training,\n,\naffine, 110\nais, see annealed importance sampling\nalmost everywhere, 71\nalmost sure convergence, 130\nancestral sampling,\n,583 598\nann, see artificial neural network\nannealed importance sampling,\u00a0\n\n268 270 533\n\n15 24 27\n\n,\n\n,\n\n,\n\n,\n\n628 670\n,\n\n719\n\napproximate bayesian computation, 718\napproximate inference, 586\nartificial intelligence, 1\nartificial neural network,\u00a0see neural net-\n\nwork\n\nbag of words, 474\nbagging, 255\nbatch normalization,\nbayes error, 117\nbayes\u2019 rule, 70\nbayesian hyperparameter optimization, 439\nbayesian\u00a0network,\u00a0see directed\u00a0graphical\n\n,266 428\n\nmodel\n\n,124 229\n\nbayesian probability, 55\nbayesian statistics, 135\nbelief network, see directed graphical model\nbernoulli distribution, 62\nbfgs, 316\nbias,\nbias parameter, 110\nbiased importance sampling, 596\nbigram, 465\nbinary relation, 485\nblock gibbs sampling, 602\nboltzmann distribution, 573\nboltzmann machine,\nbptt, see back-propagation through time\nbroadcasting, 34\nburn-in, 600\n\n,573 656\n\nasr, see automatic speech recognition\nasymptotically unbiased, 124\naudio,\nautoencoder,\nautomatic speech recognition, 461\n\n102 361 461\n,\n\n,\n,\n4 357 505\n\n,\n\nback-propagation, 203\nback-propagation through time, 385\nbackprop, see back-propagation\n\ncae, see contractive autoencoder\ncalculus of variations, 179\ncategorical distribution, see multinoulli dis-\n\ntribution\n\ncd, see contrastive divergence\ncentering trick (dbm), 675\ncentral limit theorem, 63\nchain rule (calculus), 206\nchain rule of probability, 59\n\n780\n\n "}, {"Page_number": 796, "text": "index\n\nchess, 2\nchord, 582\nchordal graph, 582\nclass-based language models, 466\nclassical dynamical system, 376\nclassification, 100\nclique potential, see factor (graphical model)\ncnn, see convolutional neural network\ncollaborative filtering, 481\ncollider, see explaining away\ncolor images, 361\ncomplex cell, 366\ncomputational graph, 204\ncomputer vision, 455\nconcept drift, 541\ncondition number, 279\nconditional computation, see dynamic struc-\n\nture\n\n,xiii 60\n\n,93 237\n\n,130 516\n\nconditional independence,\nconditional probability, 59\nconditional rbm, 687\nconnectionism,\n,17 446\nconnectionist temporal classification, 463\nconsistency,\nconstrained optimization,\ncontent-based addressing, 422\ncontent-based recommender systems, 483\ncontext-specific independence, 576\ncontextual bandits, 483\ncontinuation methods, 328\ncontractive autoencoder, 524\ncontrast, 457\ncontrastive divergence,\nconvex optimization, 141\nconvolution,\nconvolutional network, 16\nconvolutional neural network,\n\n291 613 674\n\n,252 331,\n\n,331 685\n\n,\n\n,\n\n,428\n\n463\n\n,322 673\n\ncoordinate descent,\ncorrelation, 61\ncost function, see objective function\ncovariance,\ncovariance matrix, 62\ncoverage, 427\n\n,xiii 61\n\ncritical temperature, 606\ncross-correlation, 333\ncross-entropy,\n,75 132\ncross-validation, 122\nctc, see connectionist temporal classifica-\n\ntion\n\ncurriculum learning, 329\ncurse of dimensionality, 154\ncyc, 2\n\n,111 131\n\nd-separation, 575\ndae, see denoising autoencoder\ndata generating distribution,\ndata generating process, 111\ndata parallelism, 450\ndataset, 105\ndataset augmentation,\ndbm, see deep boltzmann machine\ndcgan,\n,\ndecision tree,\ndecoder, 4\ndeep belief network,\n\n554 555 703\n,145 551\n\n,270 460\n\n,\n\n,\n\n27 532 634 659 662\n,\n\n,\n\n,\n\n,\n\n686 694\n\n,\ndeep blue, 2\ndeep boltzmann machine,\n\n,\n\n,\n\n,\n\n24 27 532 634\n,\n,\n,167 428\n\n655 659 665 674 686\n\n,\n\n,\n\n,\n\n,2 5\n\n,513 691\n\ndeep feedforward network,\ndeep learning,\ndenoising autoencoder,\ndenoising score matching, 622\ndensity estimation, 103\nderivative,\n,xiii 83\ndesign matrix, 106\ndetector layer, 340\ndeterminant, xii\ndiagonal matrix, 41\ndifferential entropy,\ndirac delta function, 65\ndirected graphical model,\ndirectional derivative, 85\ndiscriminative fine-tuning, see supervised\n\n77 510 566 694\n\n,74 649\n\n,\n\n,\n\n,\n\nfine-tuning\n\ndiscriminative rbm, 688\ndistributed representation,\ndomain adaptation, 539\n\n781\n\n17 150 549\n\n,\n\n,\n\n "}, {"Page_number": 797, "text": "index\n\ndot product,\n,34 141\ndouble backprop, 270\ndoubly block circulant matrix, 334\ndream sleep,\ndropconnect, 265\ndropout,\n,\ndynamic structure,\n\n,\n,451 452\n\n,612 655\n\n257 428 433 434 674 691\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n246 249 272 273 428\n\ne-step, 637\nearly stopping,\n,\nebm, see energy-based model\necho state network,\n24 27 406\neffective capacity, 114\neigendecomposition, 42\neigenvalue, 42\neigenvector, 42\nelbo, see evidence lower bound\nelement-wise product, see hadamard prod-\n\n,\n\nuct, see hadamard product\n\n572 598 656 665\n\n,\n\n,\n\n,\n\nem, see expectation maximization\nembedding, 519\nempirical distribution, 66\nempirical risk, 276\nempirical risk minimization, 276\nencoder, 4\nenergy function, 572\nenergy-based model,\nensemble methods, 255\nepoch, 247\nequality constraint, 94\nequivariance, 339\nerror function, see objective function\nesn, see echo state network\neuclidean norm, 39\neuler-lagrange equation, 649\nevidence lower bound,\nexample, 99\nexpectation, 60\nexpectation maximization, 637\nexpected value, see expectation\nexplaining away,\n577 634 647\nexploitation, 484\nexploration, 484\nexponential distribution, 65\n\n,636 663\n\n,\n\n,\n\nf-score, 426\nfactor (graphical model), 570\nfactor analysis, 493\nfactor graph, 582\nfactors of variation, 4\nfeature, 99\nfeature selection, 236\nfeedforward neural network, 167\nfine-tuning, 324\nfinite differences, 442\nforget gate, 306\nforward propagation, 203\nfourier transform,\nfovea, 367\nfpcd, 617\nfree energy,\nfreebase, 486\nfrequentist probability, 55\nfrequentist statistics, 135\nfrobenius norm, 46\nfully-visible bayes network, 707\nfunctional derivatives, 648\nfvbn, see fully-visible bayes network\n\n,574 682\n\n,361 363\n\ngabor function, 369\ngans, see generative adversarial networks\ngated recurrent unit, 428\ngaussian distribution, see normal distribu-\n\ntion\n\n,67 188\n\ngaussian kernel, 142\ngaussian mixture,\ngcn, see global contrast normalization\ngeneontology, 486\ngeneralization, 110\ngeneralized lagrange function, see general-\n\nized lagrangian\n\n,691 702\n\ngeneralized lagrangian, 94\ngenerative adversarial networks,\ngenerative moment matching networks, 705\ngenerator network, 695\ngibbs distribution, 571\ngibbs sampling,\nglobal contrast normalization, 457\ngpu, see graphics processing unit\ngradient, 84\n\n,584 602\n\n782\n\n "}, {"Page_number": 798, "text": "index\n\ngradient clipping,\ngradient descent,\ngraph, xii\ngraphical model, see structured probabilis-\n\n,289 417\n,83 85\n\ntic model\n\ngraphics processing unit, 447\ngreedy algorithm, 324\ngreedy layer-wise unsupervised pretraining,\n\n531\n\ngreedy supervised pretraining, 324\ngrid search, 435\n\nhadamard product,\nhard\nharmonium, see restricted boltzmann ma-\n\ntanh 196\n\n,xii 34\n\n,\n\nchine\n\nharmony theory, 574\nhelmholtz free energy, see evidence lower\n\nbound\n\n,xiii 87\n\nhessian, 223\nhessian matrix,\nheteroscedastic, 187\nhidden layer,\n,6 167\nhill climbing, 86\nhyperparameter optimization, 435\nhyperparameters,\nhypothesis space,\n\n,120 433\n,112 118\n\ni.i.d. assumptions,\nidentity matrix, 36\nilsvrc, see imagenet large-scale visual\n\n111 122 267\n\n,\n\n,\n\nrecognition challenge\n\nimagenet large-scale visual recognition\n\nchallenge, 23\n\nimmorality, 580\nimportance sampling,\nimportance weighted autoencoder, 700\nindependence,\nindependent and identically distributed, see\n\n595 627 700\n\n,xiii 60\n\n,\n\n,\n\ni.i.d. assumptions\n\nindependent component analysis, 494\nindependent subspace analysis, 496\ninequality constraint, 94\ninference,\n,\n\n,\n\n,\n\n,\n\n565 586 634 636 638 641 651\n,\n653\n\n,\n\n,\n\ninformation retrieval, 528\ninitialization, 301\nintegral, xiii\ninvariance, 343\nisotropic, 65\n\njacobian matrix,\njoint probability, 57\n\nxiii 72 86\n\n,\n\n,\n\n,365 549\n\n,143 551\n\nk-means,\nk-nearest neighbors,\nkarush-kuhn-tucker conditions,\nkarush\u2013kuhn\u2013tucker, 94\nkernel (convolution),\nkernel machine, 551\nkernel trick, 141\nkkt, see karush\u2013kuhn\u2013tucker\nkkt conditions, see karush-kuhn-tucker\n\n,332 333\n\n,95 237\n\nconditions\n\nkl divergence, see kullback-leibler diver-\n\ngence\n\nknowledge base,\nkrylov methods, 224\nkullback-leibler divergence,\n\n,2 486\n\n,xiii 74\n\n,\n\n,\n\n,94 649\n\n65 499 500\n\nlabel smoothing, 243\nlagrange multipliers,\nlagrangian, see generalized lagrangian\nlapgan, 704\nlaplace distribution,\nlatent variable, 67\nlayer (neural network), 167\nlcn, see local contrast normalization\nleaky relu, 192\nleaky units, 409\nlearning rate, 85\nline search,\n85 86 93\nlinear combination, 37\nlinear dependence, 38\nlinear factor models, 492\nlinear regression,\nlink prediction, 487\nlipschitz constant, 92\nlipschitz continuous, 92\nliquid state machine, 406\n\n107 110 140\n\n,\n\n,\n\n,\n\n,\n\n783\n\n "}, {"Page_number": 799, "text": "index\n\nlocal conditional probability distribution,\n\n567\n\nlocal contrast normalization, 459\nlogistic regression,\nlogistic sigmoid,\nlong short-term memory,\n\n,\n3 140 140\n,7 67\n\n,\n\n,\n\n,\n\n18 25 306 411,\n\n,\n\n428\n\nloop, 582\nloopy belief propagation, 588\nloss function, see objective function\nlp norm, 39\nlstm, see long short-term memory\n\nm-step, 637\nmachine learning, 2\nmachine translation, 101\nmain diagonal, 33\nmanifold, 160\nmanifold hypothesis, 161\nmanifold learning, 161\nmanifold tangent classifier, 270\nmap approximation,\n,138 508\nmarginal probability, 58\nmarkov chain, 598\nmarkov chain monte carlo, 598\nmarkov network, see undirected model\nmarkov random field, see undirected model\nmatrix,\nxi xii 32\nmatrix inverse, 36\nmatrix product, 34\nmax norm, 40\nmax pooling, 340\nmaximum likelihood, 131\nmaxout,\nmcmc, see markov chain monte carlo\n641 642 674\nmean field,\nmean squared error, 108\nmeasure theory, 71\nmeasure zero, 71\nmemory network,\nmethod of\u00a0steepest descent,\u00a0see gradient\n\n,192 428\n\n,419 421\n\n,\n\n,\n\n,\n\n,\n\ndescent\n\nminibatch, 279\nmissing inputs, 100\nmixing (markov chain), 604\n\n,\n\n,\n\n,453 551\n\n21 22 674\n\nmixture density networks, 188\nmixture distribution, 66\nmixture model,\n,188 513\nmixture of experts,\nmlp, see multilayer perception\nmnist,\nmodel averaging, 255\nmodel compression, 451\nmodel identifiability, 284\nmodel parallelism, 450\nmoment matching, 705\nmoore-penrose pseudoinverse,\nmoralized graph, 580\nmp-dbm, see multi-prediction dbm\nmrf (markov\u00a0random field), see undi-\n\n,45 240\n\nrected model\n\nmse, see mean squared error\nmulti-modal learning, 542\nmulti-prediction dbm, 676\nmulti-task learning,\nmultilayer perception, 5\nmultilayer perceptron, 27\nmultinomial distribution, 62\nmultinoulli distribution, 62\n\n,245 541\n\n473 609 611\n,\n16 24 27 368\n\nn-gram, 464\nnade, 710\nnaive bayes, 3\nnat, 73\nnatural image, 562\nnatural language processing, 464\nnearest neighbor regression, 115\nnegative definite, 89\n,\nnegative phase,\n,\nneocognitron,\n,\nnesterov momentum, 300\nnetflix grand prize,\nneural language model,\nneural network, 13\nneural turing machine, 421\nneuroscience, 15\nnewton\u2019s method,\nnlm, see neural language model\nnlp, see natural language processing\nno free lunch theorem, 116\n\n,466 479\n\n,256 482\n\n,89 310\n\n,\n\n784\n\n "}, {"Page_number": 800, "text": "index\n\n,xiv 39\n\nnoise-contrastive estimation, 623\nnon-parametric model, 114\nnorm,\n,\nnormal distribution,\nnormal equations,\n,\nnormalized initialization, 303\nnumerical differentiation, see finite differ-\n\n109 109 112 234\n\n63 64 125\n\n,\n,\n\n,\n\nences\n\nobject detection, 456\nobject recognition, 456\nobjective function, 82\nomp- ,k see orthogonal matching pursuit\none-shot learning, 541\noperation, 204\noptimization,\northodox statistics, see frequentist statistics\northogonal matching pursuit,\northogonal matrix, 42\northogonality, 41\noutput layer, 167\n\n,27 254\n\n,80 82\n\n,\n\n,\n\n251 336 374 376 389\n\nparallel distributed processing, 17\n,301 408\nparameter initialization,\nparameter sharing,\n,\n,\nparameter tying, see parameter sharing\nparametric model, 114\nparametric relu, 192\npartial derivative, 84\npartition function,\npca, see principal components analysis\npcd, see stochastic maximum likelihood\nperceptron,\npersistent contrastive divergence, see stochas-\n\n571 608 671\n\n,15 27\n\n,\n\n,\n\ntic maximum likelihood\n\nperturbation analysis, see reparametrization\n\ntrick\n\n,331 685\n\npoint estimator, 122\npolicy, 483\npooling,\npositive definite, 89\npositive phase,\n,\nprecision, 426\nprecision (of a normal distribution),\npredictive sparse decomposition, 526\n\n473 609 611 658 670\n\n,\n\n,\n\n,\n\n,63 65\n\npreprocessing, 456\npretraining,\nprimary visual cortex, 366\nprincipal components analysis,\n\n,324 531\n\n493 634\n\n,\n\n48 146 148\n,\n\n\u2013\n\n,\n\n,\n\n,\n\nprior probability distribution, 135\nprobabilistic max pooling, 685\nprobabilistic pca,\n493 494 635\nprobability density function, 58\nprobability distribution, 56\nprobability mass function, 56\nprobability mass function estimation, 103\nproduct of experts, 573\nproduct rule of probability, see chain rule\n\nof probability\n\npsd, see predictive sparse decomposition\npseudolikelihood, 618\n\nquadrature pair, 370\nquasi-newton condition, 316\nquasi-newton methods, 316\n\n,\n\n,\n\n171 192 428 510\n\nradial basis function, 196\nrandom search, 437\nrandom variable, 56\nratio matching, 621\nrbf, 196\nrbm, see restricted boltzmann machine\nrecall, 426\nreceptive field, 338\nrecommender systems, 481\nrectified linear unit,\nrecurrent network, 27\nrecurrent neural network, 379\nregression, 101\nregularization,\nregularizer, 119\nreinforce, 691\nreinforcement learning,\nrelational database, 486\nreparametrization trick, 690\nrepresentation learning, 3\nrepresentational capacity, 114\nrestricted boltzmann machine,\u00a0\n,\n\n120 120 177 228 433\n\n,\u00a0\n,\n\n,\n357 462\n482 590 634 658 659 674 678\n,\n\n25 106 483 691\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n785\n\n "}, {"Page_number": 801, "text": "index\n\n680 683 685\n\n,\n\n,\n\nridge regression, see weight decay\nrisk, 275\nrnn-rbm, 688\n\n,\n\n,\n\n,516 620\n\nsaddle points, 285\nsample mean, 125\nscalar,\nxi xii 31\nscore matching,\nsecant condition, 316\nsecond derivative, 86\nsecond derivative test, 89\nself-information, 73\nsemantic hashing, 528\nsemi-supervised learning, 244\nseparable convolution, 363\nseparation (probabilistic modeling), 575\nset, xii\nsgd, see stochastic gradient descent\nshannon entropy,\nshortlist, 469\nsigmoid,\nsigmoid belief network, 27\nsimple cell, 366\nsingular value, see singular value decompo-\n\n,xiv see logistic sigmoid\n\n,xiii 73\n\nsition\n\nsingular value decomposition,\n44 148 482\nsingular vector, see singular value decom-\n\n,\n\n,\n\nposition\n\n,\n\n,\n,\n\n183 421 453\nxiv 68 196\n\nslow feature analysis, 496\nsml, see stochastic maximum likelihood\nsoftmax,\nsoftplus,\n,\nspam detection, 3\nsparse coding,\n,\nsparse initialization,\nsparse representation,\n\n322 357 499 634 694\n\n,\n,304 408\n\n,\n\n,\n\n,\n\n146 226 253 508\n,\n\n,\n\n,\n\n559\n\nspearmint, 439\nspectral radius, 407\nspeech recognition, see\u00a0automatic speech\n\nrecognition\n\nsphering, see whitening\nspike and\u00a0slab restricted\u00a0boltzmann ma-\n\nchine, 683\n\nspn, see sum-product network\nsquare matrix, 38\nssrbm, see spike and slab restricted boltz-\n\nmann machine\nstandard deviation, 61\nstandard error, 127\nstandard error of the mean,\nstatistic, 122\nstatistical learning theory, 110\nsteepest descent, see gradient descent\nstochastic back-propagation, see reparametriza-\n\n,128 278\n\ntion trick\n\nstochastic gradient descent,\n\n294, 674\n\n15 150 279\n,\n\n,\n\n,\n\n,615 674\n\n,77 561\n\n,101 687\n\nstochastic maximum likelihood,\nstochastic pooling, 265\nstructure learning, 585\nstructured output,\nstructured probabilistic model,\nsum rule of probability, 58\nsum-product network, 556\nsupervised fine-tuning,\nsupervised learning, 105\nsupport vector machine, 140\nsurrogate loss function, 276\nsvd, see singular value decomposition\nsymmetric matrix,\n\n,532 664\n\n,41 43\n\n,\n\n,\n\n,383 384\n\nxi xii 33\n\ntangent distance, 269\ntangent plane, 519\ntangent prop, 269\ntdnn, see time-delay neural network\nteacher forcing,\ntempering, 606\ntemplate matching, 141\ntensor,\ntest set, 110\ntikhonov regularization, see weight decay\ntiled convolution, 353\ntime-delay neural network,\ntoeplitz matrix, 334\ntopographic ica, 496\ntrace operator, 46\ntraining error, 110\ntranscription, 101\n\n,369 375\n\n786\n\n "}, {"Page_number": 802, "text": "index\n\n,xii 33\n\ntransfer learning, 539\ntranspose,\ntriangle inequality, 39\ntriangulated graph, see chordal graph\ntrigram, 465\n\nwordnet, 486\n\nzero-data learning, see zero-shot learning\nzero-shot learning, 541\n\n,77 510\n\nunbiased, 124\nundirected graphical model,\nundirected model, 569\nuniform distribution, 57\nunigram, 465\nunit norm, 41\nunit vector, 41\nuniversal approximation theorem, 197\nuniversal approximator, 556\nunnormalized probability distribution, 570\nunsupervised learning,\nunsupervised pretraining,\n\n,105 146\n\n,462 531\n\nv-structure, see explaining away\nv1, 366\nvae, see variational autoencoder\nvapnik-chervonenkis dimension, 114\nvariance,\nvariational autoencoder,\nvariational derivatives, see functional deriva-\n\nxiii 61 229\n\n,691 698\n\n,\n\n,\n\ntives\n\nvariational free energy, see evidence lower\n\nbound\n\nvc dimension, see vapnik-chervonenkis di-\n\nmension\n,\nxi xii 32\n\n,\n\nvector,\nvirtual adversarial examples, 268\nvisible layer, 6\nvolumetric data, 361\n\n,\n\n,\n\n,654 663\n\n118 177 231 434\n\nwake-sleep,\nweight decay,\n,\nweight space symmetry, 284\n,15 107\nweights,\nwhitening, 458\nwikibase, 486\nwikibase, 486\nword embedding, 467\nword-sense disambiguation, 487\n\n787\n\n "}]}
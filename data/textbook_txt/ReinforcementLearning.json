{"Filename": "ReinforcementLearning", "Pages": [{"Page_number": 1, "text": "reinforcement learning\nand dynamic programming\nusing function approximators\n\nk11117_fm.indd   1\n\n3/22/10   4:10:24 am\n\n "}, {"Page_number": 2, "text": "automation and control engineering\n\na series of reference books and textbooks\n\nseries editors\n\nfrank l. lewis, ph.d.,\nfellow ieee, fellow ifac\n\nprofessor\n\nautomation and robotics research institute \n\nthe university of texas at arlington\n\nshuzhi sam ge, ph.d.,\n\nfellow ieee\n\nprofessor\n\ninteractive digital media institute\nthe national university of singapore\n\nreinforcement learning and dynamic programming using function  \napproximators, lucian bus\u00b8oniu, robert babu\u0161ka, bart de schutter, \nand damien ernst\nmodeling and control of vibration in mechanical systems, chunling du \nand lihua xie\nanalysis and synthesis of fuzzy control systems: a model-based approach, \ngang feng\nlyapunov-based control of robotic systems, aman behal, warren dixon, \ndarren m. dawson, and bin xian\nsystem modeling and control with resource-oriented petri nets, naiqi wu \nand mengchu zhou\nsliding mode control in electro-mechanical systems, second edition,  \nvadim utkin, j\u00fcrgen guldner, and jingxin shi\noptimal control: weakly coupled systems and applications, zoran gajic\u00b4, \nmyo-taeg lim, dobrila skataric\u00b4, wu-chung su, and vojislav kecman\nintelligent systems: modeling, optimization, and control, yung c. shin \nand chengying xu \noptimal and robust estimation: with an introduction to stochastic control \ntheory, second edition, frank l. lewis, lihua xie, and dan popa\nfeedback control of dynamic bipedal robot locomotion, eric r. westervelt, \njessy w. grizzle, christine chevallereau, jun ho choi, and benjamin morris\nintelligent freight transportation, edited by petros a. ioannou\nmodeling and control of complex systems, edited by petros a. ioannou \nand andreas pitsillides\nwireless ad hoc and sensor networks: protocols, performance, and control, \njagannathan sarangapani\nstochastic hybrid systems, edited by christos g. cassandras \nand john lygeros\nhard disk drive: mechatronics and control, abdullah al mamun, \nguo xiao guo, and chao bi\nautonomous mobile robots: sensing, control, decision making  \nand applications, edited by shuzhi sam ge and frank l. lewis\n\nk11117_fm.indd   2\n\n3/22/10   4:10:24 am\n\n "}, {"Page_number": 3, "text": "automation and control\n\nengineering series\n\nreinforcement learning\nand dynamic programming\nusing function approximators\n\nlucian busoniu\n\ndelft university of technology\ndelft, the netherlands\n\nrobert babuska\n\ndelft university of technology\ndelft, the netherlands\n\nbart de schutter\n\ndelft university of technology\ndelft, the netherlands\n\ndamien ernst\n\nuniversity of li\u00e8ge\nli\u00e8ge, belgium\n\nboca raton   london   new york\n\ncrc press is an imprint of the\ntaylor & francis group, an informa business\n\nk11117_fm.indd   3\n\n3/22/10   4:10:24 am\n\n "}, {"Page_number": 4, "text": "matlab\u00ae is a trademark of the mathworks, inc. and is used with permission. the mathworks does \nnot warrant the accuracy of the text or exercises in this book. this book\u2019s use or discussion of mat-\nlab\u00ae software or related products does not constitute endorsement or sponsorship by the mathworks \nof a particular pedagogical approach or particular use of the matlab\u00ae software.\n\ncrc press\ntaylor & francis group\n6000 broken sound parkway nw, suite 300\nboca raton, fl 33487-2742\n\u00a9 2010 by taylor and francis group, llc\ncrc press is an imprint of taylor & francis group, an informa business\nno claim to original u.s. government works\nprinted in the united states of america on acid-free paper\n10 9 8 7 6 5 4 3 2 1\ninternational standard book number: 978-1-4398-2108-4 (hardback)\nthis book contains information obtained from authentic and highly regarded sources. reasonable \nefforts have been made to publish reliable data and information, but the author and publisher cannot \nassume responsibility for the validity of all materials or the consequences of their use. the authors and \npublishers have attempted to trace the copyright holders of all material reproduced in this publication \nand apologize to copyright holders if permission to publish in this form has not been obtained. if any \ncopyright material has not been acknowledged please write and let us know so we may rectify in any \nfuture reprint.\nexcept as permitted under u.s. copyright law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or \nhereafter invented, including photocopying, microfilming, and recording, or in any information stor-\nage or retrieval system, without written permission from the publishers.\nfor permission to photocopy or use material electronically from this work, please access www.copy-\nright.com (http://www.copyright.com/) or contact the copyright clearance center, inc. (ccc), 222 \nrosewood drive, danvers, ma 01923, 978-750-8400. ccc is a not-for-profit organization that pro-\nvides licenses and registration for a variety of users. for organizations that have been granted a pho-\ntocopy license by the ccc, a separate system of payment has been arranged.\ntrademark notice: product or corporate names may be trademarks or registered trademarks, and are \nused only for identification and explanation without intent to infringe.\nvisit the taylor & francis web site at\nhttp://www.taylorandfrancis.com\nand the crc press web site at\nhttp://www.crcpress.com \n\nk11117_fm.indd   4\n\n3/22/10   4:10:24 am\n\n "}, {"Page_number": 5, "text": "preface\n\ncontrol systems are making a tremendous impact on our society. though invisible\nto most users, they are essential for the operation of nearly all devices \u2013 from basic\nhome appliances to aircraft and nuclear power plants. apart from technical systems,\nthe principles of control are routinely applied and exploited in a variety of disciplines\nsuch as economics, medicine, social sciences, and artificial intelligence.\n\na common denominator in the diverse applications of control is the need to in-\nfluence or modify the behavior of dynamic systems to attain prespecified goals. one\napproach to achieve this is to assign a numerical performance index to each state tra-\njectory of the system. the control problem is then solved by searching for a control\npolicy that drives the system along trajectories corresponding to the best value of the\nperformance index. this approach essentially reduces the problem of finding good\ncontrol policies to the search for solutions of a mathematical optimization problem.\n\nearly work in the field of optimal control dates back to the 1940s with the pi-\noneering research of pontryagin and bellman. dynamic programming (dp), intro-\nduced by bellman, is still among the state-of-the-art tools commonly used to solve\noptimal control problems when a system model is available. the alternative idea of\nfinding a solution in the absence of a model was explored as early as the 1960s. in\nthe 1980s, a revival of interest in this model-free paradigm led to the development of\nthe field of reinforcement learning (rl). the central theme in rl research is the de-\nsign of algorithms that learn control policies solely from the knowledge of transition\nsamples or trajectories, which are collected beforehand or by online interaction with\nthe system. most approaches developed to tackle the rl problem are closely related\nto dp algorithms.\n\na core obstacle in dp and rl is that solutions cannot be represented exactly for\nproblems with large discrete state-action spaces or continuous spaces. instead, com-\npact representations relying on function approximators must be used. this challenge\nwas already recognized while the first dp techniques were being developed. how-\never, it has only been in recent years \u2013 and largely in correlation with the advance\nof rl \u2013 that approximation-based methods have grown in diversity, maturity, and\nefficiency, enabling rl and dp to scale up to realistic problems.\n\nthis book provides an accessible in-depth treatment of reinforcement learning\nand dynamic programming methods using function approximators. we start with a\nconcise introduction to classical dp and rl, in order to build the foundation for\nthe remainder of the book. next, we present an extensive review of state-of-the-art\napproaches to dp and rl with approximation. theoretical guarantees are provided\non the solutions obtained, and numerical examples and comparisons are used to il-\nlustrate the properties of the individual methods. the remaining three chapters are\n\nv\n\n "}, {"Page_number": 6, "text": "vi\n\ndedicated to a detailed presentation of representative algorithms from the three ma-\njor classes of techniques: value iteration, policy iteration, and policy search. the\nproperties and the performance of these algorithms are highlighted in simulation and\nexperimental studies on a range of control applications.\n\nwe believe that this balanced combination of practical algorithms, theoretical\nanalysis, and comprehensive examples makes our book suitable not only for re-\nsearchers, teachers, and graduate students in the fields of optimal and adaptive con-\ntrol, machine learning and artificial intelligence, but also for practitioners seeking\nnovel strategies for solving challenging real-life control problems.\n\nthis book can be read in several ways. readers unfamiliar with the field are\nadvised to start with chapter 1 for a gentle introduction, and continue with chap-\nter 2 (which discusses classical dp and rl) and chapter 3 (which considers\napproximation-based methods). those who are familiar with the basic concepts of\nrl and dp may consult the list of notations given at the end of the book, and then\nstart directly with chapter 3. this first part of the book is sufficient to get an overview\nof the field. thereafter, readers can pick any combination of chapters 4 to 6, depend-\ning on their interests: approximate value iteration (chapter 4), approximate policy\niteration and online learning (chapter 5), or approximate policy search (chapter 6).\nsupplementary information relevant to this book, including a complete archive\n\nof the computer code used in the experimental studies, is available at the web site:\n\nhttp://www.dcsc.tudelft.nl/rlbook/\n\ncomments, suggestions, or questions concerning the book or the web site are wel-\ncome. interested readers are encouraged to get in touch with the authors using the\ncontact information on the web site.\n\nthe authors have been inspired over the years by many scientists who undoubt-\nedly left their mark on this book; in particular by louis wehenkel, pierre geurts,\nguy-bart stan, r\u00b4emi munos, martin riedmiller, and michail lagoudakis. pierre\ngeurts also provided the computer program for building ensembles of regression\ntrees, used in several examples in the book. this work would not have been pos-\nsible without our colleagues, students, and the excellent professional environments\nat the delft center for systems and control of the delft university of technology,\nthe netherlands, the montefiore institute of the university of li`ege, belgium, and at\nsup\u00b4elec rennes, france. among our colleagues in delft, justin rice deserves special\nmention for carefully proofreading the manuscript. to all these people we extend our\nsincere thanks.\n\nwe thank sam ge for giving us the opportunity to publish our book with taylor\n& francis crc press, and the editorial and production team at taylor & francis for\ntheir valuable help. we gratefully acknowledge the financial support of the bsik-\nicis project \u201cinteractive collaborative information systems\u201d (grant no. bsik03024)\nand the dutch funding organizations nwo and stw. damien ernst is a research\nassociate of the frs-fnrs, the financial support of which he acknowledges. we\nappreciate the kind permission offered by the ieee to reproduce material from our\nprevious works over which they hold copyright.\n\n "}, {"Page_number": 7, "text": "finally, we thank our families for their continual understanding, patience, and\n\nsupport.\n\nvii\n\nlucian bus\u00b8oniu\nrobert babu\u02c7ska\nbart de schutter\ndamien ernst\nnovember 2009\n\nmatlab r(cid:13) is a trademark of the mathworks, inc.\nfor product information, please contact:\n\nthe mathworks, inc.\n3 apple hill drive\nnatick, ma 01760-2098 usa\ntel: 508 647 7000\nfax: 508-647-7001\ne-mail: info@mathworks.com\nweb: www.mathworks.com\n\n "}, {"Page_number": 8, "text": "about the authors\n\nlucian bus\u00b8oniu is a postdoctoral fellow at the delft center for systems and control\nof delft university of technology, in the netherlands. he received his phd degree\n(cumlaude) in 2009 from the delft university of technology, and his msc degree in\n2003 from the technical university of cluj-napoca, romania. his current research\ninterests include reinforcement learning and dynamic programming with function\napproximation, intelligent and learning techniques for control problems, and multi-\nagent learning.\n\nrobert babu\u02c7ska is a full professor at the delft center for systems and control\nof delft university of technology in the netherlands. he received his phd degree\n(cum laude) in control in 1997 from the delft university of technology, and his\nmsc degree (with honors) in electrical engineering in 1990 from czech technical\nuniversity, prague. his research interests include fuzzy systems modeling and iden-\ntification, data-driven construction and adaptation of neuro-fuzzy systems, model-\nbased fuzzy control and learning control. he is active in applying these techniques in\nrobotics, mechatronics, and aerospace.\n\nbart de schutter is a full professor at the delft center for systems and control and\nat the marine & transport technology department of delft university of technol-\nogy in the netherlands. he received the phd degree in applied sciences (summa\ncum laude with congratulations of the examination jury) in 1996 from k.u. leuven,\nbelgium. his current research interests include multi-agent systems, hybrid systems\ncontrol, discrete-event systems, and control of intelligent transportation systems.\n\ndamien ernst received the msc and phd degrees from the university of li`ege in\n1998 and 2003, respectively. he is currently a research associate of the belgian\nfrs-fnrs and he is affiliated with the systems and modeling research unit of the\nuniversity of li`ege. damien ernst spent the period 2003\u20132006 with the university\nof li`ege as a postdoctoral researcher of the frs-fnrs and held during this period\npositions as visiting researcher at cmu, mit and eth. he spent the academic year\n2006\u20132007 working at sup\u00b4elec (france) as professor. his main research interests are\nin the fields of power system dynamics, optimal control, reinforcement learning, and\ndesign of dynamic treatment regimes.\n\nix\n\n "}, {"Page_number": 9, "text": "contents\n\n1 introduction\n\n1.1 the dynamic programming and reinforcement learning problem . .\n1.2 approximation in dynamic programming and reinforcement learning\n1.3 about this book . .\n. .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\nintroduction . . . .\n\n2.3 value iteration\n\nstochastic setting . .\n. . . . .\n\n2.1\n. . . . .\n2.2 markov decision processes .\n\n2 an introduction to dynamic programming and reinforcement learning\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n2.2.1 deterministic setting . . . . .\n. . . . .\n. . . . .\n2.2.2\n. . . . .\n. . . . .\n2.3.1 model-based value iteration .\n. . . . .\n2.3.2 model-free value iteration and the need for exploration .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n2.4.1 model-based policy iteration .\n2.4.2 model-free policy iteration . .\n. . . . .\n. . . . .\n\n2.5 policy search . . .\n. . . . .\n2.6 summary and discussion . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n\n2.4 policy iteration . .\n\n. . . . .\n\n. .\n\n3 dynamic programming and reinforcement learning in large and contin-\n\n. . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\nintroduction . . . .\n\n. . . .\n. . . .\n. . . .\n\n. . . . .\n3.4 approximate value iteration . . . . .\n\nuous spaces\n3.1\n. . . . .\n3.2 the need for approximation in large and continuous spaces . . .\n. . . . .\n3.3 approximation architectures . . . . .\nparametric approximation . .\n. . . . .\n. . . . .\n\n. .\n. .\n. .\n. . . . .\n. .\n. . . . .\n3.3.1\n3.3.2 nonparametric approximation . . . . .\n. .\n3.3.3 comparison of parametric and nonparametric approximation\n. .\n3.3.4 remarks\n. .\n3.4.1 model-based value iteration with parametric approximation\n3.4.2 model-free value iteration with parametric approximation .\n3.4.3 value iteration with nonparametric approximation . . . .\n. .\n3.4.4 convergence and the role of nonexpansive approximation .\n3.4.5 example: approximate q-iteration for a dc motor . . .\n. .\n. .\n. . . . .\n3.5.1 value iteration-like algorithms for approximate policy\n. .\n\n3.5 approximate policy iteration . . . . .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n\nevaluation .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n.\n\n1\n2\n5\n8\n\n11\n11\n14\n14\n19\n23\n23\n28\n30\n31\n37\n38\n41\n\n43\n43\n47\n49\n49\n51\n53\n54\n54\n55\n58\n62\n63\n66\n71\n\n73\n\nxi\n\n "}, {"Page_number": 10, "text": "xii\n\n3.6 finding value function approximators automatically\n\n. . . .\n\n. . . . .\n\n. . . . .\n\npolicy improvement and approximate policy iteration . .\n. . . . .\n\napproximation . . .\n. . . . .\npolicy evaluation with nonparametric approximation . .\n\n3.5.2 model-free policy evaluation with linearly parameterized\n74\n. .\n84\n3.5.3\n. .\n84\n3.5.4 model-based approximate policy evaluation with rollouts . .\n85\n. .\n3.5.5\n88\n. .\n3.5.6 theoretical guarantees . . . .\n90\n. .\n3.5.7 example: least-squares policy iteration for a dc motor\n95\n. .\n96\n. .\n. .\n98\n. . 100\n. . 101\n. . 102\n. . 107\n. . 109\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n3.7.1\n3.7.2 gradient-free policy search . .\n. . . . .\n3.7.3 example: gradient-free policy search for a dc motor . .\n\n.\n. . . .\n. . . .\n. . . .\n. . . .\npolicy gradient and actor-critic algorithms . . .\n. . . .\n\n3.6.1 basis function optimization .\n3.6.2 basis function construction . .\n. . . . .\n3.6.3 remarks\n. . . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n.\n\n. . . . .\n3.7 approximate policy search .\n\n3.8 comparison of approximate value iteration, policy iteration, and pol-\n\nicy search . . . . .\n\n. . . . .\n3.9 summary and discussion . .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n\n. . . . .\n. . . . .\n\n. . 113\n. . 114\n\n4 approximate value iteration with a fuzzy representation\n\n. . . .\n. . . .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n4.1\nintroduction . . . .\n4.2 fuzzy q-iteration .\n\n117\n. . 117\n. . 119\n4.2.1 approximation and projection mappings of fuzzy q-iteration 119\n. . 123\n4.2.2\n. . 127\n. . 127\n. . 135\n. . 140\n. . 141\n4.4.1 a general approach to membership function optimization . . 141\n4.4.2 cross-entropy optimization . .\n. . 143\n4.4.3\n\nsynchronous and asynchronous fuzzy q-iteration . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n4.4 optimizing the membership functions . . . . .\n\n4.3 analysis of fuzzy q-iteration . . . . .\n. . . . .\n4.3.1 convergence . . . .\n4.3.2 consistency . . . . .\n. . . . .\n4.3.3 computational complexity . .\n\n. . . .\n. . . .\n. . . .\n. . . .\n. . . .\n\nfuzzy q-iteration with cross-entropy optimization of the\nmembership functions\n4.5 experimental study . . . . .\n\n. . . . .\n. . . . .\n4.5.1 dc motor: convergence and consistency study . . . . .\n4.5.2 two-link manipulator: effects of action interpolation, and\n\n. . 144\n. . 145\n. . 146\n\n. . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\ncomparison with fitted q-iteration . . .\n.\ninverted pendulum: real-time control\n\n. . 152\n4.5.3\n. . 157\n4.5.4 car on the hill: effects of membership function optimization 160\n. . 164\n\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n4.6 summary and discussion . .\n\n5 approximate policy iteration for online learning and continuous-action\n\ncontrol\n5.1\n. . . . .\n5.2 a recapitulation of least-squares policy iteration\n\nintroduction . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n. . .\n\n. . . . .\n. . . . .\n\n167\n. . 167\n. . 168\n\n "}, {"Page_number": 11, "text": "xiii\n\n. . . . .\n. . . . .\n\n5.3 online least-squares policy iteration .\n5.4 online lspi with prior knowledge . .\n\n. . . .\n. . . .\n5.4.1 online lspi with policy approximation . . . .\n5.4.2 online lspi with monotonic policies\n. . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n5.5 lspi with continuous-action, polynomial approximation . . . .\n. . . . .\n5.6 experimental study . . . . .\n. . . . .\n. . . . .\n. .\n\n. . . .\n5.6.1 online lspi for the inverted pendulum . . . .\n5.6.2 online lspi for the two-link manipulator . . .\n5.6.3 online lspi with prior knowledge for the dc motor\n5.6.4 lspi with continuous-action approximation for the inverted\n\n. . 170\n. . 173\n. . 174\n. . 175\n. . 177\n. . 180\n. . 180\n. . 192\n. . 195\n\n. . . . .\n\n. . . . .\n\n.\n\n. . . . .\n5.7 summary and discussion . .\n\npendulum .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n\n. . . . .\n. . . . .\n\n. . 198\n. . 201\n\n6 approximate policy search with cross-entropy optimization of basis\n\nintroduction . . . .\n\nfunctions\n6.1\n6.2 cross-entropy optimization\n6.3 cross-entropy policy search\n\n. . . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n. . . . .\n6.3.1 general approach . .\n. . . . .\n6.3.2 cross-entropy policy search with radial basis functions .\n. . . . .\n. . . . .\n. . . . .\n\n. . . . .\n6.4.1 discrete-time double integrator\n6.4.2 bicycle balancing . .\n. . . . .\n6.4.3\n\n. . . .\n. . . .\n. . . .\n. . . .\n\n. . . . .\n. . . .\n. . . . .\n\nstructured treatment interruptions for hiv infection control\n\n. . . .\n. . . .\n. . . .\n\n6.4 experimental study . . . . .\n\n6.5 summary and discussion . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . . .\n\nappendix a\n\na.1 structure of the approximator\na.2 building and using a tree . .\n\nextremely randomized trees\n. . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n\n. . . . .\n. . . . .\n\nappendix b\n\nthe cross-entropy method\n\nb.1 rare-event simulation using the cross-entropy method\nb.2 cross-entropy optimization\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . . .\n. . . . .\n\nsymbols and abbreviations\n\nbibliography\n\nlist of algorithms\n\nindex\n\n205\n. . 205\n. . 207\n. . 209\n. . 209\n. . 213\n. . 216\n. . 216\n. . 223\n229\n. . 233\n\n235\n. . 235\n. . 236\n\n239\n. . 239\n. . 242\n\n245\n\n249\n\n267\n\n269\n\n "}, {"Page_number": 12, "text": "1\n\nintroduction\n\ndynamic programming (dp) and reinforcement learning (rl) are algorithmic meth-\nods for solving problems in which actions (decisions) are applied to a system over\nan extended period of time, in order to achieve a desired goal. dp methods require\na model of the system\u2019s behavior, whereas rl methods do not. the time variable is\nusually discrete and actions are taken at every discrete time step, leading to a sequen-\ntial decision-making problem. the actions are taken in closed loop, which means that\nthe outcome of earlier actions is monitored and taken into account when choosing\nnew actions. rewards are provided that evaluate the one-step decision-making per-\nformance, and the goal is to optimize the long-term performance, measured by the\ntotal reward accumulated over the course of interaction.\n\nsuch decision-making problems appear in a wide variety of fields, including au-\ntomatic control, artificial intelligence, operations research, economics, and medicine.\nfor instance, in automatic control, as shown in figure 1.1(a), a controller receives\noutput measurements from a process, and applies actions to this process in order to\nmake its behavior satisfy certain requirements (levine, 1996). in this context, dp\nand rl methods can be applied to solve optimal control problems, in which the be-\nhavior of the process is evaluated using a cost function that plays a similar role to\nthe rewards. the decision maker is the controller, and the system is the controlled\nprocess.\n\ncontroller\n\naction\n\nprocess\n\noutput\n\nintelligent\nagent\n\naction\n\nenvironment\n\nperception\n\n(a) automatic control.\n\n(b) artificial intelligent agents.\n\nfigure 1.1\ntwo application domains for dynamic programming and reinforcement learning.\n\nin artificial intelligence, dp and rl are useful to obtain optimal behavior for in-\ntelligent agents, which, as shown in figure 1.1(b), monitor their environment through\nperceptions and influence it by applying actions (russell and norvig, 2003). the de-\ncision maker is now the agent, and the system is the agent\u2019s environment.\n\nif a model of the system is available, dp methods can be applied. a key benefit\n\n1\n\n "}, {"Page_number": 13, "text": "2\n\nchapter 1. introduction\n\nof dp methods is that they make few assumptions on the system, which can gen-\nerally be nonlinear and stochastic (bertsekas, 2005a, 2007). this is in contrast to,\ne.g., classical techniques from automatic control, many of which require restrictive\nassumptions on the system, such as linearity or determinism. moreover, many dp\nmethods do not require an analytical expression of the model, but are able to work\nwith a simulation model instead. constructing a simulation model is often easier than\nderiving an analytical model, especially when the system behavior is stochastic.\n\nhowever, sometimes a model of the system cannot be obtained at all, e.g., be-\ncause the system is not fully known beforehand, is insufficiently understood, or ob-\ntaining a model is too costly. rl methods are helpful in this case, since they work\nusing only data obtained from the system, without requiring a model of its behavior\n(sutton and barto, 1998). offline rl methods are applicable if data can be obtained\nin advance. online rl algorithms learn a solution by interacting with the system, and\ncan therefore be applied even when data is not available in advance. for instance, in-\ntelligent agents are often placed in environments that are not fully known beforehand,\nwhich makes it impossible to obtain data in advance. note that rl methods can, of\ncourse, also be applied when a model is available, simply by using the model instead\nof the real system to generate data.\n\nin this book, we primarily adopt a control-theoretic point of view, and hence\nemploy control-theoretical notation and terminology, and choose control systems as\nexamples to illustrate the behavior of dp and rl algorithms. we nevertheless also\nexploit results from other fields, in particular the strong body of rl research from the\nfield of artificial intelligence. moreover, the methodology we describe is applicable\nto sequential decision problems in many other fields.\n\nthe remainder of this introductory chapter is organized as follows. in section 1.1,\nan outline of the dp/rl problem and its solution is given. section 1.2 then introduces\nthe challenge of approximating the solution, which is a central topic of this book.\nfinally, in section 1.3, the organization of the book is explained.\n\n1.1 the dynamic programming and reinforcement\n\nlearning\n\nproblem\n\nthe main elements of the dp and rl problem, together with their flow of interaction,\nare represented in figure 1.2: a controller interacts with a process by means of states\nand actions, and receives rewards according to a reward function. for the dp and rl\nalgorithms considered in this book, an important requirement is the availability of\na signal that completely describes the current state of the process (this requirement\nwill be formalized in chapter 2). this is why the process shown in figure 1.2 outputs\na state signal.\n\nto clarify the meaning of the elements of figure 1.2, we use a conceptual robotic\nnavigation example. autonomous mobile robotics is an application domain where\nautomatic control and artificial intelligence meet in a natural way, since a mobile\n\n "}, {"Page_number": 14, "text": "1.1. the dynamic programming and reinforcement learning problem\n\n3\n\nreward function\n\nreward\n\ncontroller\n\naction\n\nstate\n\nprocess\n\nfigure 1.2\nthe elements of dp and rl and their flow of interaction. the elements related to the reward\nare depicted in gray.\n\nrobot and its environment comprise a process that must be controlled, while the robot\nis also an artificial agent that must accomplish a task in its environment. figure 1.3\npresents the navigation example, in which the robot shown in the bottom region must\nnavigate to the goal on the top-right, while avoiding the obstacle represented by a\ngray block. (for instance, in the field of rescue robotics, the goal might represent\nthe location of a victim to be rescued.) the controller is the robot\u2019s software, and\nthe process consists of the robot\u2019s environment (the surface on which it moves, the\nobstacle, and the goal) together with the body of the robot itself. it should be empha-\nsized that in dp and rl, the physical body of the decision-making entity (if it has\none), its sensors and actuators, as well as any fixed lower-level controllers, are all\nconsidered to be a part of the process, whereas the controller is taken to be only the\ndecision-making algorithm.\n\nr\nk+1 reward\n\n,\n\nnext state xk+1\nuk\n(position)\n\n(step)\n\naction\n\nstate\n\nxk\n\nfigure 1.3\na robotic navigation example. an example transition is also shown, in which the current and\nnext states are indicated by black dots, the action by a black arrow, and the reward by a gray\narrow. the dotted silhouette represents the robot in the next state.\n\nin the navigation example, the state is the position of the robot on the surface,\ngiven, e.g., in cartesian coordinates, and the action is a step taken by the robot, sim-\nilarly given in cartesian coordinates. as a result of taking a step from the current\n\n "}, {"Page_number": 15, "text": "4\n\nchapter 1. introduction\n\nposition, the next position is obtained, according to a transition function. in this ex-\nample, because both the positions and steps are represented in cartesian coordinates,\nthe transitions are most often additive: the next position is the sum of the current\nposition and the step taken. more complicated transitions are obtained if the robot\ncollides with the obstacle. note that for simplicity, most of the dynamics of the robot,\nsuch as the motion of the wheels, have not been taken into account here. for instance,\nif the wheels can slip on the surface, the transitions become stochastic, in which case\nthe next state is a random variable.\n\nthe quality of every transition is measured by a reward, generated according to\nthe reward function. for instance, the reward could have a positive value such as 10 if\nthe robot reaches the goal, a negative value such as \u22121, representing a penalty, if the\nrobot collides with the obstacle, and a neutral value of 0 for any other transition. al-\nternatively, more informative rewards could be constructed, using, e.g., the distances\nto the goal and to the obstacle.\n\nthe behavior of the controller is dictated by its policy: a mapping from states into\n\nactions, which indicates what action (step) should be taken in each state (position).\n\nin general, the state is denoted by x, the action by u, and the reward by r. these\nquantities may be subscripted by discrete time indices, where k denotes the current\ntime index (see figure 1.3). the transition function is denoted by f , the reward func-\ntion by \u03c1, and the policy by h.\n\nin dp and rl, the goal is to maximize the return, consisting of the cumulative re-\nward over the course of interaction. we mainly consider discounted infinite-horizon\nreturns, which accumulate rewards obtained along (possibly) infinitely long trajec-\ntories starting at the initial time step k = 0, and weigh the rewards by a factor that\ndecreases exponentially as the time step increases:\n\n\u03b30r1 + \u03b31r2 + \u03b32r3 + ...\n\n(1.1)\n\nthe discount factor \u03b3 \u2208 [0, 1) gives rise to the exponential weighting, and can be\nseen as a measure of how \u201cfar-sighted\u201d the controller is in considering its rewards.\nfigure 1.4 illustrates the computation of the discounted return for the navigation\nproblem of figure 1.3.\n\nthe rewards depend of course on the state-action trajectory followed, which in\n\nturn depends on the policy being used:\n\nx0, u0 = h(x0), x1, u1 = h(x1), x2, u2 = h(x2), . . .\n\nin particular, each reward rk+1 is the result of the transition (xk, uk, xk+1). it is con-\nvenient to consider the return separately for every initial state x0, which means the\nreturn is a function of the initial state. note that, if state transitions are stochastic,\nthe goal considered in this book is to maximize the expectation of (1.1) over all the\nrealizations of the stochastic trajectory starting from x0.\n\nthe core challenge of dp and rl is therefore to arrive at a solution that optimizes\nthe long-term performance given by the return, using only reward information that\ndescribes the immediate performance. solving the dp/rl problem boils down to\nfinding an optimal policy, denoted by h\u2217, that maximizes the return (1.1) for every\n\n "}, {"Page_number": 16, "text": "1.2. approximation in dynamic programming and reinforcement learning\n\n5\n\n2r3\n\u03b3\n\n3r4\n\u03b3\n\n1r2\n\u03b3\n\n0r1\n\u03b3\n\nfigure 1.4\nthe discounted return along a trajectory of the robot. the decreasing heights of the gray\nvertical bars indicate the exponentially diminishing nature of the discounting applied to the\nrewards.\n\ninitial state. one way to obtain an optimal policy is to first compute the maximal\nreturns. for example, the so-called optimal q-function, denoted by q\u2217, contains for\neach state-action pair (x, u) the return obtained by first taking action u in state x and\nthen choosing optimal actions from the second step onwards:\n\nq\u2217(x, u) = \u03b30r1 + \u03b31r2 + \u03b32r3 + ...\n\nwhen x0 = x, u0 = u, and optimal actions are taken for x1, x2, . . .\n\n(1.2)\n\nif transitions are stochastic, the optimal q-function is defined instead as the expec-\ntation of the return on the right-hand side of (1.2) over the trajectory realizations.\nthe optimal q-function can be found using a suitable dp or rl algorithm. then,\nan optimal policy can be obtained by choosing, at each state x, an action h\u2217(x) that\nmaximizes the optimal q-function for that state:\n\nh\u2217(x) \u2208 arg max\n\nu\n\nq\u2217(x, u)\n\n(1.3)\n\nto see that an optimal policy is obtained, recall that the optimal q-function already\ncontains optimal returns starting from the second step onwards; in (1.3), an action is\nchosen that additionally maximizes the return over the first step, therefore obtaining\na return that is maximal over the entire horizon, i.e., optimal.\n\n1.2 approximation in dynamic programming and reinforcement\n\nlearning\n\nconsider the problem of representing a q-function, not necessarily the optimal one.\nsince no prior knowledge about the q-function is available, the only way to guar-\nantee an exact representation is to store distinct values of the q-function (q-values)\n\n "}, {"Page_number": 17, "text": "6\n\nchapter 1. introduction\n\nfor every state-action pair. this is schematically depicted in figure 1.5 for the navi-\ngation example of section 1.1: q-values must be stored separately for each position\nof the robot, and for each possible step that it might take from every such position.\nhowever, because the position and step variables are continuous, they can both take\nuncountably many distinct values. therefore, even in this simple example, storing\ndistinct q-values for every state-action pair is obviously impossible. the only feasi-\nble way to proceed is to use a compact representation of the q-function.\n\ngoal\n\n)\n\n2\n\nq x u(\n\n,\n\n2\n\n1\n\n)\n,\n\n2\n\nq x u(\n\nq x u(\n\n1\n\n)\n1\n\n,\nq x u(\n)\n2\nq x u(\n,\n\n,\n\n1\n\n1\n\n)\n3\n\nfigure 1.5\nillustration of an exact q-function representation for the navigation example. for every state-\naction pair, there is a corresponding q-value. the q-values are not represented explicitly, but\nonly shown symbolically near corresponding state-action pairs.\n\none type of compact q-function representation that will often be used in the se-\nquel relies on state-dependent basis functions (bfs) and action discretization. such\na representation is illustrated in figure 1.6 for the navigation problem. a finite num-\nber of bfs, \u03c61, . . . ,\u03c6n , are defined over the state space, and the action space is dis-\ncretized into a finite number of actions, in this case 4: left, right, forward, and back.\ninstead of storing distinct q-values for every state-action pair, such a representa-\n\n\u03b81,left\n\n\u03b81,forward\n\n\u03b82,left\n\n\u03b82,back\n\n\u03b81,right\n\n\u03b81,back\n\nf1\n\nfn\n\n\u03b82,forward\n\n\u03b82,right\n\n...\n\nf2\n\nstate space\n\nfigure 1.6\nillustration of a compact q-function representation for the navigation example.\n\n "}, {"Page_number": 18, "text": "1.2. approximation in dynamic programming and reinforcement learning\n\n7\n\ntion stores parameters \u03b8, one for each combination of a bf and a discrete action.\nto find the q-value of a continuous state-action pair (x, u), the action is discretized\n(e.g., using nearest-neighbor discretization). assume the result of discretization is the\ndiscrete action \u201cforward\u201d; then, the q-value is computed by adding the parameters\n\u03b81,forward, . . . ,\u03b8n,forward corresponding to this discrete action, where the parameters\nare weighted by the value of their corresponding bfs at x:\n\nbq(x, forward) =\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8i,forward\n\n(1.4)\n\nthe dp/rl algorithm therefore only needs to remember the 4n parameters, which\ncan easily be done when n is not too large. note that this type of q-function repre-\nsentation generalizes to any dp/rl problem. even in problems with a finite number\nof discrete states and actions, compact representations can still be useful by reducing\nthe number of values that must be stored.\n\nwhile not all dp and rl algorithms employ q-functions, they all generally re-\nquire compact representations, so the illustration above extends to the general case.\nconsider, e.g., the problem of representing a policy h. an exact representation would\ngenerally require storing distinct actions for every possible state, which is impos-\nsible when the state variables are continuous. note that continuous actions are not\nproblematic for policy representation.\n\nit should be emphasized at this point that, in general, a compact representation\ncan only represent the target function up to a certain approximation error, which\nmust be accounted for. hence, in the sequel such representations are called \u201cfunction\napproximators,\u201d or \u201capproximators\u201d for short.\n\napproximation in dp and rl is not only a problem of representation. assume for\ninstance that an approximation of the optimal q-function is available. to obtain an\napproximately optimal policy, (1.3) must be applied, which requires maximizing the\nq-function over the action variable. in large or continuous action spaces, this is a po-\ntentially difficult optimization problem, which can only be solved approximately in\ngeneral. however, when a discrete-action q-function of the form (1.4) is employed,\nit is sufficient to compute the q-values of all the discrete actions and to find the\nmaximum among these values using enumeration. this provides a motivation for\nusing discretized actions. besides approximate maximization, other approximation\ndifficulties also arise, such as the estimation of expected values from samples. these\nadditional challenges are outside the scope of this section, and will be discussed in\ndetail in chapter 3.\n\nthe classical dp and rl algorithms are only guaranteed to obtain an optimal so-\nlution if they use exact representations. therefore, the following important questions\nmust be kept in mind when using function approximators:\n\n\u2022 if the algorithm is iterative, does it converge when approximation is employed?\n\nor, if the algorithm is not iterative, does it obtain a meaningful solution?\n\n\u2022 if a meaningful solution is obtained, is it near optimal, and more specifically,\n\nhow far is it from the optimal solution?\n\n "}, {"Page_number": 19, "text": "8\n\nchapter 1. introduction\n\n\u2022 is the algorithm consistent, i.e., does it asymptotically obtain the optimal solu-\n\ntion as the approximation power grows?\n\nthese questions will be taken into account when discussing algorithms for approxi-\nmate dp and rl.\n\nchoosing an appropriate function approximator for a given problem is a highly\nnontrivial task. the complexity of the approximator must be managed, since it di-\nrectly influences the memory and computational costs of the dp and rl algorithm.\nthis is an important concern in both approximate dp and approximate rl. equally\nimportant in approximate rl are the restrictions imposed by the limited amount of\ndata available, since in general a more complex approximator requires more data to\ncompute an accurate solution. if prior knowledge about the function of interest is\navailable, it can be used in advance to design a lower-complexity, but still accurate,\napproximator. for instance, bfs with intuitive, relevant meanings could be defined\n(such as, in the navigation problem, bfs representing the distance between the robot\nand the goal or the obstacle). however, prior knowledge is often unavailable, espe-\ncially in the model-free context of rl. in this book, we will therefore pay special\nattention to techniques that automatically find low-complexity approximators suited\nto the problem at hand, rather than relying on manual design.\n\n1.3 about this book\n\nthis book focuses on approximate dynamic programming (dp) and reinforcement\nlearning (rl) for control problems with continuous variables. the material is aimed\nat researchers, practitioners, and graduate students in the fields of systems and control\n(in particular optimal, adaptive, and learning control), computer science (in partic-\nular machine learning and artificial intelligence), operations research, and statistics.\nalthough not primarily intended as a textbook, our book can nevertheless be used as\nsupport for courses that treat dp and rl methods.\n\nfigure 1.7 presents a road map for the remaining chapters of this book, which we\nwill detail next. chapters 2 and 3 are prerequisite for the remainder of the book and\nshould be read in sequence. in particular, in chapter 2 the dp and rl problem and\nits solution are formalized, representative classical algorithms are introduced, and the\nbehavior of several such algorithms is illustrated in an example with discrete states\nand actions. chapter 3 gives an extensive account of dp and rl methods with func-\ntion approximation, which are applicable to large and continuous-space problems. a\ncomprehensive selection of algorithms is introduced, theoretical guarantees are pro-\nvided on the approximate solutions obtained, and numerical examples involving the\ncontrol of a continuous-variable system illustrate the behavior of several representa-\ntive algorithms.\n\nthe material of chapters 2 and 3 is organized along three major classes of dp\nand rl algorithms: value iteration, policy iteration, and policy search. in order to\nstrengthen the understanding of these three classes of algorithms, each of the three\n\n "}, {"Page_number": 20, "text": "1.3. about this book\n\n9\n\nchapter 2.\n\nan introduction to dp and rl\n\nchapter 3.\n\ndp and rl in\n\nlarge and continuous spaces\n\nchapter 4.\n\napproximate value iteration\nwith a fuzzy representation\n\nchapter 5.\n\napproximate policy iteration\n\nfor online learning\n\nand continuous-action control\n\nchapter 6.\n\napproximate policy search\n\nwith cross-entropy optimization\n\nof basis functions\n\nfigure 1.7\na road map for the remainder of this book, given in a graphical form. the full arrows indicate\nthe recommended sequence of reading, whereas dashed arrows indicate optional ordering.\n\nfinal chapters of the book considers in detail an algorithm from one of these classes.\nspecifically, in chapter 4, a value iteration algorithm with fuzzy approximation is\ndiscussed, and an extensive theoretical analysis of this algorithm illustrates how con-\nvergence and consistency guarantees can be developed for approximate dp. in chap-\nter 5, an algorithm for approximate policy iteration is discussed. in particular, an\nonline variant of this algorithm is developed, and some important issues that appear\nin online rl are emphasized along the way. in chapter 6, a policy search approach\nrelying on the cross-entropy method for optimization is described, which highlights\none possibility to develop techniques that scale to relatively high-dimensional state\nspaces, by focusing the computation on important initial states. the final part of each\nof these three chapters contains an experimental evaluation on a representative selec-\ntion of control problems.\n\nchapters 4, 5, and 6 can be read in any order, although, if possible, they should\n\nbe read in sequence.\n\ntwo appendices are included at the end of the book (these are not shown in\nfigure 1.7). appendix a outlines the so-called ensemble of extremely randomized\ntrees, which is used as an approximator in chapters 3 and 4. appendix b describes\nthe cross-entropy method for optimization, employed in chapters 4 and 6. reading\nappendix b before these two chapters is not mandatory, since both chapters include\na brief, specialized introduction to the cross-entropy method, so that they can more\neasily be read independently.\n\nadditional information and material concerning this book, including the com-\n\nputer code used in the experimental studies, is available at the web site:\n\nhttp://www.dcsc.tudelft.nl/rlbook/\n\n "}, {"Page_number": 21, "text": "2\n\nan introduction to dynamic programming and\nreinforcement learning\n\nthis chapter introduces dynamic programming and reinforcement learning tech-\nniques, and the formal model behind the problem they solve: the markov decision\nprocess. deterministic and stochastic markov decision processes are discussed in\nturn, and their optimal solution is characterized. three categories of dynamic pro-\ngramming and reinforcement learning algorithms are described: value iteration, pol-\nicy iteration, and policy search.\n\n2.1 introduction\n\nin dynamic programming (dp) and reinforcement learning (rl), a controller (agent,\ndecision maker) interacts with a process (environment), by means of three signals: a\nstate signal, which describes the state of the process, an action signal, which allows\nthe controller to influence the process, and a scalar reward signal, which provides the\ncontroller with feedback on its immediate performance. at each discrete time step,\nthe controller receives the state measurement and applies an action, which causes the\nprocess to transition into a new state. a reward is generated that evaluates the quality\nof this transition. the controller receives the new state measurement, and the whole\ncycle repeats. this flow of interaction is represented in figure 2.1 (repeated from\nfigure 1.2).\n\nreward function\n\nreward\n\ncontroller\n\naction\n\nstate\n\nprocess\n\nfigure 2.1 the flow of interaction in dp and rl.\n\nthe behavior of the controller is dictated by its policy, a function from states into\nactions. the behavior of the process is described by its dynamics, which determine\n\n11\n\n "}, {"Page_number": 22, "text": "12\n\nchapter 2. an introduction to dp and rl\n\nhow the state changes as a result of the controller\u2019s actions. state transitions can be\ndeterministic or stochastic. in the deterministic case, taking a given action in a given\nstate always results in the same next state, while in the stochastic case, the next state\nis a random variable. the rule according to which rewards are generated is described\nby the reward function. the process dynamics and the reward function, together with\nthe set of possible states and the set of possible actions (respectively called state\nspace and action space), constitute a so-called markov decision process (mdp).\n\nin the dp/rl setting, the goal is to find an optimal policy that maximizes the\n(expected) return, consisting of the (expected) cumulative reward over the course\nof interaction. in this book, we will mainly consider infinite-horizon returns, which\naccumulate rewards along infinitely long trajectories. this choice is made because\ninfinite-horizon returns have useful theoretical properties. in particular, they lead to\nstationary optimal policies, which means that for a given state, the optimal action\nchoices will always be the same, regardless of the time when that state is encountered.\n\nthe dp/rl framework can be used to address problems from a variety of fields,\nincluding, e.g., automatic control, artificial intelligence, operations research, and eco-\nnomics. automatic control and artificial intelligence are arguably the most important\nfields of origin for dp and rl. in automatic control, dp can be used to solve non-\nlinear and stochastic optimal control problems (bertsekas, 2007), while rl can al-\nternatively be seen as adaptive optimal control (sutton et al., 1992; vrabie et al.,\n2009). in artificial intelligence, rl helps to build an artificial agent that learns how\nto survive and optimize its behavior in an unknown environment, without requiring\nprior knowledge (sutton and barto, 1998). because of this mixed inheritance, two\nsets of equivalent names and notations are used in dp and rl, e.g., \u201ccontroller\u201d has\nthe same meaning as \u201cagent,\u201d and \u201cprocess\u201d has the same meaning as \u201cenvironment.\u201d\nin this book, we will use the former, control-theoretical terminology and notation.\n\na taxonomy of dp and rl algorithms is shown in figure 2.2 and detailed in the\n\nremainder of this section.\n\nvalue iteration\n\ndp, model-based\nalgorithms\n\npolicy iteration\n\npolicy search\n\nvalue iteration\n\nrl, model-free\nalgorithms\n\npolicy iteration\n\npolicy search\n\noffline\n\nonline\n\noffline\n\nonline\n\noffline\n\nonline\n\nfigure 2.2 a taxonomy of dp and rl algorithms.\n\n "}, {"Page_number": 23, "text": "2.1. introduction\n\n13\n\ndp algorithms require a model of the mdp, including the transition dynamics\nand the reward function, to find an optimal policy (bertsekas, 2007; powell, 2007).\nthe model dp algorithms work offline, producing a policy which is then used to\ncontrol the process.1 usually, they do not require an analytical expression of the\ndynamics. instead, given a state and an action, the model is only required to generate\na next state and the corresponding reward. constructing such a generative model is\noften easier than deriving an analytical expression of the dynamics, especially when\nthe dynamics are stochastic.\n\nrl algorithms are model-free (bertsekas and tsitsiklis, 1996; sutton and barto,\n1998), which makes them useful when a model is difficult or costly to construct. rl\nalgorithms use data obtained from the process, in the form of a set of samples, a set of\nprocess trajectories, or a single trajectory. so, rl can be seen as model-free, sample-\nbased or trajectory-based dp, and dp can be seen as model-based rl. while dp\nalgorithms can use the model to obtain any number of sample transitions from any\nstate-action pair, rl algorithms must work with the limited data that can be obtained\nfrom the process \u2013 a greater challenge. note that some rl algorithms build a model\nfrom the data; we call these algorithms \u201cmodel-learning.\u201d\n\nboth the dp and rl classes of algorithms can be broken down into three sub-\nclasses, according to the path taken to find an optimal policy. these three sub-\nclasses are value iteration, policy iteration, and policy search, and are characterized\nas follows.\n\n\u2022 value iteration algorithms search for the optimal value function, which con-\nsists of the maximal returns from every state or from every state-action pair.\nthe optimal value function is used to compute an optimal policy.\n\n\u2022 policy iteration algorithms evaluate policies by constructing their value func-\ntions (instead of the optimal value function), and use these value functions to\nfind new, improved policies.\n\n\u2022 policy search algorithms use optimization techniques to directly search for an\n\noptimal policy.\n\nnote that, in this book, we use the name dp to refer to the class of all model-based\nalgorithms that find solutions for mdps, including model-based policy search. this\nclass is larger than the category of algorithms traditionally called dp, which only\nincludes model-based value iteration and policy iteration (bertsekas and tsitsiklis,\n1996; sutton and barto, 1998; bertsekas, 2007).\n\nwithin each of the three subclasses of rl algorithms, two categories can be fur-\nther distinguished, namely offline and online algorithms. offline rl algorithms use\ndata collected in advance, whereas online rl algorithms learn a solution by interact-\ning with the process. online rl algorithms are typically not provided with any data\n\n1there is also a class of model-based, dp-like online algorithms called model-predictive control (ma-\nciejowski, 2002; camacho and bordons, 2004). in order to restrict the scope of the book, we do not discuss\nmodel-predictive control. for details about the relationship of dp/rl with model-predictive control, see,\ne.g., (bertsekas, 2005b; ernst et al., 2009).\n\n "}, {"Page_number": 24, "text": "14\n\nchapter 2. an introduction to dp and rl\n\nin advance, but instead have to rely only on the data they collect while learning, and\nthus are useful when it is difficult or costly to obtain data in advance. most online\nrl algorithms work incrementally. for instance, an incremental, online value itera-\ntion algorithm updates its estimate of the optimal value function after each collected\nsample. even before this estimate becomes accurate, it is used to derive estimates of\nan optimal policy, which are then used to collect new data.\n\nonline rl algorithms must balance the need to collect informative data (by ex-\nploring novel action choices or novel parts of the state space) with the need to control\nthe process well (by exploiting the currently available knowledge). this exploration-\nexploitation trade-off makes online rl more challenging than offline rl. note that,\nalthough online rl algorithms are only guaranteed (under appropriate conditions) to\nconverge to an optimal policy when the process does not change over time, in prac-\ntice they are sometimes applied also to slowly changing processes, in which case\nthey are expected to adapt the solution so that the changes are taken into account.\n\nthe remainder of this chapter is structured as follows. section 2.2 describes\nmdps and characterizes the optimal solution for an mdp, in the deterministic as\nwell as in the stochastic setting. the class of value iteration algorithms is introduced\nin section 2.3, policy iteration in section 2.4, and policy search in section 2.5. when\nintroducing value iteration and policy iteration, dp and rl algorithms are described\nin turn, while the introduction of policy search focuses on the model-based, dp set-\nting. section 2.6 concludes the chapter with a summary and discussion. throughout\nthe chapter, a simulation example involving a highly abstracted robotic task is em-\nployed to illustrate certain theoretical points, as well as the properties of several\nrepresentative algorithms.\n\n2.2 markov decision processes\n\ndp and rl problems can be formalized with the help of mdps (puterman, 1994).\nwe first present the simpler case of mdps with deterministic state transitions. after-\nwards, we extend the theory to the stochastic case.\n\n2.2.1 deterministic setting\n\na deterministic mdp is defined by the state space x of the process, the action space\nu of the controller, the transition function f of the process (which describes how\nthe state changes as a result of control actions), and the reward function \u03c1 (which\nevaluates the immediate control performance).2 as a result of the action uk applied\nin the state xk at the discrete time step k, the state changes to xk+1, according to the\n\n2as mentioned earlier, control-theoretic notation is used instead of artificial intelligence notation. for\ninstance, in the artificial intelligence literature on dp and rl, the state space is usually denoted by s, the\nstate by s, the action space by a, the action by a, and the policy by \u03c0.\n\n "}, {"Page_number": 25, "text": "2.2. markov decision processes\n\n15\n\ntransition function f : x \u00d7 u \u2192 x :\n\nxk+1 = f (xk, uk)\n\nat the same time, the controller receives the scalar reward signal rk+1, according to\nthe reward function \u03c1 : x \u00d7 u \u2192 r:\n\nrk+1 = \u03c1(xk, uk)\n\nwhere we assume that k\u03c1k\u221e = supx,u|\u03c1(x, u)| is finite.3 the reward evaluates the\nimmediate effect of action uk, namely the transition from xk to xk+1, but in general\ndoes not say anything about its long-term effects.\n\nthe controller chooses actions according to its policy h : x \u2192 u , using:\n\nuk = h(xk)\n\ngiven f and \u03c1, the current state xk and the current action uk are sufficient to\ndetermine both the next state xk+1 and the reward rk+1. this is the markov property,\nwhich is essential in providing theoretical guarantees about dp/rl algorithms.\n\nsome mdps have terminal states that, once reached, can no longer be left; all\nthe rewards received in terminal states are 0. the rl literature often uses \u201ctrials\u201d\nor \u201cepisodes\u201d to refer to trajectories starting from some initial state and ending in a\nterminal state.\n\nexample 2.1 the deterministic cleaning-robot mdp. consider the deterministic\nproblem depicted in figure 2.3: a cleaning robot has to collect a used can and also\nhas to recharge its batteries.\n\nr=1\n\nr=5\n\nu=-1\n\nu=1\n\nx=0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nfigure 2.3 the cleaning-robot problem.\n\nin this problem, the state x describes the position of the robot, and the action\nu describes the direction of its motion. the state space is discrete and contains six\ndistinct states, denoted by integers 0 to 5: x = {0, 1, 2, 3, 4, 5}. the robot can move\nto the left (u = \u22121) or to the right (u = 1); the discrete action space is therefore\nu = {\u22121, 1}. states 0 and 5 are terminal, meaning that once the robot reaches either\nof them it can no longer leave, regardless of the action. the corresponding transition\nfunction is:\n\nf (x, u) =(x + u\n\nx\n\nif 1 \u2264 x \u2264 4\nif x = 0 or x = 5 (regardless of u)\n\n3to simplify the notation, whenever searching for extrema, performing summations, etc., over vari-\nables whose domains are obvious from the context, we omit these domains from the formulas. for in-\nstance, in the formula supx,u |\u03c1(x, u)|, the domains of x and u are clearly x and u , so they are omitted.\n\n "}, {"Page_number": 26, "text": "16\n\nchapter 2. an introduction to dp and rl\n\nin state 5, the robot finds a can and the transition into this state is rewarded with\n5. in state 0, the robot can recharge its batteries and the transition into this state is\nrewarded with 1. all other rewards are 0. in particular, taking any action while in a\nterminal state results in a reward of 0, which means that the robot will not accumulate\n(undeserved) rewards in the terminal states. the corresponding reward function is:\n\n\u03c1(x, u) =\uf8f1\uf8f4\uf8f2\uf8f4\uf8f3\n\n5\n\n1\n\n0\n\nif x = 4 and u = 1\nif x = 1 and u = \u22121\notherwise\n\n(cid:3)\n\noptimality in the deterministic setting\n\nin dp and rl, the goal is to find an optimalpolicy that maximizes the return from any\ninitial state x0. the return is a cumulative aggregation of rewards along a trajectory\nstarting at x0. it concisely represents the reward obtained by the controller in the\nlong run. several types of return exist, depending on the way in which the rewards\nare accumulated (bertsekas and tsitsiklis, 1996, section 2.1; kaelbling et al., 1996).\nthe infinite-horizon discounted return is given by:\n\nrh(x0) =\n\n\u221e\n\u2211\nk=0\n\n\u03b3krk+1 =\n\n\u221e\n\u2211\nk=0\n\n\u03b3k\u03c1(xk, h(xk))\n\n(2.1)\n\nwhere \u03b3\u2208 [0, 1) is the discountfactor and xk+1 = f (xk, h(xk)) for k \u2265 0. the discount\nfactor can be interpreted intuitively as a measure of how \u201cfar-sighted\u201d the controller\nis in considering its rewards, or as a way of taking into account increasing uncertainty\nabout future rewards. from a mathematical point of view, discounting ensures that\nthe return will always be bounded if the rewards are bounded. the goal is therefore\nto maximize the long-term performance (return), while only using feedback about\nthe immediate, one-step performance (reward). this leads to the so-called challenge\nof delayed rewards (sutton and barto, 1998): actions taken in the present affect the\npotential to achieve good rewards far in the future, but the immediate reward provides\nno information about these long-term effects.\n\nother types of return can also be defined. the undiscounted return, obtained by\nsetting \u03b3 equal to 1 in (2.1), simply adds up the rewards, without discounting. unfor-\ntunately, the infinite-horizon undiscounted return is often unbounded. an alternative\nis to use the infinite-horizon average return:\n\n1\n\nk\n\nk\n\n\u2211\nk=0\n\nlim\nk\u2192\u221e\n\n\u03c1(xk, h(xk))\n\nwhich is bounded in many cases. finite-horizon returns can be obtained by accumu-\nlating rewards along trajectories of a fixed, finite length k (the horizon), instead of\nalong infinitely long trajectories. for instance, the finite-horizon discounted return\ncan be defined as:\n\nk\n\n\u2211\nk=0\n\n\u03b3k\u03c1(xk, h(xk))\n\n "}, {"Page_number": 27, "text": "2.2. markov decision processes\n\n17\n\nthe undiscounted return (\u03b3 = 1) can be used more easily in the finite-horizon case,\nas it is bounded when the rewards are bounded.\n\nin this book, we will mainly use the infinite-horizon discounted return (2.1), be-\ncause it has useful theoretical properties. in particular, for this type of return, under\ncertain technical assumptions, there always exists at least one stationary, determinis-\n\ntic optimal policy h\u2217 : x \u2192 u (bertsekas and shreve, 1978, chapter 9). in contrast,\n\nin the finite-horizon case, optimal policies depend in general on the time step k, i.e.,\nthey are nonstationary (bertsekas, 2005a, chapter 1).\n\nwhile the discount factor \u03b3 can theoretically be regarded as a given part of the\nproblem, in practice, a good value of \u03b3 has to be chosen. choosing \u03b3 often involves a\ntrade-off between the quality of the solution and the convergence rate of the dp/rl\nalgorithm, for the following reasons. some important dp/rl algorithms converge\nfaster when \u03b3 is smaller (this is the case, e.g., for model-based value iteration, which\nwill be introduced in section 2.3). however, if \u03b3 is too small, the solution may be\nunsatisfactory because it does not sufficiently take into account rewards obtained\nafter a large number of steps.\n\nthere is no generally valid procedure for choosing \u03b3. consider however, as an\nexample, a typical stabilization problem from automatic control, where from every\ninitial state the process should reach a steady state and remain there. in such a prob-\nlem, \u03b3 should be chosen large enough that the rewards received upon reaching the\nsteady state and remaining there have a detectable influence on the returns from ev-\nery initial state. for instance, if the number of steps taken by a reasonable policy to\nstabilize the system from an initial state x is k(x), then \u03b3 should be chosen so that\n\u03b3kmax is not too small, where kmax = maxx k(x). however, finding kmax is a diffi-\ncult problem in itself, which could be solved using, e.g., domain knowledge, or a\nsuboptimal policy obtained by other means.\n\nvalue functions and the bellman equations in the deterministic setting\n\na convenient way to characterize policies is by using their value functions. two\ntypes of value functions exist: state-action value functions (q-functions) and state\nvalue functions (v-functions). note that the name \u201cvalue function\u201d is often used for\nv-functions in the literature. we will use the names \u201cq-function\u201d and \u201cv-function\u201d to\nclearly differentiate between the two types of value functions, and the name \u201cvalue\nfunction\u201d to refer to q-functions and v-functions collectively. we will first define\nand characterize q-functions, and then turn our attention to v-functions.\n\nthe q-function qh : x \u00d7 u \u2192 r of a policy h gives the return obtained when\n\nstarting from a given state, applying a given action, and following h thereafter:\n\nqh(x, u) = \u03c1(x, u) + \u03b3rh( f (x, u))\n\n(2.2)\n\nhere, rh( f (x, u)) is the return from the next state f (x, u). this concise formula can\nbe obtained by first writing qh(x, u) explicitly as the discounted sum of rewards\nobtained by taking u in x and then following h:\n\nqh(x, u) =\n\n\u221e\n\u2211\nk=0\n\n\u03b3k\u03c1(xk, uk)\n\n "}, {"Page_number": 28, "text": "18\n\nchapter 2. an introduction to dp and rl\n\nwhere (x0, u0) = (x, u), xk+1 = f (xk, uk) for k \u2265 0, and uk = h(xk) for k \u2265 1. then,\nthe first term is separated from the sum:\n\nqh(x, u) = \u03c1(x, u) +\n\n\u03b3k\u03c1(xk, uk)\n\n\u221e\n\u2211\nk=1\n\u221e\n\u2211\nk=1\n\n= \u03c1(x, u) + \u03b3\n\n\u03b3k\u22121\u03c1(xk, h(xk))\n\n(2.3)\n\n= \u03c1(x, u) + \u03b3rh( f (x, u))\n\nwhere the definition (2.1) of the return was used in the last step. so, (2.2) has been\nobtained.\n\nthe optimal q-function is defined as the best q-function that can be obtained by\n\nany policy:\n\nq\u2217(x, u) = max\n\nh\n\nqh(x, u)\n\n(2.4)\n\nany policy h\u2217 that selects at each state an action with the largest optimal q-value,\ni.e., that satisfies:\n\nh\u2217(x) \u2208 arg max\n\nu\n\nq\u2217(x, u)\n\n(2.5)\n\nis optimal (it maximizes the return). in general, for a given q-function q, a policy h\nthat satisfies:\n\nh(x) \u2208 arg max\n\nu\n\nq(x, u)\n\n(2.6)\n\nis said to be greedy in q. so, finding an optimal policy can be done by first finding\nq\u2217, and then using (2.5) to compute a greedy policy in q\u2217.\n\nnote that, for simplicity of notation, we implicitly assume that the maximum in\n(2.4) exists, and also in similar equations in the sequel. when the maximum does\nnot exist, the \u201cmax\u201d operator should be replaced by the supremum operator. for the\ncomputation of greedy actions in (2.5), (2.6), and in similar equations in the sequel,\nthe maximum must exist to ensure the existence of a greedy policy; this can be guar-\nanteed under certain technical assumptions (bertsekas and shreve, 1978, chapter 9).\nthe q-functions qh and q\u2217 are recursively characterized by the bellman equa-\ntions, which are of central importance for value iteration and policy iteration algo-\nrithms. the bellman equation for qh states that the value of taking action u in state x\nunder the policy h equals the sum of the immediate reward and the discounted value\nachieved by h in the next state:\n\nqh(x, u) = \u03c1(x, u) + \u03b3qh( f (x, u), h( f (x, u)))\n\n(2.7)\n\nthis bellman equation can be derived from the second step in (2.3), as follows:\n\nqh(x, u) = \u03c1(x, u) + \u03b3\n\n\u03b3k\u22121\u03c1(xk, h(xk))\n\n\u221e\n\u2211\nk=1\n\n= \u03c1(x, u) + \u03b3(cid:20)\u03c1( f (x, u), h( f (x, u))) + \u03b3\n\n\u221e\n\u2211\nk=2\n\n\u03b3k\u22122\u03c1(xk, h(xk))(cid:21)\n\n= \u03c1(x, u) + \u03b3qh( f (x, u), h( f (x, u)))\n\n "}, {"Page_number": 29, "text": "2.2. markov decision processes\n\n19\n\nwhere (x0, u0) = (x, u), xk+1 = f (xk, uk) for k \u2265 0, and uk = h(xk) for k \u2265 1.\nthe bellman optimality equation characterizes q\u2217, and states that the optimal\nvalue of action u taken in state x equals the sum of the immediate reward and the\ndiscounted optimal value obtained by the best action in the next state:\n\nq\u2217(x, u) = \u03c1(x, u) + \u03b3max\nu\u2032\n\nq\u2217( f (x, u), u\u2032)\n\n(2.8)\n\nthe v-function v h : x \u2192 r of a policy h is the return obtained by starting from\n\na particular state and following h. this v-function can be computed from the q-\nfunction of policy h:\n\nv h(x) = rh(x) = qh(x, h(x))\n\n(2.9)\n\nthe optimal v-function is the best v-function that can be obtained by any policy, and\ncan be computed from the optimal q-function:\n\nv \u2217(x) = max\n\nh\n\nv h(x) = max\n\nu\n\nq\u2217(x, u)\n\n(2.10)\n\nan optimal policy h\u2217 can be computed from v \u2217, by using the fact that it satisfies:\n\nh\u2217(x) \u2208 arg max\n\nu\n\n[\u03c1(x, u) + \u03b3v\u2217( f (x, u))]\n\n(2.11)\n\nusing this formula is more difficult than using (2.5); in particular, a model of the\nmdp is required in the form of the dynamics f and the reward function \u03c1. because\nthe q-function also depends on the action, it already includes information about the\nquality of transitions. in contrast, the v-function only describes the quality of the\nstates; in order to infer the quality of transitions, they must be explicitly taken into\naccount. this is what happens in (2.11), and this also explains why it is more difficult\nto compute policies from v-functions. because of this difference, q-functions will\nbe preferred to v-functions throughout this book, even though they are more costly\nto represent than v-functions, as they depend both on x and u.\n\nthe v-functions v h and v\u2217 satisfy the following bellman equations, which can\n\nbe interpreted similarly to (2.7) and (2.8):\n\nv h(x) = \u03c1(x, h(x)) + \u03b3v h( f (x, h(x)))\n\nv \u2217(x) = max\n\nu\n\n[\u03c1(x, u) + \u03b3v\u2217( f (x, u))]\n\n(2.12)\n\n(2.13)\n\n2.2.2 stochastic setting\n\nin a stochastic mdp, the next state is not deterministically given by the current state\nand action. instead, the next state is a random variable, and the current state and\naction give the probability density of this random variable.\n\nmore formally, the deterministic transition function f is replaced by a transition\n\nprobability function \u02dcf : x \u00d7 u \u00d7 x \u2192 [0, \u221e). after action uk is taken in state xk, the\nprobability that the next state, xk+1, belongs to a region xk+1 \u2286 x is:\n\np (xk+1 \u2208 xk+1| xk, uk) =zxk+1\n\n\u02dcf (xk, uk, x\u2032)dx\u2032\n\n "}, {"Page_number": 30, "text": "20\n\nchapter 2. an introduction to dp and rl\n\nfor any x and u, \u02dcf (x, u,\u00b7) must define a valid probability density function of the\nargument \u201c\u00b7\u201d, where the dot stands for the random variable xk+1. because rewards\nare associated with transitions, and the transitions are no longer fully determined by\nthe current state and action, the reward function also has to depend on the next state,\n\u02dc\u03c1 : x \u00d7 u \u00d7 x \u2192 r. after each transition to a state xk+1, a reward rk+1 is received\naccording to:\n\nrk+1 = \u02dc\u03c1(xk, uk, xk+1)\n\nwhere we assume that k \u02dc\u03c1k\u221e = supx,u,x\u2032 \u02dc\u03c1(x, u, x\u2032) is finite. note that \u02dc\u03c1 is a deter-\nministic function of the transition (xk, uk, xk+1). this means that, once xk+1 has been\ngenerated, the reward rk+1 is fully determined. in general, the reward can also depend\nstochastically on the entire transition (xk, uk, xk+1). if it does, to simplify notation, we\nassume that \u02dc\u03c1 gives the expected reward after the transition.\n\nwhen the state space is countable (e.g., discrete), the transition function can also\n\nbe given as \u00aff : x \u00d7 u \u00d7 x \u2192 [0, 1], where the probability of reaching x\u2032 after taking\nuk in xk is:\n\np(cid:0)xk+1 = x\u2032| xk, uk(cid:1) = \u00aff (xk, uk, x\u2032)\n\n(2.14)\n\n\u00aff (x, u, x\u2032) = 1. the function \u02dcf is a\nfor any x and u, the function \u00aff must satisfy \u2211x\u2032\ngeneralization of \u00aff to uncountable (e.g., continuous) state spaces; in such spaces, the\nprobability of ending up in a given state x\u2032 is generally 0, making a description of the\nform \u00aff inappropriate.\n\nin the stochastic case, the markov property requires that xk and uk fully determine\n\nthe probability density of the next state.\n\ndeveloping an analytical expression for the transition probability function \u02dcf is\ngenerally a difficult task. fortunately, as previously noted in section 2.1, most dp\n(model-based) algorithms can work with a generative model, which only needs to\ngenerate samples of the next state and corresponding rewards for any given pair of\ncurrent state and action taken.\n\nexample 2.2 the stochastic cleaning-robot mdp. consider again the cleaning-\nrobot problem of example 2.1. assume that, due to uncertainties in the environment,\nsuch as a slippery floor, state transitions are no longer deterministic. when trying\nto move in a certain direction, the robot succeeds with a probability of 0.8. with a\nprobability of 0.15 it remains in the same state, and it may even move in the opposite\ndirection with a probability of 0.05 (see also figure 2.4).\n\np=0.05\n\nx=0\n\n1\n\n2\n\np=0.15\n\np=0.8\n\n4\n\n5\n\nu\n\n3\n\nfigure 2.4\nthe stochastic cleaning-robot problem. the robot intends to move right, but it may instead end\nup standing still or moving left, with different probabilities.\n\n "}, {"Page_number": 31, "text": "2.2. markov decision processes\n\n21\n\nbecause the state space is discrete, a transition model of the form (2.14) is appro-\npriate. the transition function \u00aff that models the probabilistic transitions described\nabove is shown in table 2.1. in this table, the rows correspond to combinations of\ncurrent states and actions taken, while the columns correspond to future states. note\nthat the transitions from any terminal state still lead deterministically to the same\nterminal state, regardless of the action.\n\ntable 2.1 dynamics of the stochastic, cleaning-robot mdp.\n\n(x, u)\n(0,\u22121)\n(1,\u22121)\n(2,\u22121)\n(3,\u22121)\n(4,\u22121)\n(5,\u22121)\n(0, 1)\n(1, 1)\n(2, 1)\n(3, 1)\n(4, 1)\n(5, 1)\n\n\u00aff (x, u, 0)\n\n\u00aff (x, u, 1)\n\n\u00aff (x, u, 2)\n\n\u00aff (x, u, 3)\n\n\u00aff (x, u, 4)\n\n\u00aff (x, u, 5)\n\n1\n0.8\n0\n0\n0\n0\n1\n\n0.05\n\n0\n0\n0\n0\n\n0\n\n0.15\n0.8\n0\n0\n0\n0\n\n0.15\n0.05\n\n0\n0\n0\n\n0\n\n0.05\n0.15\n0.8\n0\n0\n0\n0.8\n0.15\n0.05\n\n0\n0\n\n0\n0\n\n0.05\n0.15\n0.8\n0\n0\n0\n0.8\n0.15\n0.05\n\n0\n\n0\n0\n0\n\n0.05\n0.15\n\n0\n0\n0\n0\n0.8\n0.15\n\n0\n\n0\n0\n0\n0\n\n0.05\n\n1\n0\n0\n0\n0\n0.8\n1\n\nthe robot receives rewards as in the deterministic case: upon reaching state 5, it is\nrewarded with 5, and upon reaching state 0, it is rewarded with 1. the corresponding\nreward function, in the form \u02dc\u03c1 : x \u00d7 u \u00d7 x \u2192 r, is:\n\n\u02dc\u03c1(x, u, x\u2032) =\uf8f1\uf8f4\uf8f2\uf8f4\uf8f3\n\n5\n\n1\n\n0\n\nif x 6= 5 and x\u2032 = 5\nif x 6= 0 and x\u2032 = 0\n\notherwise\n\n(cid:3)\n\noptimality in the stochastic setting\n\nthe expected infinite-horizon discounted return of an initial state x0 under a (deter-\nministic) policy h is:4\n\nrh(x0) = lim\nk\u2192\u221e\n\n= lim\nk\u2192\u221e\n\nexk+1\u223c \u02dcf (xk,h(xk),\u00b7)( k\nexk+1\u223c \u02dcf (xk,h(xk),\u00b7)( k\n\n\u2211\nk=0\n\n\u2211\nk=0\n\n\u03b3krk+1)\n\u03b3k \u02dc\u03c1(xk, h(xk), xk+1))\n\n(2.15)\n\n4we assume that the mdp and the policies h have suitable properties such that the expected return and\nthe bellman equations in the remainder of this section are well defined. see, e.g., (bertsekas and shreve,\n1978, chapter 9) and (bertsekas, 2007, appendix a) for a discussion of these properties.\n\n "}, {"Page_number": 32, "text": "22\n\nchapter 2. an introduction to dp and rl\n\nwhere e denotes the expectation operator, and the notation xk+1 \u223c \u02dcf (xk, h(xk),\u00b7)\nmeans that the random variable xk+1 is drawn from the density \u02dcf (xk, h(xk),\u00b7) at each\n\nstep k. the discussion of section 2.2.1 regarding the interpretation and choice of\nthe discount factor also applies to the stochastic case. for any stochastic or deter-\nministic mdp, when using the infinite-horizon discounted return (2.15) or (2.1), and\nunder certain technical assumptions on the elements of the mdp, there exists at least\none stationary deterministic optimal policy (bertsekas and shreve, 1978, chapter 9).\ntherefore, we will mainly consider stationary deterministic policies in the sequel.\n\nexpected undiscounted, average, and finite-horizon returns (see section 2.2.1)\n\ncan be defined analogously to (2.15).\n\nvalue functions and the bellman equations in the stochastic setting\n\nto obtain the q-function of a policy h, the definition (2.2) is generalized to the\nstochastic case, as follows. the q-function is the expected return under the stochas-\ntic transitions, when starting in a particular state, applying a particular action, and\nfollowing the policy h thereafter:\n\nqh(x, u) = ex\u2032\u223c \u02dcf (x,u,\u00b7)n \u02dc\u03c1(x, u, x\u2032) + \u03b3rh(x\u2032)o\n\n(2.16)\n\nthe definition of the optimal q-function q\u2217 remains unchanged from the determin-\nistic case (2.4), and is repeated here for easy reference:\n\nq\u2217(x, u) = max\n\nh\n\nqh(x, u)\n\nsimilarly, optimal policies can still be computed from q\u2217 as in the deterministic case,\nbecause they satisfy (2.5), also repeated here:\n\nh\u2217(x) \u2208 arg max\n\nu\n\nq\u2217(x, u)\n\nthe bellman equations for qh and q\u2217 are given in terms of expectations over the\n\none-step stochastic transitions:\n\nqh(x, u) = ex\u2032\u223c \u02dcf (x,u,\u00b7)n \u02dc\u03c1(x, u, x\u2032) + \u03b3qh(x\u2032, h(x\u2032))o\nq\u2217(x\u2032, u\u2032)(cid:27)\nq\u2217(x, u) = ex\u2032\u223c \u02dcf (x,u,\u00b7)(cid:26) \u02dc\u03c1(x, u, x\u2032) + \u03b3max\n\nu\u2032\n\n(2.17)\n\n(2.18)\n\nthe definition of the v-function v h of a policy h, as well as of the optimal v-\n\nfunction v\u2217, are the same as for the deterministic case (2.9), (2.10):\n\nv h(x) = rh(x)\nv\u2217(x) = max\n\nv h(x)\n\nh\n\nhowever, the computation of optimal policies from v \u2217 becomes more difficult, in-\nvolving an expectation that did not appear in the deterministic case:\n\nh\u2217(x) \u2208 arg max\n\nu\n\nex\u2032\u223c \u02dcf (x,u,\u00b7)(cid:8) \u02dc\u03c1(x, u, x\u2032) + \u03b3v\u2217(x\u2032)(cid:9)\n\n(2.19)\n\n "}, {"Page_number": 33, "text": "2.3. value iteration\n\n23\n\nin contrast, computing an optimal policy from q\u2217 is as simple as in the deterministic\ncase, which is yet another reason for using q-functions in practice.\n\nthe bellman equations for v h and v\u2217 are obtained from (2.12) and (2.13), by\n\nconsidering expectations over the one-step stochastic transitions:\n\nv h(x) = ex\u2032\u223c \u02dcf (x,h(x),\u00b7)n \u02dc\u03c1(x, h(x), x\u2032) + \u03b3v h(x\u2032)o\nex\u2032\u223c \u02dcf (x,u,\u00b7)(cid:8) \u02dc\u03c1(x, u, x\u2032) + \u03b3v\u2217(x\u2032)(cid:9)\n\nv \u2217(x) = max\n\nu\n\n(2.20)\n\n(2.21)\n\nnote that in the bellman equation for v\u2217 (2.21), the maximization is outside the\nexpectation operator, whereas in the bellman equation for q\u2217 (2.18), the order of the\nexpectation and maximization is reversed.\n\nclearly, all the equations for deterministic mdps are a special case of the equa-\ntions for stochastic mdps. the deterministic case is obtained by using a degenerate\n\ndensity \u02dcf (x, u,\u00b7) that assigns all the probability mass to f (x, u). the deterministic\nreward function is obtained as \u03c1(x, u) = \u02dc\u03c1(x, u, f (x, u)).\n\nthe entire class of value iteration algorithms, introduced in section 2.3, revolves\naround solving the bellman optimality equations (2.18) or (2.21) to find, respectively,\nthe optimal q-function or the optimal v-function (in the deterministic case, (2.8) or\n(2.13) are solved instead). similarly, policy evaluation, which is a core component\nof the policy iteration algorithms introduced in section 2.4, revolves around solving\n(2.17) or (2.20) to find, respectively, qh or v h (in the deterministic case (2.7) or\n(2.12) are solved instead).\n\n2.3 value iteration\n\nvalue iteration techniques use the bellman optimality equation to iteratively com-\npute an optimal value function, from which an optimal policy is derived. we first\npresent dp (model-based) algorithms for value iteration, followed by rl (model-\nfree) algorithms. dp algorithms like v-iteration (bertsekas, 2007, section 1.3) solve\nthe bellman optimality equation by using knowledge of the transition and reward\nfunctions. rl techniques either learn a model, e.g., dyna (sutton, 1990), or do not\nuse an explicit model at all, e.g., q-learning (watkins and dayan, 1992).\n\n2.3.1 model-based value iteration\n\nwe will next introduce the model-based q-iteration algorithm, as an illustrative ex-\nample from the class of model-based value iteration algorithms. let the set of all\nthe q-functions be denoted by q. then, the q-iteration mapping t : q \u2192 q, com-\nputes the right-hand side of the bellman optimality equation (2.8) or (2.18) for any\n\n "}, {"Page_number": 34, "text": "24\n\nchapter 2. an introduction to dp and rl\n\nq-function.5 in the deterministic case, this mapping is:\n\n[t (q)](x, u) = \u03c1(x, u) + \u03b3max\n\nu\u2032\n\nq( f (x, u), u\u2032)\n\n(2.22)\n\nand in the stochastic case, it is:\n\n[t (q)](x, u) = ex\u2032\u223c \u02dcf (x,u,\u00b7)(cid:26) \u02dc\u03c1(x, u, x\u2032) + \u03b3max\n\nu\u2032\n\nq(x\u2032, u\u2032)(cid:27)\n\n(2.23)\n\nnote that if the state space is countable (e.g., finite), a transition model of the form\n(2.14) is appropriate, and the q-iteration mapping for the stochastic case (2.23) can\nbe written as the simpler summation:\n\n[t (q)](x, u) = \u2211\nx\u2032\n\n\u00aff (x, u, x\u2032)(cid:20) \u02dc\u03c1(x, u, x\u2032) + \u03b3max\n\nu\u2032\n\nq(x\u2032, u\u2032)(cid:21)\n\n(2.24)\n\nthe same notation is used for the q-iteration mapping both in the deterministic case\nand in the stochastic case, because the analysis given below applies to both cases,\nand the definition (2.22) of t is a special case of (2.23) (or of (2.24) for countable\nstate spaces).\n\nthe q-iteration algorithm starts from an arbitrary q-function q0 and at each\n\niteration \u2113 updates the q-function using:\n\nq\u2113+1 = t (q\u2113)\n\n(2.25)\n\nit can be shown that t is a contraction with factor \u03b3 < 1 in the infinity norm, i.e., for\nany pair of functions q and q\u2032, it is true that:\n\nkt (q)\u2212 t (q\u2032)k\u221e \u2264 \u03b3kq\u2212 q\u2032k\u221e\n\nbecause t is a contraction, it has a unique fixed point (istratescu, 2002). addition-\nally, when rewritten using the q-iteration mapping, the bellman optimality equation\n(2.8) or (2.18) states that q\u2217 is a fixed point of t , i.e.:\n\nq\u2217 = t (q\u2217)\n\n(2.26)\n\nhence, the unique fixed point of t is actually q\u2217, and q-iteration asymptotically\nconverges to q\u2217 as \u2113 \u2192 \u221e. moreover, q-iteration converges to q\u2217 at a rate of \u03b3, in\nthe sense that kq\u2113+1 \u2212 q\u2217k\u221e \u2264 \u03b3kq\u2113 \u2212 q\u2217k\u221e. an optimal policy can be computed\nfrom q\u2217 with (2.5).\n\nalgorithm 2.1 presents q-iteration for deterministic mdps in an explicit, proce-\ndural form, wherein t is computed using (2.22). similarly, algorithm 2.2 presents\nq-iteration for stochastic mdps with countable state spaces, using the expression\n(2.24) for t .\n\n5the term \u201cmapping\u201d is used to refer to functions that work with other functions as inputs and/or\noutputs; as well as to compositions of such functions. the term is used to differentiate mappings from\nordinary functions, which only have numerical scalars, vectors, or matrices as inputs and/or outputs.\n\n "}, {"Page_number": 35, "text": "2.3. value iteration\n\n25\n\nalgorithm 2.1 q-iteration for deterministic mdps.\ninput: dynamics f , reward function \u03c1, discount factor \u03b3\n1: initialize q-function, e.g., q0 \u2190 0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\nfor every (x, u) do\n\n4:\n\nq\u2113+1(x, u) \u2190 \u03c1(x, u) + \u03b3maxu\u2032 q\u2113( f (x, u), u\u2032)\n\nend for\n\n5:\n6: until q\u2113+1 = q\u2113\noutput: q\u2217 = q\u2113\n\nalgorithm 2.2 q-iteration for stochastic mdps with countable state spaces.\ninput: dynamics \u00aff , reward function \u02dc\u03c1, discount factor \u03b3\n1: initialize q-function, e.g., q0 \u2190 0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\n\u00aff (x, u, x\u2032) [ \u02dc\u03c1(x, u, x\u2032) + \u03b3maxu\u2032 q\u2113(x\u2032, u\u2032)]\n\n4:\n\nfor every (x, u) do\nq\u2113+1(x, u) \u2190 \u2211x\u2032\n\nend for\n\n5:\n6: until q\u2113+1 = q\u2113\noutput: q\u2217 = q\u2113\n\nthe results given above only guarantee the asymptotic convergence of q-\niteration, hence the stopping criterion of algorithms 2.1 and 2.2 may only be sat-\nisfied asymptotically. in practice, it is also important to guarantee the performance\nof q-iteration when the algorithm is stopped after a finite number of iterations. the\nfollowing result holds both in the deterministic case and in the stochastic case. given\na suboptimality bound \u03c2qi > 0, where the subscript \u201cqi\u201d stands for \u201cq-iteration,\u201d a\nfinite number l of iterations can be (conservatively) chosen with:\n\nl =(cid:24)log\u03b3\n\n\u03c2qi(1\u2212 \u03b3)2\n\n2k\u03c1k\u221e (cid:25)\n\n(2.27)\n\nso that the suboptimality of a policy hl that is greedy in ql is guaranteed to be at most\n\u03c2qi, in the sense that kv hl \u2212 v\u2217k\u221e \u2264 \u03c2qi. here, \u2308\u00b7\u2309 is the smallest integer larger than\n\nor equal to the argument (ceiling). equation (2.27) follows from the bound (ernst\net al., 2005):\n\nkv hl \u2212 v\u2217k\u221e \u2264 2\n\n\u03b3lk\u03c1k\u221e\n(1\u2212 \u03b3)2\n\u03b3lk\u03c1k\u221e\n(1\u2212\u03b3)2 \u2264 \u03c2qi.\n\non the suboptimality of hl, by requiring that 2\n\nalternatively, q-iteration could be stopped when the difference between two\nconsecutive q-functions decreases below a given threshold \u03b5qi > 0, i.e., when\nkq\u2113+1 \u2212 q\u2113k\u221e \u2264 \u03b5qi. this can also be guaranteed to happen after a finite number\nof iterations, due to the contracting nature of the q-iteration updates.\n\n "}, {"Page_number": 36, "text": "26\n\nchapter 2. an introduction to dp and rl\n\na v-iteration algorithm that computes the optimal v-function can be developed\nalong similar lines, using the bellman optimality equation (2.13) in the deterministic\ncase, or (2.21) in the stochastic case. note that the name \u201cvalue iteration\u201d is typically\nused for the v-iteration algorithm in the literature, whereas we use it to refer more\ngenerally to the entire class of algorithms that use the bellman optimality equations\nto compute optimal value functions. (recall that we similarly use \u201cvalue function\u201d\nto refer to q-functions and v-functions collectively.)\n\ncomputational cost of q-iteration for finite mdps\n\nnext, we investigate the computational cost of q-iteration when applied to an mdp\nwith a finite number of states and actions. denote by |\u00b7| the cardinality of the argu-\nment set \u201c\u00b7\u201d, so that |x| denotes the finite number of states and |u| denotes the finite\nnumber of actions.\nconsider first the deterministic case, for which algorithm 2.1 can be used. as-\nsume that, when updating the q-value for a given state-action pair (x, u), the max-\nimization over the action space u is solved by enumeration over its |u| elements,\nand f (x, u) is computed once and then stored and reused. updating the q-value then\nrequires 2 +|u| function evaluations, where the functions being evaluated are f , \u03c1,\nand the current q-function q\u2113. since at every iteration, the q-values of |x||u| state-\naction pairs have to be updated, the cost per iteration is |x||u| (2 +|u|). so, the total\ncost of l q-iterations for a deterministic, finite mdp is:\n\nl|x||u| (2 +|u|)\n\n(2.28)\n\nthe number l of iterations can be chosen, e.g., by imposing a suboptimality bound\n\u03c2qi and using (2.27).\n\nin the stochastic case, because the state space is finite, algorithm 2.2 can be used.\nassuming that the maximization over u\u2032 is implemented using enumeration, the cost\nof updating the q-value for a given pair (x, u) is |x| (2 +|u|), where the functions\nbeing evaluated are \u00aff , \u02dc\u03c1, and q\u2113. the cost per iteration is |x|2|u| (2 +|u|), and the\n\ntotal cost of l q-iterations for a stochastic, finite mdp is thus:\n\nl|x|2|u| (2 +|u|)\n\n(2.29)\n\nwhich is larger by a factor |x| than the cost (2.28) for the deterministic case.\n\nexample 2.3 q-iteration for the cleaning robot. in this example, we apply q-\niteration to the cleaning-robot problem of examples 2.1 and 2.2. the discount factor\n\u03b3 is set to 0.5.\n\nconsider first the deterministic variant of example 2.1. for this variant, q-\niteration is implemented as algorithm 2.1. starting from an identically zero initial\nq-function, q0 = 0, this algorithm produces the sequence of q-functions given in the\nfirst part of table 2.2 (above the dashed line), where each cell shows the q-values of\nthe two actions in a certain state, separated by a semicolon. for instance:\n\nq3(2, 1) = \u03c1(2, 1)+\u03b3max\n\nu\n\nq2( f (2, 1), u) = 0+0.5 max\n\nu\n\nq2(3, u) = 0+0.5\u00b72.5 = 1.25\n\n "}, {"Page_number": 37, "text": "2.3. value iteration\n\n27\n\ntable 2.2\nq-iteration results for the deterministic cleaning-robot problem.\n\nx = 0\n\nx = 1\n\nx = 2\n\nx = 3\n\nx = 4\n\nx = 5\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n1 ; 0\n\n0 ; 0\n0.5 ; 0\n\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n\n0 ; 0\nq0\n0 ; 0\nq1\n0 ; 0\nq2\n0 ; 0\nq3\n0 ; 0\nq4\n0 ; 0\nq5\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nh\u2217\n\u2217\nv \u2217\n0\n\n0.25 ; 0\n0.25 ; 2.5\n0.625 ; 2.5\n0.625 ; 2.5\n0.625 ; 2.5\n\n0.125 ; 5\n1.25 ; 5\n1.25 ; 5\n1.25 ; 5\n1.25 ; 5\n\n0.5 ; 0.125\n0.5 ; 1.25\n0.5 ; 1.25\n0.5 ; 1.25\n\n1 ; 0.25\n1 ; 0.25\n1 ; 0.625\n1 ; 0.625\n\n\u22121\n1\n\n1\n2.5\n\n1.25\n\n\u2217\n0\n\n1\n5\n\n1\n\nthe algorithm converges after 5 iterations; q5 = q4 = q\u2217. the last two rows\nof the table (below the dashed line) also give the optimal policies, computed from\nq\u2217 with (2.5), and the optimal v-function v\u2217, computed from q\u2217 with (2.10). in the\npolicy representation, the symbol \u201c\u2217\u201d means that any action can be taken in that state\nwithout changing the quality of the policy. the total number of function evaluations\nrequired by the algorithm in the deterministic case is:\n\n5|x||u| (2 +|u|) = 5\u00b7 6\u00b7 2\u00b7 4 = 240\n\nconsider next the stochastic variant of the cleaning-robot problem, introduced in\nexample 2.2. for this variant, q-iteration is implemented by using algorithm 2.2,\nand produces the sequence of q-functions illustrated in the first part of table 2.3\n(not all the iterations are shown). the algorithm fully converges after 22 iterations.\n\ntable 2.3\nq-iteration results for the stochastic cleaning-robot problem. q-function and v-function val-\nues are rounded to 3 decimal places.\n\nx = 0\n\nx = 1\n\nx = 2\n\nx = 3\n\nx = 4\n\nx = 5\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n\u00b7\u00b7\u00b7\n0 ; 0\n\u00b7\u00b7\u00b7\n0 ; 0\n\n0.128 ; 0.018\n0.260 ; 1.639\n0.515 ; 1.878\n0.581 ; 1.911\n\n0.320 ; 0.044\n0.374 ; 0.101\n0.419 ; 0.709\n0.453 ; 0.826\n\n0.800 ; 0.110\n0.868 ; 0.243\n0.874 ; 0.265\n0.883 ; 0.400\n\n0 ; 0\nq0\n0 ; 0\nq1\n0 ; 0\nq2\n0 ; 0\nq3\n0 ; 0\nq4\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n0 ; 0\nq12\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n0 ; 0\nq22\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nh\u2217\n\u2217\nv\u2217\n0\n\n0.301 ; 4.026\n1.208 ; 4.343\n1.327 ; 4.373\n1.342 ; 4.376\n\n1.344 ; 4.376\n\n1.344 ; 4.376\n\n0.888 ; 0.458\n\n0.888 ; 0.458\n\n0.467 ; 0.852\n\n0.594 ; 1.915\n\n0.467 ; 0.852\n\n0.594 ; 1.915\n\n\u22121\n0.888\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\n4.376\n\n0.852\n\n1.915\n\n\u2217\n0\n\n1\n\n1\n\n1\n\n "}, {"Page_number": 38, "text": "28\n\nchapter 2. an introduction to dp and rl\n\nthe optimal policies and the optimal v-function obtained are also shown in table 2.3\n(below the dashed line). while the optimal q-function and the optimal v-function are\ndifferent from those obtained in the deterministic case, the optimal policies remain\nthe same. the total number of function evaluations required by the algorithm in the\nstochastic case is:\n\n22|x|2|u| (2 +|u|) = 22\u00b7 62 \u00b7 2\u00b7 4 = 6336\n\nwhich is considerably greater than in the deterministic case.\n\nif we impose a suboptimality bound \u03c2qi = 0.01 and apply (2.27), we find that q-\niteration should run for l = 12 iterations in order to guarantee this bound, where the\nmaximum absolute reward k\u03c1k\u221e = 5 and the discount factor \u03b3 = 0.5 have also been\nused. so, in the deterministic case, the algorithm fully converges to its fixed point\nin fewer iterations than the conservative number given by (2.27). in the stochastic\ncase, even though the algorithm does not fully converge after 12 iterations (instead\nrequiring 22 iterations), the q-function at iteration 12 (shown in table 2.3) is already\nvery accurate, and a policy that is greedy in this q-function is fully optimal. the\nsuboptimality of such a policy is 0, which is smaller than the imposed bound \u03c2qi.\nin fact, for any iteration \u2113 \u2265 3, the q-function q\u2113 would produce an optimal policy,\nwhich means that l = 12 is also conservative in the stochastic case.\n\n(cid:3)\n\n2.3.2 model-free value iteration and the need for exploration\n\nwe have discussed until now model-based value iteration. we next consider rl,\nmodel-free value iteration algorithms, and discuss q-learning, the most widely used\nalgorithm from this class. q-learning starts from an arbitrary initial q-function q0\nand updates it without requiring a model, using instead observed state transitions\nand rewards, i.e., data tuples of the form (xk, uk, xk+1, rk+1) (watkins, 1989; watkins\nand dayan, 1992). after each transition, the q-function is updated using such a data\ntuple, as follows:\n\nqk+1(xk, uk) = qk(xk, uk) + \u03b1k[rk+1 + \u03b3max\n\nu\u2032\n\nqk(xk+1, u\u2032)\u2212 qk(xk, uk)]\n\n(2.30)\n\nwhere \u03b1k \u2208 (0, 1]\nis the learning rate. the term between square brackets is\nthe temporal difference, i.e., the difference between the updated estimate rk+1 +\n\u03b3maxu\u2032 qk(xk+1, u\u2032) of the optimal q-value of (xk, uk), and the current estimate\nqk(xk, uk). in the deterministic case, the new estimate is actually the q-iteration map-\nping (2.22) applied to qk in the state-action pair (xk, uk), where \u03c1(xk, uk) has been\nreplaced by the observed reward rk+1, and f (xk, uk) by the observed next-state xk+1.\nin the stochastic case, these replacements provide a single sample of the random\nquantity whose expectation is computed by the q-iteration mapping (2.23), and thus\nq-learning can be seen as a sample-based, stochastic approximation procedure based\non this mapping (singh et al., 1995; bertsekas and tsitsiklis, 1996, section 5.6).\n\nas the number of transitions k approaches infinity, q-learning asymptotically\nconverges to q\u2217 if the state and action spaces are discrete and finite, and under the\nfollowing conditions (watkins and dayan, 1992; tsitsiklis, 1994; jaakkola et al.,\n1994):\n\n "}, {"Page_number": 39, "text": "2.3. value iteration\n\n29\n\n\u2022 the sum \u2211\u221e\n\nk=0 \u03b12\nan infinite value.\n\nk produces a finite value, whereas the sum \u2211\u221e\n\nk=0 \u03b1k produces\n\n\u2022 all the state-action pairs are (asymptotically) visited infinitely often.\nthe first condition is not difficult to satisfy. for instance, a satisfactory standard\n\nchoice is:\n\n\u03b1k =\n\n1\n\nk\n\n(2.31)\n\nin practice, the learning rate schedule may require tuning, because it influences the\nnumber of transitions required by q-learning to obtain a good solution. a good\nchoice for the learning rate schedule depends on the problem at hand.\n\nthe second condition can be satisfied if, among other things, the controller has a\nnonzero probability of selecting any action in every encountered state; this is called\nexploration. the controller also has to exploit its current knowledge in order to ob-\ntain good performance, e.g., by selecting greedy actions in the current q-function.\nthis is a typical illustration of the exploration-exploitation trade-off in online rl.\na classical way to balance exploration with exploitation in q-learning is \u03b5-greedy\nexploration (sutton and barto, 1998, section 2.2), which selects actions according\nto:\n\nuk =(u \u2208 arg max \u00afu qk(xk, \u00afu)\n\na uniformly random action in u with probability \u03b5k\n\nwith probability 1\u2212 \u03b5k\n\n(2.32)\n\nwhere \u03b5k \u2208 (0, 1) is the exploration probability at step k. another option is to use\nboltzmann exploration (sutton and barto, 1998, section 2.3), which at step k selects\nan action u with probability:\n\np (u| xk) =\n\neqk(xk,u)/\u03c4k\n\n\u2211 \u00afu eqk(xk, \u00afu)/\u03c4k\n\n(2.33)\n\nwhere the temperature \u03c4k \u2265 0 controls the randomness of the exploration. when \u03c4k \u2192\n0, (2.33) is equivalent to greedy action selection, while for \u03c4k \u2192 \u221e, action selection\nis uniformly random. for nonzero, finite values of \u03c4k, higher-valued actions have a\ngreater chance of being selected than lower-valued ones.\n\nusually, the exploration diminishes over time, so that the policy used asymptoti-\n\ncally becomes greedy and therefore (as qk \u2192 q\u2217) optimal. this can be achieved by\nmaking \u03b5k or \u03c4k approach 0 as k grows. for instance, an \u03b5-greedy exploration sched-\nule of the form \u03b5k = 1/k diminishes to 0 as k \u2192 \u221e, while still satisfying the second\nconvergence condition of q-learning, i.e., while allowing infinitely many visits to\nall the state-action pairs (singh et al., 2000). notice the similarity of this explo-\nration schedule with the learning rate schedule (2.31). for a schedule of the boltz-\nmann exploration temperature \u03c4k that decreases to 0 while satisfying the convergence\nconditions, see (singh et al., 2000). like the learning rate schedule, the exploration\nschedule has a significant effect on the performance of q-learning.\n\nalgorithm 2.3 presents q-learning with \u03b5-greedy exploration. note that an ideal-\nized, infinite-time online setting is considered for this algorithm, in which no termi-\nnation condition is specified and no explicit output is produced. instead, the result of\n\n "}, {"Page_number": 40, "text": "30\n\nchapter 2. an introduction to dp and rl\n\nthe algorithm is the improvement of the control performance achieved while inter-\nacting with the process. a similar setting will be considered for other online learning\nalgorithms described in this book, with the implicit understanding that, in practice,\nthe algorithms will of course be stopped after a finite number of steps. when q-\nlearning is stopped, the resulting q-function and the corresponding greedy policy\ncan be interpreted as outputs and reused.\n\nalgorithm 2.3 q-learning with \u03b5-greedy exploration.\ninput: discount factor \u03b3,\n\nexploration schedule {\u03b5k}\u221e\n\nk=0, learning rate schedule {\u03b1k}\u221e\n\nk=0\n\n1: initialize q-function, e.g., q0 \u2190 0\n2: measure initial state x0\n3: for every time step k = 0, 1, 2, . . . do\n\nuk \u2190(u \u2208 arg max \u00afu qk(xk, \u00afu)\napply uk, measure next state xk+1 and reward rk+1\nqk+1(xk, uk) \u2190 qk(xk, uk) + \u03b1k[rk+1 + \u03b3maxu\u2032 qk(xk+1, u\u2032)\u2212 qk(xk, uk)]\n\na uniformly random action in u with probability \u03b5k (explore)\n\nwith probability 1\u2212 \u03b5k (exploit)\n\n4:\n\n5:\n\n6:\n7: end for\n\nnote that this discussion has not been all-encompassing; the \u03b5-greedy and boltz-\nmann exploration procedures can also be used in other online rl algorithms besides\nq-learning, and a variety of other exploration procedures exist. for instance, the pol-\nicy can be biased towards actions that have not recently been taken, or that may lead\nthe system towards rarely visited areas of the state space (thrun, 1992). the value\nfunction can also be initialized to be larger than the true returns, in a method known\nas \u201coptimism in the face of uncertainty\u201d (sutton and barto, 1998, section 2.7). be-\ncause the return estimates have been adjusted downwards for any actions already\ntaken, greedy action selection leads to exploring novel actions. confidence intervals\nfor the returns can be estimated, and the action with largest upper confidence bound,\ni.e., with the best potential for good returns, can be chosen (kaelbling, 1993). many\nauthors have also studied the exploration-exploitation trade-off for specific types of\nproblems, such as problems with linear transition dynamics (feldbaum, 1961), and\nproblems without any dynamics, for which the state space reduces to a single element\n(auer et al., 2002; audibert et al., 2007).\n\n2.4 policy iteration\n\nhaving introduced value iteration in section 2.3, we now consider policy iteration,\nthe second major class of dp/rl algorithms. policy iteration algorithms evaluate\npolicies by constructing their value functions, and use these value functions to find\nnew, improved policies (bertsekas, 2007, section 1.3). as a representative example\n\n "}, {"Page_number": 41, "text": "2.4. policy iteration\n\n31\n\nof policy iteration, consider an offline algorithm that evaluates policies using their\nq-functions. this algorithm starts with an arbitrary policy h0. at every iteration \u2113,\nthe q-function qh\u2113 of the current policy h\u2113 is determined; this step is called policy\nevaluation. policy evaluation is performed by solving the bellman equation (2.7) in\nthe deterministic case, or (2.17) in the stochastic case. when policy evaluation is\ncomplete, a new policy h\u2113+1 that is greedy in qh is found:\n\nh\u2113+1(x) \u2208 arg max\n\nu\n\nqh\u2113(x, u)\n\n(2.34)\n\nthis step is called policyimprovement. the entire procedure is summarized in algo-\nrithm 2.4. the sequence of q-functions produced by policy iteration asymptotically\n\nconverges to q\u2217 as \u2113 \u2192 \u221e. simultaneously, an optimal policy h\u2217 is obtained.\n\nalgorithm 2.4 policy iteration with q-functions.\n\n1: initialize policy h0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\nfind qh\u2113 , the q-function of h\u2113\nh\u2113+1(x) \u2208 arg maxu qh\u2113(x, u)\n\n4:\n5: until h\u2113+1 = h\u2113\noutput: h\u2217 = h\u2113, q\u2217 = qh\u2113\n\n\u22b2 policy evaluation\n\u22b2 policy improvement\n\nthe crucial component of policy iteration is policy evaluation. policy improve-\nment can be performed by solving static optimization problems, e.g., of the form\n(2.34) when q-functions are used \u2013 often an easier challenge.\n\nin the remainder of this section, we first discuss dp (model-based) policy itera-\ntion, followed by rl (model-free) policy iteration. we pay special attention to the\npolicy evaluation component.\n\n2.4.1 model-based policy iteration\n\nin the model-based setting, the policy evaluation step employs knowledge of the tran-\nsition and reward functions. a model-based iterative algorithm for policy evaluation\ncan be given that is similar to q-iteration, which will be called policy evaluation for\nq-functions. analogously to the q-iteration mapping t (2.22), a policy evaluation\n\nmapping t h : q \u2192 q is defined, which computes the right-hand side of the bellman\n\nequation for an arbitrary q-function. in the deterministic case, this mapping is:\n\n[t h(q)](x, u) = \u03c1(x, u) + \u03b3q( f (x, u), h( f (x, u)))\n\n(2.35)\n\nand in the stochastic case, it is:\n\n[t h(q)](x, u) = ex\u2032\u223c \u02dcf (x,u,\u00b7)(cid:8) \u02dc\u03c1(x, u, x\u2032) + \u03b3q(x\u2032, h(x\u2032))(cid:9)\n\n(2.36)\n\nnote that when the state space is countable, the transition model (2.14) is appropriate,\nand the policy evaluation mapping for the stochastic case (2.36) can be written as the\n\n "}, {"Page_number": 42, "text": "32\n\nchapter 2. an introduction to dp and rl\n\nsimpler summation:\n\n[t h(q)](x, u) = \u2211\nx\u2032\n\n\u00aff (x, u, x\u2032)(cid:2) \u02dc\u03c1(x, u, x\u2032) + \u03b3q(x\u2032, h(x\u2032))(cid:3)\n\n(2.37)\n\npolicy evaluation for q-functions starts from an arbitrary q-function qh\n\n0 and at\n\neach iteration \u03c4 updates the q-function using:6\n\nqh\n\n\u03c4+1 = t h(qh\n\u03c4)\n\n(2.38)\n\nlike the q-iteration mapping t , the policy evaluation mapping t h is a contraction\nwith a factor \u03b3 < 1 in the infinity norm, i.e., for any pair of functions q and q\u2032:\n\nkt h(q)\u2212 t h(q\u2032)k\u221e \u2264 \u03b3kq\u2212 q\u2032k\u221e\n\nso, t h has a unique fixed point. written in terms of the mapping t h, the bellman\nequation (2.7) or (2.17) states that this unique fixed point is actually qh:\n\nqh = t h(qh)\n\n(2.39)\n\ntherefore, policy evaluation for q-functions (2.38) asymptotically converges to qh.\nmoreover, also because t h is a contraction with factor \u03b3, this variant of policy evalu-\n\u03c4\u2212 qhk\u221e.\nation converges to qh at a rate of \u03b3, in the sense that kqh\n\n\u03c4+1 \u2212 qhk\u221e \u2264 \u03b3kqh\n\nalgorithm 2.5 presents policy evaluation for q-functions in deterministic mdps,\nwhile algorithm 2.6 is used for stochastic mdps with countable state spaces. in\nalgorithm 2.5, t h is computed with (2.35), while in algorithm 2.6, (2.37) is em-\nployed. since the convergence condition of these algorithms is only guaranteed to\nbe satisfied asymptotically, in practice they can be stopped, e.g., when the differ-\nence between consecutive q-functions decreases below a given threshold, i.e., when\n\u03c4k\u221e \u2264 \u03b5pe, where \u03b5pe > 0. here, the subscript \u201cpe\u201d stands for \u201cpolicy\n\n\u03c4+1 \u2212 qh\n\nkqh\n\nevaluation.\u201d\n\nalgorithm 2.5 policy evaluation for q-functions in deterministic mdps.\ninput: policy h to be evaluated, dynamics f , reward function \u03c1, discount factor \u03b3\n\n1: initialize q-function, e.g., qh\n2: repeat at every iteration \u03c4 = 0, 1, 2, . . .\n3:\n\nfor every (x, u) do\n\n0 \u2190 0\n\n4:\n\n\u03c4+1(x, u) \u2190 \u03c1(x, u) + \u03b3qh\nqh\nend for\n\u03c4+1 = qh\n\u03c4\n\n5:\n6: until qh\n\noutput: qh = qh\n\u03c4\n\n\u03c4( f (x, u), h( f (x, u)))\n\n6a different iteration index \u03c4 is used for policy evaluation, because it runs in the inner loop of every\n\n(offline) policy iteration \u2113.\n\n "}, {"Page_number": 43, "text": "2.4. policy iteration\n\n33\n\nalgorithm 2.6\npolicy evaluation for q-functions in stochastic mdps with countable state spaces.\ninput: policy h to be evaluated, dynamics \u00aff , reward function \u02dc\u03c1, discount factor \u03b3\n\n1: initialize q-function, e.g., qh\n2: repeat at every iteration \u03c4 = 0, 1, 2, . . .\n3:\n\n0 \u2190 0\n\n4:\n\nfor every (x, u) do\nqh\n\u03c4+1(x, u) \u2190 \u2211x\u2032\nend for\n\u03c4+1 = qh\n\u03c4\n\n5:\n6: until qh\n\noutput: qh = qh\n\u03c4\n\n\u00aff (x, u, x\u2032)(cid:2) \u02dc\u03c1(x, u, x\u2032) + \u03b3qh\n\n\u03c4(x\u2032, h(x\u2032))(cid:3)\n\nthere are also other ways to compute qh. for example, in the deterministic case,\nthe mapping t h (2.35) and equivalently the bellman equation (2.7) are obviously\nlinear in the q-values. in the stochastic case, because x has a finite cardinality, the\npolicy evaluation mapping t h and equivalently the bellman equation (2.39) can be\nwritten by using the summation (2.37), and are therefore also linear. hence, when the\nstate and action spaces are finite and the cardinality of x \u00d7 u is not too large (e.g.,\nup to several thousands), qh can be found by directly solving the linear system of\nequations given by the bellman equation.\n\nthe entire derivation can be repeated and similar algorithms can be given for v-\nfunctions instead of q-functions. such algorithms are more popular in the literature,\nsee, e.g., (sutton and barto, 1998, section 4.1) and (bertsekas, 2007, section 1.3).\nrecall however, that policy improvements are more problematic when v-functions\nare used, because a model is required to find greedy policies, as seen in (2.11). ad-\nditionally, in the stochastic case, expectations over the one-step stochastic transitions\nhave to be computed to find greedy policies, as seen in (2.19).\n\nan important advantage of policy iteration over value iteration stems from the\nlinearity of the bellman equation for qh in the q-values. in contrast, the bellman\noptimality equation (for q\u2217) is highly nonlinear due to the maximization at the right-\nhand side. this makes policy evaluation generally easier to solve than value iteration.\nmoreover, in practice, offline policy iteration algorithms often converge in a small\nnumber of iterations (madani, 2002; sutton and barto, 1998, section 4.3), possi-\nbly smaller than the number of iterations taken by offline value iteration algorithms.\nhowever, this does not mean that policy iteration is computationally less costly than\nvalue iteration. for instance, even though policy evaluation using q-functions is gen-\nerally less costly than q-iteration, every single policy iteration requires a complete\npolicy evaluation.\n\ncomputational cost of policy evaluation for q-functions in finite mdps\n\nwe next investigate the computational cost of policy evaluation for q-functions\n(2.38) for an mdp with a finite number of states and actions. we also provide a com-\nparison with the computational cost of q-iteration. note again that policy evaluation\n\n "}, {"Page_number": 44, "text": "34\n\nchapter 2. an introduction to dp and rl\n\nis only one component of policy iteration; for an illustration of the computational\ncost of the entire policy iteration algorithm, see the upcoming example 2.4.\n\nin the deterministic case, policy evaluation for q-functions can be implemented\nas in algorithm 2.5. the computational cost of one iteration of this algorithm, mea-\nsured by the number of function evaluations, is:\n\n4|x||u|\n\nwhere the functions being evaluated are \u03c1, f , h, and the current q-function qh\n\u03c4. in\nthe stochastic case, algorithm 2.6 can be used, which requires at each iteration the\nfollowing number of function evaluations:\n\n4|x|2|u|\n\n\u03c4. the cost in the stochastic\n\nwhere the functions being evaluated are \u02dc\u03c1, \u00aff , h, and qh\ncase is thus larger by a factor |x| than the cost in the deterministic case.\ntable 2.4 collects the computational cost of policy evaluation for q-functions,\nand compares it with the computational cost of q-iteration (section 2.3.1). a sin-\ngle q-iteration requires |x||u| (2 + |u|) function evaluations in the deterministic\ncase (2.28), and |x|2|u| (2 +|u|) function evaluations in the stochastic case (2.29).\nwhenever |u| > 2, the cost of a single q-iteration is therefore larger than the cost of\na policy evaluation iteration.\n\ntable 2.4\ncomputational cost of policy evaluation for q-functions and of q-\niteration, measured by the number of function evaluations. the cost\nfor a single iteration is shown.\n\npolicy evaluation\nq-iteration\n\ndeterministic case\n\nstochastic case\n\n4|x||u|\n\n|x||u| (2 +|u|)\n\n4|x|2|u|\n\n|x|2|u| (2 +|u|)\n\nnote also that evaluating the policy by directly solving the linear system given by\n\nthe bellman equation typically requires o(|x|3|u|3) computation. this is an asymp-\n\ntotic measure of computational complexity (knuth, 1976), and is no longer directly\nrelated to the number of function evaluations. by comparison, the complexity of the\ncomplete iterative policy evaluation algorithm is o(l|x||u|) in the deterministic\ncase and o(l|x|2|u|) in the stochastic case, where l is the number of iterations.\n\nexample 2.4 model-based policy iteration for the cleaning robot. in this exam-\nple, we apply a policy iteration algorithm to the cleaning-robot problem introduced\nin examples 2.1 and 2.2. recall that every single policy iteration requires a com-\npleteexecution of policy evaluation for the current policy, together with a policy im-\nprovement. the (model-based) policy evaluation for q-functions (2.38) is employed,\nstarting from identically zero q-functions. each policy evaluation is run until the\n\n "}, {"Page_number": 45, "text": "2.4. policy iteration\n\n35\n\nq-function fully converges. the same discount factor is used as for q-iteration in\nexample 2.3, namely \u03b3 = 0.5.\n\nconsider first the deterministic variant of example 2.1, in which policy evalua-\ntion for q-functions takes the form shown in algorithm 2.5. starting from a policy\nthat always moves right (h0(x) = 1 for all x), policy iteration produces the sequence of\nq-functions and policies given in table 2.5. in this table, the sequence of q-functions\nproduced by a given execution of policy evaluation is separated by dashed lines from\nthe policy being evaluated (shown above the sequence of q-functions) and from the\nimproved policy (shown below the sequence). the policy iteration algorithm con-\nverges after 2 iterations. in fact, the policy is already optimal after the first policy\nimprovement: h2 = h1 = h\u2217.\n\ntable 2.5\npolicy iteration results for the deterministic cleaning-robot problem. q-values\nare rounded to 3 decimal places.\n\n1\n\n1\n\n1\n\n1\n\nx = 4\n\nx = 3\n\nx = 2\n\nx = 1\n\n0 ; 1.25\n\n0 ; 0\n0 ; 5\n\n0 ; 0\n0 ; 0\n0 ; 0\n\n0 ; 0\n1 ; 0\n1 ; 0\n1 ; 0\n\n1 ; 0.625\n1 ; 0.625\n\n0 ; 0\n0 ; 0\n0 ; 2.5\n\n0.313 ; 1.25\n0.313 ; 1.25\n\nx = 0\n\u2217\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n\n1.25 ; 5\n0.625 ; 2.5 1.25 ; 5\n0.625 ; 2.5 1.25 ; 5\n0.625 ; 2.5 1.25 ; 5\n\nx = 5\n\u2217\nh0\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n0 ; 0\nq0\n0 ; 0\nq1\n0 ; 0\nq2\n0 ; 0\nq3\n0 ; 0\nq4\n0 ; 0\nq5\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\u2217\nh1\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n0 ; 0\nq0\n0 ; 0\nq1\n0 ; 0\nq2\n0 ; 0\nq3\n0 ; 0\nq4\n0 ; 0\nq5\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\u2217\nh2\n\n1.25 ; 5\n0.625 ; 2.5 1.25 ; 5\n0.625 ; 2.5 1.25 ; 5\n0.625 ; 2.5 1.25 ; 5\n\n\u2217\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n\n0.5 ; 1.25\n0.5 ; 1.25\n0.5 ; 1.25\n\n\u22121\n0 ; 0\n1 ; 0\n1 ; 0\n1 ; 0\n\n0 ; 0\n0.5 ; 0\n0.5 ; 0\n\n0 ; 0\n0 ; 0\n0 ; 2.5\n\n1 ; 0.625\n1 ; 0.625\n\n0 ; 0\n0 ; 5\n\n\u22121\n\n\u2217\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\nfive iterations of the policy evaluation algorithm are required for the first policy,\nand the same number of iterations are required for the second policy. recall that the\ncomputational cost of every iteration of the policy evaluation algorithm, measured by\nthe number of function evaluations, is 4|x||u|, leading to a total cost of 5\u00b7 4\u00b7|x||u|\nfor each of the two policy evaluations. assuming that the maximization over u in\nthe policy improvement is solved by enumeration, the computational cost of every\npolicy improvement is |x||u|. each of the two policy iterations consists of a policy\nevaluation and a policy improvement, requiring:\n\n5\u00b7 4\u00b7|x||u| +|x||u| = 21|x||u|\n\n "}, {"Page_number": 46, "text": "36\n\nchapter 2. an introduction to dp and rl\n\nfunction evaluations, and thus the entire policy iteration algorithm has a cost of:\n\n2\u00b7 21\u00b7|x||u| = 2\u00b7 21\u00b7 6\u00b7 2 = 504\n\ncompared to the cost 240 of q-iteration in example 2.3, policy iteration is in this\ncase more computationally expensive. this is true even though the cost of any single\npolicy evaluation, 5\u00b7 4\u00b7|x||u| = 240, is the same as the cost of q-iteration. the latter\nfact is expected from the theory (table 2.4), which indicated that policy evaluation\nfor q-functions and q-iteration have similar costs when |u| = 2, as is the case here.\nconsider now the stochastic case of example 2.2. for this case, policy evalua-\ntion for q-functions takes the form shown in algorithm 2.6. starting from the same\npolicy as in the deterministic case (always going right), policy iteration produces the\nsequence of q-functions and policies illustrated in table 2.6 (not all q-functions are\nshown). although the q-functions are different from those in the deterministic case,\nthe same sequence of policies is produced.\n\ntable 2.6\npolicy iteration results for the stochastic cleaning-robot problem. q-values are rounded to 3\ndecimal places.\n\n1\n\n1\n\n1\n\n1\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n\n\u00b7\u00b7\u00b7\n\nx = 4\n\nx = 3\n\nx = 2\n\nx = 1\n\n0.250 ; 4\n\n0.001 ; 0\n\n0.852 ; 0.417\n\n1.190 ; 4.340\n1.324 ; 4.372\n1.342 ; 4.376\n\n0.101 ; 1.600\n0.485 ; 1.872\n0.572 ; 1.909\n\n0.020 ; 0.001\n0.022 ; 0.001\n0.062 ; 0.641\n0.219 ; 0.805\n\n0.800 ; 0.050\n0.804 ; 0.054\n0.804 ; 0.055\n0.820 ; 0.311\n\nx = 0\n\u2217\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n\u00b7\u00b7\u00b7\n0 ; 0\n\nx = 5\n\u2217\nh0\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n0 ; 0\nq0\n0 ; 0\nq1\n0 ; 0\nq2\n0 ; 0\nq3\n0 ; 0\nq4\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n0 ; 0\nq24\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\u2217\nh1\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n0 ; 0\nq0\n0 ; 0\nq1\n0 ; 0\nq2\n0 ; 0\nq3\n0 ; 0\nq4\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n0 ; 0\nq22\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\u2217\nh2\n\n0.800 ; 0.110\n0.861 ; 0.123\n0.865 ; 0.124\n0.881 ; 0.382\n\n0.320 ; 0.020\n0.346 ; 0.023\n0.388 ; 0.664\n0.449 ; 0.821\n\n0.008 ; 0.001\n0.109 ; 1.601\n0.494 ; 1.873\n0.578 ; 1.910\n\n1.190 ; 4.340\n1.325 ; 4.372\n1.342 ; 4.376\n\n\u2217\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n0 ; 0\n\u00b7\u00b7\u00b7\n0 ; 0\n\n0.888 ; 0.458\n\n0.589 ; 1.915\n\n1.344 ; 4.376\n\n0.467 ; 0.852\n\n0.594 ; 1.915\n\n1.344 ; 4.376\n\n0.278 ; 0.839\n\n\u22121\n0 ; 0\n\n0.250 ; 4\n\n\u22121\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n0 ; 0\n\n0 ; 0\n\n0 ; 0\n\n\u2217\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\ntwenty-four iterations of the policy evaluation algorithm are required to eval-\nuate the first policy, and 22 iterations are required for the second. recall that the\ncost of every iteration of the policy evaluation algorithm, measured by the number\n\nof function evaluations, is 4|x|2|u| in the stochastic case, while the cost for policy\nimprovement is the same as in the deterministic case: |x||u|. so, the first policy\n\n "}, {"Page_number": 47, "text": "2.4. policy iteration\n\n37\n\niteration requires 24 \u00b7 4 \u00b7 |x|2|u| + |x||u| function evaluations, and the second re-\nquires 22\u00b7 4\u00b7|x|2|u| +|x||u| function evaluations. the total cost of policy iteration\n\nis obtained by adding these two costs:\n\n46\u00b7 4\u00b7|x|2|u| + 2|x||u| = 46\u00b7 4\u00b7 62 \u00b7 2 + 2\u00b7 6\u00b7 2 = 13272\n\ncomparing this to the 6336 function evaluations necessary for q-iteration in the\nstochastic problem (see example 2.3), it appears that policy iteration is also more\ncomputationally expensive in the stochastic case. moreover, policy iteration is more\ncomputationally costly in the stochastic case than in the deterministic case; in the\nlatter case, policy iteration required only 504 function evaluations.\n(cid:3)\n\n2.4.2 model-free policy iteration\n\nafter having discussed above model-based policy iteration, we now turn our attention\nto the class of class of rl, model-free policy iteration algorithms, and within this\nclass, we focus on sarsa, an online algorithm proposed by rummery and niranjan\n(1994) as an alternative to the value-iteration based q-learning. the name sarsa is\nobtained by joining together the initials of every element in the data tuples employed\nby the algorithm, namely: state, action, reward, (next) state, (next) action. formally,\nsuch a tuple is denoted by (xk, uk, rk+1, xk+1, uk+1). sarsa starts with an arbitrary\ninitial q-function q0 and updates it at each step using tuples of this form, as follows:\n\nqk+1(xk, uk) = qk(xk, uk) + \u03b1k[rk+1 + \u03b3qk(xk+1, uk+1)\u2212 qk(xk, uk)]\n\n(2.40)\n\nwhere \u03b1k \u2208 (0, 1] is the learning rate. the term between square brackets is the tem-\nporal difference, obtained as the difference between the updated estimate rk+1 +\n\u03b3qk(xk+1, uk+1) of the q-value for (xk, uk), and the current estimate qk(xk, uk). this\nis not the same as the temporal difference used in q-learning (2.30). while the q-\nlearning temporal difference includes the maximal q-value in the next state, the\nsarsa temporal difference includes the q-value of the action actually taken in\nthis next state. this means that sarsa performs online, model-free policy evalu-\nation of the policy that is currently being followed. in the deterministic case, the new\nestimate rk+1 +\u03b3qk(xk+1, uk+1) of the q-value for (xk, uk) is actually the policy eval-\nuation mapping (2.35) applied to qk in the state-action pair (xk, uk). here, \u03c1(xk, uk)\nhas been replaced by the observed reward rk+1, and f (xk, uk) by the observed next\nstate xk+1. in the stochastic case, these replacements provide a single sample of the\nrandom quantity whose expectation is found by the policy evaluation mapping (2.36).\nnext, the policy employed by sarsa is considered. unlike offline policy it-\neration, sarsa cannot afford to wait until the q-function has (almost) converged\nbefore it improves the policy. this is because convergence may take a long time,\nduring which the unimproved (and possibly bad) policy would be used. instead, to\nselect actions, sarsa combines a greedy policy in the current q-function with ex-\nploration, using, e.g., \u03b5-greedy (2.32) or boltzmann (2.33) exploration. because of\nthe greedy component, sarsa implicitly performs a policy improvement at every\ntime step, and is therefore a type of online policy iteration. such a policy iteration\n\n "}, {"Page_number": 48, "text": "38\n\nchapter 2. an introduction to dp and rl\n\nalgorithm, which improves the policy after every sample, is sometimes called fully\noptimistic (bertsekas and tsitsiklis, 1996, section 6.4).\n\nalgorithm 2.3 presents sarsa with \u03b5-greedy exploration. in this algorithm, be-\ncause the update at step k involves the action uk+1, this action has to be chosen prior\nto updating the q-function.\n\nalgorithm 2.7 sarsa with \u03b5-greedy exploration.\ninput: discount factor \u03b3,\n\n1: initialize q-function, e.g., q0 \u2190 0\n2: measure initial state x0\n\nexploration schedule {\u03b5k}\u221e\n3: u0 \u2190(u \u2208 arg max \u00afu q0(x0, \u00afu)\n\nk=0, learning rate schedule {\u03b1k}\u221e\n\nk=0\n\nwith probability 1\u2212 \u03b50 (exploit)\n\na uniformly random action in u with probability \u03b50 (explore)\n\n4: for every time step k = 0, 1, 2, . . . do\n5:\n\napply uk, measure next state xk+1 and reward rk+1\n\n6:\n\nuk+1 \u2190(u \u2208 arg max \u00afu qk(xk+1, \u00afu)\n\na uniformly random action in u with probability \u03b5k+1\n\nwith probability 1\u2212 \u03b5k+1\n\n7:\n8: end for\n\nqk+1(xk, uk) \u2190 qk(xk, uk) + \u03b1k[rk+1 + \u03b3qk(xk+1, uk+1)\u2212 qk(xk, uk)]\n\nin order to converge to the optimal q-function q\u2217, sarsa requires conditions\nsimilar to those of q-learning, which demand exploration, and additionally that\nthe exploratory policy being followed asymptotically becomes greedy (singh et al.,\n2000). such a policy can be obtained by using, e.g., \u03b5-greedy (2.32) exploration with\nan exploration probability \u03b5k that asymptotically decreases to 0, or boltzmann (2.33)\nexploration with an exploration temperature \u03c4k that asymptotically decreases to 0.\nnote that, as already explained in section 2.3.2, the exploratory policy used by q-\nlearning can also be made greedy asymptotically, even though the convergence of\nq-learning does not rely on this condition.\n\nalgorithms like sarsa, which evaluate the policy they are currently using to\ncontrol the process, are also called \u201con-policy\u201d in the rl literature (sutton and barto,\n1998). in contrast, algorithms like q-learning, which act on the process using one\npolicy and evaluate another policy, are called \u201coff-policy.\u201d in q-learning, the pol-\nicy used to control the system typically includes exploration, whereas the algorithm\nimplicitly evaluates a policy that is greedy in the current q-function, since maximal\nq-values are used in the q-function updates (2.30).\n\n2.5 policy search\n\nthe previous two sections have introduced value iteration and policy iteration. in this\nsection, we consider the third major class of dp/rl methods, namely policy search\n\n "}, {"Page_number": 49, "text": "2.5. policy search\n\n39\n\nalgorithms. these algorithms use optimization techniques to directly search for an\noptimal policy, which maximizes the return from every initial state. the optimiza-\ntion criterion should therefore be a combination (e.g., average) of the returns from\nevery initial state. in principle, any optimization technique can be used to search for\nan optimal policy. for a general problem, however, the optimization criterion may\nbe a nondifferentiable function with multiple local optima. this means that global,\ngradient-free optimization techniques are more appropriate than local, gradient-based\ntechniques. particular examples of global, gradient-free techniques include genetic\nalgorithms (goldberg, 1989), tabu search (glover and laguna, 1997), pattern search\n(torczon, 1997; lewis and torczon, 2000), cross-entropy optimization (rubinstein\nand kroese, 2004), etc.\n\nconsider the return estimation procedure of a model-based policy search algo-\nrithm. the returns are infinite sums of discounted rewards (2.1), (2.15). however,\nin practice, the returns have to be estimated in a finite time. to this end, the infinite\nsum in the return can be approximated with a finite sum over the first k steps. to\nguarantee that the approximation obtained in this way is within a bound \u03b5mc > 0 of\nthe infinite sum, k can be chosen with (e.g., mannor et al., 2003):\n\nk =(cid:24)log\u03b3\n\n\u03b5mc(1\u2212 \u03b3)\n\nk\u03c1k\u221e\n\n(cid:25)\n\n(2.41)\n\nnote that, in the stochastic case, usually many sample trajectories need to be simu-\nlated to obtain an accurate estimate of the expected return.\n\nevaluating the optimization criterion of policy search requires the accurate es-\ntimation of returns from all the initial states. this procedure is likely to be com-\nputationally expensive, especially in the stochastic case. since optimization algo-\nrithms typically require many evaluations of the criterion, policy search algorithms\nare therefore computationally expensive, usually more so than value iteration and\npolicy iteration.\n\ncomputational cost of exhaustive policy search for finite mdps\n\nwe next investigate the computational cost of a policy search algorithm for deter-\nministic mdps with a finite number of states and actions. since the state and action\nspaces are finite and therefore discrete, any combinatorial optimization technique\ncould be used to look for an optimal policy. however, for simplicity, we consider an\nalgorithm that exhaustively searches the entire policy space.\n\nin the deterministic case, a single trajectory consisting of k simulation steps\nsuffices to estimate the return from a given initial state. the number of possible\n\nfollows that the total number of simulation steps that have to be performed to find an\n\npolicies is |u||x| and the return has to be evaluated for all the |x| initial states. it\noptimal policy is at most k|u||x||x|. since f , \u03c1, and h are each evaluated once at\n\nevery simulation step, the computational cost, measured by the number of function\nevaluations, is:\n\n3k|u||x||x|\n\n "}, {"Page_number": 50, "text": "40\n\nchapter 2. an introduction to dp and rl\n\ncompared to the cost l|x||u| (2 + |u|) of q-iteration for deterministic systems\n(2.28), this implementation of policy search is, in most cases, clearly more costly.\nin the stochastic case, when computing the expected return from a given initial\nstate x0, the exhaustive search algorithm considers all the possible realizations of a\ntrajectory of length k. starting from initial state x0 and taking action h(x0), there\nare |x| possible values of x1, the state at step 1 of the trajectory. the algorithm con-\nsiders all these possible values, together with their respective probabilities of being\nreached, namely \u00aff (x0, h(x0), x1). then, for each of these values of x1, given the re-\nspective actions h(x1), there are again |x| possible values of x2, each reachable with\na certain probability, and so on until k steps have been considered. with a recursive\nimplementation, a total number of |x|+|x|2 +\u00b7\u00b7\u00b7+|x|k steps have to be considered.\nare \u00aff , \u02dc\u03c1, and h. moreover, |u||x| policies have to be evaluated for |x| initial states,\n\neach such step requires 3 function evaluations, where the functions being evaluated\n\nso the total cost of exhaustive policy search in the stochastic case is:\n\nk=1|x|k!|u||x||x| = 3|x|k+1 \u2212|x|\n3  k\n\n\u2211\n\n|x|\u2212 1\n\n|u||x||x|\n\nunsurprisingly, this cost grows roughly exponentially with k, rather than linearly\nas in the deterministic case, so exhaustive policy search is more computationally\nexpensive in the stochastic case than in the deterministic case. in most problems, the\ncost of exhaustive policy search in the stochastic case is also greater than the cost\n\nl|x|2|u| (2 +|u|) of q-iteration (2.29).\n\nof course, much more efficient optimization techniques than exhaustive search\nare available, and the estimation of the expected returns can also be accelerated. for\ninstance, after the return of a state has been estimated, this estimate can be reused\nat every occurrence of that state along subsequent trajectories, thereby reducing the\ncomputational cost. nevertheless, the costs derived above can be seen as worst-case\nvalues that illustrate the inherently large complexity of policy search.\n\nexample 2.5 exhaustive policy search for the cleaning robot. consider again the\ncleaning-robot problem introduced in examples 2.1 and 2.2, and assume that the ex-\nhaustive policy search described above is applied. take the approximation tolerance\nin the evaluation of the return to be \u03b5mc = 0.01, which is equal to the suboptimal-\nity bound \u03c2qi for q-iteration in example 2.3. using \u03c2qi, maximum absolute reward\nk\u03c1k\u221e = 5, and discount factor \u03b3 = 0.5 in (2.41), a time horizon of k = 10 steps is ob-\ntained. therefore, in the deterministic case, the computational cost of the algorithm,\nmeasured by the number of function evaluations, is:\n\n3k|u||x||x| = 3\u00b7 10\u00b7 26 \u00b7 6 = 11520\n\nwhereas in the stochastic case, it is:\n\n3|x|k+1 \u2212|x|\n\n|x|\u2212 1\n\n|u||x||x| = 3\u00b7\n\n611 \u2212 6\n6\u2212 1 \u00b7 26 \u00b7 6 \u2248 8\u00b7 1010\n\n "}, {"Page_number": 51, "text": "2.6. summary and discussion\n\n41\n\nby observing that it is unnecessary to look for optimal actions and to evaluate returns\n\nin the terminal states, the cost can further be reduced to 3\u00b7 10\u00b7 24 \u00b7 4 = 1920 in the\ndeterministic case, and to 3\u00b7 611\u22126\n6\u22121 \u00b7 24 \u00b7 4 \u2248 1\u00b7 1010 in the stochastic case. additional\n\nreductions in cost can be obtained by stopping the simulation of trajectories as soon\nas they reach a terminal state, which will often happen in fewer than 10 steps.\n\ntable 2.7 compares the computational cost of exhaustive policy search with the\ncost of q-iteration from example 2.3 and of policy iteration from example 2.4. for\nthe cleaning-robot problem, the exhaustive implementation of direct policy search is\nvery likely to be more expensive than both q-iteration and policy iteration.\n(cid:3)\n\ntable 2.7\ncomputational cost of exhaustive policy search for the cleaning robot, compared with\nthe cost of q-iteration and of policy iteration. the cost is measured by the number of\nfunction evaluations.\n\ndeterministic case stochastic case\n\nexhaustive policy search\nexhaustive policy search,\n\nno optimization in terminal states\n\nq-iteration\npolicy iteration\n\n11520\n\n1920\n\n240\n504\n\n8\u00b7 1010\n1\u00b7 1010\n\n6336\n13272\n\n2.6 summary and discussion\n\nin this chapter, deterministic and stochastic mdps have been introduced, and their\noptimal solution has been characterized. three classes of dp and rl algorithms\nhave been described: value iteration, policy iteration, and direct search for control\npolicies. this presentation provides the necessary background for the remainder of\nthis book, but is by no means exhaustive. for the reader interested in more details\nabout classical dp and rl, we recommend the textbook of bertsekas (2007) on dp,\nand that of sutton and barto (1998) on rl.\n\na central challenge in the dp and rl fields is that, in their original form, dp and\nrl algorithms cannot be implemented for general problems. they can only be im-\nplemented when the state and action spaces consist of a finite number of discrete ele-\nments, because (among other reasons) they require the exact representation of value\nfunctions or policies, which is generally impossible for state spaces with an infinite\nnumber of elements. in the case of q-functions, an infinite number of actions also\nprohibits an exact representation. for instance, most problems in automatic control\nhave continuous states and actions, which can take infinitely many distinct values.\neven when the states and actions take finitely many values, the cost of representing\nvalue functions and policies grows exponentially with the number of state variables\n(and action variables, for q-functions). this problem is called the curse of dimen-\n\n "}, {"Page_number": 52, "text": "42\n\nchapter 2. an introduction to dp and rl\n\nsionality, and makes the classical dp and rl algorithms impractical when there are\nmany state and action variables.\n\nto cope with these problems, versions of the classical algorithms that approxi-\nmately represent value functions and/or policies must be used. such algorithms for\napproximate dp and rl form the subject of the remainder of this book.\n\nin practice, it is essential to provide more comprehensive performance guarantees\nthan simply the asymptotical maximization of the return. for instance, online rl\nalgorithms should guarantee an increase in performance over time. note that the\nperformance cannot increase monotonically, since exploration is necessary, which\ncan cause temporary degradations of the performance. in order to use dp and rl\nalgorithms in industrial applications of automatic control, it should be guaranteed\nthat they can never destabilize the process. for instance, perkins and barto (2002);\nbalakrishnan et al. (2008) discuss dp and rl approaches that guarantee stability\nusing the lyapunov framework (khalil, 2002, chapters 4 and 14).\n\ndesigning a good reward function is an important and nontrivial step of apply-\ning dp and rl. classical texts on rl recommend making the reward function as\nsimple as possible; it should only reward the achievement of the final goal (sutton\nand barto, 1998). however, a simple reward function often makes online rl slow,\nand including more information may be required for successful learning. moreover,\nother high-level requirements on the behavior of the controller often have to be con-\nsidered in addition to achieving the final goal. for instance, in automatic control the\ncontrolled state trajectories often have to satisfy requirements on overshoot and the\nrate of convergence to an equilibrium, etc. translating such requirements into the\nlanguage of rewards can be very challenging.\n\ndp and rl algorithms can also be greatly helped by domain knowledge. al-\nthough rl is usually envisioned as purely model-free, it can be very beneficial to use\nprior knowledge about the problem, if such knowledge is available. if a partial model\nis available, a dp algorithm can be run with this partial model, in order to obtain a\nrough initial solution for the rl algorithm. prior knowledge about the policy can also\nbe used to restrict the class of policies considered by (model-based or model-free)\npolicy iteration and policy search. a good way to provide domain knowledge to any\ndp or rl algorithm is to encode it in the reward function (dorigo and colombetti,\n1994; matari\u00b4c, 1997; randl\u00f8v and alstr\u00f8m, 1998; ng et al., 1999). this procedure is\nrelated to the problem of reward function design discussed above. for instance, prior\nknowledge about promising control actions can be exploited by associating these ac-\ntions with high rewards. encoding prior knowledge in the reward function should be\ndone with care, because doing so incorrectly can lead to unexpected and possibly\nundesirable behavior.\n\nother work aiming to expand the boundaries of dp and rl includes: problems in\nwhich the state is not fully measurable, called partially observable mdps (lovejoy,\n1991; kaelbling et al., 1998; singh et al., 2004; pineau et al., 2006; porta et al., 2006),\nexploiting modular and hierarchical task decompositions (dietterich, 2000; hengst,\n2002; russell and zimdars, 2003; barto and mahadevan, 2003; ghavamzadeh and\nmahadevan, 2007), and applying dp and rl to distributed, multi-agent problems\n(panait and luke, 2005; shoham et al., 2007; bus\u00b8oniu et al., 2008a).\n\n "}, {"Page_number": 53, "text": "3\n\ndynamic programming and reinforcement\nlearning in large and continuous spaces\n\nthis chapter describes dynamic programming and reinforcement learning for large\nand continuous-space problems. in such problems, exact solutions cannot be found in\ngeneral, and approximation is necessary. the algorithms of the previous chapter can\ntherefore no longer be applied in their original form. instead, approximate versions of\nvalue iteration, policy iteration, and policy search are introduced. theoretical guar-\nantees are provided on the performance of the algorithms, and numerical examples\nare used to illustrate their behavior. techniques to automatically find value function\napproximators are reviewed, and the three categories of algorithms are compared.\n\n3.1 introduction\n\nthe classical dynamic programming (dp) and reinforcement learning (rl) algo-\nrithms introduced in chapter 2 require exact representations of the value func-\ntions and policies. in general, an exact value function representation can only be\nachieved by storing distinct estimates of the return for every state-action pair (when\nq-functions are used) or for every state (in the case of v-functions). similarly, to rep-\nresent policies exactly, distinct actions have to be stored for every state. when some\nof the variables have a very large or infinite number of possible values (e.g., when\nthey are continuous), such exact representations are no longer possible, and value\nfunctions and policies need to be represented approximately. since most problems of\npractical interest have large or continuous state and action spaces, approximation is\nessential in dp and rl.\n\napproximators can be separated into two main types: parametric and nonpara-\nmetric. parametric approximators are mappings from a parameter space into the\nspace of functions they aim to represent. the form of the mapping and the number of\nparameters are given a priori, while the parameters themselves are tuned using data\nabout the target function. a representative example is a weighted linear combination\nof a fixed set of basis functions, in which the weights are the parameters. in contrast,\nthe structure of a nonparametric approximator is derived from the data. despite its\nname, a nonparametric approximator typically still has parameters, but unlike in the\nparametric case, the number of parameters (as well as their values) is determined\n\n43\n\n "}, {"Page_number": 54, "text": "44\n\nchapter 3. dp and rl in large and continuous spaces\n\nfrom the data. for instance, kernel-based approximators considered in this book de-\nfine one kernel per data point, and represent the target function as a weighted linear\ncombination of these kernels, where again the weights are the parameters.\n\nthis chapter provides an extensive, in-depth review of approximate dp and rl\nin large and continuous-space problems. the three basic classes of dp and rl al-\ngorithms discussed in chapter 2, namely value iteration, policy iteration, and policy\nsearch, are all extended to use approximation, resulting in approximate value itera-\ntion, approximatepolicyiteration, and approximatepolicysearch. algorithm deriva-\ntions are complemented by theoretical guarantees on their performance, by numerical\nexamples illustrating their behavior, and by comparisons of the different approaches.\nseveral other important topics in value function and policy approximation are also\ntreated. to help in navigating this large body of material, figure 3.1 presents a road\nmap of the chapter in graphical form, and the remainder of this section details this\nroad map.\n\nsection 3.1\nintroduction\n\nsection 3.2\n\nthe need for approximation\n\nsection 3.3\n\napproximation architectures\n\nsection 3.4\n\nsection 3.5\n\napproximate value iteration\n\napproximate policy iteration\n\nsection 3.7\n\napproximate policy search\n\nsection 3.6\n\nfinding value function\n\napproximators automatically\n\nsection 3.8\ncomparison\n\nsection 3.9\n\nsummary and discussion\n\nfigure 3.1\na road map of this chapter. the arrows indicate the recommended sequence of reading. dashed\narrows indicate optional ordering.\n\nin section 3.2, the need for approximation in dp and rl for large and continu-\nous spaces is explained. approximation is not only a problem of compact represen-\ntation, but also plays a role in several other parts of dp and rl algorithms. in sec-\n\n "}, {"Page_number": 55, "text": "3.1. introduction\n\n45\n\ntion 3.3, parametric and nonparametric approximation architectures are introduced\nand compared.\n\nthis introduction is followed by an in-depth discussion of approximate value iter-\nation in section 3.4, and of approximate policy iteration in section 3.5. techniques to\nautomatically derive value function approximators, useful in approximate value iter-\nation and policy iteration, are reviewed right after these two classes of algorithms, in\nsection 3.6. approximate policy search is discussed in detail in section 3.7. repre-\nsentative algorithms from each of the three classes are applied to a numerical example\ninvolving the optimal control of a dc motor.\n\nin closing the chapter, approximate value iteration, policy iteration, and poli-\ncy search are compared in section 3.8, while section 3.9 provides a summary and\ndiscussion.\n\nin order to reasonably restrict the scope of this chapter, several choices are made\n\nregarding the material that will be presented:\n\n\u2022 in the context of value function approximation, we focus on q-function ap-\nproximation and q-function based algorithms, because a significant portion of\nthe remainder of this book concerns such algorithms. nevertheless, a majority\nof the concepts and algorithms introduced extend in a straightforward manner\nto the case of v-function approximation.\n\n\u2022 we mainly consider parametric approximation, because the remainder of the\nbook relies on this type of approximation, but we also review nonparametric\napproaches to approximate value iteration and policy iteration.\n\n\u2022 when discussing parametric approximation, whenever appropriate, we con-\nsider general (possibly nonlinear) parametrizations. sometimes, however, we\nconsider linear parametrizations in more detail, e.g., because they allow\nthe derivation of better theoretical guarantees on the resulting approximate\nsolutions.\n\nnext, we give some additional details about the organization of the core material\nof this chapter, which consists of approximate value iteration (section 3.4), approx-\nimate policy iteration (section 3.5), and approximate policy search (section 3.7). to\nthis end, figure 3.2 shows how the algorithms selected for presentation are orga-\nnized, using a graphical tree format. this organization will be explained below. all\nthe terminal (right-most) nodes in the trees correspond to subsections in sections 3.4,\n3.5, and 3.7. note that figure 3.2 does not contain an exhaustive taxonomy of all the\napproaches.\n\nwithin the context of approximate value iteration, algorithms employing para-\nmetric approximation are presented first, separating model-based from model-free\napproaches. then, value iteration with nonparametric approximation is reviewed.\n\napproximate policy iteration consists of two distinct problems: approximate pol-\nicy evaluation, i.e., finding an approximate value function for a given policy, and pol-\nicy improvement. out of these two problems, approximate policy evaluation poses\nmore interesting theoretical questions, because, like approximate value iteration, it\n\n "}, {"Page_number": 56, "text": "46\n\nchapter 3. dp and rl in large and continuous spaces\n\nmodel-based value iteration with\nparametric approximation\n\napproximate\nvalue iteration\n\nmodel-free value iteration with\nparametric approximation\n\nvalue iteration with\nnonparametric approximation\n\napproximate\npolicy iteration\n\napproximate\npolicy evaluation\n\npolicy improvement\n\nvalue iteration-like algorithms for\napproximate policy evaluation\n\nmodel-free policy evaluation with\nlinearly parameterized approximation\n\npolicy evaluation with\nnonparametric approximation\n\nmodel-based approximate policy evaluation with\nrollouts\n\napproximate\npolicy search\n\ngradient-based policy search,\nactor-critic methods\n\ngradient-free policy search\n\nfigure 3.2\nthe organization of the algorithms for approximate value iteration, policy iteration, and policy\nsearch presented in this chapter.\n\ninvolves finding an approximate solution to a bellman equation. special require-\nments have to be imposed to ensure that a meaningful approximate solution exists\nand can be found by appropriate algorithms. in contrast, policy improvement relies\non solving maximization problems over the action variables, which involve fewer\ntechnical difficulties (although they may still be hard to solve when the action space\nis large). therefore, we pay special attention to approximate policy evaluation in\nour presentation. we first describe a class of algorithms for policy evaluation that\nare derived along the same lines as approximate value iteration. then, we introduce\nmodel-free policy evaluation with linearly parameterized approximation, and briefly\nreview nonparametric approaches to approximate policy evaluation. additionally, a\nmodel-based, direct simulation approach for policy evaluation is discussed that em-\nploys monte carlo estimates called \u201crollouts.\u201d\n\nfrom the class of approximate policy search methods (section 3.7), gradient-\nbased and gradient-free methods for policy optimization are discussed in turn. in the\ncontext of gradient-based methods, special attention is paid to the important category\nof actor-critic techniques.\n\n "}, {"Page_number": 57, "text": "3.2. the need for approximation in large and continuous spaces\n\n47\n\n3.2 the need for approximation in large and continuous spaces\n\nthe algorithms for exact value iteration (section 2.3) require the storage of distinct\nreturn estimates for every state (if v-functions are used) or for every state-action pair\n(in the case of q-functions). when some of the state variables have a very large or in-\nfinite number of possible values (e.g., when they are continuous), exact storage is no\nlonger possible, and the value functions must be represented approximately. large or\ncontinuous action spaces make the representation of q-functions additionally chal-\nlenging. in policy iteration (section 2.4), value functions and sometimes policies also\nneed to be represented approximately in general. similarly, in policy search (sec-\ntion 2.5), policies must be represented approximately when the state space is large or\ncontinuous.\n\napproximation in dp/rl is not only a problem of representation. two additional\ntypes of approximation are needed. first, sample-based approximation is necessary\nin any dp/rl algorithm. second, value iteration and policy iteration must repeatedly\nsolve potentially difficult nonconcave maximization problems over the action vari-\nables, whereas policy search must find optimal policy parameters, which involves\nsimilar difficulties. in general, these optimization problems can only be solved ap-\nproximately. these two types of approximation are detailed below.\n\nsample-based approximation is required for two distinct purposes in value func-\ntion estimation. consider first, as an example, the q-iteration algorithm for determin-\nistic problems, namely algorithm 2.1. every iteration of this algorithm would have\nto be implemented as follows:\n\nfor every (x, u) do: q\u2113+1(x, u) = \u03c1(x, u) + \u03b3max\n\nu\u2032\n\nq\u2113( f (x, u), u\u2032)\n\n(3.1)\n\nwhen the state-action space contains an infinite number of elements, it is impos-\nsible to loop over all the state-action pairs in finite time. instead, a sample-based,\napproximate update has to be used that only considers a finite number of state-action\nsamples.\n\nsuch sample-based updates are also necessary in stochastic problems. moreover,\nin the stochastic case, sample-based approximation is required for a second, distinct\npurpose. consider, e.g., the q-iteration algorithm for general stochastic problems,\nwhich for every state-action pair (x, u) considered would have to be implemented as\nfollows:\n\nq\u2113+1(x, u) = ex\u2032\u223c \u02dcf (x,u,\u00b7)(cid:26) \u02dc\u03c1(x, u, x\u2032) + \u03b3max\n\nu\u2032\n\nq\u2113(x\u2032, u\u2032)(cid:27)\n\n(3.2)\n\nclearly, the expectation on the right-hand side of (3.2) cannot be computed exactly in\ngeneral, and must be estimated from a finite number of samples, e.g., by using monte\ncarlo methods. note that, in many rl algorithms, the estimation of the expectation\ndoes not appear explicitly, but is performed implicitly while processing samples.\nfor instance, q-learning (algorithm 2.3) is such an algorithm, in which stochastic\napproximation is employed to estimate the expectation.\n\nthe maximization over the action variable in (3.1) or (3.2) (as well as in other\n\n "}, {"Page_number": 58, "text": "48\n\nchapter 3. dp and rl in large and continuous spaces\n\nvalue iteration algorithms) has to be solved for every sample considered. in large or\ncontinuous action spaces, this maximization is a potentially difficult nonconcave op-\ntimization problem, which can only be solved approximately in general. to simplify\nthis problem, many algorithms discretize the action space in a small number of val-\nues, compute the value function for all the discrete actions, and find the maximum\namong these values using enumeration.\n\nin policy iteration, sample-based approximation is required at the policy eval-\nuation step, for reasons similar to those explained above. the maximization issues\naffect the policy improvement step, which in the case of q-functions computes a\npolicy h\u2113+1 using (2.34), repeated here for easy reference:\n\nh\u2113+1(x) \u2208 arg max\n\nu\n\nqh\u2113(x, u)\n\nnote that these sampling and maximization issues also affect algorithms that em-\n\nploy v-functions.\n\nin policy search, some methods (e.g., actor-critic algorithms) estimate value func-\ntions and are therefore affected by the sampling issues mentioned above. even meth-\nods that do not employ value functions must estimate returns in order to evaluate the\npolicies, and return estimation requires sample-based approximation, as described\nnext. in principle, a policy that maximizes the return from every initial state should\nbe found. however, the return can only be estimated for a finite subset of initial states\n(samples) from the possibly infinite state space. additionally, in stochastic problems,\nfor every initial state considered, the expected return (2.15) must be evaluated using\na finite set of sampled trajectories, e.g., by using monte carlo methods.\n\nbesides these sampling problems, policy search methods must of course find the\nbest policy within the class of policies considered. this is a difficult optimization\nproblem, which can only be solved approximately in general. however, it only needs\nto be solved once, unlike the maximization over actions in value iteration and pol-\nicy iteration, which needs to be solved for every sample considered. in this sense,\npolicy search methods are less affected from the maximization difficulties than value\niteration or policy iteration.\n\na different view on the benefits of approximation can be taken in the model-free,\nrl setting. consider a value iteration algorithm that estimates q-functions, such\nas q-learning (algorithm 2.3). without approximation, the q-value of every state-\naction pair must be estimated separately (assuming it is possible to do so). if little or\nno data is available for some states, their q-values are poorly estimated, and the al-\ngorithm makes poor control decisions in those states. however, when approximation\nis used, the approximator can be designed so that the q-values of each state influence\nthe q-values of other, usually nearby, states (this requires the assumption of a certain\ndegree of smoothness for the q-function). then, if good estimates of the q-values\nof a certain state are available, the algorithm can also make reasonable control deci-\nsions in nearby states. this is called generalization in the rl literature, and can help\nalgorithms work well despite using only a limited number of samples.\n\n "}, {"Page_number": 59, "text": "3.3. approximation architectures\n\n49\n\n3.3 approximation architectures\n\ntwo major classes of approximators can be identified, namely parametric and non-\nparametric approximators. we introduce parametric approximators in section 3.3.1,\nnonparametric approximators in section 3.3.2, and compare the two classes in sec-\ntion 3.3.3. section 3.3.4 contains some additional remarks.\n\n3.3.1 parametric approximation\n\nparametric approximators are mappings from a parameter space into the space of\nfunctions they aim to represent (in dp/rl, value functions or policies). the func-\ntional form of the mapping and the number of parameters are typically established\nin advance and do not depend on the data. the parameters of the approximator are\ntuned using data about the target function.\n\nconsider a q-function approximator parameterized by an n-dimensional vector1\n\u03b8. the approximator is denoted by an approximation mapping f : rn \u2192 q, where\nrn is the parameter space and q is the space of q-functions. every parameter vector\n\u03b8 provides a compact representation of a corresponding approximate q-function:\n\nor equivalently, element-wise:\n\nbq = f(\u03b8)\n\nbq(x, u) = [f(\u03b8)](x, u)\n\nwhere [f(\u03b8)](x, u) denotes the q-function f(\u03b8) evaluated at the state-action pair\n(x, u). so, instead of storing distinct q-values for every pair (x, u), which would be\nimpractical in many cases, it is only necessary to store n parameters. when the state-\naction space is discrete, n is usually much smaller than |x|\u00b7|u|, thereby providing\na compact representation (recall that, when applied to sets, the notation |\u00b7| stands\nfor cardinality). however, since the set of q-functions representable by f is only a\nsubset of q, an arbitrary q-function can generally only be represented up to a certain\napproximation error, which must be accounted for.\n\nin general, the mapping f can be nonlinear in the parameters. a typical exam-\nple of a nonlinearly parameterized approximator is a feed-forward neural network\n(hassoun, 1995; bertsekas and tsitsiklis, 1996, chapter 3). however, linearly pa-\nrameterized approximators are often preferred in dp and rl, because they make\nit easier to analyze the theoretical properties of the resulting dp/rl algorithms.\na linearly parameterized q-function approximator employs n basis functions (bfs)\n\u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r and an n-dimensional parameter vector \u03b8. approximate q-\nvalues are computed with:\n\n[f(\u03b8)](x, u) =\n\nn\n\n\u2211\nl=1\n\n\u03c6l(x, u)\u03b8l = \u03c6t(x, u)\u03b8\n\n(3.3)\n\n1all the vectors used in this book are column vectors.\n\n "}, {"Page_number": 60, "text": "50\n\nchapter 3. dp and rl in large and continuous spaces\n\nwhere \u03c6(x, u) = [\u03c61(x, u), . . . ,\u03c6n(x, u)]t is the vector of bfs. in the literature, the bfs\nare also called features (bertsekas and tsitsiklis, 1996).\n\nexample 3.1 approximating q-functions with state-dependent bfs and dis-\ncrete actions. as explained in section 3.2, in order to simplify the maximization\nover actions, in many dp/rl algorithms the action space is discretized into a small\nnumber of values. in this example we consider such a discrete-action approximator,\nwhich additionally employs state-dependent bfs to approximate over the state space.\na discrete, finite set of actions u1, . . . , um is chosen from the original action space\nu . the resulting discretized action space is denoted by ud = {u1, . . . , um}. a number\nof n state-dependent bfs \u00af\u03c61, . . . , \u00af\u03c6n : x \u2192 r are defined and replicated for each\ndiscrete action in ud. approximate q-values can be computed for any state-discrete\naction pair with:\n\n[f(\u03b8)](x, u j) = \u03c6t(x, u j)\u03b8,\n\n(3.4)\n\nwhere, in the state-action bf vector \u03c6t(x, u j), all the bfs that do not correspond to\nthe current discrete action are taken to be equal to 0:\n\n\u03c6(x, u j) = [0, . . . , 0\n\n, . . . , 0, \u00af\u03c61(x), . . . , \u00af\u03c6n(x)\n\n, 0, . . . , 0, . . . , 0\n\nu1\n\n| {z }\n\n|\n\nu j\n\n{z\n\n}\n\num\n\n| {z }\n\n]t \u2208 rnm\n\n(3.5)\n\nthe parameter vector \u03b8 therefore has nm elements. this type of approximator can\nbe seen as representing m distinct state-dependent slices through the q-function, one\nslice for each of the m discrete actions. note that it is only meaningful to use such an\napproximator for the discrete actions in ud; for any other actions, the approximator\noutputs 0. for this reason, only the discrete actions are considered in (3.4) and (3.5).\nin this book, we will often use such discrete-action approximators. for instance,\nconsider normalized (elliptical) gaussian radial basis functions (rbfs). this type of\nrbf can be defined as follows:\n\n\u00af\u03c6i(x) =\n\n\u03c6\u2032i (x)\ni\u2032=1 \u03c6\u2032i\u2032(x)\n\u2211n\n\n, \u03c6\u2032i (x) = exp(cid:18)\u2212\n\n1\n\n2\n\n[x\u2212 ci]tbi\u22121[x\u2212 ci](cid:19)\n\n(3.6)\n\nhere, \u03c6\u2032i are the nonnormalized rbfs, the vector ci = [ci,1, . . . , ci,d]t \u2208 rd is the cen-\nter of the ith rbf, and the symmetric positive-definite matrix bi \u2208 rd\u00d7d is its width.\n\ndepending on the structure of the width matrix, rbfs of various shapes can be ob-\ntained. for a general width matrix, the rbfs are elliptical, while axis-aligned rbfs\nare obtained if the width matrix is diagonal, i.e., if bi = diag (bi,1, . . . , bi,d). in this\ncase, the width of an rbf can also be expressed using a vector bi = [bi,1, . . . , bi,d]t.\nfurthermore, spherical rbfs are obtained if, in addition, bi,1 = \u00b7\u00b7\u00b7 = bi,d.\nanother class of discrete-action approximators uses stateaggregation (bertsekas\nand tsitsiklis, 1996, section 6.7). for state aggregation, the state space is partitioned\ninto n disjoint subsets. let xi be the ith subset in this partition, for i = 1, . . . , n.\nfor a given action, the approximator assigns the same q-values for all the states in\nxi. this corresponds to a bf vector of the form (3.5), with binary-valued (0 or 1)\n\n "}, {"Page_number": 61, "text": "state-dependent bfs:\n\n3.3. approximation architectures\n\n\u00af\u03c6i(x) =(1 if x \u2208 xi\n\n0 otherwise\n\n51\n\n(3.7)\n\nbecause the subsets xi are disjoint, exactly one bf is active at any point in the state\nspace. all the individual states belonging to xi can thus be seen as a single, larger\naggregate (or quantized) state; hence the name \u201cstate aggregation\u201d (or state quantiza-\ntion). by additionally identifying each subset xi with a prototype state xi \u2208 xi, state\naggregation can also be seen as statediscretization, where the discretized state space\nis xd = {x1, . . . , xn}. the prototype state can be, e.g., the geometrical center of xi\n(assuming this center belongs to xi), or some other representative state.\nusing the definition (3.7) of the state-dependent bfs and the expression (3.5) for\n\nthe state-action bfs, the state-action bfs can be written compactly as follows:\n\n\u03c6[i, j](x, u) =(1\n\n0\n\nif x \u2208 xi and u = u j\notherwise\n\n(3.8)\n\nthe notation [i, j] represents the scalar index corresponding to i and j, which can be\ncomputed as [i, j] = i + ( j \u2212 1)n. if the n elements of the bf vector were arranged\ninto an n \u00d7 m matrix, by first filling in the first column with the first n elements,\nthen the second column with the subsequent n elements, etc., then the element at\nindex [i, j] of the vector would be placed at row i and column j of the matrix. note\nthat exactly one state-action bf (3.8) is active at any point of x \u00d7 ud, and no bf is\nactive if u /\u2208 ud.\n\n(cid:3)\n\nother types of linearly parameterized approximators used in the literature in-\nclude tile coding (watkins, 1989; sherstov and stone, 2005), multilinear interpola-\ntion (davies, 1997), and kuhn triangulation (munos and moore, 2002).\n\n3.3.2 nonparametric approximation\n\nnonparametric approximators, despite their name, still have parameters. however,\nunlike in the parametric case, the number of parameters, as well as the form of the\nnonparametric approximator, are derived from the available data.\n\nkernel-based approximators are typical representatives of the nonparametric\nclass. consider a kernel-based approximator of the q-function. in this case, the ker-\nnelfunction is a function defined over two state-action pairs, \u03ba : x \u00d7u \u00d7 x \u00d7u \u2192 r:\n(3.9)\n\n(x, u, x\u2032, u\u2032) 7\u2192 \u03ba((x, u), (x\u2032, u\u2032))\n\nthat must also satisfy certain additional conditions (see, e.g., smola and sch\u00a8olkopf,\n2004). under these conditions, the function \u03ba can be interpreted as an inner prod-\nuct between feature vectors of its two arguments (the two state-action pairs) in a\nhigh-dimensional feature space. using this property, a powerful approximator can\nbe obtained by only computing the kernels, without ever working explicitly in the\n\n "}, {"Page_number": 62, "text": "52\n\nchapter 3. dp and rl in large and continuous spaces\n\nfeature space. note that in (3.9), as well as in the sequel, the state-action pairs are\ngrouped together for clarity.\n\na widely used type of kernel is the gaussian kernel, which for the problem of\n\napproximating the q-function is given by:\n\n\u03ba((x, u), (x\u2032, u\u2032)) = exp \u2212\n\n1\n\nu\u2212 u\u2032(cid:21)t\n2(cid:20)x\u2212 x\u2032\n\nu\u2212 u\u2032(cid:21)!\nb\u22121(cid:20)x\u2212 x\u2032\n\n(3.10)\n\nwhere the kernel width matrix b \u2208 r(d+c)\u00d7(d+c) must be symmetric and positive\ndefinite. here, d denotes the number of state variables and c denotes the number of\naction variables. for instance, a diagonal matrix b = diag (b1, . . . , bd+c) can be used.\nnote that, when the pair (x\u2032, u\u2032) is fixed, the kernel (3.10) has the same shape as a\ngaussian state-action rbf centered on (x\u2032, u\u2032).\n\nassume that a set of state-action samples is available: {(xls, uls) | ls = 1, . . . , ns}.\n\nfor this set of samples, the kernel-based approximator takes the form:\n\nns\n\u2211\nls=1\n\n\u03ba((x, u), (xls, uls))\u03b8ls\n\n(3.11)\n\nbq(x, u) =\n\nwhere \u03b81, . . . ,\u03b8ns are the parameters. this form is superficially similar to the linearly\nparameterized approximator (3.3). however, there is a crucial difference between\nthese two approximators. in the parametric case, the number and form of the bfs\nwere defined in advance, and therefore led to a fixed functional form f of the ap-\nproximator. in contrast, in the nonparametric case, the number of kernels and their\nform, and thus also the number of parameters and the functional form of the approx-\nimator, are determined from the samples.\n\none situation in which the kernel-based approximator can be seen as a parametric\napproximator is when the set of samples is selected in advance. then, the resulting\nkernels can be identified with predefined bfs:\n\n\u03c6ls(x, u) = \u03ba((x, u), (xls, uls)),\n\nls = 1, . . . , ns\n\nand the kernel-based approximator (3.11) is equivalent to a linearly parameterized\napproximator (3.3). however, in many cases, such as in online rl, the samples are\nnot available in advance.\n\nimportant classes of nonparametric approximators that have been used in dp\nand rl include kernel-based methods (shawe-taylor and cristianini, 2004), among\nwhich support vector machines are the most popular (sch\u00a8olkopf et al., 1999; cris-\ntianini and shawe-taylor, 2000; smola and sch\u00a8olkopf, 2004), gaussian processes,\nwhich also employ kernels (rasmussen and williams, 2006), and regression trees\n(breiman et al., 1984; breiman, 2001). for instance, kernel-based and related\napproximators have been applied to value iteration (ormoneit and sen, 2002; deisen-\nroth et al., 2009; farahmand et al., 2009a) and to policy evaluation and policy itera-\ntion (lagoudakis and parr, 2003b; engel et al., 2003, 2005; xu et al., 2007; jung and\npolani, 2007a; bethke et al., 2008; farahmand et al., 2009b). ensembles of regres-\nsion trees have been used with value iteration by ernst et al. (2005, 2006a) and with\npolicy iteration by jodogne et al. (2006).\n\n "}, {"Page_number": 63, "text": "3.3. approximation architectures\n\n53\n\nnote that nonparametric approximators are themselves driven by certain meta-\nparameters, such as the width b of the gaussian kernel (3.10). these meta-parameters\ninfluence the accuracy of the approximator and may require tuning, which can be\ndifficult to perform manually. however, there also exist methods for automating the\ntuning process (deisenroth et al., 2009; jung and stone, 2009).\n\n3.3.3 comparison of parametric and nonparametric approximation\n\nbecause they are designed in advance, parametric approximators have to be flexi-\nble enough to accurately model the target functions solely by tuning the parameters.\nhighly flexible, nonlinearly parameterized approximators are available, such as neu-\nral networks. however, when used to approximate value functions in dp and rl,\ngeneral nonlinear approximators make it difficult to guarantee the convergence of the\nresulting algorithms, and indeed can sometimes lead to divergence. often, linearly\nparameterized approximators (3.3) must be used to guarantee convergence. such ap-\nproximators are specified by their bfs. when prior knowledge is not available to\nguide the selection of bfs (as is usually the case), a large number of bfs must be de-\nfined to evenly cover the state-action space. this is impractical in high-dimensional\nproblems. to address this issue, methods have been proposed to automatically derive\na small number of good bfs from data. we review these methods in section 3.6.\nbecause they derive bfs from data, such methods can be seen as residing between\nparametric and nonparametric approximation.\n\nnonparametric approximators are highly flexible. however, because their shape\ndepends on the data, it may change while the dp/rl algorithm is running, which\nmakes it difficult to provide convergence guarantees. a nonparametric approximator\nadapts its complexity to the amount of available data. this is beneficial in situations\nwhere data is costly or difficult to obtain. it can, however, become a disadvantage\nwhen a large amount of data is used, because the computational and memory de-\nmands of the approximator usually grow with the number of samples. for instance,\nthe kernel-based approximator (3.11) has a number of parameters equal to the num-\nber of samples ns used. this is especially problematic in online rl algorithms, which\nkeep receiving new samples for their entire lifetime. there exist approaches to mit-\nigate this problem. for instance, in kernel-based methods, the number of samples\nused to derive the approximator can be limited by only employing a subset of sam-\nples that contribute significantly to the accuracy of the approximation, and discarding\nthe rest. various measures can be used for the contribution of a given sample to the\napproximation accuracy. such kernel sparsification methods were employed by xu\net al. (2007); engel et al. (2003, 2005), and a related, so-called subset of regressors\nmethod was applied by jung and polani (2007a). ernst (2005) proposed the selec-\ntion of informative samples for an offline rl algorithm by iteratively choosing those\nsamples for which the error in the bellman equation is maximal under the current\nvalue function.\n\n "}, {"Page_number": 64, "text": "54\n\nchapter 3. dp and rl in large and continuous spaces\n\n3.3.4 remarks\n\nn\n\nthe approximation architectures introduced above for q-functions can be extended\nin a straightforward manner to v-function and policy approximation. for instance,\na linearly parameterized policy approximator can be described as follows. a set of\nstate-dependent bfs \u03d51, . . . ,\u03d5n : x \u2192 r are defined, and given a parameter vector\n\u03d1 \u2208 rn , the approximate policy is:\nbh(x) =\n\nwhere \u03d5(x) = [\u03d51(x), . . . ,\u03d5n (x)]t. for simplicity, the parametrization (3.12) is only\ngiven for scalar actions, but it can easily be extended to the case of multiple action\nvariables. note that we use calligraphic notation to differentiate variables related to\npolicy approximation from variables related to value function approximation. so, the\npolicy parameter is \u03d1 and the policy bfs are denoted by \u03d5, whereas the value func-\ntion parameter is \u03b8 and the value function bfs are denoted by \u03c6. furthermore, the\nnumber of policy parameters and bfs is n . when samples are used to approximate\nthe policy, their number is denoted by ns.\n\n\u03d5i(x)\u03d1i = \u03d5t(x)\u03d1\n\n\u2211\ni=1\n\n(3.12)\n\nin the parametric case, whenever we wish to explicitly highlight the dependence\n\nto denote q-functions and v-functions, respectively.\n\nof an approximate policy bh on the parameter vector \u03d1, we will use the notation\nbh(x;\u03d1). similarly, when the dependence of a value function on the parameters needs\nto be made explicit without using the mapping f, we will use bq(x, u;\u03b8) andbv (x;\u03b8)\n\nthroughout the remainder of this chapter, we will mainly focus on dp and rl\nwith parametric approximation, because the remainder of the book relies on this type\nof approximation, but we will also overview nonparametric approaches to value iter-\nation and policy iteration.\n\n3.4 approximate value iteration\n\nin order to apply value iteration to large or continuous-space problems, the value\nfunction must be approximated. figure 3.3 (repeated from the relevant part of fig-\nure 3.2) shows how our presentation of the algorithms for approximate value iteration\nis organized. first, we describe value iteration with parametric approximation in de-\ntail. specifically, in section 3.4.1 we present model-based algorithms from this class,\nand in section 3.4.2 we describe offline and online model-free algorithms. then, in\nsection 3.4.3, we briefly review value iteration with nonparametric approximation.\n\nhaving completed our review of the algorithms for approximate value iteration,\nwe then provide convergence guarantees for these algorithms, in section 3.4.4. fi-\nnally, in section 3.4.5, we apply two representative algorithms for approximate value\niteration to a dc motor control problem.\n\n "}, {"Page_number": 65, "text": "3.4. approximate value iteration\n\n55\n\nmodel-based value iteration with\nparametric approximation\n\napproximate\nvalue iteration\n\nmodel-free value iteration with\nparametric approximation\n\nvalue iteration with\nnonparametric approximation\n\nfigure 3.3\nthe organization of the algorithms for approximate value iteration presented in this section.\n\n3.4.1 model-based value iteration with parametric approximation\n\nthis section considers q-iteration with a general parametric approximator, which is\na representative model-based algorithm for approximate value iteration.\n\napproximate q-iteration is an extension of the exact q-iteration algorithm in-\ntroduced in section 2.3.1. recall that exact q-iteration starts from an arbitrary q-\nfunction q0 and at each iteration \u2113 updates the q-function using the rule (2.25),\nrepeated here for easy reference:\n\nq\u2113+1 = t (q\u2113)\n\nwhere t is the q-iteration mapping (2.22) or (2.23). in approximate q-iteration,\nthe q-function q\u2113 cannot be represented exactly. instead, an approximate version is\ncompactly represented by a parameter vector \u03b8\u2113 \u2208 rn, using a suitable approximation\nmapping f : rn \u2192 q (see section 3.3):\n\nthis approximate q-function is provided, instead of q\u2113, as an input to the q-iteration\nmapping t . so, the q-iteration update would become:\n\nbq\u2113 = f(\u03b8\u2113)\n\nq\u2021\n\u2113+1 = (t \u25e6 f)(\u03b8\u2113)\nhowever, in general, the newly found q-function q\u2021\n\u2113+1 cannot be explicitly stored,\neither. instead, it must also be represented approximately, using a new parameter\n\n(3.13)\n\nvector \u03b8\u2113+1. this parameter vector is obtained by a projectionmapping p : q \u2192 rn:\n\n\u03b8\u2113+1 = p(q\u2021\n\n\u2113+1)\n\nwhich ensures that bq\u2113+1 = f(\u03b8\u2113+1) is as close as possible to q\u2021\n\nfor p is least-squares regression, which, given a q-function q, produces:2\n\n\u2113+1. a natural choice\n\np(q) = \u03b8\u2021, where \u03b8\u2021 \u2208 arg min\n\n\u03b8\n\n(q(xls, uls)\u2212 [f(\u03b8)](xls, uls))2\n\n(3.14)\n\nns\n\u2211\nls=1\n\n2in the absence of additional restrictions, the use of least-squares projections can cause convergence\n\nproblems, as we will discuss in section 3.4.4.\n\n "}, {"Page_number": 66, "text": "56\n\nchapter 3. dp and rl in large and continuous spaces\n\nfor some set of state-action samples {(xls, uls) | ls = 1, . . . , ns}. some care is required\nto ensure that \u03b8\u2021 exists and that it is not too difficult to find. for instance, when the\napproximator f is linearly parameterized, (3.14) is a convex quadratic optimization\nproblem.\n\nto summarize, approximate q-iteration starts with an arbitrary (e.g., identically\n0) parameter vector \u03b80, and updates this vector at every iteration \u2113 using the compo-\nsition of mappings p, t , and f:\n\n\u03b8\u2113+1 = (p\u25e6 t \u25e6 f)(\u03b8\u2113)\n\n(3.15)\n\nof course, in practice, the intermediate results of f and t cannot be fully computed\nand stored. instead, p\u25e6 t \u25e6 f can be implemented as a single mapping, or t and f can\nbe sampled at a finite number of points. the algorithm is stopped once a satisfactory\n\nconditions under which a unique fixed point exists and is obtained asymptotically as\n\u2113 \u2192 \u221e.\n\nparameter vector b\u03b8\u2217 has been found (see below for examples of stopping criteria).\nideally, b\u03b8\u2217 is near to a fixed point \u03b8\u2217 of p \u25e6 t \u25e6 f. in section 3.4.4, we will give\ngivenb\u03b8\u2217, a greedy policy in f(b\u03b8\u2217) can be found, i.e., a policy h that satisfies:\n\n(3.16)\n\nh(x) \u2208 arg max\n\nu\n\nhere, as well as in the sequel, we assume that the q-function approximator is struc-\ntured in a way that guarantees the existence of at least one maximizing action for any\nstate. because the approximator is under the control of the designer, ensuring this\nproperty should not be too difficult.\n\nfigure 3.4 illustrates approximate q-iteration and the relations between the vari-\n\nous mappings, parameter vectors, and q-functions considered by the algorithm.\n\nspace of q-functions\n\nt\n\nf \u03b8(\n\n)0\n\nf\n\n\u03b80\n\nt\n(\n\n\u25cbf \u03b80\n\n)(\n\n)\n\nf \u03b8*(\n\n)\n\np\n\n\u03b81= p\n\n(\n\nt\n\n\u25cb \u25cb\n\nf\n\n)(\n\n\u03b80\n\n.....\n\n)\n\n\u03b82\n\n\u03b8*\nparameter space\n\nfigure 3.4\na conceptual illustration of approximate q-iteration. at every iteration, the approximation\nmapping f is applied to the current parameter vector to obtain an approximate q-function,\nwhich is then passed through the q-iteration mapping t . the result of t is then projected back\nonto the parameter space with the projection mapping p. ideally, the algorithm asymptotically\n\nconverges to a fixed point \u03b8\u2217, which leads back to itself when passed through p\u25e6 t \u25e6 f. the\nasymptotically obtained solution of approximate q-iteration is then the q-function f(\u03b8\u2217).\n\n[f(b\u03b8\u2217)](x, u)\n\n "}, {"Page_number": 67, "text": "3.4. approximate value iteration\n\n57\n\nalgorithm 3.1 presents an example of approximate q-iteration for a deterministic\nmarkov decision process (mdp), using the least-squares projection (3.14). at line 4\nof this algorithm, q\u2021\n\u2113+1(xls, uls) has been computed according to (3.13), in which the\ndefinition (2.22) of the q-iteration mapping has been substituted.\n\nalgorithm 3.1 least-squares approximate q-iteration for deterministic mdps.\ninput: dynamics f , reward function \u03c1, discount factor \u03b3,\n\napproximation mapping f, samples {(xls, uls) | ls = 1, . . . , ns}\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\nfor ls = 1, . . . , ns do\n\n4:\n\n5:\n\nq\u2021\n\u2113+1(xls, uls) \u2190 \u03c1(xls, uls) + \u03b3maxu\u2032[f(\u03b8\u2113)]( f (xls, uls), u\u2032)\nend for\n\n\u03b8\u2113+1 \u2190 \u03b8\u2021, where \u03b8\u2021 \u2208 arg min\u03b8 \u2211ns\n\n6:\n7: until \u03b8\u2113+1 is satisfactory\n\nls=1(cid:16)q\u2021\n\n\u2113+1(xls, uls)\u2212 [f(\u03b8)](xls, uls)(cid:17)2\n\noutput: b\u03b8\u2217 = \u03b8\u2113+1\n\nthere still remains the question of when to stop approximate q-iteration, i.e.,\nwhen to consider the parameter vector satisfactory. one possibility is to stop after\na predetermined number of iterations l. under the (reasonable) assumption that, at\n\nevery iteration \u2113, the approximate q-function bq\u2113 = f(\u03b8\u2113) is close to the q-function\n\nq\u2113 that would have been obtained by exact q-iteration, the number l of iterations\ncan be chosen with equation (2.27) of section 2.3.1, repeated here:\n\nl =(cid:24)log\u03b3\n\n\u03c2qi(1\u2212 \u03b3)2\n\n2k\u03c1k\u221e (cid:25)\n\nwhere \u03c2qi > 0 is a desired bound on the suboptimality of a policy greedy in the q-\nfunction obtained at iteration l. of course, because f(\u03b8\u2113) is not identical to q\u2113, it\ncannot be guaranteed that this bound is achieved. nevertheless, l is still useful as an\ninitial guess for the number of iterations needed to achieve a good performance.\n\nanother possibility is to stop the algorithm when the distance between \u03b8\u2113+1 and\n\u03b8\u2113 decreases below a certain threshold \u03b5qi > 0. this criterion is only useful if approx-\nimate q-iteration is convergent to a fixed point (see section 3.4.4 for convergence\nconditions). when convergence is not guaranteed, this criterion should be combined\nwith a maximum number of iterations, to ensure that the algorithm stops in finite\ntime.\n\nnote that we have not explicitly considered the maximization issues or the es-\ntimation of expected values in the stochastic case. as explained in section 3.2, one\nway to address the maximization difficulties is to discretize the action space. the\nexpected values in the q-iteration mapping for the stochastic case (2.23) need to\nbe estimated from samples. for additional insight into this problem, see the fitted\nq-iteration algorithm introduced in the next section.\n\na similar derivation can be given for approximate v-iteration, which is more pop-\n\n "}, {"Page_number": 68, "text": "58\n\nchapter 3. dp and rl in large and continuous spaces\n\nular in the literature (gonzalez and rofman, 1985; chow and tsitsiklis, 1991; gor-\ndon, 1995; tsitsiklis and van roy, 1996; munos and moore, 2002; gr\u00a8une, 2004).\nmany results from the literature deal with the discretization of continuous-variable\nproblems (gonzalez and rofman, 1985; chow and tsitsiklis, 1991; munos and\nmoore, 2002; gr\u00a8une, 2004). such discretization procedures sometimes use interpo-\nlation, which leads to linearly parameterized approximators similar to (3.3).\n\n3.4.2 model-free value iteration with parametric approximation\n\nfrom the class of model-free algorithms for approximate value iteration, we first\ndiscuss offline, batch algorithms, followed by online algorithms. online algorithms,\nmainly approximate versions of q-learning, have been studied since the beginning\nof the nineties (lin, 1992; singh et al., 1995; horiuchi et al., 1996; jouffe, 1998;\nglorennec, 2000; tuyls et al., 2002; szepesv\u00b4ari and smart, 2004; murphy, 2005;\nsherstov and stone, 2005; melo et al., 2008). a strong research thread in offline\nmodel-free value iteration emerged later (ormoneit and sen, 2002; ernst et al., 2005;\nriedmiller, 2005; szepesv\u00b4ari and munos, 2005; ernst et al., 2006b; antos et al.,\n2008a; munos and szepesv\u00b4ari, 2008; farahmand et al., 2009a).\n\noffline model-free approximate value iteration\n\nin the offline model-free case, the transition dynamics f and the reward function \u03c1\nare unknown.3 instead, only a batch of transition samples is available:\n\n{(xls, uls, x\u2032ls, rls)| ls = 1, . . . , ns}\n\nwhere for every ls, the next state x\u2032ls\nand the reward rls have been obtained as a result\nof taking action uls in the state xls . the transition samples may be independent, they\nmay belong to a set of trajectories, or to a single trajectory. for instance, when the\nsamples come from a single trajectory, they are typically ordered so that xls+1 = x\u2032ls\nfor all ls < ns.\n\nin this section, we present fitted q-iteration (ernst et al., 2005), a model-free\nversion of approximate q-iteration (3.15) that employs such a batch of samples. to\nobtain this version, two changes are made in the original, model-based algorithm.\nfirst, one has to use a sample-based projection mapping that considers only the sam-\nples (xls, uls), such as the least-squares regression (3.14). second, because f and \u03c1\nare not available, the updated q-function q\u2021\n\u2113+1 = (t \u25e6 f)(\u03b8\u2113) (3.13) at a given itera-\ntion \u2113 cannot be computed directly. instead, the q-values q\u2021\n\u2113+1(xls, uls) are replaced\nby quantities derived from the available data.\n\nto understand how this is done, consider first the deterministic case. in this case,\n\n3we take the point of view prevalent in the rl literature, which considers that the learning controller\nhas no prior information about the problem to be solved. this means the reward function is unknown. in\npractice, of course, the reward function is almost always designed by the experimenter and is therefore\nknown.\n\n "}, {"Page_number": 69, "text": "3.4. approximate value iteration\n\n59\n\nthe updated q-values are:\n\nq\u2021\n\n\u2113+1(xls, uls) = \u03c1(xls, uls) + \u03b3max\nu\u2032\n\n[f(\u03b8\u2113)]( f (xls, uls), u\u2032)\n\n(3.17)\n\nwhere the q-iteration mapping (2.22) has been used. recall that \u03c1(xls, uls) = rls and\nthat f (xls, uls) = x\u2032ls\n\n. by performing these substitutions in (3.17), we get:\n\nq\u2021\n\n\u2113+1(xls, uls) = rls + \u03b3max\nu\u2032\n\n[f(\u03b8\u2113)](x\u2032ls, u\u2032)\n\n(3.18)\n\nand hence the updated q-value can be computed exactly from the transition sample\n(xls, uls, x\u2032ls\n\n, rls), without using f or \u03c1.\n\nfitted q-iteration works in deterministic and stochastic problems, and replaces\n\neach q-value q\u2021\n\n\u2113+1(xls, uls) by the quantity:\n\nq\u2021\n\n\u2113+1,ls\n\n= rls + \u03b3max\n\nu\u2032\n\n[f(\u03b8\u2113)](x\u2032ls, u\u2032)\n\n(3.19)\n\nidentical to the right-hand side of (3.18). as already discussed, in the deterministic\ncase, this replacement is exact. in the stochastic case, the updated q-value is the\nexpectation of a random variable, of which q\u2021\nis only a sample. this updated\nq-value is:\n\n\u2113+1,ls\n\nq\u2021\n\n\u2113+1(xls, uls) = ex\u2032\u223c \u02dcf (xls ,uls ,\u00b7)(cid:26) \u02dc\u03c1(xls, uls, x\u2032) + \u03b3max\n\nu\u2032\n\n[f(\u03b8\u2113)](x\u2032, u\u2032)(cid:27)\n\nwhere the q-iteration mapping (2.23) has been used (note that q\u2021\n\u2113+1(xls, uls) is the\ntrue q-value and not a data point, so it is no longer subscripted by the sample index\nls). nevertheless, most projection algorithms, including the least-squares regression\n(3.14), seek to approximate the expected value of their output variable conditioned\nby the input. in fitted q-iteration, this means that the projection actually looks for\n\u2113+1, even though only samples of the form (3.19) are used.\n\n\u03b8\u2113 such that f(\u03b8\u2113) \u2248 q\u2021\n\ntherefore, the algorithm remains valid in the stochastic case.\n\nalgorithm 3.2 presents fitted q-iteration using least-squares projection (3.14).\nnote that, in the deterministic case, fitted q-iteration is identical to model-based ap-\nproximate q-iteration (e.g., algorithm 3.1), whenever both algorithms use the same\napproximator f, the same projection p, and the same state-action samples (xls, uls).\nthe considerations of section 3.4.1 about the stopping criteria of approximate q-\niteration also apply to fitted q-iteration, so they will not be repeated here. moreover,\nonce fitted q-iteration has found a satisfactory parameter vector, a policy can be\nderived with (3.16).\n\nwe have introduced fitted q-iteration in the parametric case, to clearly establish\nits link with model-based approximate q-iteration. neural networks are one class of\nparametric approximators that have been combined with fitted q-iteration, leading\nto the so-called \u201cneural fitted q-iteration\u201d (riedmiller, 2005). however, fitted q-\niteration is more popular in combination with nonparametric approximators, so we\nwill revisit it in the nonparametric context in section 3.4.3.\n\n "}, {"Page_number": 70, "text": "60\n\nchapter 3. dp and rl in large and continuous spaces\n\nalgorithm 3.2 least-squares fitted q-iteration with parametric approximation.\ninput: discount factor \u03b3,\n\napproximation mapping f, samples {(xls, uls, x\u2032ls\n\n, rls)| ls = 1, . . . , ns}\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\nfor ls = 1, . . . , ns do\n\n4:\n\n5:\n\nq\u2021\n\u2113+1,ls \u2190 rls + \u03b3maxu\u2032[f(\u03b8\u2113)](x\u2032ls\nend for\n\u03b8\u2113+1 \u2190 \u03b8\u2021, where \u03b8\u2021 \u2208 arg min\u03b8 \u2211ns\n\n6:\n7: until \u03b8\u2113+1 is satisfactory\n\n, u\u2032)\n\nls=1(cid:16)q\u2021\n\n\u2113+1,ls \u2212 [f(\u03b8)](xls, uls)(cid:17)2\n\noutput: b\u03b8\u2217 = \u03b8\u2113+1\n\nalthough we have assumed that the batch of samples is given in advance, fitted q-\niteration, together with other offline rl algorithms, can also be modified to use dif-\nferent batches of samples at different iterations. this property can be exploited, e.g.,\nto add new, more informative samples in-between iterations. ernst et al. (2006b) pro-\nposed a different, but related approach that integrates fitted q-iteration into a larger\niterative process. at every larger iteration, the entire fitted q-iteration algorithm is\nrun on the current batch of samples. then, the solution obtained by fitted q-iteration\nis used to generate new samples, e.g., by using an \u03b5-greedy policy in the obtained\nq-function. the entire cycle is then repeated.\n\nonline model-free approximate value iteration\n\nfrom the class of online algorithms for approximate value iteration, approximate\nversions of q-learning are the most popular (lin, 1992; singh et al., 1995; horiuchi\net al., 1996; jouffe, 1998; glorennec, 2000; tuyls et al., 2002; szepesv\u00b4ari and smart,\n2004; murphy, 2005; sherstov and stone, 2005; melo et al., 2008). recall from\nsection 2.3.2 that the original q-learning updates the q-function with (2.30):\n\nqk+1(xk, uk) = qk(xk, uk) + \u03b1k[rk+1 + \u03b3max\n\nu\u2032\n\nqk(xk+1, u\u2032)\u2212 qk(xk, uk)]\n\nafter observing the next state xk+1 and reward rk+1, as a result of taking action uk\nin state xk. a straightforward way to integrate approximation in q-learning is by\nusing gradient descent. we next explain how gradient-based q-learning is obtained,\nfollowing sutton and barto (1998, chapter 8). we require that the approximation\nmapping f is differentiable in the parameters.\n\nto simplify the formulas below, we denote the approximate q-function at time k\n\nby bqk(xk, uk) = [f(\u03b8k)](xk, uk), leaving the dependence on the parameter vector im-\n\nplicit. in order to derive gradient-based q-learning, assume for now that after taking\naction uk in state xk, the algorithm is provided with the true optimal q-value of the\ncurrent state action pair, q\u2217(xk, uk), in addition to the next state xk+1 and reward rk+1.\nunder these circumstances, the algorithm could aim to minimize the squared error\n\n "}, {"Page_number": 71, "text": "3.4. approximate value iteration\n\n61\n\nbetween this optimal value and the current q-value:\n\n\u03b8k+1 = \u03b8k \u2212\n\n2\n\n1\n\n\u2202\n\n\u03b1k\n\n\u2202\u03b8khq\u2217(xk, uk)\u2212bqk(xk, uk)i2\n= \u03b8k + \u03b1khq\u2217(xk, uk)\u2212bqk(xk, uk)i \u2202\n\n\u2202\u03b8k bqk(xk, uk)\n\nof course, q\u2217(xk, uk) is not available, but it can be replaced by an estimate derived\nfrom the q-iteration mapping (2.22) or (2.23):\n\nrk+1 + \u03b3max\n\nu\u2032 bqk(xk+1, u\u2032)\n\nnote the similarity with the q-function samples (3.19) used in fitted q-iteration. the\nsubstitution leads to the approximate q-learning update:\n\n\u03b8k+1 = \u03b8k + \u03b1k(cid:20)rk+1 + \u03b3max\n\nu\u2032 bqk(xk+1, u\u2032)\u2212bqk(xk, uk)(cid:21) \u2202\n\n\u2202\u03b8k bqk(xk, uk)\n\nwe have actually obtained, in the square brackets, an approximation of the tempo-\nral difference. with a linearly parameterized approximator (3.3), the update (3.20)\nsimplifies to:\n\n(3.20)\n\n\u03b8k+1 = \u03b8k + \u03b1k(cid:20)rk+1 + \u03b3max\n\nu\u2032 (cid:0)\u03c6t(xk+1, u\u2032)\u03b8k(cid:1)\u2212 \u03c6t(xk, uk)\u03b8k(cid:21)\u03c6(xk, uk)\n\n(3.21)\n\nnote that, like the original q-learning algorithm of section 2.3.2, approximate\nq-learning requires exploration. as an example, algorithm 3.3 presents gradient-\nbased q-learning with a linear parametrization and \u03b5-greedy exploration. for an ex-\nplanation and examples of the learning rate and exploration schedules used in this\nalgorithm, see section 2.3.2.\n\nalgorithm 3.3 q-learning with a linear parametrization and \u03b5-greedy exploration.\ninput: discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r,\nexploration schedule {\u03b5k}\u221e\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: measure initial state x0\n3: for every time step k = 0, 1, 2, . . . do\n\nk=0, learning rate schedule {\u03b1k}\u221e\n\nk=0\n\na uniform random action in u with probability \u03b5k (explore)\n\nuk \u2190(u \u2208 arg max \u00afu(cid:0)\u03c6t(xk, \u00afu)\u03b8k(cid:1)\n\u03b8k+1 \u2190 \u03b8k + \u03b1k(cid:2)rk+1 + \u03b3maxu\u2032(cid:0)\u03c6t(xk+1, u\u2032)\u03b8k(cid:1)\u2212 \u03c6t(xk, uk)\u03b8k(cid:3)\u03c6(xk, uk)\n\nwith probability 1\u2212 \u03b5k (exploit)\n\napply uk, measure next state xk+1 and reward rk+1\n\n4:\n\n5:\n\n6:\n7: end for\n\n "}, {"Page_number": 72, "text": "62\n\nchapter 3. dp and rl in large and continuous spaces\n\nin the literature, q-learning has been combined with a variety of approximators,\n\nfor example:\n\n\u2022 linearly parameterized approximators, including tile coding (watkins, 1989;\nsherstov and stone, 2005), as well as so-called interpolative representations\n(szepesv\u00b4ari and smart, 2004) and \u201csoft\u201d state aggregation (singh et al., 1995).\n\n\u2022 fuzzy rule-bases (horiuchi et al., 1996; jouffe, 1998; glorennec, 2000), which\n\ncan also be linear in the parameters.\n\n\u2022 neural networks (lin, 1992; touzet, 1997).\nwhile approximate q-learning is easy to use, it typically requires many transi-\ntion samples (i.e., many steps, k) before it can obtain a good approximation of the\noptimal q-function. one possible approach to alleviate this problem is to store tran-\nsition samples in a database and reuse them multiple times, similarly to how the\nbatch algorithms of the previous section work. this procedure is known as expe-\nrience replay (lin, 1992; kalyanakrishnan and stone, 2007). another option is to\nemploy so-called eligibility traces, which allow the parameter updates at the current\nstep to also incorporate information about recently observed transitions (e.g., singh\nand sutton, 1996). this mechanism makes use of the fact that the latest transition is\nthe causal result of an entire trajectory.\n\n3.4.3 value iteration with nonparametric approximation\n\nin this section, we first describe fitted q-iteration with nonparametric approximation.\nwe then point out some other algorithms that combine value iteration with nonpara-\nmetric approximators.\n\nthe fitted q-iteration algorithm was introduced in a parametric context in sec-\ntion 3.4.2, see algorithm 3.2. in the nonparametric case, fitted q-iteration can no\nlonger be described using approximation and projection mappings that remain un-\nchanged from one iteration to the next. instead, fitted q-iteration can be regarded\nas generating an entirely new, nonparametric approximator at every iteration. algo-\nrithm 3.4 outlines a general template for fitted q-iteration with nonparametric ap-\nproximation. the nonparametric regression at line 6 is responsible for generating\n\u2113+1,\n\na new approximator bq\u2113+1 that accurately represents the updated q-function q\u2021\n\nusing the information provided by the available samples q\u2021\n\nfitted q-iteration has been combined with several types of nonparametric ap-\nproximators, including kernel-based approximators (farahmand et al., 2009a) and\nensembles of regression trees (ernst et al., 2005, 2006b); see appendix a for a de-\nscription of such an ensemble.\n\n, ls = 1, . . . ns.\n\n\u2113+1,ls\n\nof course, other dp and rl algorithms besides fitted q-iteration can also be\ncombined with nonparametric approximation. for instance, deisenroth et al. (2009)\nemployed gaussian processes in approximate value iteration. they proposed two\nalgorithms: one that assumes that a model of the (deterministic) dynamics is known,\nand another that estimates a gaussian-process approximation of the dynamics from\n\n "}, {"Page_number": 73, "text": "3.4. approximate value iteration\n\n63\n\nalgorithm 3.4 fitted q-iteration with nonparametric approximation.\ninput: discount factor \u03b3,\nsamples {(xls, uls, x\u2032ls\n1: initialize q-function approximator, e.g., bq0 \u2190 0\n\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\n, rls)| ls = 1, . . . , ns}\n\nfor ls = 1, . . . , ns do\n\n4:\n\n5:\n\nq\u2021\nend for\n\n, u\u2032)\n\n\u2113+1,ls \u2190 rls + \u03b3maxu\u2032 bq\u2113(x\u2032ls\nnonparametric regression on {((xls, uls), q\u2021\n\n6:\n\nfind bq\u2113+1 using\n7: until bq\u2113+1 is satisfactory\noutput: bq\u2217 = bq\u2113+1\n\n\u2113+1,ls\n\n)| ls = 1, . . . , ns}\n\ntransition data. ormoneit and sen (2002) employed kernel-based approximation in\nmodel-free approximate value iteration for discrete-action problems.\n\n3.4.4 convergence and the role of nonexpansive approximation\n\nan important question in approximate dp/rl is whether the approximate solution\ncomputed by the algorithm converges, and, if it does converge, how far the conver-\ngence point is from the optimal solution. convergence is important because a conver-\ngent algorithm is more amenable to analysis and meaningful performance guarantees.\n\nconvergence of model-based approximate value iteration\n\nthe convergence proofs for approximate value iteration often rely on contraction\nmapping arguments. consider for instance approximate q-iteration (3.15). the q-\niteration mapping t is a contraction in the infinity norm with factor \u03b3 < 1, as already\nexplained in section 2.3.1. if the composite mapping p\u25e6 t \u25e6 f of approximate q-\niteration is also a contraction, i.e., if for any pair of parameter vectors \u03b8,\u03b8\u2032 and for\nsome \u03b3\u2032 < 1:\n\nk(p\u25e6 t \u25e6 f)(\u03b8)\u2212 (p\u25e6 t \u25e6 f)(\u03b8\u2032)k\u221e \u2264 \u03b3\u2032k\u03b8\u2212 \u03b8\u2032k\u221e\n\nthen approximate q-iteration asymptotically converges to a unique fixed point, which\nwe denote by \u03b8\u2217.\n\none way to ensure that p \u25e6 t \u25e6 f is a contraction is to require f and p to be\n\nnonexpansions, i.e.:\n\nkf(\u03b8)\u2212 f(\u03b8\u2032)k\u221e \u2264 k\u03b8\u2212 \u03b8\u2032k\u221e\nfor all pairs \u03b8,\u03b8\u2032\nkp(q)\u2212 p(q\u2032)k\u221e \u2264 kq\u2212 q\u2032k\u221e for all pairs q, q\u2032\n\nnote that in this case the contraction factor of p \u25e6 t \u25e6 f is the same as that of t :\n\u03b3\u2032 = \u03b3 < 1. under these conditions, as we will describe next, suboptimality bounds\n\n "}, {"Page_number": 74, "text": "bh\u2217(x) \u2208 arg max\n\nu\n\n64\n\nchapter 3. dp and rl in large and continuous spaces\n\ncan be derived on the approximate q-function f(\u03b8\u2217) and on any policybh\u2217 that is\n\ngreedy in this q-function, i.e., that satisfies:\n\n[f(\u03b8\u2217)](x, u)\n\n(3.22)\n\ndenote by ff\u25e6p \u2282 q the set of fixed points of the composite mapping f \u25e6 p,\nwhich is assumed nonempty. define the minimum distance between q\u2217 and any fixed\npoint of f \u25e6 p:4\n\n\u03c2\u2217qi = min\n\nq\u2032\u2208ff\u25e6pkq\u2217 \u2212 q\u2032k\u221e\n\nthis distance characterizes the representation power of the approximator: the better\n\nthe representation power, the closer the nearest fixed point of f \u25e6 p will be to q\u2217, and\nthe smaller \u03c2\u2217qi will be. using this distance, the convergence point \u03b8\u2217 of approximate\nq-iteration satisfies the following suboptimality bounds:\n\nkq\u2217 \u2212 f(\u03b8\u2217)k\u221e \u2264\nkq\u2217 \u2212 qbh\u2217k\u221e \u2264\n\n2\u03c2\u2217qi\n1\u2212 \u03b3\n4\u03b3\u03c2\u2217qi\n(1\u2212 \u03b3)2\n\n(3.23)\n\n(3.24)\n\nwhere qbh\u2217 is the q-function of the near-optimal policybh\u2217 (3.22). these bounds can\n\nbe derived similarly to those for approximate v-iteration found by gordon (1995);\ntsitsiklis and van roy (1996). equation (3.23) gives the suboptimality bound of the\napproximately optimal q-function, whereas (3.24) gives the suboptimality bound of\nthe resulting approximately optimal policy, and may be more relevant in practice. the\nfollowing relationship between the policy suboptimality and the q-function subopti-\nmality was used to obtain (3.24), and is also valid in general:\n\nkq\u2217 \u2212 qhk\u221e \u2264\n\n2\u03b3\n\n(1\u2212 \u03b3)kq\u2217 \u2212 qk\u221e\n\n(3.25)\n\nwhere the policy h is greedy in the (arbitrary) q-function q.\n\nideally, the optimal q-function q\u2217 is a fixed point of f \u25e6 p, in which case \u03c2\u2217qi = 0,\nand approximate q-iteration asymptotically converges to q\u2217. for instance, when q\u2217\nhappens to be exactly representable by f, a well-chosen tandem of approximation\n\nand projection mappings should ensure that q\u2217 is in fact a fixed point of f \u25e6 p.\nin practice, of course, \u03c2\u2217qi will rarely be 0, and only near-optimal solutions can be\nobtained.\n\nin order to take advantage of these theoretical guarantees, f and p should be\nnonexpansions. when f is linearly parameterized (3.3), it is fairly easy to ensure its\nnonexpansiveness by normalizing the bfs \u03c6l, so that for every x and u, we have:\n\nn\n\n\u2211\nl=1\n\n\u03c6l(x, u) = 1\n\n4for simplicity, we assume that the minimum in this equation exists. if the minimum does not exist,\nthen \u03c2\u2217qi should be taken as small as possible so that there still exists a q\u2032 \u2208 ff\u25e6p with kq\u2032 \u2212 q\u2217k\u221e \u2264 \u03c2\u2217qi.\n\n "}, {"Page_number": 75, "text": "3.4. approximate value iteration\n\n65\n\nensuring that p is nonexpansive is more difficult. for instance, the least-squares pro-\njection (3.14) can in general be an expansion, and examples of divergence when\nusing it have been given (tsitsiklis and van roy, 1996; wiering, 2004). one way to\nmake least-squares projection nonexpansive is to choose exactly ns = n state-action\nsamples (xl, ul), l = 1, . . . , n, and require that:\n\n\u03c6l(xl, ul) = 1, \u03c6l\u2032(xl, ul) = 0 \u2200l\u2032 6= l\n\nthese samples could be, e.g., the centers of the bfs. then, the projection (3.14)\nsimplifies to an assignment that associates each parameter with the q-value of the\ncorresponding sample:\n\n[p(q)]l = q(xl, ul)\n\n(3.26)\n\nwhere the notation [p(q)]l refers to the lth component in the parameter vector p(q).\nthis mapping is clearly nonexpansive. more general, but still restrictive conditions\non the bfs under which convergence and near optimality are guaranteed are given in\n(tsitsiklis and van roy, 1996).\n\nconvergence of model-free approximate value iteration\n\nlike in the model-based case, convergence guarantees for offline, batch model-free\nvalue iteration typically rely on nonexpansive approximation. in fitted q-iteration\nwith parametric approximation (algorithm 3.2), care must be taken when selecting\nf and p, to prevent possible expansion and divergence. similarly, in fitted q-iteration\nwith nonparametric approximation (algorithm 3.4), the nonparametric regression\nalgorithm should have nonexpansive properties. certain types of kernel-based ap-\nproximators satisfy this condition (ernst et al., 2005). the convergence of the kernel-\nbased v-iteration algorithm of ormoneit and sen (2002) is also guaranteed under\nnonexpansiveness assumptions.\n\nmore recently, a different class of theoretical results for batch value iteration\nhave been developed, which do not rely on nonexpansion properties and do not con-\ncern the asymptotic case. instead, these results provide probabilistic bounds on the\nsuboptimality of the policy obtained by using a finite number of samples, after a\nfinite number of iterations. besides the number of samples and iterations, such finite-\nsample bounds typically depend on the representation power of the approximator\nand on certain properties of the mdp. for instance, munos and szepesv\u00b4ari (2008)\nprovided finite-sample bounds for approximate v-iteration in discrete-action mdps,\nwhile farahmand et al. (2009a) focused on fitted q-iteration in the same type of\nmdps. antos et al. (2008a) gave finite-sample bounds for fitted q-iteration in the\nmore difficult case of continuous-action mdps.\n\nin the area of online approximate value iteration, as already discussed in sec-\ntion 3.4.2, the main representative is approximate q-learning. many variants of ap-\nproximate q-learning are heuristic and do not guarantee convergence (horiuchi et al.,\n1996; touzet, 1997; jouffe, 1998; glorennec, 2000; mill\u00b4an et al., 2002). conver-\ngence of approximate q-learning has been proven for linearly parameterized ap-\nproximators, under the requirement that the policy followed by q-learning remains\nunchanged during the learning process (singh et al., 1995; szepesv\u00b4ari and smart,\n\n "}, {"Page_number": 76, "text": "66\n\nchapter 3. dp and rl in large and continuous spaces\n\n2004; melo et al., 2008). this requirement is restrictive, because it does not allow the\ncontroller to improve its performance, even if it has gathered knowledge that would\nenable it to do so. among these results, singh et al. (1995) and szepesv\u00b4ari and smart\n(2004) proved the convergence of approximate q-learning with nonexpansive, lin-\nearly parameterized approximation. melo et al. (2008) showed that gradient-based\nq-learning (3.21) converges without requiring nonexpansive approximation, but at\nthe cost of other restrictive assumptions.\n\nconsistency of approximate value iteration\n\nbesides convergence, another important theoretical property of algorithms for ap-\nproximate dp and rl is consistency. in model-based value iteration, and more gen-\nerally in dp, an algorithm is said to be consistent if the approximate value function\nconverges to the optimal one as the approximation accuracy increases (e.g., gonza-\nlez and rofman, 1985; chow and tsitsiklis, 1991; santos and vigo-aguiar, 1998).\nin model-free value iteration, and more generally in rl, consistency is sometimes\nunderstood as the convergence to a well-defined solution as the number of samples\nincreases. the stronger result of convergence to an optimal solution as the approxi-\nmation accuracy also increases was proven in (ormoneit and sen, 2002; szepesv\u00b4ari\nand smart, 2004).\n\n3.4.5 example: approximate q-iteration for a dc motor\n\nin closing the discussion on approximate value iteration, we provide a numerical\nexample involving a dc motor control problem. this example shows how approxi-\nmate value iteration algorithms can be used in practice. the first part of the example\nconcerns a basic version of approximate q-iteration that relies on a gridding of the\nstate space and on a discretization of the action space, while the second part employs\nthe state-of-the-art, fitted q-iteration algorithm with nonparametric approximation\n(algorithm 3.4).\n\nconsider a second-order discrete-time model of an electrical dc (direct current)\n\nmotor:\n\nxk+1 = f (xk, uk) = axk + buk\n\na =(cid:20)1\n\n0\n\n0.0049\n\n0.8505(cid:21)\n0.9540(cid:21) , b =(cid:20)0.0021\n\n(3.27)\n\nthis model was obtained by discretizing a continuous-time model of the dc mo-\ntor, which was developed by first-principles modeling (e.g., khalil, 2002, chapter\n1) of a real dc motor. the discretization was performed with the zero-order-hold\nmethod (franklin et al., 1998), using a sampling time of ts = 0.005 s. using satura-\ntion, the shaft angle x1,k = \u03b1 is bounded to [\u2212\u03c0,\u03c0] rad, the angular velocity x2,k = \u02d9\u03b1\nto [\u221216\u03c0, 16\u03c0] rad/s, and the control input uk to [\u221210, 10] v.\n\nthe control goal is to stabilize the dc motor in the zero equilibrium (x = 0). the\n\n "}, {"Page_number": 77, "text": "3.4. approximate value iteration\n\n67\n\nfollowing quadratic reward function is chosen to express this goal:\n\nrk+1 = \u03c1(xk, uk) = \u2212xt\nqrew =(cid:20)5\n\n0.01(cid:21) , rrew = 0.01\n\nk qrewxk \u2212 rrewu2\n\n0\n\n0\n\nk\n\n(3.28)\n\nthis reward function leads to a discounted quadratic regulation problem. a\n(near-)optimal policy will drive the state (close) to 0, while also minimizing the mag-\nnitude of the states along the trajectory and the control effort. the discount factor was\nchosen to be \u03b3 = 0.95, which is sufficiently large to lead to an optimal policy that\nproduces a good stabilizing control behavior.5\n\nfigure 3.5 presents a near-optimal solution to this problem, including a represen-\ntative state-dependent slice through the q-function (obtained by setting the action\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) slice through a near-optimal q-function, for u =\n0.\n\n(b) a near-optimal policy.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(c) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nfigure 3.5 a near-optimal solution for the dc motor.\n\n5note that a distinction is made between the optimality under the chosen reward function and discount\n\nfactor, and the actual (albeit subjective) quality of the control behavior.\n\n "}, {"Page_number": 78, "text": "68\n\nchapter 3. dp and rl in large and continuous spaces\n\nargument u to 0), a greedy policy in this q-function, and a representative trajectory\nthat is controlled by this policy. to find the near-optimal solution, the convergent and\nconsistent fuzzy q-iteration algorithm (which will be discussed in detail in chap-\nter 4) was applied. an accurate approximator over the state space was used, together\nwith a fine discretization of the action space, which contains 31 equidistant actions.\n\ngrid q-iteration\n\nas an example of approximate value iteration, we apply a q-iteration algorithm that\nrelies on state aggregation and action discretization, a type of approximator intro-\nduced in example 3.1. the state space is partitioned into n disjoint rectangles. de-\nnote by xi the ith rectangle in the state space partition. for this problem, the following\nthree discrete actions suffice to produce an acceptable stabilizing control behavior:\nu1 = \u221210, u2 = 0, u3 = 10 (i.e., applying maximum torque in either direction, and no\ntorque at all). so, the discrete action space is ud = {\u221210, 0, 10}. recall from exam-\nple 3.1 that the state-action bfs are given by (3.8), repeated here for easy reference:\n\n\u03c6[i, j](x, u) =(1\n\n0\n\nif x \u2208 xi and u = u j\notherwise\n\n(3.29)\n\nwhere [i, j] = i + ( j \u2212 1)n. to derive the projection mapping p, the least-squares\nprojection (3.14) is used, taking the cross-product of the sets {x1, . . . , xn} and ud\nas state-action samples, where xi denotes the center of the ith rectangle xi. these\nsamples satisfy the conditions to simplify p to an assignment of the form (3.26),\nnamely:\n\n[p(q)][i, j] = q(xi, u j)\n\n(3.30)\n\nusing a linearly parameterized approximator with the bfs (3.29) and the projection\n(3.30) yields the grid q-iteration algorithm. because f and p are nonexpansions, the\nalgorithm is convergent.\n\nto apply grid q-iteration to the dc motor problem, two different grids over the\nstate space are used: a coarse grid, with 20 equidistant bins on each axis (leading to\n202 = 400 rectangles); and a fine grid, with 400 equidistant bins on each axis (leading\nto 4002 = 160 000 rectangles). the algorithm is considered to have converged when\nthe maximum amount by which any parameter changes between two consecutive\niterations does not exceed \u03b5qi = 0.001. for the coarse grid, convergence occurred\nafter 160 iterations, and for the fine grid, after 123. this shows that the number of\niterations required for convergence does not necessarily increase with the number of\nparameters.\n\nfigure 3.6 shows slices through the resulting q-functions, together with corre-\nsponding policies and representative controlled trajectories. the accuracy in repre-\nsenting the q-function and policy is better for the fine grid (figures 3.6(b) and 3.6(d))\nthan for the coarse grid (figures 3.6(a) and 3.6(c)). axis-oriented policy artifacts ap-\npear for both grid sizes, due to the limitations of the chosen type of approximator.\nfor instance, the piecewise-constant nature of the approximator is clearly visible in\nfigure 3.6(a). compared to the near-optimal trajectory of figure 3.5(c), the grid q-\niteration trajectories in figures 3.6(e) and 3.6(f) do not reach the goal state x = 0 with\n\n "}, {"Page_number": 79, "text": "3.4. approximate value iteration\n\n69\n\nthe same accuracy. with the coarse-grid policy, there is a large steady-state error of\nthe angle \u03b1, while the fine-grid policy leads to chattering of the control action.\n\nthe execution time of grid q-iteration was 0.06 s for the coarse grid, and 7.80 s\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) slice through coarse-grid q-function, for u = 0.\n\n(b) slice through fine-grid q-function, for u = 0.\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(c) coarse-grid policy.\n\n(d) fine-grid policy.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.4\nt [s]\n\n(e) trajectory from x0 = [\u2212\u03c0, 0]t, controlled by\n\nthe coarse-grid policy.\n\n(f) trajectory from x0 = [\u2212\u03c0, 0]t, controlled by\n\nthe fine-grid policy.\n\nfigure 3.6\ngrid q-iteration solutions for the dc motor. the results obtained with the coarse grid are\nshown on the left-hand side of the figure, and those obtained with the fine grid on the right-\nhand side.\n\n "}, {"Page_number": 80, "text": "70\n\nchapter 3. dp and rl in large and continuous spaces\n\nfor the fine grid.6 the fine grid is significantly more computationally expensive to\nuse, because it has a much larger number of parameters to update (480 000, versus\n1200 for the coarse grid).\n\nfitted q-iteration\n\nnext, we apply fitted q-iteration (algorithm 3.4) to the dc motor problem, using\nensembles of extremely randomized trees (geurts et al., 2006) to approximate the q-\nfunction. for a description of this approximator, see appendix a. the same discrete\nactions are employed as for grid q-iteration: ud = {\u221210, 0, 10}. a distinct ensemble\nof regression trees is used to approximate the q-function for each of these discrete\nactions \u2013 in analogy to the discrete-action grid approximator. the construction of the\ntree ensembles is driven by three meta-parameters:\n\n\u2022 each ensemble contains ntr trees. we set this parameter equal to 50.\n\n\u2022 to split a node, ktr randomly chosen cut directions are evaluated, and the one\nthat maximizes a certain score is selected. we set ktr equal to the dimension-\nality 2 of the input to the regression trees (the 2-dimensional state variable),\nwhich is its recommended default value (geurts et al., 2006).\n\n\u2022 a node is only split further when it is associated with at least nmin\n\nsamples.\nto its default value of 2, which\n\notherwise, it remains a leaf node. we set nmin\nmeans that the trees are fully developed.\n\ntr\n\ntr\n\nfitted q-iteration is supplied with a set of samples consisting of the cross-product be-\ntween a regular grid of 100\u00d7 100 points in the state space, and the 3 discrete actions.\nthis ensures the meaningfulness of the comparison with grid q-iteration, which em-\nployed similarly placed samples. fitted q-iteration is run for a predefined number\nof 100 iterations, and the q-function found after the 100th iteration is considered\nsatisfactory.\n\nfigure 3.7 shows the solution obtained. this is similar in quality to the solution\nobtained by grid q-iteration with the fine grid, and better than the solution obtained\nwith the coarse grid (figure 3.6).\n\nthe execution time of fitted q-iteration was approximately 2151 s, several orders\nof magnitude larger than the execution time of grid q-iteration (recall that the latter\nwas 0.06 s for the coarse grid, and 7.80 s for the fine grid). clearly, finding a more\npowerful nonparametric approximator is much more computationally intensive than\nupdating the parameters of the simple, grid-based approximator.\n\n6all the execution times reported in this chapter were recorded while running the algorithms in\nmatlab r(cid:13) 7 on a pc with an intel core 2 duo t9550 2.66 ghz cpu and with 3 gb ram. for value\niteration and policy iteration, the reported execution times do not include the time required to simulate the\nsystem for every state-action sample in order to obtain the next state and reward.\n\n "}, {"Page_number": 81, "text": "3.5. approximate policy iteration\n\n71\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n50\n\n0\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n\u221250\n\n \n\n\u22122\n\n(a) slice through q-function for u = 0.\n\n0\n\n\u03b1 [rad]\n\n2\n\n(b) policy.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(c) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nfigure 3.7 fitted q-iteration solution for the dc motor.\n\n3.5 approximate policy iteration\n\npolicy iteration algorithms evaluate policies by constructing their value functions,\nand use these value functions to find new, improved policies. they were introduced\nin section 2.4. in large or continuous spaces, policy evaluation cannot be solved ex-\nactly, and the value function has to be approximated. approximate policy evaluation\nis a difficult problem, because, like approximate value iteration, it involves finding\nan approximate solution to a bellman equation. special requirements must be im-\nposed to ensure that a meaningful approximate solution exists and can be found by\nappropriate algorithms. policy improvement relies on solving maximization prob-\nlems over the action variables, which involve fewer technical difficulties (although\nthey may still be hard to solve when the action space is large). often, an explicit rep-\nresentation of the policy can be avoided, by computing improved actions on demand\nfrom the current value function. alternatively, the policy can be represented explic-\n\n "}, {"Page_number": 82, "text": "72\n\nchapter 3. dp and rl in large and continuous spaces\n\nitly, in which case policy approximation is generally required. in this case, solving a\nclassical supervised learning problem is necessary to perform policy improvement.\n\nalgorithm 3.5 outlines a general template for approximate policy iteration with\nq-function policy evaluation. note that at line 4, when there are multiple maximiz-\ning actions, the expression \u201c\u2248 arg maxu . . . \u201d should be interpreted as \u201capproximately\nequal to one of the maximizing actions.\u201d\n\nalgorithm 3.5 approximate policy iteration with q-functions.\n\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n\nfind bqbh\u2113 , an approximate q-function ofbh\u2113\nfindbh\u2113+1 so thatbh\u2113+1(x) \u2248 arg maxubqbh\u2113(x, u),\u2200x \u2208 x \u22b2 policy improvement\n\n\u22b2 policy evaluation\n\n4:\n\n3:\n\n1: initialize policybh0\n5: untilbh\u2113+1 is satisfactory\noutput: bh\u2217 =bh\u2113+1\n\nfigure 3.8 (repeated from the relevant part of figure 3.2) illustrates the struc-\nture of our upcoming presentation. we first discuss in detail the approximate policy\nevaluation component, starting in section 3.5.1 with a class of algorithms that can\nbe derived along the same lines as approximate value iteration. in section 3.5.2,\nmodel-free policy evaluation algorithms with linearly parameterized approximation\nare introduced, which aim to solve a projected form of the bellman equation. sec-\ntion 3.5.3 briefly reviews policy evaluation with nonparametric approximation, and\nsection 3.5.4 outlines a model-based, direct simulation approach for policy evalu-\nation. in section 3.5.5, we move on to the policy improvement component and the\nresulting approximate policy iteration. theoretical results about approximate policy\niteration are reviewed in section 3.5.6, and a numerical example is provided in sec-\ntion 3.5.7 (the material of these last two sections is not represented in figure 3.8).\n\napproximate\npolicy iteration\n\napproximate\npolicy evaluation\n\npolicy improvement\n\nvalue iteration-like algorithms for\napproximate policy evaluation\n\nmodel-free policy evaluation with\nlinearly parameterized approximation\n\npolicy evaluation with\nnonparametric approximation\n\nmodel-based approximate policy evaluation with\nrollouts\n\nfigure 3.8\nthe organization of the algorithms for approximate policy evaluation and policy improvement\npresented in the sequel.\n\n "}, {"Page_number": 83, "text": "3.5. approximate policy iteration\n\n73\n\n3.5.1 value iteration-like algorithms for approximate policy evaluation\n\nwe start our discussion of approximate policy evaluation with a class of algorithms\nthat can be derived along entirely similar lines to approximate value iteration. these\nalgorithms can be model-based or model-free, and can use parametric or nonparamet-\nric approximation. we focus here on the parametric case, and discuss two represen-\ntative algorithms, one model-based and the other model-free. these two algorithms\nare similar to approximate q-iteration (section 3.4.1) and to fitted q-iteration (sec-\ntion 3.4.2), respectively. in order to streamline the presentation, we will often refer\nto these counterparts and to their derivation.\n\nthe first algorithm that we develop is based on the model-based, iterative policy\nevaluation for q-functions (section 2.3.1). denote the policy to be evaluated by h.\nrecall that policy evaluation for q-functions starts from an arbitrary q-function qh\n0,\nwhich is updated at each iteration \u03c4 using (2.38), repeated here for easy reference:\n\nqh\n\n\u03c4+1 = t h(qh\n\u03c4)\n\nwhere t h is the policy evaluation mapping, given by (2.35) in the deterministic case\nand by (2.36) in the stochastic case. the algorithm asymptotically converges to the\nq-function qh of the policy h, which is the solution of the bellman equation (2.39),\nalso repeated here:\n\nqh = t h(qh)\n\n(3.31)\n\npolicy evaluation for q-functions can be extended to the approximate case in\na similar way as approximate q-iteration (see section 3.4.1). as with approximate\n\nq-iteration, an approximation mapping f : rn \u2192 q is used to compactly represent\nq-functions using parameter vectors \u03b8h \u2208 rn, and a projection mapping p : q \u2192 rn\n\nis used to find parameter vectors that represent the updated q-functions well.\n\nthe iterative, approximate policy evaluation for q-functions starts with an ar-\n0 , and updates this vector at every\n\nbitrary (e.g., identically 0) parameter vector \u03b8h\niteration \u03c4 using the composition of mappings p, t h, and f:\n\n\u03c4+1 = (p\u25e6 t h \u25e6 f)(\u03b8h\n\u03b8h\n\u03c4 )\n\n(3.32)\n\nunder conditions similar to those for value iteration (section 3.4.4), the composite\n\nthe algorithm is stopped once a satisfactory parameter vector b\u03b8h has been found.\nmapping p \u25e6 t h \u25e6 f is a contraction, and therefore has a fixed point \u03b8h to which\n\nthe update (3.32) asymptotically converges. this is true, e.g., if both f and p are\nnonexpansions.\n\nas an example, algorithm 3.6 shows approximate policy evaluation for q-\nfunctions in the case of deterministic mdps, using the least-squares projection\n(3.14). in this algorithm, qh,\u2021\n\n\u03c4+1 denotes the intermediate, updated q-function:\n\nqh,\u2021\n\u03c4+1 = (t h \u25e6 f)(\u03b8h\n\u03c4 )\nbecause deterministic mdps are considered, qh,\u2021\n\u03c4+1(xls, uls) is computed at line 4 us-\ning the policy evaluation mapping (2.35). note the similarity of algorithm 3.6 with\nthe approximate q-iteration for mdps (algorithm 3.1).\n\n "}, {"Page_number": 84, "text": "74\n\nchapter 3. dp and rl in large and continuous spaces\n\nalgorithm 3.6 approximate policy evaluation for q-functions in deterministic mdps.\ninput: policy h to be evaluated, dynamics f , reward function \u03c1, discount factor \u03b3,\n\napproximation mapping f, samples {(xls, uls) | ls = 1, . . . , ns}\n\n1: initialize parameter vector, e.g., \u03b8h\n0 \u2190 0\n2: repeat at every iteration \u03c4 = 0, 1, 2, . . .\n3:\n\nfor ls = 1, . . . , ns do\n\n4:\n\n5:\n\n6:\n\nqh,\u2021\n\u03c4+1(xls, uls) \u2190 \u03c1(xls, uls) + \u03b3[f(\u03b8h\nend for\n\u03c4+1 \u2190 \u03b8h,\u2021, where \u03b8h,\u2021 \u2208 arg min\u03b8 \u2211ns\n\u03b8h\n\u03c4+1 is satisfactory\n\n7: until \u03b8h\n\n\u03c4 )]( f (xls, uls), h( f (xls, uls)))\n\nls=1(cid:16)qh,\u2021\n\n\u03c4+1(xls, uls)\u2212 [f(\u03b8)](xls, uls)(cid:17)2\n\noutput: b\u03b8h = \u03b8h\n\n\u03c4+1\n\nthe second algorithm that we develop is an analogue of fitted q-iteration, so it\nwill be called fittedpolicyevaluationforq-functions. it can also be seen as a model-\nfree variant of the approximate policy evaluation for q-functions developed above.\nin this variant, a batch of transition samples is assumed to be available:\n\n{(xls, uls, x\u2032ls, rls)| ls = 1, . . . , ns}\n\nwhere for every ls, the next state x\u2032ls\nand the reward rls have been obtained after taking\naction uls in the state xls . at every iteration, samples of the updated q-function qh,\u2021\n\u03c4+1\nare computed with:\n\nqh,\u2021\n\n\u03c4+1,ls\n\n= rls + \u03b3[f(\u03b8\u03c4)](x\u2032ls, h(x\u2032ls))\n\nin the deterministic case, the quantity qh,\u2021\n\u03c4+1(xls, uls) (see, e.g., line 4 of algorithm 3.6). in the stochastic case, qh,\u2021\nqh,\u2021\nis\na sample of the random variable that has the updated q-value as its expectation. a\ncomplete iteration of the algorithm is obtained by computing an updated parameter\nvector with a projection mapping, using the samples ((xls, uls), qh,\u2021\n\nis identical to the updated q-value\n\n\u03c4+1,ls\n\n\u03c4+1,ls\n\n).\n\n\u03c4+1,ls\n\nalgorithm 3.7 presents fitted policy evaluation for q-functions, using the least-\nsquares projection (3.14). note that, in the deterministic case, fitted policy evaluation\nis identical to model-based, approximate policy evaluation (e.g., algorithm 3.6), if\nboth algorithms use the same approximation and projection mappings, together with\nthe same state-action samples (xls, uls).\n\n3.5.2 model-free policy evaluation with linearly parameterized\n\napproximation\n\na different, dedicated framework for approximate policy evaluation can be devel-\noped when linearly parameterized approximators are employed. by exploiting the\nlinearity of the approximator in combination with the linearity of the policy evalua-\ntion mapping (see section 2.4.1), it is possible to derive a specific approximate form\n\n "}, {"Page_number": 85, "text": "3.5. approximate policy iteration\n\n75\n\nalgorithm 3.7 fitted policy evaluation for q-functions.\ninput: policy h to be evaluated, discount factor \u03b3,\napproximation mapping f, samples {(xls, uls, x\u2032ls\n\n1: initialize parameter vector, e.g., \u03b8h\n0 \u2190 0\n2: repeat at every iteration \u03c4 = 0, 1, 2, . . .\n3:\n\nfor ls = 1, . . . , ns do\n\n4:\n\n5:\n\n6:\n\n\u03c4 )](x\u2032ls\n\nqh,\u2021\n\u03c4+1,ls \u2190 rls + \u03b3[f(\u03b8h\nend for\n\u03c4+1 \u2190 \u03b8h,\u2021, where \u03b8h,\u2021 \u2208 arg min\u03b8 \u2211ns\n\u03b8h\n\u03c4+1 is satisfactory\n\n, h(x\u2032ls\n\n))\n\n7: until \u03b8h\n\n, rls)| ls = 1, . . . , ns}\n\nls=1(cid:16)qh,\u2021\n\n\u03c4+1,ls \u2212 [f(\u03b8)](xls, uls)(cid:17)2\n\noutput: b\u03b8h = \u03b8h\n\n\u03c4+1\n\nof the bellman equation, called the \u201cprojected bellman equation,\u201d which is linear in\nthe parameter vector.7 efficient algorithms can be developed to solve this equation. in\ncontrast, in approximate value iteration, the maximum operator leads to nonlinearity\neven when the approximator is linearly parameterized.\n\nwe next introduce the projected bellman equation, along with several important\n\nmodel-free algorithms that can be used to solve it.\n\nprojected bellman equation\n\nassume for now that x and u have a finite number of elements, x = {x1, . . . , x \u00afn},\nu = {u1, . . . , u \u00afm}. because the state space is finite, a transition model of the form\n(2.14) is appropriate, and the policy evaluation mapping t h can be written as a sum\n(2.37), repeated here for easy reference:\n\n[t h(q)](x, u) = \u2211\nx\u2032\n\n\u00aff (x, u, x\u2032)(cid:2) \u02dc\u03c1(x, u, x\u2032) + \u03b3q(x\u2032, h(x\u2032))(cid:3)\n\n(3.33)\n\nin the linearly parameterized case, an approximate q-function bqh that has the form\n\n(3.3) is sought:\n\nwhere \u03c6(x, u) = [\u03c61(x, u), . . . ,\u03c6n(x, u)]t is the vector of bfs and \u03b8h is the parameter\nvector. this approximate q-function satisfies the following approximate version of\n\nbqh(x, u) = \u03c6t(x, u)\u03b8h\n\n7another important class of policy evaluation approaches aims to minimize the bellman error (resid-\nual), which is the difference between the two sides of the bellman equation (baird, 1995; antos et al.,\n2008b; farahmand et al., 2009b). for instance, in the case of the bellman equation for qh (3.31), the\n\n(quadratic) bellman error is rx\u00d7u (bqh(x, u) \u2212 [t h(bqh)](x, u))2d(x, u). we choose to focus on projected\n\npolicy evaluation instead, as this class of methods will be required later in the book.\n\n "}, {"Page_number": 86, "text": "76\n\nchapter 3. dp and rl in large and continuous spaces\n\nthe bellman equation for qh (3.31), called the projected bellman equation:8\n\nwhere pw performs a weighted least-squares projection onto the space of repre-\nsentable (approximate) q-functions, i.e., the space spanned by the bfs:\n\n(3.34)\n\nbqh = (pw \u25e6 t h)(bqh)\n(cid:8)\u03c6t(x, u)\u03b8 |\u03b8 \u2208 rn(cid:9)\n\nthe projection pw is defined by:\n\n[pw(q)](x, u) = \u03c6t(x, u)\u03b8\u2021, where\n\u03b8\u2021 \u2208 arg min\n\n\u2211\n\n\u03b8\n\n(x,u)\u2208x\u00d7u\n\nw(x, u)(cid:0)\u03c6t(x, u)\u03b8\u2212 q(x, u)(cid:1)2\n\n(3.35)\n\nin which the weight function w : x \u00d7 u \u2192 [0, 1] controls the distribution of the ap-\nproximation error. the weight function is always interpreted as a probability distri-\nbution over the state-action space, so it must satisfy \u2211x,u w(x, u) = 1. for instance,\nthe distribution given by w will later be used to generate the samples used by some\nmodel-free policy evaluation algorithms. under appropriate conditions, the projected\n\nbellman mapping pw\u25e6 t h is a contraction, and so the solution (fixed point) bqh of the\n\nprojected bellman equation exists and is unique (see bertsekas (2007, section 6.3)\nfor a discussion of the conditions in the context of v-function approximation).\n\nfigure 3.9 illustrates the projected bellman equation.\n\nmatrix form of the projected bellman equation\n\nwe will now derive a matrix form of the projected bellman equation, which is given\nin terms of the parameter vector. this form will be useful in the sequel, when devel-\noping algorithms to solve the projected bellman equation. to introduce the matrix\nform, it will be convenient to refer to the state and the actions using explicit indices,\ne.g., xi, u j (recall that the states and actions were temporarily assumed to be discrete).\nas a first step, the policy evaluation mapping (3.33) is written in matrix form\n\nttt h : r \u00afn \u00afm \u2192 r \u00afn \u00afm, as:\n\nttt h(qqq) = \u02dc\u03c1\u03c1\u03c1+ \u03b3 \u00affff hhhqqq\n\n(3.36)\n\ndenote by [i, j] the scalar index corresponding to i and j, computed with [i, j] =\ni + ( j\u2212 1) \u00afn. the vectors and matrices in (3.36) are then defined as follows:9\n\n8a multistep version of this equation can also be given. instead of the (single-step) policy evaluation\n\nmapping t h, this version uses the following multistep mapping, parameterized by the scalar \u03bb \u2208 [0, 1):\n\nt h\n\u03bb (q) = (1\u2212 \u03bb)\n\n\u221e\n\u2211\nk=0\n\n\u03bbk(t h)k+1(q)\n\nwhere (t h)k denotes the k-times composition of t h with itself, i.e., t h \u25e6 t h \u25e6\u00b7\u00b7\u00b7\u25e6 t h. in this chapter, as\nwell as in the remainder of the book, we only consider the single-step case, i.e., the case in which \u03bb = 0.\n9note that boldface notation is used for vector or matrix representations of functions and mappings.\n\nordinary vectors and matrices are displayed in normal font.\n\n "}, {"Page_number": 87, "text": "3.5. approximate policy iteration\n\n77\n\nspace of all q-functions\n\n^\nt qh(\n)\n\nt h\n\npw\n\n^\nh\nt qh\n\n(\n\n)\n\nt h\n\npw\n\n^\nq\n\nw\n\n^\np t q\n)\n\n(\n\n(\n\nh\n\n)\n\nqh^\n\nspace of approximate q-functions\n\nfigure 3.9\na conceptual illustration of the projected bellman equation. applying t h and then pw to an\n\nbellman equation leads back to the same point (right).\n\nordinary approximate q-function bq leads to a different point in the space of approximate q-\nfunctions (left). in contrast, applying t h and then pw to the fixed point bqh of the projected\n\u2022 qqq \u2208 r \u00afn \u00afm is a vector representation of q, with qqq[i, j] = q(xi, u j).\n\u2022 \u02dc\u03c1\u03c1\u03c1 \u2208 r \u00afn \u00afm is a vector representation of\n\n\u02dc\u03c1, where the element\ni.e.,\n\ntaking action u j\n\nin state xi,\n\n\u02dc\u03c1\u03c1\u03c1[i, j]\nis\n\u02dc\u03c1\u03c1\u03c1[i, j] =\n\nthe expected reward after\n\u00aff (xi, u j, xi\u2032) \u02dc\u03c1(xi, u j, xi\u2032).\n\u2211i\u2032\n\n\u2022 \u00affff \u2208 r \u00afn \u00afm\u00d7 \u00afn is a matrix representation of \u00aff , with \u00affff [i, j],i\u2032 = \u00aff (xi, u j, xi\u2032). here,\n\n\u00affff [i, j],i\u2032 denotes the element at row [i, j] and column i\u2032 of matrix \u00affff .\n\n\u2022 hhh \u2208 r \u00afn\u00d7 \u00afn \u00afm is a matrix representation of h, with hhhi\u2032,[i, j] = 1 if i\u2032 = i and h(xi) =\nu j, and 0 otherwise. note that stochastic policies can easily be represented, by\nmaking hhhi,[i, j] equal to the probability of taking u j in xi, and hhhi\u2032,[i, j] = 0 for all\ni\u2032 6= i.\n\nconsider now the setting of approximate policy evaluation. define the bf matrix\n\n\u03c6\u03c6\u03c6 \u2208 r \u00afn \u00afm\u00d7n and the diagonal weighting matrix www \u2208 r \u00afn \u00afm\u00d7 \u00afn \u00afm by:\n\n\u03c6\u03c6\u03c6[i, j],l = \u03c6l(xi, u j)\nwww[i, j],[i, j] = w(xi, u j)\n\nusing \u03c6\u03c6\u03c6, the approximate q-vector corresponding to a parameter \u03b8 is:\n\nthe projected bellman equation (3.34) can now be written as follows:\n\nwhere pppw is a matrix representation of the projection operator pw, which can be\nwritten in a closed form (see, e.g., lagoudakis and parr, 2003a):\n\n(3.37)\n\nbqqq = \u03c6\u03c6\u03c6\u03b8\n) =bqqq\npppwttt h(bqqq\n\nh\n\nh\n\npppw = \u03c6\u03c6\u03c6(\u03c6\u03c6\u03c6twww\u03c6\u03c6\u03c6)\u22121\u03c6\u03c6\u03c6twww\n\n "}, {"Page_number": 88, "text": "78\n\nchapter 3. dp and rl in large and continuous spaces\n\nby substituting this closed-form expression for pppw, the formula (3.36) for ttt h, and\n\nh\n\nthe expressionbqqq\n\n= \u03c6\u03c6\u03c6\u03b8h for the approximate q-vector into (3.37), we get:\n\n\u03c6\u03c6\u03c6(\u03c6\u03c6\u03c6twww\u03c6\u03c6\u03c6)\u22121\u03c6\u03c6\u03c6twww( \u02dc\u03c1\u03c1\u03c1+ \u03b3 \u00affff hhh\u03c6\u03c6\u03c6\u03b8h) = \u03c6\u03c6\u03c6\u03b8h\n\nnotice that this is a linear equation in the parameter vector \u03b8h. after a left-\nmultiplication with \u03c6\u03c6\u03c6twww and a rearrangement of the terms, we have:\n\n\u03c6\u03c6\u03c6twww\u03c6\u03c6\u03c6\u03b8h = \u03b3\u03c6\u03c6\u03c6twww \u00affff hhh\u03c6\u03c6\u03c6\u03b8h + \u03c6\u03c6\u03c6twww \u02dc\u03c1\u03c1\u03c1\n\nby introducing the matrices \u03b3, \u03bb \u2208 rn\u00d7n and the vector z \u2208 rn, given by:\n\n\u03b3 = \u03c6\u03c6\u03c6twww\u03c6\u03c6\u03c6, \u03bb = \u03c6\u03c6\u03c6twww \u00affff hhh\u03c6\u03c6\u03c6,\n\nz = \u03c6\u03c6\u03c6twww \u02dc\u03c1\u03c1\u03c1\n\nthe projected bellman equation can be written in the final, matrix form:\n\n\u03b3\u03b8h = \u03b3\u03bb\u03b8h + z\n\n(3.38)\n\nso, instead of the original, high-dimensional bellman equation (3.31), approximate\npolicy evaluation only needs to solve the low-dimensional system (3.38). a solution\n\u03b8h of this system can be employed to find an approximate q-function using (3.3).\n\nit can also be shown that matrices \u03b3, \u03bb and vector z can be written as sums of\n\nsimpler matrices and vectors (e.g., lagoudakis and parr, 2003a):\n\n\u03b3 =\n\n\u03bb =\n\nz =\n\n\u00afn\n\u2211\ni=1\n\n\u00afn\n\u2211\ni=1\n\n\u00afn\n\u2211\ni=1\n\n\u00afm\n\u2211\n\n\u00afm\n\u2211\n\nj=1h\u03c6(xi, u j)w(xi, u j)\u03c6t(xi, u j)i\nj=1\"\u03c6(xi, u j)w(xi, u j)\nj=1h\u03c6(xi, u j)w(xi, u j)\n\ni\u2032=1(cid:16) \u00aff (xi, u j, xi\u2032)\u03c6t(xi\u2032 , h(xi\u2032))(cid:17)#\ni\u2032=1(cid:16) \u00aff (xi, u j, xi\u2032 )\u03c1(xi, u j, xi\u2032)(cid:17)i\n\n\u00afm\n\u2211\n\n\u00afn\n\u2211\n\n\u00afn\n\u2211\n\n(3.39)\n\nto understand why the summation over i\u2032 enters the equation for z, recall that each\nelement \u02dc\u03c1\u03c1\u03c1[i, j] of the vector \u02dc\u03c1\u03c1\u03c1 is the expected reward after taking action u j in state xi.\n\nmodel-free projected policy evaluation\n\nsome of the most powerful algorithms for approximate policy evaluation solve the\nmatrix form (3.38) of the projected bellman equation in a model-free fashion, by\nestimating \u03b3, \u03bb, and z from transition samples. because (3.38) is a linear system\nof equations, these algorithms are computationally efficient. they are also sample-\nefficient, i.e., they approach their solution quickly as the number of samples they\nconsider increases, as shown in the context of v-function approximation by konda\n(2002, chapter 6) and by yu and bertsekas (2006, 2009).\n\nconsider a set of transition samples:\n\n{(xls , uls, x\u2032ls \u223c \u00aff (xls, uls,\u00b7), rls = \u02dc\u03c1(xls, uls, x\u2032ls))| ls = 1, . . . , ns}\n\n "}, {"Page_number": 89, "text": "3.5. approximate policy iteration\n\n79\n\nthis set is constructed by drawing state-action samples (x, u) from a distribution\ngiven by the weight function w: the probability of each pair (x, u) is equal to its\nweight w(x, u). using this set of samples, estimates of \u03b3, \u03bb, and z can be constructed\nas follows:\n\nz0 = 0\n\n\u03b30 = 0, \u03bb0 = 0,\n\u03b3ls = \u03b3ls\u22121 + \u03c6(xls, uls)\u03c6t(xls, uls)\n\u03bbls = \u03bbls\u22121 + \u03c6(xls, uls)\u03c6t(x\u2032ls, h(x\u2032ls))\nzls = zls\u22121 + \u03c6(xls, uls)rls\n\n(3.40)\n\nthese updates can be derived from (3.39).\n\nthe least-squares temporal difference for q-functions (lstd-q) (lagoudakis\net al., 2002; lagoudakis and parr, 2003a) is a policy evaluation algorithm that pro-\ncesses the samples using (3.40) and then solves the equation:\n\n1\nns\n\n1\nns\n\n\u03b3nsb\u03b8h = \u03b3\n\n\u03bbnsb\u03b8h +\n\n1\nns\n\nzns\n\n(3.41)\n\nto find an approximate parameter vectorb\u03b8h. notice thatb\u03b8h appears on both sides of\n\n(3.41), so this equation can be simplified to:\n\n1\nns\n\n(\u03b3ns \u2212 \u03b3\u03bbns)b\u03b8h =\n\n1\nns\n\nzns\n\nalthough the division by ns is not necessary from a formal point of view, it helps to\nincrease the numerical stability of the algorithm (the elements in the \u03b3ns , \u03bbns , zns can\nbe very large when ns is large). lstd-q is an extension of an earlier, similar algo-\nrithm for v-functions, called least-squares temporal difference (bradtke and barto,\n1996; boyan, 2002).\n\nanother method, the least-squares policy evaluation for q-functions (lspe-q)\n(e.g., jung and polani, 2007a) starts with an arbitrary initial parameter vector \u03b80 and\nupdates it incrementally, with:\n\n\u03b8ls = \u03b8ls\u22121 + \u03b1(\u03b8\u2021\n\u03bbls\u03b8ls\u22121 +\n\n= \u03b3\n\n1\nls\n\n\u03b3ls\u03b8\u2021\nls\n\n1\nls\n\n1\nls\n\nzls\n\nls \u2212 \u03b8ls\u22121), where:\n\n(3.42)\n\nin which \u03b1 is a step size parameter. to ensure the invertibility of the matrix \u03b3 at the\nstart of the learning process, when only a few samples have been processed, it can\nbe initialized to a small multiple of the identity matrix. the division by ls increases\nthe numerical stability of the updates. like lstd-q, lspe-q is an extension of\nan earlier algorithm for v-functions, called least-squares policy evaluation (lspe)\n(bertsekas and ioffe, 1996).\n\nalgorithms 3.8 and 3.9 present lstd-q and lspe-q in a procedural form.\nlstd-q is a one-shot algorithm, and the parameter vector it computes does not\ndepend on the order in which the samples are processed. on the other hand, lspe-q\n\n "}, {"Page_number": 90, "text": "80\n\nchapter 3. dp and rl in large and continuous spaces\n\nalgorithm 3.8 least-squares temporal difference for q-functions.\ninput: policy h to be evaluated, discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r, samples {(xls, uls, x\u2032ls\n\n, rls)| ls = 1, . . . , ns}\n\n1: \u03b30 \u2190 0, \u03bb0 \u2190 0, z0 \u2190 0\n2: for ls = 1, . . . , ns do\n3:\n\n\u03b3ls \u2190 \u03b3ls\u22121 + \u03c6(xls, uls)\u03c6t(xls, uls)\n\u03bbls \u2190 \u03bbls\u22121 + \u03c6(xls, uls)\u03c6t(x\u2032ls\n, h(x\u2032ls\nzls \u2190 zls\u22121 + \u03c6(xls, uls)rls\n\u03bbnsb\u03b8h + 1\n\u03b3nsb\u03b8h = \u03b3 1\n\nzns forb\u03b8h\n\nns\n\nns\n\n))\n\n4:\n\n5:\n6: end for\n7: solve 1\nns\n\noutput: b\u03b8h\n\nalgorithm 3.9 least-squares policy evaluation for q-functions.\ninput: policy h to be evaluated, discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r, samples {(xls, uls, x\u2032ls\nstep size \u03b1, a small constant \u03b2\u03b3 > 0\n\n, rls)| ls = 1, . . . , ns},\n\n1: \u03b30 \u2190 \u03b2\u03b3i, \u03bb0 \u2190 0, z0 \u2190 0\n2: for ls = 1, . . . , ns do\n3:\n\n\u03b3ls \u2190 \u03b3ls\u22121 + \u03c6(xls, uls)\u03c6t(xls, uls)\n\u03bbls \u2190 \u03bbls\u22121 + \u03c6(xls, uls)\u03c6t(x\u2032ls\n, h(x\u2032ls\nzls \u2190 zls\u22121 + \u03c6(xls, uls)rls\n\u03b8ls \u2190 \u03b8ls\u22121 + \u03b1(\u03b8\u2021\n\nls \u2212 \u03b8ls\u22121), where 1\n\nls\n\n))\n\n4:\n\n5:\n\n6:\n7: end for\n\n\u03b3ls\u03b8\u2021\nls\n\n= \u03b3 1\nls\n\n\u03bbls\u03b8ls\u22121 + 1\n\nls\n\nzls\n\noutput: b\u03b8h = \u03b8ns\n\nis an incremental algorithm, so the current parameter vector \u03b8ls depends on the pre-\nvious values \u03b80, . . . ,\u03b8ls\u22121, and therefore the order in which samples are processed is\nimportant.\n\nin the context of v-function approximation, such least-squares algorithms have\nbeen shown to converge to the fixed point of the projected bellman equation, namely\nby nedi\u00b4c and bertsekas (2003) for the v-function analogue of lstd-q, and by nedi\u00b4c\nand bertsekas (2003); bertsekas et al. (2004) for the analogue of lspe-q. these\nresults also extend to q-function approximation. to ensure convergence, the weight\n(probability of being sampled) w(x, u) of each state-action pair (x, u) should be equal\nto the steady-state probability of this pair along an infinitely-long trajectory generated\nwith the policy h.10\n\nnote that collecting samples by using only a deterministic policy h is insuffi-\n\n10from a practical point of view, note that lstd-q is a one-shot algorithm and will produce a solution\nwhenever \u03b3ls is invertible. this means the experimenter need not worry excessively about divergence per\nse. rather, the theoretical results concern the uniqueness and meaning of the solution obtained. lstd-q\ncan, in fact, produce meaningful results for many weight functions w, as we illustrate later in section 3.5.7\nand in chapter 5.\n\n "}, {"Page_number": 91, "text": "3.5. approximate policy iteration\n\n81\n\ncient for the following reason. if only state-action pairs of the form (x, h(x)) were\ncollected, no information about pairs (x, u) with u 6= h(x) would be available (equiv-\nalently, the corresponding weights w(x, u) would all be zero). as a result, the ap-\nproximate q-values of such pairs would be poorly estimated and could not be relied\nupon for policy improvement. to alleviate this problem, exploration is necessary:\nsometimes, actions different from h(x) have to be selected, e.g., in a random fashion.\ngiven a stationary (time-invariant) exploration procedure, lstd-q and lspe-q are\nsimply evaluating the new, exploratory policy, and so they remain convergent.\n\nns\n\nns\n\nexpected value, given x and u.\n\n\u03bbns \u2192 \u03bb, and 1\n\nthe following intuitive (albeit informal) line of reasoning is useful to understand\nthe convergence of lstd-q and lspe-q. asymptotically, as ns \u2192 \u221e, it is true that\n1\n\u03b3ns \u2192 \u03b3, 1\nzns \u2192 z, for the following two reasons. first, as the\nns\nnumber ns of state-action samples generated grows, their empirical distribution con-\nverges to w. second, as the number of transition samples involving a given state-\naction pair (x, u) grows, the empirical distribution of the next states x\u2032 converges to\nthe distribution \u00aff (x, u,\u00b7), and the empirical average of the rewards converges to its\nsince the estimates of \u03b3, \u03bb, and z asymptotically converge to their true values,\nthe equation solved by lstd-q asymptotically converges to the projected bellman\nequation (3.38). under the assumptions for convergence, this equation has a unique\nsolution \u03b8h, so the parameter vector of lstd-q asymptotically reaches this solu-\ntion. for similar reasons, whenever it converges, lspe-q asymptotically becomes\nequivalent to lstd-q and the projected bellman equation. therefore, if lspe-q\nconverges, it must in fact converge to \u03b8h. in fact, it can additionally be shown that,\nas ns grows, the solutions of lstd-q and lspe-q converge to each other faster\nthan they converge to their limit \u03b8h. this was proven in the context of v-function\napproximation by yu and bertsekas (2006, 2009).\n\none possible advantage of lstd-q over lspe-q may arise when their assump-\ntions are violated, e.g., when the policy to be evaluated changes as samples are being\ncollected. this situation can arise in the important context of optimistic policy itera-\ntion, which will be discussed in section 3.5.5. violating the assumptions may intro-\nduce instability and possibly divergence in the iterative lspe-q updates (3.42). in\ncontrast, because it only computes one-shot solutions, lstd-q (3.41) may be more\nresilient to such instabilities. on the other hand, the incremental nature of lspe-\nq offers some advantages over lstd-q. for instance, lspe-q can benefit from a\ngood initial value of the parameter vector. additionally, by lowering the step size \u03b1,\nit may be possible to mitigate the destabilizing effects of violating the assumptions.\nnote that an incremental version of lstd-q can also be given, but the benefits of\nthis version are unclear.\n\nwhile for the derivation above it was assumed that x and u are finite, the updates\n(3.40), together with lstd-q and lspe-q, can also be applied without any change\nin infinite and uncountable (e.g., continuous) state-action spaces.\n\nfrom a computational point of view, the linear systems in (3.41) and (3.42) can\nbe solved in several ways, e.g., by matrix inversion, by gaussian elimination, or by\nincrementally computing the inverse with the sherman-morrison formula. the com-\nputational cost is o(n3) for \u201cnaive\u201d matrix inversion. more efficient algorithms than\n\n "}, {"Page_number": 92, "text": "82\n\nchapter 3. dp and rl in large and continuous spaces\n\nmatrix inversion can be obtained, e.g., by incrementally computing the inverse, but\nthe cost of solving the linear system will still be larger than o(n2). in an effort to\nfurther reduce the computational costs, variants of the least-squares temporal differ-\nence have been proposed in which only a few of the parameters are updated at a\ngiven iteration (geramifard et al., 2006, 2007). note also that, when the bf vector\n\u03c6(x, u) is sparse, the computational efficiency of the updates (3.40) can be improved\nby exploiting this sparsity.11\n\nas already outlined, analogous least-squares algorithms can be given to com-\npute approximate v-functions (bertsekas and ioffe, 1996; bradtke and barto, 1996;\nboyan, 2002; bertsekas, 2007, chapter 6). however, as explained in section 2.2,\npolicy improvement is more difficult to perform using v-functions. namely, a model\nof the mdp is required, and in the stochastic case, expectations over the transitions\nmust be estimated.\n\ngradient-based policy evaluation\n\ngradient-based algorithms for policy evaluation historically precede the least-\nsquares methods discussed above (sutton, 1988). however, under appropriate condi-\ntions, they find, in fact, a solution of the projected bellman equation (3.34). these\nalgorithms are called temporal-difference learning in the literature, and are more pop-\nular in the context of v-function approximation (sutton, 1988; jaakkola et al., 1994;\ntsitsiklis and van roy, 1997). nevertheless, given the focus of this chapter, we will\npresent gradient-based policy evaluation for the case of q-function approximation.\n\nwe use sarsa as a starting point in developing such an algorithm. recall that\nsarsa (algorithm 2.7) uses tuples (xk, uk, rk+1, xk+1, uk+1) to update a q-function\nonline (2.40):\n\nqk+1(xk, uk) = qk(xk, uk) + \u03b1k[rk+1 + \u03b3qk(xk+1, uk+1)\u2212 qk(xk, uk)]\n\n(3.43)\n\nwhere \u03b1k is the learning rate. when uk is chosen according to a fixed policy h,\nsarsa actually performs policy evaluation (see also section 2.4.2). we exploit this\nproperty and combine (3.43) with gradient-based updates to obtain the desired policy\nevaluation algorithm. as before, linearly parameterized approximation is considered.\nby a derivation similar to that given for gradient-based q-learning in section 3.4.2,\nthe following update rule is obtained:\n\n\u03b8k+1 = \u03b8k + \u03b1k(cid:2)rk+1 + \u03b3\u03c6t(xk+1, uk+1)\u03b8k \u2212 \u03c6t(xk, uk)\u03b8k(cid:3)\u03c6(xk, uk)\n\nwhere the quantity in square brackets is an approximation of the temporal difference.\nthe resulting algorithm for policy evaluation is called temporal difference for q-\nfunctions (td-q) . note that td-q can be seen as an extension of a corresponding\nalgorithm for v-functions, which is called temporal difference (td) (sutton, 1988).\nlike the least-squares algorithms presented earlier, td-q requires exploration to\n\n(3.44)\n\n11the bf vector is sparse, e.g., for the discrete-action approximator described in example 3.1. this is\nbecause the bf vector contains zeros for all the discrete actions that are different from the current discrete\naction.\n\n "}, {"Page_number": 93, "text": "3.5. approximate policy iteration\n\n83\n\nobtain samples (x, u) with u 6= h(x). algorithm 3.10 presents td-q with \u03b5-greedy\nexploration. in this algorithm, because the update at step k involves the action uk+1\nat the next step, this action is chosen prior to updating the parameter vector.\n\nalgorithm 3.10 temporal difference for q-functions, with \u03b5-greedy exploration.\ninput: discount factor \u03b3, policy h to be evaluated,\n\nk=0, learning rate schedule {\u03b1k}\u221e\n\nk=0\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: measure initial state x0\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r,\nexploration schedule {\u03b5k}\u221e\n3: u0 \u2190(h(x0)\n\n4: for every time step k = 0, 1, 2, . . . do\n5:\n\na uniform random action in u with probability \u03b50 (explore)\n\nwith probability 1\u2212 \u03b50\n\napply uk, measure next state xk+1 and reward rk+1\n\nuk+1 \u2190(h(xk+1)\n\u03b8k+1 \u2190 \u03b8k + \u03b1k(cid:2)rk+1 + \u03b3\u03c6t(xk+1, uk+1)\u03b8k \u2212 \u03c6t(xk, uk)\u03b8k(cid:3)\u03c6(xk, uk)\n\na uniform random action in u with probability \u03b5k+1\n\nwith probability 1\u2212 \u03b5k+1\n\n6:\n\n7:\n8: end for\n\na comprehensive convergence analysis of gradient-based policy evaluation\nwas provided by tsitsiklis and van roy (1997) in the context of v-function ap-\nproximation. this analysis extends to q-function approximation under appropriate\nconditions. an important condition is that the stochastic policy \u02dch resulting from the\ncombination of h with exploration should be time-invariant, which can be achieved\nby simply making the exploration time-invariant, e.g., in the case of \u03b5-greedy explo-\nration, by making \u03b5k the same for all steps k. the main result is that td-q asymptoti-\ncally converges to the solution of the projected bellman equation for the exploratory\npolicy \u02dch, for a weight function given by the steady-state distribution of the state-\naction pairs under \u02dch.\n\ngradient-based algorithms such as td-q are less computationally demanding\nthan least-squares algorithms such as lstd-q and lspe-q. the time and memory\ncomplexity of td-q are both o(n), since they store and update vectors of length n.\nthe memory complexity of lstd-q and lspe-q is at least o(n2) (since they store\nmatrices of size n) and their time complexity is o(n3) (when \u201cnaive\u201d matrix inversion\nis used to solve the linear system). on the other hand, gradient-based algorithms\ntypically require more samples than least-squares algorithms to achieve a similar\naccuracy (konda, 2002; yu and bertsekas, 2006, 2009), and are more sensitive to\nthe learning rate (step size) schedule. lstd-q has no step size at all, and lspe-q\nworks for a wide range of constant step sizes, as shown in the context of v-functions\nby bertsekas et al. (2004) (this range includes \u03b1 = 1, leading to a nonincremental\nvariant of lspe-q).\n\nefforts have been made to extend gradient-based policy evaluation algorithms to\noff-policy learning, i.e., evaluating one policy while using another policy to gener-\n\n "}, {"Page_number": 94, "text": "84\n\nchapter 3. dp and rl in large and continuous spaces\n\nate the samples (sutton et al., 2009b,a). these extensions perform gradient descent\non error measures that are different from the measure used in the basic temporal-\ndifference algorithms such as td-q (i.e., different from the squared value function\nerror for the current sample).\n\n3.5.3 policy evaluation with nonparametric approximation\n\nnonparametric approximators have been combined with a number of algorithms for\napproximate policy evaluation. for instance, kernel-based approximators were com-\nbined with lstd by xu et al. (2005), with lstd-q by xu et al. (2007); jung and\npolani (2007b); farahmand et al. (2009b), and with lspe-q by jung and polani\n(2007a,b). rasmussen and kuss (2004) and engel et al. (2003, 2005) used the re-\nlated framework of gaussian processes to approximate v-functions in policy eval-\nuation. taylor and parr (2009) showed that, in fact, the algorithms in (rasmussen\nand kuss, 2004; engel et al., 2005; xu et al., 2005) produce the same solution when\nthey use the same samples and the same kernel function. fitted policy evaluation\n(algorithm 3.7) can be extended to the nonparametric case along the same lines as\nfitted q-iteration in section 3.4.3. such an algorithm was proposed by jodogne et al.\n(2006), who employed ensembles of extremely randomized trees to approximate the\nq-function.\n\nas explained in section 3.3.2, a kernel-based approximator can be seen as lin-\nearly parameterized if all the samples are known in advance. in certain cases, this\nproperty can be exploited to extend the theoretical guarantees about approximate pol-\nicy evaluation from the parametric case to the nonparametric case (xu et al., 2007).\nfarahmand et al. (2009b) provided performance guarantees for their kernel-based\nlstd-q variant for the case when only a finite number of samples is available.\n\nan important concern in the nonparametric case is controlling the complexity\nof the approximator. originally, the computational demands of many nonparametric\napproximators, including kernel-based methods and gaussian processes, grow with\nthe number of samples considered. many of the approaches mentioned above employ\nkernel sparsification techniques to limit the number of samples that contribute to the\nsolution (xu et al., 2007; engel et al., 2003, 2005; jung and polani, 2007a,b).\n\n3.5.4 model-based approximate policy evaluation with rollouts\n\nall the policy evaluation algorithms discussed above obtain a value function by solv-\ning the bellman equation (3.31) approximately. while this is a powerful approach, it\nalso has its drawbacks. a core problem is that a good value function approximator is\nrequired, which is often difficult to find. nonparametric approximation alleviates this\nproblem to some extent. another problem is that the convergence requirements of the\nalgorithms, such as the linearity of the approximate q-function in the parameters, can\nsometimes be too restrictive.\n\nanother class of policy evaluation approaches sidesteps these difficulties by\navoiding an explicit representation of the value function. instead, the value function\nis evaluated on demand, by monte carlo simulations. a model is required to perform\n\n "}, {"Page_number": 95, "text": "3.5. approximate policy iteration\n\n85\n\nthe simulations, so these approaches are model-based. for instance, to estimate the\n\nq-value bqh(x, u) of a given state-action pair (x, u), a number nmc of trajectories are\n\nsimulated, where each trajectory is generated using the policy h, has length k, and\nstarts from the pair (x, u). the estimated q-value is then the average of the sample\nreturns obtained along these trajectories:\n\nnmc\n\u2211\n\ni0=1\" \u02dc\u03c1(x, u, xi0,1) +\n\n\u03b3k \u02dc\u03c1(xi0,k, h(xi0,k), xi0,k+1)#\n\nk\n\n\u2211\nk=1\n\n1\n\nnmc\n\n(3.45)\n\nbqh(x, u) =\n\nwhere nmc is the number of trajectories to simulate. for each trajectory i0, the first\nstate-action pair is fixed to (x, u) and leads to a next state xi0,1 \u223c \u02dcf (x, u,\u00b7). thereafter,\nactions are chosen using the policy h, which means that for k \u2265 1:\n\nxi0,k+1 \u223c f (xi0,k, h(xi0,k),\u00b7)\n\nsuch a simulation-based estimation procedure is called a rollout (lagoudakis and\nparr, 2003b; bertsekas, 2005b; dimitrakakis and lagoudakis, 2008). the length k\nof the trajectories can be chosen using (2.41) to ensure \u03b5mc-accurate returns, where\n\u03b5mc > 0. note that if the mdp is deterministic, a single trajectory suffices. in the\nstochastic case, an appropriate value for the number nmc of trajectories will depend\non the problem.\n\nrollouts can be computationally expensive, especially in the stochastic case.\ntheir computational cost is proportional to the number of points at which the value\nfunction must be evaluated. therefore, rollouts are most beneficial when this num-\nber is small. if the value function must be evaluated at many (or all) points of the\nstate(-action) space, then methods that solve the bellman equation approximately\n(sections 3.5.1 \u2013 3.5.3) may be computationally less costly than rollouts.\n\n3.5.5 policy improvement and approximate policy iteration\n\nup to this point, approximate policy evaluation has been considered. to obtain a\ncomplete algorithm for approximate policy iteration, a method to perform policy\nimprovement is also required.\n\nexact and approximate policy improvement\n\nconsider first policy improvement in the case where the policy is not represented\nexplicitly. instead, greedy actions are computed on demand from the value function,\nfor every state where a control action is required. for instance, when q-functions are\nemployed, an improved action for the state x can be found with:\n\nh\u2113+1(x) = u, where u \u2208 arg max\n\n\u00afu\n\n(3.46)\n\nbqh\u2113(x, \u00afu)\n\nthe policy is thus implicitly defined by the value function. in (3.46), it was assumed\nthat a greedy action can be computed exactly. this is true, e.g., when the action space\nonly contains a small, discrete set of actions, and the maximization in the policy\n\n "}, {"Page_number": 96, "text": "86\n\nchapter 3. dp and rl in large and continuous spaces\n\nimprovement step is solved by enumeration. in this situation, policy improvement\nis exact, but if greedy actions cannot be computed exactly, then the result of the\nmaximization is approximate, and the (implicitly defined) policy thus becomes an\napproximation.\n\nalternatively, the policy can also be represented explicitly, in which case it gen-\nerally must be approximated. the policy can be approximated, e.g., by a linear\nparametrization (3.12):\n\nn\n\n\u2211\ni=1\n\n\u03d5i(x)\u03d1i = \u03d5t(x)\u03d1\n\nbh(x) =\n\nwhere \u03d5i(x), i = 1, . . . , n are the state-dependent bfs and \u03d1 is the policy parameter\nvector (see section 3.3.4 for a discussion of the notation used for policy approxima-\ntion). a scalar action was assumed, but the parametrization can easily be extended to\nmultiple action variables. for this parametrization, approximate policy improvement\ncan be performed by solving the linear least-squares problem:\n\n\u03d1\u2113+1 = \u03d1\u2021, where \u03d1\u2021 \u2208 arg min\n\n\u03d1\n\nns\n\u2211\n\nis=1(cid:0)\u03d5t(xis)\u03d1\u2212 uis(cid:1)2\n\n(3.47)\n\nto find a parameter vector \u03d1\u2113+1, where {x1, . . . , xns} is a set of state samples to be\nused for policy improvement, and u1, . . . , uns are corresponding greedy actions:\n\nuis \u2208 arg max\n\nu\n\nbqbh\u2113(xis, u)\n\n(3.48)\n\nnote that the previous policybh\u2113 is now also an approximation. in (3.48), it was im-\n\nplicitly assumed that greedy actions can be computed exactly; if this is not the case,\nthen uis will only be approximations of the true greedy actions.\n\nsuch a policy improvement is therefore a two-step procedure: first, greedy actions\nuis are chosen using (3.48), and then these actions are used to solve the least-squares\nproblem (3.47). the solution depends on the greedy actions chosen, but remains\nmeaningful for any combination of choices, since for any such combination, it ap-\nproximates one of the possible greedy policies in the q-function.\nalternatively, policy improvement could be performed with:\n\n\u03d1\u2113+1 = \u03d1\u2021, where \u03d1\u2021 \u2208 arg max\n\n\u03d1\n\nns\n\u2211\n\nis=1bqbh\u2113(xis,\u03d5t(xis)\u03d1)\n\n(3.49)\n\nwhich maximizes the approximate q-values of the actions chosen by the policy in the\nstate samples. however, (3.49) is generally a difficult nonlinear optimization prob-\nlem, whereas (3.47) is (once greedy actions have been chosen) a convex optimization\nproblem, which is easier to solve.\n\nmore generally, for any policy representation (e.g., for a nonlinear parametriza-\ntion), a regression problem generalizing either (3.47) or (3.49) must be solved to\nperform policy improvement.\n\n "}, {"Page_number": 97, "text": "3.5. approximate policy iteration\n\n87\n\noffline approximate policy iteration\n\napproximate policy iteration algorithms can be obtained by combining a policy eval-\nuation algorithm (e.g., one of those described in sections 3.5.1 \u2013 3.5.3) with a policy\nimprovement technique (e.g., one of those described above); see again algorithm 3.5\nfor a generic template of approximate policy iteration. in the offline case, the approx-\nimate policy evaluation is run until (near) convergence, to ensure the accuracy of the\nvalue function and therefore an accurate policy improvement.\n\nfor example, the algorithm resulting from combining lstd-q (algorithm 3.8)\nwith exact policy improvement is called least-squares policy iteration (lspi). lspi\nwas proposed by lagoudakis et al. (2002) and by lagoudakis and parr (2003a), and\nhas been studied often since then (e.g., mahadevan and maggioni, 2007; xu et al.,\n2007; farahmand et al., 2009b). algorithm 3.11 shows lspi, in a simple variant that\nuses the same set of transition samples at every policy evaluation. in general, different\nsets of samples can be used at different iterations. the explicit policy improvement at\nline 4 is included for clarity. in practice, the policy h\u2113+1 does not have to be computed\nand stored for every state. instead, it is computed on demand from the current q-\nfunction, only for those states where an improved action is necessary. in particular,\nlstd-q only evaluates the policy at the state samples x\u2032ls\n\n.\n\nalgorithm 3.11 least-squares policy iteration.\ninput: discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r, samples {(xls, uls, x\u2032ls\n\n, rls)| ls = 1, . . . , ns}\n\n1: initialize policy h0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\nevaluate h\u2113 using lstd-q (algorithm 3.8), yielding \u03b8\u2113 \u22b2 policy evaluation\nh\u2113+1(x) \u2190 u, u \u2208 arg max \u00afu \u03c6t(x, \u00afu)\u03b8\u2113 for each x \u2208 x \u22b2 policy improvement\n\n4:\n5: until h\u2113+1 is satisfactory\n\noutput: bh\u2217 = h\u2113+1\n\npolicy iteration with rollout policy evaluation (section 3.5.4) was studied, e.g.,\nby lagoudakis and parr (2003b) and by dimitrakakis and lagoudakis (2008), who\nemployed nonparametric approximation to represent the policy. note that rollout pol-\nicy evaluation (which represents value functions implicitly) should not be combined\nwith implicit policy improvement. such an algorithm would be impractical, because\nneither the value function nor the policy would be represented explicitly.\n\nonline, optimistic approximate policy iteration\n\nin online learning, the performance should improve once every few transition sam-\nples. this is in contrast to the offline case, in which only the performance at the end\nof the learning process is important. one way in which policy iteration can take this\nrequirement into account is by performing policy improvements once every few tran-\nsition samples, before an accurate evaluation of the current policy can be completed.\nsuch a variant is sometimes called optimistic policy iteration (bertsekas and tsitsik-\n\n "}, {"Page_number": 98, "text": "88\n\nchapter 3. dp and rl in large and continuous spaces\n\nlis, 1996, section 6.4; sutton 1988; tsitsiklis 2002). in the extreme case, the policy\nis improved after every transition, and then applied to obtain a new transition sample\nthat is fed into the policy evaluation algorithm. then, another policy improvement\ntakes place, and the cycle repeats. this variant is called fully optimistic. in general,\nthe policy is improved once every several (but not too many) transitions; this variant\nis partially optimistic. as in any online rl algorithm, exploration is also necessary\nin optimistic policy iteration.\n\noptimistic policy iteration was already outlined in section 2.4.2, where it was\nalso explained that sarsa (algorithm 2.7) belongs to this class. so, an approximate\nversion of sarsa will naturally be optimistic, as well. a gradient-based version of\nsarsa can be easily obtained from td-q (algorithm 3.10), by choosing actions\nwith a policy that is greedy in the current q-function, instead of with a fixed policy\nas in td-q. of course, exploration is required in addition to greedy action selection.\nalgorithm 3.12 presents approximate sarsa with an \u03b5-greedy exploration proce-\ndure. approximate sarsa has been studied, e.g., by sutton (1996); santamaria et al.\n(1998); gordon (2001); melo et al. (2008).\n\nalgorithm 3.12 sarsa with a linear parametrization and \u03b5-greedy exploration.\ninput: discount factor \u03b3,\n\nk=0, learning rate schedule {\u03b1k}\u221e\n\nk=0\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: measure initial state x0\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r,\nexploration schedule {\u03b5k}\u221e\n3: u0 \u2190(u \u2208 arg max \u00afu(cid:0)\u03c6t(x0, \u00afu)\u03b80(cid:1)\n\n4: for every time step k = 0, 1, 2, . . . do\n5:\n\na uniform random action in u with probability \u03b50 (explore)\n\nwith probability 1\u2212 \u03b50 (exploit)\n\napply uk, measure next state xk+1 and reward rk+1\n\nuk+1 \u2190(u \u2208 arg max \u00afu(cid:0)\u03c6t(xk+1, \u00afu)\u03b8k(cid:1)\n\u03b8k+1 \u2190 \u03b8k + \u03b1k(cid:2)rk+1 + \u03b3\u03c6t(xk+1, uk+1)\u03b8k \u2212 \u03c6t(xk, uk)\u03b8k(cid:3)\u03c6(xk, uk)\n\na uniform random action in u with probability \u03b5k+1\n\nwith probability 1\u2212 \u03b5k+1\n\n6:\n\n7:\n8: end for\n\nother policy evaluation algorithms can also be used in optimistic policy itera-\ntion. for instance, optimistic policy iteration with lspe-q was applied by jung and\npolani (2007a,b), while a v-function based algorithm similar to approximate sarsa\nwas proposed by jung and uthmann (2004). in chapter 5 of this book, an online, op-\ntimistic variant of lspi will be introduced in detail and evaluated experimentally.\n\n3.5.6 theoretical guarantees\n\nunder appropriate assumptions, offline policy iteration eventually produces policies\nwith a bounded suboptimality. however, in general it cannot be guaranteed to con-\nverge to a fixed policy. the theoretical understanding of optimistic policy iteration\n\n "}, {"Page_number": 99, "text": "3.5. approximate policy iteration\n\n89\n\nis currently limited, and guarantees can only be provided in some special cases. we\nfirst discuss the properties of policy iteration in the offline setting, and then continue\nto the online, optimistic setting.\n\ntheoretical guarantees for offline approximate policy iteration\n\nas long as the policy evaluation and improvement errors are bounded, offline ap-\nproximate policy iteration eventually produces policies with a bounded suboptimal-\nity. this result applies to any type of value function or policy approximator, and can\nbe formalized as follows.\n\nconsider the general case where both the value functions and the policies are\napproximated. consider also the case where q-functions are used, and assume that\nthe error at every policy evaluation step is bounded by \u03c2q:\n\nkbqbh\u2113 \u2212 qbh\u2113k\u221e \u2264 \u03c2q,\n\nfor any \u2113 \u2265 0\n\nand that the error at every policy improvement step is bounded by \u03c2h, in the following\nsense:\n\nktbh\u2113+1(bqbh\u2113)\u2212 t (bqbh\u2113)k\u221e \u2264 \u03c2h,\n\nfor any \u2113 \u2265 0\n\nwhere tbh\u2113+1 is the policy evaluation mapping for the improved (approximate) policy,\n\nand t is the q-iteration mapping (2.22). then, approximate policy iteration eventu-\nally produces policies with performances that lie within a bounded distance from the\noptimal performance (e.g., lagoudakis and parr, 2003a):\n\nlim sup\n\n\u2113\u2192\u221e (cid:13)(cid:13)(cid:13)bqbh\u2113 \u2212 q\u2217(cid:13)(cid:13)(cid:13)\u221e \u2264\n\n\u03c2h + 2\u03b3\u03c2q\n(1\u2212 \u03b3)2\n\n(3.50)\n\nfor an algorithm that performs exact policy improvements, such as lspi, \u03c2h = 0 and\nthe bound is tightened to:\n\nlim sup\n\n\u2113\u2192\u221e kbqh\u2113 \u2212 q\u2217k\u221e \u2264\n\n2\u03b3\u03c2q\n(1\u2212 \u03b3)2\n\n(3.51)\n\nwhere kbqh\u2113 \u2212 qh\u2113k\u221e \u2264 \u03c2q, for any \u2113 \u2265 0. note that finding \u03c2q and (when approximate\n\npolicies are used) \u03c2h may be difficult in practice, and the existence of these bounds\nmay require additional assumptions.\n\nthese guarantees do not necessarily imply the convergence to a fixed policy. for\ninstance, both the value function and policy parameters might converge to limit cy-\ncles, so that every point on the cycle yields a policy that satisfies the bound. conver-\ngence to limit cycles can indeed happen, as will be seen in the upcoming example of\nsection 3.5.7. similarly, when exact policy improvements are used, the value func-\ntion parameter may oscillate, implicitly leading to an oscillating policy. this is a\ndisadvantage with respect to offline approximate value iteration, which under appro-\npriate assumptions converges monotonically to a unique fixed point (section 3.4.4).\nsimilar results hold when v-functions are used instead of q-functions (bertsekas\n\nand tsitsiklis, 1996, section 6.2).\n\n "}, {"Page_number": 100, "text": "90\n\nchapter 3. dp and rl in large and continuous spaces\n\ntheoretical guarantees for online, optimistic policy iteration\n\nthe performance guarantees given above for offline policy iteration rely on bounded\npolicy evaluation errors. because optimistic policy iteration improves the policy be-\nfore an accurate value function is available, the policy evaluation error can be very\nlarge, and the performance guarantees for offline policy iteration are not useful in the\nonline case.\n\nthe behavior of optimistic policy iteration has not been properly understood yet,\nand can be very complicated. optimistic policy iteration can, e.g., exhibit a phe-\nnomenon called chattering, whereby the value function converges to a stationary\nfunction, while the policy sequence oscillates, because the limit of the value function\nparameter corresponds to multiple policies (bertsekas and tsitsiklis, 1996, section\n6.4).\n\ntheoretical guarantees can, however, be provided in certain special cases. gor-\ndon (2001) showed that the parameter vector of approximate sarsa cannot diverge\nwhen the mdp has terminal states and the policy is only improved in-between trials\n(see section 2.2.1 for the meaning of terminal states and trials). melo et al. (2008)\nimproved on this result, by showing that approximate sarsa converges with prob-\nability 1 to a fixed point, if the dependence of the policy on the parameter vector\nsatisfies a certain lipschitz continuity condition. this condition prohibits using fully\ngreedy policies, because those generally depend on the parameters in a discontinuous\nfashion.\n\nthese theoretical results concern the gradient-based sarsa algorithm. how-\never, in practice, least-squares algorithms may be preferable due to their improved\nsample efficiency. while no theoretical guarantees are available when using least-\nsquares algorithms in the optimistic setting, some promising empirical results have\nbeen reported (jung and polani, 2007a,b); see also chapter 5 for an empirical evalu-\nation of optimistic lspi.\n\n3.5.7 example: least-squares policy iteration for a dc motor\n\nin this example, approximate policy iteration is applied to the dc motor problem\nintroduced in section 3.4.5. in a first experiment, the original lspi (algorithm 3.11)\nis applied. this algorithm represents policies implicitly and performs exact pol-\nicy improvements. the results of this experiment are compared with the results of\napproximate q-iteration from section 3.4.5. in a second experiment, lspi is modi-\nfied to use approximate policies and sample-based, approximate policy improve-\nments. the resulting solution is compared with the solution found with exact policy\nimprovements.\n\nin both experiments, the policies are evaluated using their q-functions, which are\napproximated with a discrete-action parametrization of the type described in exam-\nple 3.1. recall that such an approximator replicates state-dependent bfs for every\ndiscrete action, and in order to obtain the state-action bfs, it sets to 0 all the bfs\nthat do not correspond to the current discrete action. like in section 3.4.5, the action\nspace is discretized into the set ud = {\u221210, 0, 10}, so the number of discrete actions\n\n "}, {"Page_number": 101, "text": "3.5. approximate policy iteration\n\n91\n\nis m = 3. the state-dependent bfs are axis-aligned, normalized gaussian rbfs (see\nexample 3.1). the centers of the rbfs are arranged on a 9\u00d7 9 equidistant grid over\nthe state space, so there are n = 81 rbfs in total. all the rbfs are identical in shape,\n2/2, where b\u2032d is the distance\nand their width bd along each dimension d is equal to b\u2032d\nbetween adjacent rbfs along that dimension (the grid step). these rbfs yield a\nsmooth interpolation of the q-function over the state space. recalling that the do-\nmains of the state variables are [\u2212\u03c0,\u03c0] for the angle and [\u221216\u03c0, 16\u03c0] for the angular\nvelocity, we obtain b\u20321 = 2\u03c0\n9\u22121 \u2248 12.57, which lead to b1 \u2248 0.31\nand b2 \u2248 78.96. the parameter vector \u03b8 contains n = nm = 243 parameters.\nleast-squares policy iteration with exact policy improvement\n\n9\u22121 \u2248 0.79 and b\u20322 = 32\u03c0\n\nin the first part of the example, the original lspi algorithm is applied to the dc\nmotor problem. recall that lspi combines lstd-q policy evaluation with exact\npolicy improvement.\n\nthe same set of ns = 7500 samples is used at every lstd-q policy evaluation.\nthe samples are random, uniformly distributed over the state-discrete action space\nx \u00d7ud. the initial policy h0 is identically equal to \u221210 throughout the state space. to\nillustrate the results of lstd-q, figure 3.10 presents the first improved policy found\nby the algorithm, h1, and its approximate q-function, computed with lstd-q. note\nthat this q-function is the second found by lspi; the first q-function evaluates the\ninitial policy h0.\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) policy h1.\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n)\n0\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u2212400\n\n\u2212600\n\n\u2212800\n\n\u22121000\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n2\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n(b) slice through the q-function for u = 0. note\nthe difference in vertical scale from the other q-\nfunctions shown in this chapter.\n\nfigure 3.10\nan early policy and its approximate q-function, for lspi with exact policy improvements.\n\nin this problem, lspi fully converged in 11 iterations. figure 3.11 shows the\nresulting policy and q-function, together with a representative controlled trajectory.\nthe policy and the q-function in figure 3.11 are good approximations of the near-\noptimal solution in figure 3.5.\n\ncompared to the results of grid q-iteration in figure 3.6, lspi needs fewer bfs\n(81 rather than 400 or 160 000) while still being able to find a similarly accurate\napproximation of the policy. this is mainly because the q-function is largely smooth\n(see figure 3.5(a)), and thus can be represented more easily by the wide rbfs of\n\n "}, {"Page_number": 102, "text": "0\n\n\u03b1 [rad]\n\n2\n\n(a) policybh\u2217.\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n92\n\nchapter 3. dp and rl in large and continuous spaces\n\n50\n\n0\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221250\n\n \n\n\u22122\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(b) slice through the q-function for u = 0.\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(c) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nfigure 3.11 results of lspi with exact policy improvements for the dc motor.\n\nthe approximator employed in lspi. in contrast, the grid bfs give a discontinuous\napproximate q-function, which is less appropriate for this problem. although certain\ntypes of continuous bfs can be used with q-iteration, using wide rbfs such as these\nin combination with the least-squares projection (3.14) is unfortunately not possible,\nbecause they do not satisfy the assumptions for convergence, and indeed lead to\ndivergence when they are too wide. the controlled trajectory in figure 3.11(c) is\ncomparable in quality with the trajectory controlled by the fine-grid policy, shown in\nfigure 3.6(f); however, it does produce more chattering.\n\nanother observation is that lspi converged in significantly fewer iterations than\ngrid q-iteration did in section 3.4.5 (12 iterations for lspi, instead of 160 for grid\nq-iteration with the coarse grid, and 123 with the fine grid). such a convergence rate\nadvantage of policy iteration over value iteration is often observed in practice. how-\never, while lspi did converge faster, it was actually more computationally intensive\nthan grid q-iteration: it required approximately 23 s to run, whereas grid q-iteration\nrequired only 0.06 s for the coarse grid and 7.80 s for the fine grid. some insight into\nthis difference can be obtained by examining the asymptotic complexity of the two\n\n "}, {"Page_number": 103, "text": "3.5. approximate policy iteration\n\n93\n\nalgorithms. the complexity of policy evaluation with lstd-q is larger than o(n2)\ndue to solving a linear system of size n. for grid q-iteration, when binary search\nis used to locate the position of a state on the grid, the cost is o(n log(n)), where\nn = nm, n is the number of elements on the grid, and m the number of discrete\nactions. on the other hand, while the convergence of grid q-iteration to a fixed point\nwas guaranteed by the theory, this is not the case for lspi (although for this problem\nlspi did, in fact, fully converge).\n\ncompared to the results of fitted q-iteration in figure 3.7, the lspi solution is\nof a similar quality. lspi introduces some curved artifacts in the policy, due to the\nlimitations of the wide rbfs employed. on the other hand, the execution time of\n2151 s for fitted q-iteration is much larger than the 23 s for lspi.\n\nleast-squares policy iteration with policy approximation\n\nthe aim of the second part of the example is to illustrate the effects of approximating\npolicies. to this end, lspi is modified to work with approximate policies and sample-\nbased, approximate policy improvement.\n\nthe policy approximator is linearly parameterized (3.12) and uses the same rbfs\nas the q-function approximator. such an approximate policy produces continuous\nactions, which must be quantized (into discrete actions belonging to ud) before per-\nforming policy evaluation, because the q-function approximator only works for dis-\ncrete actions. policy improvement is performed with the linear least-squares pro-\ncedure (3.47), using a number ns = 2500 of random, uniformly distributed state\nsamples. the same samples are used at every iteration. as before, policy evaluation\nemploys ns = 7500 samples.\n\nin this experiment, both the q-functions and the policies oscillate in the steady\nstate of the algorithm, with a period of 2 iterations. the execution time until the os-\ncillation was detected was 58 s. the differences between the two distinct policies and\nq-functions on the limit cycle are too small to be noticed in a figure. instead, fig-\nure 3.12 shows the evolution of the policy parameter that changes the most in steady\nstate, for which the oscillation is clearly visible. the appearance of oscillations may\nbe related to the fact that the weaker suboptimality bound (3.50) applies when ap-\nproximate policies are used, rather than the stronger bound (3.51), which applies for\nexact policy improvements.\n\nfigure 3.13 presents one of the two policies from the limit cycle, one of the\nq-functions, and a representative controlled trajectory. the policy and q-function\nhave a similar accuracy to those computed with exact, discrete-action policy im-\nprovements. one advantage of the approximate policy is that it produces continuous\nactions. the beneficial effects of continuous actions on the control performance are\napparent in the trajectory shown in figure 3.13(c), which is very close to the near-\noptimal trajectory of figure 3.5(c).\n\n "}, {"Page_number": 104, "text": "94\n\nchapter 3. dp and rl in large and continuous spaces\n\n10\n\n0\n\n\u03d1\n\n\u221210\n\n\u221220\n\n\u221230\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\niteration\n\nfigure 3.12\nthe variation of one of the policy parameters for lspi with policy approximation on the dc\nmotor.\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n)\n0\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(b) slice through the q-function for u = 0.\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) policy.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(c) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nfigure 3.13 results of lspi with policy approximation for the dc motor.\n\n "}, {"Page_number": 105, "text": "3.6. finding value function approximators automatically\n\n95\n\n3.6 finding value function approximators automatically\n\nparametric approximators of the value function play an important role in approximate\nvalue iteration and approximate policy iteration, as seen in sections 3.4 and 3.5.\ngiven the functional form of such an approximator, the dp/rl algorithm computes\nits parameters. however, there still remains the problem of finding a good functional\nform, well suited to the problem at hand. for concreteness, we will consider linearly\nparameterized approximators (3.3), in which case a good set of bfs has to be found.\nthis focus is motivated by the fact that many methods to find good approximators\nwork in such a linear setting.\n\nthe most straightforward solution is to design the bfs in advance, in which case\ntwo approaches are possible. the first is to design the bfs so that a uniform resolution\nis obtained over the entire state space (for v-functions) or over the entire state-action\nspace (for q-functions). unfortunately, such an approach suffers from the curse of\ndimensionality: the complexity of a uniform approximator grows exponentially with\nthe number of state variables, and in the case of q-functions, also with the number\nof action variables. the second approach is to focus the resolution on certain parts of\nthe state (or state-action) space, where the value function has a more complex shape,\nor where it is more important to approximate it accurately. prior knowledge about\nthe shape of the value function or about the importance of certain regions of the state\n(or state-action) space is necessary in this case. unfortunately, such prior knowledge\nis often nonintuitive and very difficult to obtain without actually computing the value\nfunction.\n\na more general alternative is to devise a method to automatically find bfs suited\nto the problem at hand, rather than designing them manually. two major categories\nof methods to find bfs automatically are bf optimization and bf construction. bf\noptimization methods search for the best placement and shape of a (usually fixed)\nnumber of bfs. bf construction methods are not constrained by a fixed number of\nbfs, but add new or remove old bfs to improve the approximation accuracy. the\nnewly added bfs may have different shapes, or they may all have the same shape.\nseveral subcategories of bf construction can be distinguished, some of the most\nimportant of which are defined next.\n\n\u2022 bf refinement methods work in a top-down fashion. they start with a few bfs\n\n(a coarse resolution) and refine them as needed.\n\n\u2022 bf selection methods work oppositely, in a bottom-up fashion. starting from a\nlarge number of bfs (a fine resolution), they select a small subset of bfs that\nstill ensure a good accuracy.\n\n\u2022 bellman error methods for bf construction define new bfs using the bell-\nman error of the value function represented with the current bfs. the bellman\nerror (or bellman residual) is the difference between the two sides of the bell-\nman equation, where the current value function has been filled in (see also the\nupcoming section 3.6.1 and, e.g., (3.52)).\n\n "}, {"Page_number": 106, "text": "96\n\nchapter 3. dp and rl in large and continuous spaces\n\nfigure 3.14 summarizes this taxonomy.\n\nmethods for automatic\n\nbf discovery\n\nbf optimization\n\nbf refinement\n\nbf construction\n\nbf selection\n\nbellman error\nfor\n\nbf construction\n\nmethods\n\nfigure 3.14 a taxonomy of methods for the automatic discovery of bfs.\n\nin the remainder of this section, we first describe bf optimization, in sec-\ntion 3.6.1, followed by bf construction in section 3.6.2, and by some additional\nremarks in section 3.6.3.\n\n3.6.1 basis function optimization\n\nbf optimization methods search for the best placement and shape of a (typi-\ncally fixed) number of bfs. consider, e.g., the linear parametrization (3.3) of the\nq-function. to optimize the n bfs, they are parameterized by a vector of bf param-\neters \u03be that encodes their locations and shapes. the approximate q-function is:\n\nwhere the parameterized bfs have been denoted by:\n\nbq(x, u) = \u03c6t(x, u;\u03be)\u03b8\n\n\u03c6t(x, u;\u03be) : x \u00d7 u \u2192 r,\n\nl = 1, . . . , n\n\nto highlight their dependence on \u03be. for instance, an rbf is characterized by its center\nand width, so for an rbf approximator, the vector \u03be contains the centers and widths\nof all the rbfs.\n\nthe bf optimization algorithm searches for an optimal parameter vector \u03be\u2217 that\noptimizes a criterion related to the accuracy of the value function approximator.\nmany optimization algorithms can be applied to this problem. for instance, gradient-\nbased optimization has been used for policy evaluation with temporal difference\n(singh et al., 1995), with lstd (menache et al., 2005; bertsekas and yu, 2009),\nand with lspe (bertsekas and yu, 2009). among these works, bertsekas and yu\n(2009) gave a general framework for gradient-based bf optimization in approximate\npolicy evaluation, and provided an efficient recursive procedure to estimate the gra-\ndient. the cross-entropy method has been applied to lstd (menache et al., 2005).\nin chapter 4 of this book, we will employ the cross-entropy method to optimize\napproximators for q-iteration.\n\nthe most widely used optimization criterion (score function) is the bellman er-\nror, also called bellman residual (singh et al., 1995; menache et al., 2005; bertsekas\nand yu, 2009). this error measures how much the estimated value function violates\n\n "}, {"Page_number": 107, "text": "[t h(bqh)](x, u)\u2212bqh(x, u)\nzx\u00d7u(cid:16)[t h(bqh)](x, u)\u2212bqh(x, u)(cid:17)2\nzx\u00d7u(cid:16)[t (bq)](x, u)\u2212bq(x, u)(cid:17)2\n\n3.6. finding value function approximators automatically\n\n97\n\nthe bellman equation, which would be precisely satisfied by the exact value function.\nfor instance, in the context of policy evaluation for a policy h, the bellman error for\n\nan estimate bqh of the q-function qh can be derived from the bellman equation (3.31)\n\n(3.52)\n\nas:\n\nat the state-action pair (x, u), where t h is the policy evaluation mapping. this error\nwas derived from the bellman equation (3.31). a quadratic bellman error over the\nentire state-action space can therefore be defined as:\n\nd(x, u)\n\n(3.53)\n\nin the context of value iteration, the quadratic bellman error for an estimate bq of the\n\noptimal q-function q\u2217 can be defined similarly:\n\nd(x, u)\n\n(3.54)\n\nwhere t is the q-iteration mapping. in practice, approximations of the bellman er-\nrors are computed using a finite set of samples. a weight function can additionally\nbe used to adjust the contribution of the errors according to the importance of each\nregion of the state-action space.\n\nin the context of policy evaluation, the distance between an approximate q-\n\nfunction bqh and qh is related to the infinity norm of the bellman error as follows\n\n(williams and baird, 1994):\n\n1\n\nkbqh \u2212 qhk\u221e \u2264\n\n1\u2212 \u03b3kt h(bqh)\u2212bqhk\u221e\n\na similar result holds in the context of value iteration, where the suboptimality of\n\nan approximate q-function bq satisfies (williams and baird, 1994; bertsekas and\n\ntsitsiklis, 1996, section 6.10):\n\n1\n\nkbq\u2212 q\u2217k\u221e \u2264\n\n1\u2212 \u03b3kt (bq)\u2212bqk\u221e\n\nfurthermore, the suboptimality of bq is related to the suboptimality of the resulting\n\npolicy by (3.25), hence, in principle, minimizing the bellman error is useful. how-\never, in practice, quadratic bellman errors (3.53), (3.54) are often employed. because\nminimizing such quadratic errors may still lead to large infinity-norm bellman errors,\nit is unfortunately unclear whether this procedure leads to accurate q-functions.\n\nother optimization criteria can, of course, be used. for instance, in approximate\nvalue iteration, the return of the policy obtained by the dp/rl algorithm can be\ndirectly maximized:\n\nw(x0)rh(x0)\n\n\u2211\nx0\u2208x0\n\n(3.55)\n\nwhere h is obtained by running approximate value iteration to (near-)convergence\n\n "}, {"Page_number": 108, "text": "98\n\nchapter 3. dp and rl in large and continuous spaces\n\nusing the current approximator, x0 is a finite set of representative initial states, and w :\nx0 \u2192 (0, \u221e) is a weight function. the set x0 and the weight function w determine the\nperformance of the resulting policy, and an appropriate choice of x0 and w depends\non the problem at hand. the returns rh(x0) can be estimated by simulation, as in\napproximate policy search, see section 3.7.2.\n\nin approximate policy evaluation, if accurate q-values qh(xls, uls) can be ob-\ntained for a set of ns samples (xls, uls), then the following error measure can be min-\nimized instead of the bellman error (menache et al., 2005; bertsekas and yu, 2009):\n\nns\n\u2211\n\nls=1(cid:16)qh(xls, uls)\u2212bqh(xls, uls)(cid:17)2\n\nthe q-values qh(xls, uls) can be obtained by simulation, as explained in sec-\ntion 3.5.4.\n\n3.6.2 basis function construction\n\nfrom the class of bf construction methods, we discuss in turn bf refinement, bf\nselection, and bellman error methods for bf construction (see again figure 3.14).\nadditionally, we explain how some nonparametric approximators can be seen as\ntechniques to construct bfs automatically.\n\nbasis function refinement\n\nbf refinement is a widely used subclass of bf construction methods. refinement\nmethods work in a top-down fashion, by starting with a few bfs (a coarse resolution)\nand refining them as needed. they can be further classified into two categories:\n\n\u2022 local refinement (splitting) methods evaluate whether the value function is\nrepresented with a sufficient accuracy in a particular region of the state space\n(corresponding to one or several neighboring bfs), and add new bfs when the\naccuracy is deemed insufficient. such methods have been proposed, e.g., for\nq-learning (reynolds, 2000; ratitch and precup, 2004; waldock and carse,\n2008), v-iteration (munos and moore, 2002), and q-iteration (munos, 1997;\nuther and veloso, 1998).\n\n\u2022 global refinement methods evaluate the global accuracy of the representation\nand, if the accuracy is deemed insufficient, they refine the bfs using various\ntechniques. all the bfs may be refined uniformly (chow and tsitsiklis, 1991),\nor the algorithm may decide that certain regions of the state space require\nmore resolution (munos and moore, 2002; gr\u00a8une, 2004). for instance, chow\nand tsitsiklis (1991); munos and moore (2002); and gr\u00a8une (2004) applied\nglobal refinement to v-iteration, while szepesv\u00b4ari and smart (2004) used it for\nq-learning.\n\na variety of criteria are used to decide when the bfs should be refined. an\noverview of typical criteria, and a comparison between them in the context of\n\n "}, {"Page_number": 109, "text": "3.6. finding value function approximators automatically\n\n99\n\nv-iteration, was given by munos and moore (2002). for instance, local refinement\nin a certain region can be performed:\n\n\u2022 when the value function is not (approximately) constant in that region (munos\n\nand moore, 2002; waldock and carse, 2008);\n\n\u2022 when the value function is not (approximately) linear in that region (munos\n\nand moore, 2002; munos, 1997);\n\n\u2022 when the bellman error (see section 3.6.1) is large in that region (gr\u00a8une,\n\n2004);\n\n\u2022 using various other heuristics (uther and veloso, 1998; ratitch and precup,\n\n2004).\n\nglobal refinement can be performed, e.g., until a desired level of solution accu-\nracy is met (chow and tsitsiklis, 1991). the approach of munos and moore (2002)\nworks for discrete-action problems, and globally identifies the regions of the state\nspace that must be more accurately approximated to find a better policy. to this end,\nit refines regions that satisfy two conditions: (i) the v-function is poorly approxi-\nmated in these regions, and (ii) this poor approximation affects, in a certain sense,\n(other) regions where the actions that are dictated by the policy change.\n\nbf refinement methods increase the memory and computational demands of the\ndp/rl algorithm when they increase the resolution. thus, care must be taken to\nprevent the memory and computation costs from becoming prohibitive, especially in\nthe online case. this is an important concern in both approximate dp and approxi-\nmate rl. equally important in approximate rl are the restrictions imposed on bf\nrefinement by the limited amount of data available. increasing the power of the ap-\nproximator means that more data will be required to compute an accurate solution,\nso the resolution cannot be refined to arbitrary levels for a given amount of data.\n\nbasis function selection\n\nbf selection methods work in a bottom-up fashion, by starting from a large number\nof bfs (a fine resolution), and then selecting a smaller subset of bfs that still provide\na good accuracy. when using this type of methods, care should be taken to ensure\nthat selecting the bfs and running the dp/rl algorithm with the selected bfs is less\nexpensive than running the dp/rl algorithm with the original bfs. the cost may be\nexpressed in terms of computational complexity or in terms of the number of samples\nrequired.\n\nkolter and ng (2009) employed regularization to select bfs for policy evaluation\nwith lstd. regularization is a technique that penalizes functional complexity in the\napproximate value function. in practice, the effect of regularization in the linear case\nis to drive some of the value function parameters (close) to 0, which means that the\ncorresponding bfs can be ignored. by incrementally selecting the bfs, kolter and\nng (2009) obtained a computational complexity that is linear in the total number of\nbfs, in contrast to the original complexity of lstd which is at least quadratic (see\nsection 3.5.2).\n\n "}, {"Page_number": 110, "text": "100\n\nchapter 3. dp and rl in large and continuous spaces\n\nbellman error basis functions\n\nanother class of bf construction approaches define new bfs by employing the bell-\nman error of the value function represented with the currently available bfs (3.53),\n(3.54). for instance, bertsekas and casta\u02dcnon (1989) proposed a method to interleave\nautomatic state aggregation steps with iterations of a model-based policy evaluation\nalgorithm. the aggregation steps group together states with similar bellman errors.\nin this work, convergence speed was the main concern, rather than limited represen-\ntation power, so the value function and the bellman error function were assumed to\nbe exactly representable.\n\nmore recently, keller et al. (2006) proposed a method that follows similar\nlines, but that explicitly addresses the approximate case, by combining lstd with\nbellman-error based bf construction. at every bf construction step, this method\ncomputes a linear projection of the state space onto a space in which points with\nsimilar bellman errors are close to each other. several new bfs are defined in this\nprojected space. then, the augmented set of bfs is used to generate a new lstd so-\nlution, and the cycle repeats. parr et al. (2008) showed that in policy evaluation with\nlinear parametrization, the bellman error can be decomposed into two components: a\ntransition error component and a reward error component, and proposed adding new\nbfs defined in terms of these error components.\n\nnonparametric approximators as methods for basis function construction\n\nas previously explained in section 3.3, some nonparametric approximators can be\nseen as methods to automatically generate bfs from the data. a typical example is\nkernel-based approximation, which, in its original form, generates a bf for every\nsample considered. an interesting effect of nonparametric approximators is that they\nadapt the complexity of the approximator to the amount of available data, which is\nbeneficial in situations where obtaining data is costly.\n\nwhen techniques to control the complexity of the nonparametric approximator\nare applied, they can sometimes be viewed as bf selection. for instance, regular-\nization techniques were used in lstd-q by farahmand et al. (2009a) and in fitted\nq-iteration by farahmand et al. (2009b). (in both of these cases, however, the advan-\ntage of regularization is a reduced functional complexity of the solution, while the\ncomputational complexity is not reduced.) kernel sparsification techniques also fall\nin this category (xu et al., 2007; engel et al., 2003, 2005), as well as sample selection\nmethods for regression tree approximators (ernst, 2005).\n\n3.6.3 remarks\n\nsome of the methods for automatic bf discovery work offline (e.g., menache et al.,\n2005; mahadevan and maggioni, 2007), while others adapt the bfs while the dp/rl\nalgorithm is running (e.g., munos and moore, 2002; ratitch and precup, 2004). since\nconvergence guarantees for approximate value iteration and approximate policy eval-\nuation typically rely on a fixed set of bfs, adapting the bfs online invalidates these\nguarantees. convergence guarantees can be recovered by ensuring that bf adaptation\n\n "}, {"Page_number": 111, "text": "3.7. approximate policy search\n\n101\n\nis stopped after a finite number of updates; fixed-bf proofs can then be applied to\nguarantee asymptotic convergence (ernst et al., 2005).\n\nthe presentation above has not been exhaustive, and bfs can also be found using\nvarious other methods. for instance, in (mahadevan, 2005; mahadevan and mag-\ngioni, 2007), a spectral analysis of the mdp transition dynamics is performed to\nfind bfs for use with lspi. because the bfs represent the underlying topology of\nthe state transitions, they provide a good accuracy in representing the value func-\ntion. moreover, while we have focused above on the popular approach of finding\nlinearly parameterized approximators, nonlinearly parameterized approximators can\nalso be found automatically. for example, whiteson and stone (2006) introduced an\napproach to optimize the parameters and the structure of neural network approxima-\ntors for a tailored variant of q-learning. this approach works in episodic tasks, and\noptimizes the total reward accumulated along episodes.\n\nfinally, note that a fully worked-out example of finding an approximator auto-\nmatically is beyond the scope of this chapter. instead, we direct the interested reader\nto section 4.4, where an approach to optimize the approximator for a value itera-\ntion algorithm is developed in detail, and to section 4.5.4, where this approach is\nempirically evaluated.\n\n3.7 approximate policy search\n\nalgorithms for approximate policy search represent the policy approximately, most\noften using a parametric approximator. an optimal parameter vector is then sought\nusing optimization techniques. in some special cases, the policy parametrization may\nrepresent an optimal policy exactly. for instance, when the transition dynamics are\nlinear in the state and action variables and the reward function is quadratic, the op-\ntimal policy is linear in the state variables. so, a linear parametrization in the state\nvariables can exactly represent this optimal policy. however, in general, optimal poli-\ncies can only be represented approximately.\n\nfigure 3.15 (repeated from the relevant part of figure 3.2) shows in a graphical\nform how our upcoming presentation of approximate policy search is organized. in\nsection 3.7.1, gradient-based methods for policy search are described, including the\nimportant category of actor-critic techniques. then, in section 3.7.2, gradient-free\npolicy optimization methods are discussed.\n\napproximate\npolicy search\n\ngradient-based policy search,\nactor-critic methods\n\ngradient-free policy search\n\nfigure 3.15\nthe organization of the algorithms for approximate policy search presented next.\n\n "}, {"Page_number": 112, "text": "102\n\nchapter 3. dp and rl in large and continuous spaces\n\nhaving completed our review, we then provide a numerical example involving\n\npolicy search for a dc motor in section 3.7.3.\n\n3.7.1 policy gradient and actor-critic algorithms\n\nan important class of methods for approximate policy search relies on gradient-\nbased optimization. in such policy gradient methods, the policy is represented using\na differentiable parametrization, and gradient updates are performed to find param-\neters that lead to (locally) maximal returns. some policy gradient methods estimate\nthe gradient without using a value function (marbach and tsitsiklis, 2003; munos,\n2006; riedmiller et al., 2007). other methods compute an approximate value func-\ntion of the current policy and use it to form the gradient estimate. these are called\nactor-critic methods, where the actor is the approximate policy and the critic is the\napproximate value function. by extension, policy gradient methods that do not use\nvalue functions are sometimes called actor-only methods (bertsekas, 2007, section\n6.7).\n\nactor-critic algorithms were introduced by barto et al. (1983) and have been in-\nvestigated often since then (berenji and khedkar, 1992; sutton et al., 2000; konda\nand tsitsiklis, 2003; berenji and vengerov, 2003; borkar, 2005; nakamura et al.,\n2007). many actor-critic algorithms approximate the policy and the value function\nusing neural networks (prokhorov and wunsch, 1997; p\u00b4erez-uribe, 2001; liu et al.,\n2008). actor-critic methods are similar to policy iteration, which also improves the\npolicy on the basis of its value function. the main difference is that in policy iter-\nation, the improved policy is greedy in the value function, i.e., it fully maximizes\nthis value function over the action variables (3.46). in contrast, actor-critic methods\nemploy gradient rules to update the policy in a direction that increases the received\nreturns. the gradient estimate is constructed using the value function.\n\nsome important results for policy gradient methods have been developed under\nthe expected average return criterion for optimality. we therefore discuss this set-\nting first, in a temporary departure from the main focus of the book, which is the\ndiscounted return. we then return to the discounted setting, and present an online\nactor-critic algorithm for this setting.\n\npolicy gradient and actor-critic methods for average returns\n\npolicy gradient and actor-critic methods have often been given in the average return\nsetting (see also section 2.2.1). we therefore introduce these methods in the average-\nreturn case, mainly following the derivation of bertsekas (2007, section 6.7). we\nassume that the mdp has a finite state-action space, but under appropriate condi-\ntions these methods can also be extended to continuous state-action spaces (see, e.g.,\nkonda and tsitsiklis, 2003).\n\nconsider a stochastic mdp with a finite state space x = {x1, . . . , x \u00afn}, a finite\naction space u = {u1, . . . , u \u00afm}, a transition function \u00aff of the form (2.14), and a re-\nward function \u02dc\u03c1. a stochastic policy of the form \u02dch : x \u00d7 u \u2192 [0, 1] is employed,\nparameterized by the vector \u03d1 \u2208 rn . this policy takes an action u in state x with the\n\n "}, {"Page_number": 113, "text": "3.7. approximate policy search\n\n103\n\nprobability:\n\np (u| x) = \u02dch(x, u;\u03d1)\n\nthe functional dependence of the policy on the parameter vector must be designed\nin advance, and must be differentiable.\n\nthe expected average return of state x0 under the policy parameterized by \u03d1 is:\n\nr\u03d1(x0) = lim\nk\u2192\u221e\n\n1\n\nk\n\nxk+1\u223c \u00aff (xk,uk,\u00b7)( k\n\ne uk\u223c\u02dch(xk,\u00b7;\u03d1)\n\n\u2211\nk=0\n\n\u02dc\u03c1(xk, uk, xk+1))\n\nnote that we have directly highlighted the dependence of the return on the parameter\nvector \u03d1, rather than on the policy \u02dch. a similar notation will be used for other policy-\ndependent quantities in this section.\n\nunder certain conditions (see, e.g., bertsekas, 2007, chapter 4), the average re-\n\nturn is the same for every initial state, i.e., r\u03d1(x0) = r\u03d1 for all x0 \u2208 x , and together\nwith the so-called differential v-function, v \u03d1 : x \u2192 r, satisfies the bellman equa-\n\ntion:\n\nr\u03d1 + v \u03d1(xi) = \u02dc\u03c1\u03d1(xi) +\n\n\u00afn\n\u2211\ni\u2032=1\n\n\u00aff \u03d1(xi, xi\u2032)v \u03d1(xi\u2032)\n\n(3.56)\n\nthe differential value of a state x can be interpreted as the expected excess return,\non top of the average return, obtained from x (konda and tsitsiklis, 2003). the other\nquantities appearing in (3.56) are defined as follows:\n\n\u2022 \u00aff \u03d1 : x \u00d7 x \u2192 [0, 1] gives the state transition probabilities under the policy\nconsidered, from which the influence of the actions has been integrated out.12\nthese probabilities can be computed with:\n\n\u00aff \u03d1(xi, xi\u2032) =\n\n\u00afm\n\u2211\n\nj=1(cid:2)\u02dch(xi, u j;\u03d1) \u00aff (xi, u j, xi\u2032)(cid:3)\n\n\u2022 \u02dc\u03c1\u03d1 : x \u2192 r gives the expected rewards obtained from every state by the policy\n\nconsidered, and can be computed with:\n\n\u02dc\u03c1\u03d1(xi) =\n\n\u00afm\n\u2211\n\nj=1\"\u02dch(xi, u j;\u03d1)\n\n\u00afn\n\u2211\n\ni\u2032=1(cid:0) \u00aff (xi, u j, xi\u2032) \u02dc\u03c1(xi, u j, xi\u2032)(cid:1)#\n\npolicy gradient methods aim to find a (locally) optimal policy within the class\nof parameterized policies considered. an optimal policy maximizes the average re-\nturn, which is the same for every initial state. so, a parameter vector that (locally)\n\n12for simplicity, a slight abuse of notation is made by using \u00aff to denote both the original transition\nfunction and the transition probabilities from which the actions have been factored out. similarly, the\nexpected rewards are denoted by \u02dc\u03c1, like the original reward function.\n\n "}, {"Page_number": 114, "text": "104\n\nchapter 3. dp and rl in large and continuous spaces\n\nmaximizes the average return must be found. to this end, policy gradient methods\nperform gradient ascent on the average return:\n\n\u03d1 \u2190 \u03d1 + \u03b1\n\n\u2202r\u03d1\n\u2202\u03d1\n\n(3.57)\n\nwhere \u03b1 is the step size. when a local optimum has been reached, the gradient is\nzero, i.e., \u2202r\u03d1\n\n\u2202\u03d1 = 0.\n\nthe core problem is to estimate the gradient \u2202r\u03d1\n\n\u2202\u03d1 . by differentiating the bellman\nequation (3.56) with respect to \u03d1 and after some calculations (see bertsekas, 2007,\nsection 6.7), the following formula for the gradient is obtained:\n\n\u2202r\u03d1\n\u2202\u03d1\n\n=\n\n\u03b6\u03d1(xi)\"\u2202 \u02dc\u03c1\u03d1(xi)\n\n\u2202\u03d1\n\n\u00afn\n\u2211\ni=1\n\n+\n\n\u00afn\n\u2211\n\ni\u2032=1(cid:18)\u2202 \u00aff \u03d1(xi, xi\u2032)\n\n\u2202\u03d1\n\nv \u03d1(xi\u2032)(cid:19)#\n\n(3.58)\n\nwhere \u03b6\u03d1(xi) is the steady-state probability of encountering the state xi when us-\ning the policy given by \u03d1. note that all the gradients in (3.58) are n -dimensional\nvectors.\n\nthe right-hand side of (3.58) can be estimated using simulation, as proposed,\ne.g., by marbach and tsitsiklis (2003), and the convergence of the resulting pol-\nicy gradient algorithms to a locally optimal parameter vector can be ensured under\nmild conditions. an important concern is controlling the variance of the gradient es-\ntimate, and marbach and tsitsiklis (2003) focused on this problem. munos (2006)\nconsidered policy gradient methods in the continuous-time setting. because the usual\nmethods to estimate the gradient lead to a variance that grows very large as the sam-\npling time decreases, other methods are necessary to keep the variance small in the\ncontinuous-time case (munos, 2006).\n\nactor-critic methods explicitly approximate the v-function in (3.58). this ap-\nproximate v-function can be found, e.g., by using variants of the td, lstd, and\nlspe techniques adapted to the average return setting (bertsekas, 2007, section 6.6).\nthe gradient can also be expressed in terms of a q-function, which can be defined\n\nin the average return setting by using the differential v-function, as follows:\n\nq\u03d1(xi, u j) =\n\n\u00afn\n\u2211\n\ni\u2032=1h \u00aff (xi, u j, xi\u2032 )(cid:16) \u02dc\u03c1(xi, u j, xi\u2032)\u2212 r\u03d1 + v \u03d1(xi\u2032)(cid:17)i\n\nusing the q-function, the gradient of the average return can be written as (sutton\net al., 2000; konda and tsitsiklis, 2000, 2003):\n\n\u2202r\u03d1\n\u2202\u03d1\n\n=\n\n\u00afn\n\u2211\ni=1\n\n\u00afm\n\u2211\n\nj=1hw\u03d1(xi, u j)q\u03d1(xi, u j)\u03c6\u03d1(xi, u j)i\n\n(3.59)\n\nwhere w\u03d1(xi, u j) = \u03b6\u03d1(xi)\u02dch(xi, u j;\u03d1) is the steady-state probability of encountering\nthe state-action pair (xi, u j) when using the policy considered, and:\n\n\u03c6\u03d1 : x \u00d7 u \u2192 rn , \u03c6\u03d1(xi, u j) =\n\n1\n\n\u2202 \u02dch(xi, u j;\u03d1)\n\n\u02dch(xi, u j;\u03d1)\n\n\u2202\u03d1\n\n(3.60)\n\n "}, {"Page_number": 115, "text": "3.7. approximate policy search\n\n105\n\nthe function \u03c6\u03d1 is regarded as a vector of state-action bfs, for reasons that will\nbecome clear shortly. it can be shown that (3.59) is equal to (sutton et al., 2000;\nkonda and tsitsiklis, 2003):\n\n\u2202r\u03d1\n\u2202\u03d1\n\n=\n\n\u00afn\n\u2211\ni=1\n\n\u00afm\n\u2211\n\nj=1hw(xi, u j)[pw\u03d1\n\n(q\u03d1)](xi, u j)\u03c6\u03d1(xi, u j)i\n\nwhere the exact q-function has been substituted by its weighted least-squares pro-\njection (3.35) onto the space spanned by the bfs \u03c6\u03d1. so, in order to find the exact\ngradient, it is sufficient to compute an approximate q-function \u2013 provided that the\nbfs \u03c6\u03d1, computed with (3.60) from the policy parametrization, are used. in the liter-\nature, such bfs are sometimes called \u201ccompatible\u201d with the policy parametrization\n(sutton et al., 2000) or \u201cessential features\u201d (bertsekas, 2007, section 6.7). note that\nother bfs can be used in addition to these.\n\nusing this property, actor-critic algorithms that linearly approximate the q-\nfunction using the bfs (3.60) can be given. these algorithms converge to a locally\noptimal policy, as shown by sutton et al. (2000); konda and tsitsiklis (2000, 2003).\nkonda and tsitsiklis (2003) additionally extended their analysis to the case of con-\ntinuous state-action spaces. this theoretical framework was used by berenji and\nvengerov (2003) to prove the convergence of an actor-critic algorithm relying on\nfuzzy approximation.\n\nkakade (2001) proposed an improvement to the gradient update formula (3.57),\nby scaling it with the inverse of the (expected) fisher information matrix of the\nstochastic policy (schervish, 1995, section 2.3.1), and thereby obtaining the so-\ncalled natural policy gradient. peters and schaal (2008) and bhatnagar et al. (2009)\nemployed this idea to develop some natural actor-critic algorithms. riedmiller et al.\n(2007) provided an experimental comparison of several policy gradient methods, in-\ncluding the natural policy gradient.\n\nan online actor-critic algorithm for discounted returns\n\nwe now come back to the discounted return criterion for optimality, and describe\nan actor-critic algorithm for this discounted setting (rather than in the average-return\nsetting, as above). this algorithm works online, in problems with continuous states\n\nand actions. denote bybh(x;\u03d1) the (deterministic) approximate policy, parameterized\nby \u03d1 \u2208 rn , and bybv (x;\u03b8) the approximate v-function, parameterized by \u03b8 \u2208 rn .\n\nthe algorithm does not distinguish between the value functions of different policies,\nso the value function notation is not superscripted by the policy. although a deter-\nministic approximate policy is considered, a stochastic policy could also be used.\n\nat each time step, an action uk is chosen by adding a random, exploratory term to\n\nthe action recommended by the policybh(x;\u03d1). this term could be drawn, e.g., from a\n\nzero-mean gaussian distribution. after the transition from xk to xk+1, an approximate\ntemporal difference is computed with:\n\nthis temporal difference can be obtained from the bellman equation for the policy\n\n\u03b4td,k = rk+1 + \u03b3bv (xk+1;\u03b8k)\u2212bv (xk;\u03b8k)\n\n "}, {"Page_number": 116, "text": "106\n\nchapter 3. dp and rl in large and continuous spaces\n\nv-function (2.20). it is analogous to the temporal difference for q-functions, used,\ne.g., in approximate sarsa (algorithm 3.12). once the temporal difference is com-\nputed, the policy and v-function parameters are updated with the following gradient\nformulas:\n\n\u03d1k+1 = \u03d1k + \u03b1a,k\n\n\u03b8k+1 = \u03b8k + \u03b1c,k\n\n\u2202\u03d1\n\n\u2202bh(xk;\u03d1k)\n\u2202bv (xk;\u03b8k)\n\n\u2202\u03b8\n\n[uk \u2212bh(xk;\u03d1k)]\u03b4td,k\n\n\u03b4td,k\n\n(3.61)\n\n(3.62)\n\nwhere \u03b1a,k and \u03b1c,k are the (possibly time-varying) step sizes for the actor and the\ncritic, respectively. note that the action signal is assumed to be scalar, but the method\ncan be extended to multiple action variables.\n\nin the actor update (3.61), due to exploration, the actual action uk applied at step\nk can be different from the action recommended by the policy. when the exploratory\naction uk leads to a positive temporal difference, the policy is adjusted towards this\naction. conversely, when \u03b4td,k is negative, the policy is adjusted away from uk. this\nis because the temporal difference is interpreted as a correction of the predicted per-\nformance, so that, e.g., if the temporal difference is positive, the obtained perfor-\nmance is considered to be better than the predicted one. in the critic update (3.62), the\n\nequation (2.20), thus leading to the temporal difference.\n\nv (xk) is the exact value of xk, given the current policy. since this exact value is not\n\ntemporal difference takes the place of the prediction error v (xk)\u2212bv (xk;\u03b8k), where\navailable, it is replaced by the estimate rk+1 +\u03b3bv (xk+1;\u03b8k) suggested by the bellman\n\nthis actor-critic method is summarized in algorithm 3.13, which generates ex-\nploratory actions using a gaussian density with a standard deviation that can vary\nover time.\n\nalgorithm 3.13 actor-critic with gaussian exploration.\ninput: discount factor \u03b3,\n\nk=0,(cid:8)\u03b1c,k(cid:9)\u221e\n\nk=0\n\npolicy parametrizationbh, v-function parametrizationbv ,\nk=0, step size schedules(cid:8)\u03b1a,k(cid:9)\u221e\nexploration schedule {\u03c3k}\u221e\n\n1: initialize parameter vectors, e.g., \u03d10 \u2190 0, \u03b80 \u2190 0\n2: measure initial state x0\n3: for every time step k = 0, 1, 2, . . . do\n4:\n\napply uk, measure next state xk+1 and reward rk+1\n\nuk \u2190bh(xk;\u03d1k) + \u00afu, where \u00afu \u223c n (0,\u03c3k)\n\u03b4td,k = rk+1 + \u03b3bv (xk+1;\u03b8k)\u2212bv (xk;\u03b8k)\n\n\u03d1k+1 = \u03d1k + \u03b1a,k\n\u03b8k+1 = \u03b8k + \u03b1c,k\n\n\u2202\u03d1\n\u2202\u03b8 \u03b4td,k\n\n[uk \u2212bh(xk;\u03d1k)]\u03b4td,k\n\n\u2202bh(xk;\u03d1k)\n\u2202bv (xk;\u03b8k)\n\n8:\n9: end for\n\n5:\n\n6:\n\n7:\n\n "}, {"Page_number": 117, "text": "3.7. approximate policy search\n\n107\n\n3.7.2 gradient-free policy search\n\ngradient-based policy optimization is based on the assumption that the locally op-\ntimal parameters found by the gradient method are good enough. this may be\ntrue when the policy parametrization is simple and well suited to the problem at\nhand. however, in order to design such a parametrization, prior knowledge about a\n(near-)optimal policy is required.\n\nwhen prior knowledge about\n\nthe policy is not available, a richer policy\nparametrization must be used. in this case, the optimization criterion is likely to have\nmany local optima, and may also be nondifferentiable. this means that gradient-\nbased algorithms are unsuitable, and global, gradient-free optimization algorithms\nare required. even when a simple policy parametrization can be designed, global\noptimization can help by avoiding local optima.\n\nconsider the dp/rl problem under the expected discounted return criterion. de-\n\nnote bybh(x;\u03d1) the approximate policy, parameterized by \u03d1 \u2208 rn . policy search\nalgorithms look for an optimal parameter vector that maximizes the return rbh(\u00b7;\u03d1)(x)\nfor all x \u2208 x . when x is large or continuous, computing the return for every ini-\ntial state is not possible. a practical procedure to circumvent this difficulty requires\nchoosing a finite set x0 of representative initial states. returns are estimated only\nfor the states in x0, and the score function (optimization criterion) is the weighted\naverage return over these states:\n\ns(\u03d1) = \u2211\nx0\u2208x0\n\nw(x0)rbh(\u00b7;\u03d1)(x0)\n\n(3.63)\n\nwhere w : x0 \u2192 (0, 1] is the weight function.13 the return from each representative\nstate is estimated by simulation. a number of nmc \u2265 1 independent trajectories are\nsimulated from every representative state, and an estimate of the expected return is\nobtained by averaging the returns obtained along these sample trajectories:\n\nrbh(\u00b7;\u03d1)(x0) =\n\n1\n\nnmc\n\nnmc\n\u2211\ni0=1\n\nk\n\n\u2211\nk=0\n\n\u03b3k \u02dc\u03c1(xi0,k, h(xi0,k;\u03d1), xi0,k+1)\n\n(3.64)\n\nfor each trajectory i0, the initial state xi0,0 is equal to x0, and actions are chosen with\nthe policy h, which means that for k \u2265 0:\n\nxi0,k+1 \u223c f (xi0,k, h(xi0,k;\u03d1),\u00b7)\n\nif the system is deterministic, a single trajectory suffices, i.e., nmc = 1. in the stochas-\ntic case, a good value for nmc will depend on the problem at hand. note that this\nmonte carlo estimation procedure is similar to a rollout (3.45).\n\nthe infinite-horizon return is approximated by truncating each simulated trajec-\ntory after k steps. a value of k that guarantees that this truncation introduces an\n\n13more generally, a density \u02dcw over the initial states can be considered, and the score function is then\n\nex0\u223c \u02dcw(\u00b7)nrh(\u00b7;\u03be,\u03d1)(x0)o, i.e., the expected value of the return when x0 \u223c \u02dcw(\u00b7).\n\n "}, {"Page_number": 118, "text": "108\n\nchapter 3. dp and rl in large and continuous spaces\n\nerror of at most \u03b5mc > 0 can be chosen using (2.41), repeated here:\n\nk =(cid:24)log\u03b3\n\n\u03b5mc(1\u2212 \u03b3)\n\nk \u02dc\u03c1k\u221e\n\n(cid:25)\n\n(3.65)\n\nin the stochastic context, ng and jordan (2000) assumed the availability of a\nsimulation model that offers access to the random variables driving the stochastic\ntransitions. they proposed to pregenerate sequences of values for these random vari-\nables, and to use the same sequences when evaluating every policy. this leads to a\ndeterministic optimization problem.\n\nrepresentative set of initial states and weight function. the set x0 of represen-\ntative states, together with the weight function w, determines the performance of the\nresulting policy. of course, this performance is in general only approximately op-\ntimal, since maximizing the returns from states in x0 cannot guarantee that returns\nfrom other states in x are maximal. a good choice of x0 and w will depend on the\nproblem at hand. for instance, if the process only needs to be controlled starting\nfrom a known set xinit of initial states, then x0 should be equal to xinit, or included\nin it when xinit is too large. initial states that are deemed more important can be as-\nsigned larger weights. when all initial states are equally important, the elements of\nx0 should be uniformly spread over the state space and identical weights equal to 1\n|x0|\nshould be assigned to every element of x0.\n\na wide range of gradient-free, global optimization techniques can be employed\nin policy search, including evolutionary optimization (e.g., genetic algorithms, see\ngoldberg, 1989), tabu search (glover and laguna, 1997), pattern search (torczon,\n1997; lewis and torczon, 2000), the cross-entropy method (rubinstein and kroese,\n2004), etc. for instance, evolutionary computation was applied to policy search by\nbarash (1999); chin and jafari (1998); gomez et al. (2006); chang et al. (2007,\nchapter 3), and cross-entropy optimization was applied by mannor et al. (2003).\nchang et al. (2007, chapter 4) described an approach to find a policy by using the\nso-called \u201cmodel-reference adaptive search,\u201d which is closely related to the cross-\nentropy method. in chapter 6 of this book, we will employ the cross-entropy method\nto develop a policy search algorithm. a dedicated algorithm that optimizes the pa-\nrameters and structure of neural network policy approximators was given by white-\nson and stone (2006). general policy modification heuristics were proposed by\nschmidhuber (2000).\n\nin another class of model-based policy search approaches, near-optimal actions\nare sought online, by executing at every time step a search over open-loop sequences\nof actions (hren and munos, 2008). the controller selects a sequence leading to\na maximal estimated return and applies the first action in this sequence. then, the\nentire cycle repeats.14 the total number of open-loop action sequences grows expo-\nnentially with the time horizon considered, but by limiting the search to promising\nsequences only, such an approach can avoid incurring excessive computational costs.\n\n14this is very similar to how model-predictive control works (maciejowski, 2002; camacho and bor-\n\ndons, 2004).\n\n "}, {"Page_number": 119, "text": "3.7. approximate policy search\n\n109\n\nhren and munos (2008) studied this method of limiting the computational cost in\na deterministic setting. in a stochastic setting, open-loop sequences are suboptimal.\nhowever, some approaches exist to extend this open-loop philosophy to the stochas-\ntic case. these approaches model the sequences of random transitions by scenario\ntrees (birge and louveaux, 1997; dupacov\u00b4a et al., 2000) and optimize the actions\nattached to the tree nodes (defourny et al., 2008, 2009).\n\n3.7.3 example: gradient-free policy search for a dc motor\n\nin this example, approximate, gradient-free policy search is applied to the dc mo-\ntor problem introduced in section 3.4.5. in a first experiment, a general policy\nparametrization is used that does not rely on prior knowledge, whereas in a second\nexperiment, a tailored policy parametrization is derived from prior knowledge. the\nresults obtained with these two parametrizations are compared.\n\nto compute the score function (3.63), a set x0 of representative states and a\nweight function w have to be selected. we aim to obtain a uniform performance\nacross the state space, so a regular grid of representative states is chosen:\n\nx0 = {\u2212\u03c0,\u22122\u03c0/3,\u2212\u03c0/3, . . . ,\u03c0}\u00d7{\u221216\u03c0,\u221212\u03c0,\u22128\u03c0, . . . , 16\u03c0}\n\nand these initial states are weighted uniformly by w(x0) = 1\n, where the number\n|x0|\nof states is |x0| = 63. a maximum error \u03b5mc = 0.01 is imposed in the estimation of\nthe return. a bound on the reward function (3.28) for the dc motor problem can be\ncomputed with:\n\nk\u03c1k\u221e = sup\n\nx,u(cid:12)(cid:12)\u2212xt\n=(cid:12)(cid:12)(cid:12)(cid:12)\u2212[\u03c0 16\u03c0](cid:20)5\n\nk(cid:12)(cid:12)\nk qrewxk \u2212 rrewu2\n16\u03c0(cid:21)\u2212 0.01\u00b7 102(cid:12)(cid:12)(cid:12)(cid:12)\n0 0.01(cid:21)(cid:20) \u03c0\n\n\u2248 75.61\n\n0\n\nto find the trajectory length k required to achieve the precision \u03b5mc, the values of\n\u03b5mc, k\u03c1k\u221e, and \u03b3 = 0.95 are substituted into (3.65); this yields k = 233. because the\nproblem is deterministic, simulating multiple trajectories from every initial state is\nnot necessary; instead, a single trajectory from every initial state will suffice.\n\nwe use the global, gradient-free pattern search algorithm to optimize the policy\n(torczon, 1997; lewis and torczon, 2000). the algorithm is considered convergent\nwhen the score variation decreases below the threshold \u03b5ps = 0.01 (equal to \u03b5mc).15\n\npolicy search with a general parametrization\n\nconsider first the case in which no prior knowledge about the optimal policy is avail-\nable, which means that a general policy parametrization must be used. the linear\n\n15we use the pattern search algorithm from the genetic algorithm and direct search toolbox of\nmatlab 7.4.0. the algorithm is configured to use the threshold \u03b5ps and to cache the score values for\nthe parameter vectors it already evaluated, in order to avoid recomputing them. besides these changes, the\ndefault settings of the algorithm are employed.\n\n "}, {"Page_number": 120, "text": "110\n\nchapter 3. dp and rl in large and continuous spaces\n\npolicy parametrization (3.12) is chosen:\n\nn\n\n\u2211\ni=1\n\n\u03d5i(x)\u03d1i = \u03d5t(x)\u03d1\n\nbh(x) =\n\naxis-aligned, normalized rbfs (see example 3.1) are defined, with their centers\narranged on an equidistant 7\u00d7 7 grid in the state space. all the rbfs are identical\n2/2, where b\u2032d is\nin shape, and their width bd along each dimension d is equal to b\u2032d\nthe distance between adjacent rbfs along that dimension (the grid step). namely,\nb\u20321 = 2\u03c0\n7\u22121 \u2248 1.05 and b\u20322 = 32\u03c0\n7\u22121 \u2248 16.76, which lead to b1 \u2248 0.55 and b2 \u2248 140.37.\nin total, 49 parameters (for 7\u00d7 7 rbfs) must be optimized.\npattern search optimization is applied to find an optimal parameter vector \u03d1\u2217,\nstarting from an identically zero parameter vector. figure 3.16 shows the policy ob-\ntained and a representative trajectory that is controlled by this policy. the policy is\nlargely linear in the state variables (within the saturation limits), and leads to a good\nconvergence to the zero state.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n5\n\n0\n\n\u22125\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(b) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) policy.\n\nfigure 3.16\nresults of policy search with the general policy parametrization for the dc motor.\n\nin this experiment, the pattern search algorithm required 18173 s to converge.\nthis execution time is larger than for all other algorithms applied earlier to the\ndc motor (grid q-iteration and fitted q-iteration in section 3.4.5, and lspi in sec-\ntion 3.5.7), illustrating the large computational demands of policy search with general\nparametrizations.\n\npolicy search spends the majority of its execution time estimating the score func-\ntion (3.63), which is a computationally expensive operation. for this experiment, the\nscore of 11440 different parameter vectors had to be computed until convergence.\nthe computational cost of evaluating each parameter vector can be decreased by tak-\ning a smaller x0 or larger \u03b5mc and \u03b5ps, at the expense of a possible decrease in control\nperformance.\n\n "}, {"Page_number": 121, "text": "3.7. approximate policy search\n\n111\n\npolicy search with a tailored parametrization\n\nin this second part of the example, we employ a simple policy parametrization that\nis well suited to the dc motor problem. this parametrization is derived by using\nprior knowledge. because the system is linear and the reward function is quadratic,\nthe optimal policy would be a linear state feedback if the constraints on the state and\naction variables were disregarded (bertsekas, 2007, section 3.2).16 now taking into\naccount the constraints on the action, we assume that a good approximation of an\noptimal policy is linear in the state variables, within the constraints on the action:\n\n(3.66)\n\nbh(x;\u03d1) = sat{\u03d11x1 + \u03d12x2,\u221210, 10}\n\nwhere \u201csat\u201d denotes saturation. in fact, an examination of the near-optimal policy in\nfigure 3.5(b) on page 67 reveals that this assumption is largely correct: the only non-\nlinearities appear in the top-left and bottom-right corners of the figure; they are prob-\nably due to the constraints on the state variables, which were not taken into account\nwhen deriving the parametrization (3.66). we employ this tailored parametrization to\nperform policy search. note that only 2 parameters must be optimized, significantly\nfewer than the 49 parameters required by the general parametrization used earlier.\n\nfigure 3.17 shows the policy obtained by pattern search optimization, together\nwith a representative controlled trajectory. as expected, the policy closely resembles\nthe near-optimal policy of figure 3.5(b), with the exception of the nonlinearities\nin the corners of the state space. the trajectory obtained is also close to the near-\noptimal one in figure 3.5(c). compared to the general-parametrization solution of\nfigure 3.16, the policy varies more quickly in the linear portion, which results in a\nmore aggressive control signal. this is because the tailored parametrization can lead\nto a large slope of the policy, whereas the wide rbfs used in figure 3.16 lead to a\nsmoother interpolation. the score obtained by the policy of figure 3.17 is \u2212229.25,\nslightly better than the score of \u2212230.69 obtained by the rbf policy of figure 3.16.\nthe execution time of pattern search with the tailored parametrization was ap-\nproximately 75 s. as expected, the computational cost is much smaller than for the\ngeneral parametrization, because only 2 parameters must be optimized, instead of 49.\nthis illustrates the benefits of using a compact policy parametrization that is appro-\npriate for the problem at hand. unfortunately, deriving an appropriate parametriza-\ntion requires prior knowledge, which is not always available. the execution time is\nlarger than that of grid q-iteration in section 3.4.5, which was 7.80 s for the fine grid\nand 0.06 s for the coarse grid. it has the same order of magnitude as the execution\ntime of lspi in section 3.5.7, which was 23 s when using exact policy improvements,\nand 58 s with approximate policy improvements; but it is smaller than the execution\n\n16 this optimal linear state feedback is given by:\n\nwhere y is the stabilizing solution of the riccati equation:\n\nh(x) = kx = \u2212\u03b3(\u03b3bty b + rrew)\u22121btyax\n\ny = at[\u03b3y \u2212 \u03b32y b(\u03b3bty b + rrew)\u22121bt]a + qrew\n\nsubstituting a, b, qrew, rrew, and \u03b3 in these equations leads to a state feedback gain of k \u2248\n[\u221211.16,\u22120.67]t for the dc motor.\n\n "}, {"Page_number": 122, "text": "112\n\nchapter 3. dp and rl in large and continuous spaces\n\ntime 2151 s of fitted q-iteration. to enable an easy comparison of all these execution\ntimes, they are collected in table 3.1.17\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n50\n\n0\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) policy.\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(b) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nfigure 3.17\nresults of policy search with the tailored policy parametrization (3.66) on the dc motor. the\n\npolicy parameter is b\u03d1\u2217 = [\u221216.69,\u22121]t.\n\ntable 3.1\nexecution time of approximate dp and rl algorithms for the dc motor problem.\n\nalgorithm\n\nexecution time [s]\n\ngrid q-iteration with a coarse grid\ngrid q-iteration with a fine grid\nfitted q-iteration\nlspi with exact policy improvement\nlspi with exact policy approximation\npolicy search with a general parametrization\npolicy search with a tailored parametrization\n\n0.06\n7.80\n2151\n\n23\n58\n\n18173\n\n75\n\n17recall that all these execution times were recorded on a pc with an intel core 2 duo t9550 2.66 ghz\n\ncpu and with 3 gb ram.\n\n "}, {"Page_number": 123, "text": "3.8. comparison\n\n113\n\n3.8 comparison of approximate value iteration, policy iteration,\n\nand policy search\n\nthis section provides a general, qualitative comparison of approximate value itera-\ntion, approximate policy iteration, and approximate policy search. a more specific\ncomparison would of course depend on the particular algorithms considered and on\nthe problem at hand.\n\napproximate value iteration versus approximate policy iteration\n\noffline approximate policy iteration often converges in a small number of iterations,\npossibly smaller than the number of iterations taken by offline approximate value it-\neration. this was illustrated for the dc motor example, in which lspi (section 3.5.7)\nconverged faster than grid q-iteration (section 3.4.5). however, this does not mean\nthat approximate policy iteration is computationally less demanding than approxi-\nmate value iteration, since approximate policy evaluation is a difficult problem by\nitself, which must be solved at every single policy iteration. one advantage of ap-\nproximate value iteration is that it usually guarantees convergence to a unique solu-\ntion, whereas approximate policy iteration is generally only guaranteed to converge\nto a sequence of policies that all provide a guaranteed level of performance. this was\nillustrated in section 3.5.7, where lspi with policy approximation converged to a\nlimit cycle.\n\nconsider now the approximate policy evaluation step of policy iteration, in\ncomparison to approximate value iteration. some approximate policy evaluation\nalgorithms closely parallel approximate value iteration and converge under similar\nconditions (section 3.5.1). however, approximate policy evaluation can addition-\nally benefit from the linearity of the bellman equation for a policy\u2019s value function,\ne.g., (2.7), whereas the bellman optimality equation, which characterizes the optimal\nvalue function, e.g., (2.8), is highly nonlinear due to the maximization in the right-\nhand side. a class of algorithms for approximate policy evaluation exploit this linear-\nity property by solving a projected form of the bellman equation (section 3.5.2). one\nadvantage of such algorithms is that they only require the approximator to be linearly\nparameterized, whereas in approximate value iteration the approximator must lead to\ncontracting updates (section 3.4.4). moreover, some of these algorithms, such as\nlstd-q and lspe-q, are highly sample-efficient. however, a disadvantage of these\nalgorithms is that their convergence guarantees typically require a sample distribu-\ntion identical with the steady-state distribution under the policy being evaluated.\n\napproximate policy search versus approximate value iteration and policy\niteration\n\nfor some problems, deriving a good policy parametrization using prior knowledge\nmay be easier and more natural than deriving a good value function parametrization.\nif a good policy parametrization is available and this parametrization is differentiable,\n\n "}, {"Page_number": 124, "text": "114\n\nchapter 3. dp and rl in large and continuous spaces\n\npolicy gradient algorithms can be used (section 3.7.1). such algorithms are backed\nby useful convergence guarantees and have moderate computational demands. pol-\nicy gradient algorithms have the disadvantage that they can only find local optima\nin the class of parameterized policies considered, and may also suffer from slow\nconvergence.\n\nnote that the difficulty of designing a good value function parametrization can be\nalleviated either by automatically finding the parametric approximator (section 3.6)\nor by using nonparametric approximators. both of these options require less tun-\ning than a predefined parametric approximator, but may increase the computational\ndemands of the algorithm.\n\neven when prior knowledge is not available and a good policy parametrization\ncannot be obtained, approximate policy search can still be useful in its gradient-\nfree forms, which do not employ value functions (section 3.7.2). one situation in\nwhich value functions are undesirable is when value-function based algorithms fail\nto obtain a good solution, or require too restrictive assumptions. in such situations, a\ngeneral policy parametrization can be defined, and a global, gradient-free optimiza-\ntion technique can be used to search for optimal parameters. these techniques are\nusually free from numerical problems \u2013 such as divergence to infinity \u2013 even when\nused with general nonlinear parametrizations, which is not the case for value and pol-\nicy iteration. however, because of its generality, this approach typically incurs large\ncomputational costs.\n\n3.9 summary and discussion\n\nin this chapter, we have introduced approximate dynamic programming (dp) and ap-\nproximate reinforcement learning (rl) for large or continuous-space problems. after\nexplaining the need for approximation in such problems, parametric and nonparamet-\nric approximation architectures have been presented. then, approximate versions for\nthe three main categories of algorithms have been described: value iteration, policy\niteration, and policy search. theoretical results have been provided and the behavior\nof representative algorithms has been illustrated using numerical examples. addi-\ntionally, techniques to automatically determine value function approximators have\nbeen reviewed, and the three categories of algorithms have been compared. exten-\nsive accounts of approximate dp and rl, presented from different perspectives, can\nalso be found in the books of bertsekas and tsitsiklis (1996); powell (2007); chang\net al. (2007); cao (2007).\n\napproximate dp/rl is a young, but active and rapidly expanding, field of re-\nsearch. important challenges still remain to be overcome in this field, some of which\nare pointed out next.\n\nwhen the problem considered is high-dimensional and prior knowledge is not\navailable, it is very difficult to design a good parametrization that does not lead to\nexcessive computational costs. an additional, related difficulty arises in the model-\n\n "}, {"Page_number": 125, "text": "3.9. summary and discussion\n\n115\n\nfree (rl) setting, when only a limited amount of data is available. in this case,\nif the approximator is too complex, the data may be insufficient to compute its\nparameters. one alternative to designing the approximator in advance is to find a\ngood parametrization automatically, while another option is to exploit the powerful\nframework of nonparametric approximators, which can also be viewed as deriving a\nparametrization from the data. adaptive and nonparametric approximators are often\nstudied in the context of value iteration and policy iteration (sections 3.4.3, 3.5.3, and\n3.6). in policy search, finding good approximators automatically is a comparatively\nunderexplored but promising idea.\n\nactions that take continuous values are important in many problems of practical\ninterest. for instance, in the context of automatic control, stabilizing a system around\nan unstable equilibrium requires continuous actions to avoid chattering, which would\notherwise damage the system in the long run. however, in dp and rl, continuous-\naction problems are more rarely studied than discrete-action problems. a major diffi-\nculty of value iteration and policy iteration in the continuous-action case is that they\nrely on solving many potentially difficult, nonconcave maximization problems over\nthe action variables (section 3.2). continuous actions are easier to handle in actor-\ncritic and policy search algorithms, in the sense that explicit maximization over the\naction variables is not necessary.\n\ntheoretical results about approximate value iteration traditionally rely on the\nrequirement of nonexpansive approximation. to satisfy this requirement, the ap-\nproximators are often confined to restricted subclasses of linear parameterizations.\nanalyzing approximate value iteration without assuming nonexpansiveness can be\nvery beneficial, e.g., by allowing powerful nonlinearly parameterized approximators,\nwhich may alleviate the difficulties of designing a good parametrization in advance.\nthe work on finite-sample performance guarantees, outlined in section 3.4.4, pro-\nvides encouraging results in this direction.\n\nin the context of approximate policy iteration, least-squares techniques for policy\nevaluation are very promising, owing to their sample efficiency and ease of tuning.\nhowever, currently available performance guarantees for these algorithms require\nthat they process relatively many samples generated using a fixed policy. from a\nlearning perspective, it would be very useful to analyze how these techniques behave\nin online, optimistic policy iteration, in which the policy is not kept fixed for a long\ntime, but is improved once every few samples. promising empirical results have been\nreported using such algorithms, but their theoretical understanding is still limited (see\nsection 3.5.6).\n\nthe material in this chapter provides a broad understanding of approximate value\niteration, policy iteration, and policy search. in order to deepen and strengthen this\nunderstanding, in each of the upcoming three chapters we treat in detail a particular\nalgorithm from one of these three classes. namely, in chapter 4, a model-based value\niteration algorithm with fuzzy approximation is introduced, theoretically analyzed,\nand experimentally evaluated. the theoretical analysis illustrates how convergence\nand consistency guarantees can be developed for approximate dp. in chapter 5,\nleast-squares policy iteration is revisited, and several extensions to this algorithm\nare introduced and empirically studied. in particular, an online variant is devel-\n\n "}, {"Page_number": 126, "text": "116\n\nchapter 3. dp and rl in large and continuous spaces\n\noped, and some important issues that appear in online rl are emphasized along\nthe way. in chapter 6, a policy search approach relying on the gradient-free cross-\nentropy method for optimization is described and experimentally evaluated. this\napproach highlights one possibility for developing techniques that scale better to\nhigh-dimensional state spaces, by focusing the computation only on important initial\nstates.\n\n "}, {"Page_number": 127, "text": "4\n\napproximate value iteration with a fuzzy\nrepresentation\n\nthis chapter introduces fuzzy q-iteration, an algorithm for approximate value it-\neration that relies on a fuzzy representation of the q-function. this representation\ncombines a fuzzy partition defined over the state space with a discretization of the\naction space. the convergence and consistency of fuzzy q-iteration are analyzed.\nas an alternative to designing the membership functions for the fuzzy partition in\nadvance, a technique to optimize the membership functions using the cross-entropy\nmethod is described. the performance of fuzzy q-iteration is evaluated in an exten-\nsive experimental study.\n\n4.1 introduction\n\nvalue iteration algorithms (introduced in section 2.3) search for the optimal value\nfunction, and then employ a policy that is greedy in this value function to control\nthe system. in large or continuous spaces, the value function must be approximated,\nleading to approximate value iteration, which was introduced in section 3.4.\n\nin this chapter, we design and study in detail an algorithm for approximate value\niteration, building on \u2013 and at the same time adding depth to \u2013 the knowledge gained\nin the previous chapters. we exploit the fuzzy approximation paradigm (fantuzzi and\nrovatti, 1996) to develop fuzzyq-iteration: an algorithm for approximate q-iteration\nthat represents q-functions using a fuzzy partition of the state space and a discretiza-\ntion of the action space. fuzzy q-iteration requires a model and works for problems\nwith deterministic dynamics. the fuzzy sets in the partition are described by mem-\nbership functions (mfs), and the discrete actions are selected beforehand from the\n(possibly large or continuous) original action space. the q-value of a given state-\ndiscrete action pair is computed as a weighted sum of parameters, where the weights\nare given by the mfs. the fuzzy representation can therefore also be seen as a lin-\nearly parameterized approximator, and in this context the mfs are state-dependent\nbasis functions.\n\nin addition to the fuzzy approximator, an important new development in this\nchapter is a variant of fuzzy q-iteration that works asynchronously, by employing\nthe most recently updated values of the parameters at each step of the computation.\n\n117\n\n "}, {"Page_number": 128, "text": "118\n\nchapter 4. fuzzy q-iteration\n\nthis variant is called asynchronous fuzzy q-iteration. the original algorithm, which\nkeeps the parameters unchanged while performing the computations of the current\niteration, is called synchronousfuzzyq-iteration, in order to differentiate it from the\nasynchronous variant. for the sake of conciseness, the name \u201cfuzzy q-iteration\u201d is\nused to refer collectively to both of these variants; e.g., from the statement \u201cfuzzy\nq-iteration converges,\u201d it should be understood that both the asynchronous and syn-\nchronous variants are convergent. whenever the distinction between the two variants\nis important, we use the \u201csynchronous\u201d and \u201casynchronous\u201d qualifiers.\n\ntwo desirable properties of algorithms for approximate value iteration are con-\nvergence to a near-optimal value function and consistency. consistency means the\nasymptotical convergence to the optimal value function as the approximation accu-\nracy increases. by using the theoretical framework of nonexpansive approximators\ndeveloped in section 3.4.4, and by extending this framework to handle the asyn-\nchronous case, we show that fuzzy q-iteration asymptotically converges to a fixed\npoint. this fixed point corresponds to an approximate q-function that lies within a\nbounded distance from the optimal q-function; moreover, the suboptimality of the\nq-function obtained after a finite number of iterations is also bounded. both of these\nq-functions lead to greedy policies with a bounded suboptimality. additionally, in\na certain sense, the asynchronous algorithm converges at least as fast as the syn-\nchronous one. in a second part of our analysis, we also show that fuzzy q-iteration\nis consistent: under appropriate continuity assumptions on the process dynamics and\non the reward function, the approximate q-function converges to the optimal one as\nthe approximation accuracy increases.\n\nthe accuracy of the solution found by fuzzy q-iteration crucially depends on\nthe mfs. in its original form, fuzzy q-iteration requires the mfs to be designed be-\nforehand. either prior knowledge about the optimal q-function is required to design\ngood mfs, or many mfs must be defined to provide a good coverage and resolution\nover the entire state space. neither of these approaches always works well. as an\nalternative to designing the mfs in advance, we consider a method to optimize the\nlocation and shape of a fixed number of mfs. this method belongs to the class of\napproximator optimization techniques introduced in section 3.6.1. to evaluate each\nconfiguration of the mfs, a policy is computed with fuzzy q-iteration using these\nmfs, and the performance of this policy is estimated by simulation. using the cross-\nentropy method for optimization, we design an algorithm to optimize triangular mfs.\n\nthe theoretical analysis of fuzzy q-iteration provides confidence in its results.\nwe complement this analysis with an extensive numerical and experimental study,\nwhich is organized in four parts, each focusing on different aspects relevant to the\npractical application of the algorithm. the first example illustrates the convergence\nand consistency of fuzzy q-iteration, using a dc motor problem. the second ex-\nample employs a two-link manipulator to demonstrate the effects of interpolating the\nactions, and also to compare fuzzy q-iteration with fitted q-iteration (algorithm 3.4).\nin the third example, the real-life control performance of fuzzy q-iteration is illus-\ntrated using an inverted pendulum swing-up problem. for these three examples, the\nmfs are designed in advance. in the fourth and final example, the effects of opti-\n\n "}, {"Page_number": 129, "text": "4.2. fuzzy q-iteration\n\n119\n\nmizing the mfs are studied in the classical car-on-the-hill benchmark (moore and\natkeson, 1995; munos and moore, 2002; ernst et al., 2005).\n\nnext, section 4.2 describes fuzzy q-iteration. in section 4.3, the convergence,\nconsistency, and computational demands of fuzzy q-iteration are analyzed, while\nsection 4.4 presents our approach to optimize the mfs. section 4.5 describes the\nexperimental evaluation outlined above, and section 4.6 closes the chapter with a\nsummary and discussion.\n\n4.2 fuzzy q-iteration\n\nfuzzy q-iteration belongs to the class of value iteration algorithms with parametric\napproximation (section 3.4.1). similarly to other algorithms in this class, it works\nby combining the q-iteration mapping (2.22) with an approximation mapping and\na projection mapping. after introducing the fuzzy approximation mapping and the\nprojection mapping used by fuzzy q-iteration, the two versions of the algorithm are\ndescribed, namely synchronous and asynchronous fuzzy q-iteration.\n\n4.2.1 approximation and projection mappings of fuzzy q-iteration\n\nconsider a deterministic markov decision process (mdp) (see section 2.2.1). the\nstate space x and the action space u of the mdp may be either continuous or discrete,\nbut they are assumed to be subsets of euclidean spaces, such that the euclidean norm\nof the states and actions is well-defined.\n\nfuzzy approximation\n\nthe proposed approximator relies on a fuzzy partition of the state space and on a\ndiscretization of the action space. the fuzzy partition of x contains n fuzzy sets \u03c7i,\neach described by a membership function (mf):\n\n\u00b5i : x \u2192 [0, 1]\n\nwhere i = 1, . . . , n. a state x then belongs to each set i with a degree of membership\n\u00b5i(x). the following requirement is imposed on the mfs:\n\nrequirement 4.1 each mf has its maximum at a single point, i.e., for every i there\nexists a unique xi for which \u00b5i(xi) > \u00b5i(x) \u2200x 6= xi. additionally, the other mfs take\nzero values at xi, i.e., \u00b5i\u2032(xi) = 0 \u2200i\u2032 6= i.\n\nthis requirement will be useful later on for obtaining a projection mapping that\nhelps with the convergence of fuzzy q-iteration, as well as for proving consistency.\nbecause the other mfs take zero values in xi, it can be assumed without loss of\ngenerality that \u00b5i(xi) = 1, and hence \u00b5i is normal. the state xi is then called the core\nof the ith mf.\n\n "}, {"Page_number": 130, "text": "120\n\nchapter 4. fuzzy q-iteration\n\nexample 4.1 triangular fuzzy partitions. a simple type of fuzzy partition that\nsatisfies requirement 4.1 can be obtained as follows. for each state variable xd,\nwhere d \u2208 {1, . . . , d} and d = dim(x), a number nd of triangular mfs are defined\nas follows:\n\ncd,2 \u2212 xd\n\n\u03c6d,1(xd) = max(cid:18)0,\ncd,2 \u2212 cd,1(cid:19)\n\u03c6d,i(xd) = max(cid:20)0, min(cid:18) xd \u2212 cd,i\u22121\n\u03c6d,nd (xd) = max(cid:18)0,\ncd,nd \u2212 cd,nd\u22121(cid:19)\n\nxd \u2212 cd,nd\u22121\n\ncd,i \u2212 cd,i\u22121\n\n,\n\ncd,i+1 \u2212 xd\n\ncd,i+1 \u2212 cd,i(cid:19)(cid:21) ,\n\nfor i = 2, . . . , nd \u2212 1\n\nwhere cd,1, . . . , cd,nd are the cores along dimension d and must satisfy cd,1 < \u00b7\u00b7\u00b7 <\ncd,nd . these cores fully determine the shape of the mfs. the state space should be\ncontained in the support of the mfs, i.e., xd \u2208 [cd,1, cd,nd ] for d = 1, . . . , d. adjacent\nsingle-dimensional mfs always intersect at a level of 0.5. the product of each com-\nbination of single-dimensional mfs thus gives a pyramid-shaped d-dimensional mf\nin the fuzzy partition of x . examples of single-dimensional and two-dimensional\ntriangular partitions are given in figure 4.1.\n(cid:3)\n\n)\n\n1\n\nx\n(\n\u00b5\n\n1\n\n0.5\n\n0\n\u22122\n\n0\nx\n1\n\n2\n\n1\n\n)\nx\n(\n\u00b5\n\n0.5\n\n0\n2\n\n0\n\nx\n\n2\n\n\u22122\n\n\u22122\n\n0\n\nx\n\n1\n\n2\n\n(a) a set of single-dimensional trian-\ngular mfs, each shown in a different\nline style.\n\n(b) two-dimensional mfs, obtained by combining\ntwo sets of single-dimensional mfs, each identical to\nthe set in figure 4.1(a).\n\nfigure 4.1 examples of triangular mfs.\n\nother types of mfs that satisfy requirement 4.1 can be obtained, e.g., by using\nhigher-order b-splines (brown and harris, 1994, ch. 8) (triangular mfs are second-\norder b-splines), or kuhn triangulations combined with barycentric interpolation\n(munos and moore, 2002; abonyi et al., 2001). kuhn triangulations can lead to a\nsmaller number of mfs than triangular or b-spline partitions; in the latter types of\npartitions, the number of mfs grows exponentially with the dimensionality of the\nstate space. although fuzzy q-iteration is not limited to triangular mfs, these will\nnevertheless be used in the examples, because they are the simplest mfs that satisfy\nthe requirements for the convergence and consistency of the algorithm.\n\nrequirement 4.1 can be relaxed so that other mfs can take nonzero values at\nthe core xi of a given mf i. if these values are sufficiently small, fuzzy q-iteration\n\n "}, {"Page_number": 131, "text": "4.2. fuzzy q-iteration\n\n121\n\ncan still be proven to converge to a near-optimal solution by extending the results of\ntsitsiklis and van roy (1996). this relaxation allows other types of localized mfs,\nsuch as gaussian mfs. note that, in practice, fuzzy q-iteration can indeed diverge\nwhen the other mfs have too large values at xi.\n\nuntil now, the approximation over the state space was discussed. to approximate\nover the (continuous or discrete) action space u , a discrete subset of actions ud is\nchosen:\n\nud =(cid:8)u j|u j \u2208 u, j = 1, . . . , m(cid:9)\n\n(4.1)\n\nthe fuzzy approximator stores a parameter vector \u03b8 with n = nm elements. each\nparameter \u03b8[i, j] corresponds to the mf-discrete action pair (\u00b5i, u j), where [i, j] =\ni + ( j \u2212 1)n denotes the scalar index corresponding to i and j. to compute the q-\nvalue of the state-action pair (x, u), first the action u is discretized by selecting a\ndiscrete action u j \u2208 ud that is closest to u:\nj \u2208 arg min\n\nku\u2212 u j\u2032k2\n\nj\u2032\n\nwhere k \u00b7 k2 denotes the euclidean norm of the argument. then, the approximate\nq-value is computed as a weighted sum of the parameters \u03b8[1, j], . . . ,\u03b8[n, j]:\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8[i, j]\n\nbq(x, u) =\n\nwhere the weights \u03c6i(x) are the normalized mfs (degrees of fulfillment):1\n\n\u03c6i(x) =\n\n\u00b5i(x)\n\u2211n\ni\u2032=1 \u00b5i\u2032(x)\n\n(4.2)\n\nthis entire procedure can be written concisely as the following approximation\n\nmapping:\n\nbq(x, u) = [f(\u03b8)](x, u) =\n\nwhere j \u2208 arg min\n\nku\u2212 u j\u2032k2\n\nj\u2032\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8[i, j]\n\n(4.3)\n\nto ensure that f(\u03b8) is a well-defined function, any ties in the minimization from (4.3)\nhave to be broken consistently. in the sequel we assume that they are broken in favor\nof the smallest index that satisfies the condition. for a fixed x, such an approximator\nis constant over each subset of actions u j, j = 1, . . . , m, defined by:\n\nu \u2208 u j\n\nif (ku\u2212 u jk2 \u2264 ku\u2212 u j\u2032k2 for all j\u2032 6= j, and:\n\nj < j\u2032 for any j\u2032 6= j such that ku\u2212 u jk2 = ku\u2212 u j\u2032k2\n\n(4.4)\n\n1the mfs are already normal (see the discussion after requirement 4.1), but they may not yet be\nnormalized, because their sum may be different from 1 for some values of x. the sum of normalized mfs\nmust be 1 for any value of x.\n\n "}, {"Page_number": 132, "text": "122\n\nchapter 4. fuzzy q-iteration\n\nwhere the second condition is due to the manner in which ties are broken. the sets\nu j form a partition of u .\n\nnote that (4.3) describes a linearly parameterized approximator (3.3). more\nspecifically, the fuzzy approximator is closely related to the discrete-action, linearly\nparameterized approximators introduced in example 3.1. it extends these approxi-\nmators by introducing an explicit action discretization procedure. in this context, the\nnormalized mfs can be seen as state-dependent basis functions or features (bertsekas\nand tsitsiklis, 1996).\n\ninterpretation of the approximator as a fuzzy rule base\n\nwe next provide an interpretation of the q-function approximator as the output of a\nfuzzy rule base (kruse et al., 1994; klir and yuan, 1995; yen and langari, 1999).\nconsider a so-called takagi-sugeno fuzzy rule base (takagi and sugeno, 1985;\nkruse et al., 1994, section 4.2.2), which describes the relationship between inputs\nand outputs using if-then rules of the form:\n\nri : if x is \u03c7i then y = gi(x)\n\n(4.5)\n\nwhere i \u2208 1, . . . , n is the index of the rule, x \u2208 x is the input variable (which for now\ndoes not need to be the state of an mdp), \u03c71, . . . ,\u03c7n are the input fuzzy sets, y \u2208 y\nis the output variable, and g1, . . . , gn : x \u2192 y are the (algebraic) output functions.\nthe input and output variables can be scalars or vectors. each fuzzy set \u03c7i is defined\nby an mf \u00b5i : x \u2192 [0, 1] and can be seen as describing a fuzzy region in the input\nspace, in which the corresponding consequent expression holds. a particular input x\nbelongs to each fuzzy set (region) \u03c7i with membership degree \u00b5i(x). the output of\nthe rule base (4.5) is a weighted sum of the output functions gi, where the weights\nare the normalized mfs \u03c6i (see again (4.2)):\n\ny =\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)gi(x)\n\n(4.6)\n\nin this expression, if y is a vector (which means that gi(x) is also a vector), algebraic\noperations are understood to be performed element-wise. note that, more generally\nthan in (4.5), the consequents can also be propositions of the form y is yi, where\ny1, . . . , yn are fuzzy sets defined over the output space. the fuzzy rule base resulting\nfrom this change is called a mamdani rule base (mamdani, 1977; kruse et al., 1994,\nsection 4.2.1).\n\nwith this framework in place, we can now interpret the q-function approximator\nas a takagi-sugeno fuzzy rule base that takes the state x as input and produces as\noutputs the q-values q1, . . . , qm of the m discrete actions:\n\nri : if x is \u03c7i then q1 = \u03b8[i,1]; q2 = \u03b8[i,2]; . . . ; qm = \u03b8[i,m]\n\n(4.7)\n\nwhere the m outputs have been shown separately to enhance readability. the output\nfunctions are in this case constant and consist of the parameters \u03b8[i, j]. to obtain the\napproximate q-value (4.3), the action is discretized and the output q j corresponding\n\n "}, {"Page_number": 133, "text": "4.2. fuzzy q-iteration\n\n123\n\nto this discretized action is selected. this output is computed with (4.6), thereby\nleading to the approximation mapping (4.3).\n\nin classical fuzzy theory, the fuzzy sets are associated with linguistic terms de-\nscribing the corresponding antecedent regions. for instance, if x represents a tem-\nperature, the fuzzy sets could be associated with linguistic values such as \u201ccold,\u201d\n\u201cwarm,\u201d and \u201chot.\u201d in fuzzy q-iteration, the rule base is simply used as an approx-\nimator, and the fuzzy sets do not necessarily have to be associated with meaningful\nlinguistic terms. nevertheless, if prior knowledge is available on the shape of the\noptimal q-function, then fuzzy sets with meaningful linguistic terms can be defined.\nhowever, such knowledge is typically difficult to obtain without actually computing\nthe optimal q-function.\n\nprojection mapping\n\nthe projection mapping of fuzzy q-iteration is a special case of the least-squares\nprojection mapping (3.14), repeated here for easy reference:\n\np(q) = \u03b8\u2021, where \u03b8\u2021 \u2208 arg min\n\n\u03b8\n\nns\n\u2211\n\nls=1(cid:16)q(xls, uls)\u2212 [f(\u03b8)](xls, uls)(cid:17)2\n\n(4.8)\n\nin which a set of state action samples {(xls, uls) | ls = 1, . . . , ns} is used. in fuzzy q-\niteration, nm samples are used, obtained as the cross-product between the set of mf\ncores {x1, . . . , xn} and the set of discrete actions ud. due to requirement 4.1, with\nthese samples the least-squares projection (4.8) reduces to an assignment of the form\n(3.26), specifically:\n\n\u03b8[i, j] = [p(q)][i, j] = q(xi, u j)\n\n(4.9)\n\nthe parameter vector \u03b8 given by (4.9) reduces the least-squares error for the samples\nconsidered to zero:\n\nn\n\n\u2211\ni=1\n\nm\n\n\u2211\n\nj=1(cid:16)q(xi, u j)\u2212 [f(\u03b8)](xi, u j)(cid:17)2\n\n= 0\n\n4.2.2 synchronous and asynchronous fuzzy q-iteration\n\nthe synchronous fuzzy q-iteration algorithm is obtained by using the approxima-\ntion mapping (4.3) and the projection mapping (4.9) in the approximate q-iteration\nupdates given by (3.15), and also repeated here:\n\n\u03b8\u2113+1 = (p\u25e6 t \u25e6 f)(\u03b8\u2113)\n\n(4.10)\n\nwhen the difference between two consecutive parameter vectors decreases below a\n\nthe algorithm starts with an arbitrary initial parameter vector \u03b80 \u2208 rn and stops\nthreshold \u03b5qi, i.e., when k\u03b8\u2113+1 \u2212 \u03b8\u2113k\u221e \u2264 \u03b5qi. a near-optimal parameter vector b\u03b8\u2217 =\n\nbecause all the q-functions considered by fuzzy q-iteration are of the form f(\u03b8),\n\n\u03b8\u2113+1 is obtained.\n\n "}, {"Page_number": 134, "text": "124\n\nchapter 4. fuzzy q-iteration\n\nthey are constant in every region u j given by (4.4). therefore, when computing max-\nimal q-values, it suffices to consider only the discrete actions in ud:2\n\nmax\n\nu\n\n[f(\u03b8)](x, u) = max\n\nj\n\n[f(\u03b8)](x, u j)\n\nby exploiting this property, the following discrete-action version of the q-iteration\nmapping can be used in practical implementations of fuzzy q-iteration:\n\n[td(q)](x, u) = \u03c1(x, u) + \u03b3max\n\nj\n\nq( f (x, u), u j)\n\neach iteration (4.10) can be implemented as:\n\n\u03b8\u2113+1 = (p\u25e6 td \u25e6 f)(\u03b8\u2113)\n\n(4.11)\n\n(4.12)\n\nwithout changing the sequence of parameter vectors obtained. the maximization\nover u in the original updates has been replaced with an easier maximization over\nthe discrete set ud, which can be solved by enumeration. furthermore, the norms in\n(4.3) no longer have to be computed to implement (4.12).\n\nsynchronous fuzzy q-iteration using the update (4.12) can be written in a proce-\ndural form as algorithm 4.1. to establish the equivalence between algorithm 4.1 and\nthe form (4.12), notice that the right-hand side of line 4 in algorithm 4.1 corresponds\nto [td(f(\u03b8\u2113))](xi, u j). hence, line 4 can be written as \u03b8\u2113+1,[i, j] \u2190 [(p\u25e6 td\u25e6 f)(\u03b8\u2113)][i, j]\nand the entire for loop described by lines 3\u20135 is equivalent to (4.12).\n\nalgorithm 4.1 synchronous fuzzy q-iteration.\ninput: dynamics f , reward function \u03c1, discount factor \u03b3,\n\nmfs \u03c6i, i = 1, . . . , n, set of discrete actions ud, threshold \u03b5qi\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\nfor i = 1, . . . , n, j = 1, . . . , m do\n\n4:\n\n5:\n\n\u03b8\u2113+1,[i, j] \u2190 \u03c1(xi, u j) + \u03b3max j\u2032 \u2211n\n\ni\u2032=1 \u03c6i\u2032( f (xi, u j))\u03b8\u2113,[i\u2032, j\u2032]\n\nend for\n\n6: until k\u03b8\u2113+1 \u2212 \u03b8\u2113k\u221e \u2264 \u03b5qi\n\noutput: b\u03b8\u2217 = \u03b8\u2113+1\n\nalgorithm 4.1 computes the new parameters \u03b8\u2113+1 using the parameters \u03b8\u2113 found\nat the previous iteration, which remain unchanged throughout the current iteration.\nalgorithm 4.2 is a different version of fuzzy q-iteration that makes more efficient\nuse of the updates: at each step of the computation, the latest updated values of\n\n2this property would also hold if, instead of pure discretization, a triangular fuzzy partition would\nbe defined over the action space, since the maximal q-values would always be attained in the cores of\nthe triangular mfs (recall that triangular mfs lead to multilinear interpolation). such a partition may be\nhelpful in a model-free context, to extract information from action samples that do not fall precisely on\nthe mf cores. in this chapter, however, we remain within a model-based context, where action samples\ncan be generated at will.\n\n "}, {"Page_number": 135, "text": "4.2. fuzzy q-iteration\n\n125\n\nalgorithm 4.2 asynchronous fuzzy q-iteration.\ninput: dynamics f , reward function \u03c1, discount factor \u03b3,\n\nmfs \u03c6i, i = 1, . . . , n, set of discrete actions ud, threshold \u03b5qi\n\n1: initialize parameter vector, e.g., \u03b80 \u2190 0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\n\u03b8 \u2190 \u03b8\u2113\nfor i = 1, . . . , n, j = 1, . . . , m do\n\u03b8[i, j] \u2190 \u03c1(xi, u j) + \u03b3max j\u2032 \u2211n\n\n4:\n\n5:\n\n6:\n\n7:\n\nend for\n\u03b8\u2113+1 \u2190 \u03b8\n\n8: until k\u03b8\u2113+1 \u2212 \u03b8\u2113k\u221e \u2264 \u03b5qi\n\noutput: b\u03b8\u2217 = \u03b8\u2113+1\n\ni\u2032=1 \u03c6i\u2032( f (xi, u j))\u03b8[i\u2032, j\u2032]\n\nthe parameters are employed. since the parameters are updated in an asynchronous\nfashion, this version is called asynchronous fuzzy q-iteration. in algorithm 4.2 the\nparameters are shown being updated in sequence, but our analysis still holds even if\nthey are updated in any order.\n\neither of the two variants of fuzzy q-iteration produces an approximately opti-\n\nmal q-function f(b\u03b8\u2217). a greedy policy in this q-function can then be employed to\n\ncontrol the system, i.e., a policy that satisfies (3.16):\n\n\u02c6\u02c6h\u2217(x) \u2208 arg max\n\nu\n\n[f(b\u03b8\u2217)](x, u)\n\nlike before, because the q-function f(b\u03b8\u2217) is constant in every region u j, the compu-\n\ntation of the greedy policy can be simplified by only considering the discrete actions\nin ud:\n\nthe notation\n\nsection 4.3.1):\n\nin f(\u03b8\u2217), where \u03b8\u2217 is the parameter vector obtained asymptotically, as \u2113 \u2192 \u221e (see\n\nj\n\n(4.13)\n\n[f(b\u03b8\u2217)](x, u j)\n\n\u02c6\u02c6h\u2217(x) = u j\u2217 , where j\u2217 \u2208 arg max\n\u02c6\u02c6h\u2217 is used to differentiate this policy from a policybh\u2217 that is greedy\nbh\u2217(x) = u j\u2217 , where j\u2217 \u2208 arg max\n\n[f(\u03b8\u2217)](x, u j)\n\n(4.14)\n\nj\n\nit is also possible to obtain a continuous-action policy using the following heuris-\ntic. for any state, an action is computed by interpolating between the best local ac-\ntions for every mf core, using the mfs as weights:\n\nh(x) =\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)u j\u2217i\n\n, where j\u2217i \u2208 arg max\n\nj\n\n[f(b\u03b8\u2217)](xi, u j)\n\n(4.15)\n\nthe index j\u2217i corresponds to a locally optimal action for the core xi. for instance,\n\n "}, {"Page_number": 136, "text": "126\n\nchapter 4. fuzzy q-iteration\n\nwhen used with triangular mfs (cf. example 4.1), the interpolation procedure (4.15)\nis well suited to problems where (near-)optimal policies are locally affine with re-\nspect to the state. interpolated policies may, however, be a poor choice for other\nproblems, as illustrated in the example below. theoretical guarantees about policies\nof the form (4.15) are therefore difficult to provide, and the analysis of section 4.3\nonly considers discrete-action policies of the form (4.13).\n\nexample 4.2 interpolated policies may perform poorly. consider the problem\nschematically represented in figure 4.2(a), in which a robot must avoid an obstacle.\n\nobstacle\n\nrobot\n\nmembership\n\ndegree m\n\nm1\n\nx\n1\n\nm2\n\nrobot\n\nposition\n\nx\n\n2\n\nposition x\n\n(a) an obstacle avoidance problem.\n\n(b) membership functions.\n\nlocally\noptimal\naction\n\ninterpolated\n\naction\n\nrobot\n\nposition\n\nlocally\noptimal\naction\n\nobstacle\n\ninterpolated\n\naction\n\nrobot\n\n(c) locally optimal and interpolated actions, with\nthe mfs repeated in gray color.\n\n(d) chosen action and outcome, with the mfs and\nthe locally optimal actions repeated in gray color.\n\nfigure 4.2\nan obstacle avoidance problem, where interpolation between two locally optimal actions leads\nto undesirable behavior.\n\ntwo mfs are defined for the position variable, and the robot is located at the mid-\npoint of the distance between the cores x1 and x2 of these two mfs, see figure 4.2(b).\nas shown in figure 4.2(c), the action of steering left is locally optimal for the mf\ncore to the left of the robot, while the action of steering right is locally optimal for\nthe mf core to the right. these actions are locally optimal because they would take\nthe robot around the obstacle, if the robot were located in the respective cores. how-\never, interpolating between these two actions at the robot\u2019s current position makes it\n(incorrectly) move forward and collide with the obstacle, as seen in figure 4.2(d). in\nthis problem, rather than interpolating, a good policy would apply either of the two\nlocally optimal actions, e.g., by randomly picking one of them.\n(cid:3)\n\n "}, {"Page_number": 137, "text": "4.3. analysis of fuzzy q-iteration\n\n127\n\n4.3 analysis of fuzzy q-iteration\n\nnext, we analyze the convergence, consistency, and computational complexity of\n(synchronous and asynchronous) fuzzy q-iteration. specifically, in section 4.3.1, we\nshow that fuzzy q-iteration is convergent and we characterize the suboptimality of its\nsolution, making use of the theoretical framework developed earlier, in section 3.4.4.\nin section 4.3.2, we prove that fuzzy q-iteration is consistent, i.e., that its solution\nasymptotically converges to q\u2217 as the approximation accuracy increases. these re-\nsults show that fuzzy q-iteration is a theoretically sound algorithm. in section 4.3.3,\nthe computational complexity of fuzzy q-iteration is briefly examined.\n\n4.3.1 convergence\n\nin this section,\nestablished:\n\nthe following theoretical results about fuzzy q-iteration are\n\n\u2022 synchronous and asynchronous fuzzy q-iteration asymptotically converge to\n\na fixed point (parameter vector) \u03b8\u2217 as the number of iterations grows.\n\n\u2022 asynchronous fuzzy q-iteration converges faster than synchronous fuzzy q-\n\niteration, in a well-defined sense that will be described later.\n\n\u2022 for any strictly positive convergence threshold \u03b5qi, synchronous and asyn-\n\nchronous fuzzy q-iteration terminate in a finite number of iterations.\n\nfunction that is within a bound of the optimal q-function, and the correspond-\ning greedy policy has a bounded suboptimality. similar bounds hold for the\n\n\u2022 the asymptotically obtained parameter vector \u03b8\u2217 yields an approximate q-\nparameter vectorb\u03b8\u2217 obtained in a finite number of iterations.\n\ntheorem 4.1 (convergence of synchronous fuzzy q-iteration) synchronous fuzzy\nq-iteration (algorithm 4.1) converges to a unique fixed point.\n\nproof: to prove convergence, we use the framework of nonexpansive approxima-\ntors developed in section 3.4.4. in this framework, the convergence of approximate\nq-iteration is guaranteed by ensuring that the composite mapping p\u25e6 t \u25e6 f is a con-\ntraction in the infinity norm. for synchronous fuzzy q-iteration, this will be done by\nshowing that f and p are nonexpansions.\n\nsince the approximation mapping f (4.3) is a weighted linear combination of\n\nnormalized mfs, it is a nonexpansion. formally:\n\n(cid:12)(cid:12)[f(\u03b8)](x, u)\u2212 [f(\u03b8\u2032)](x, u)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8[i, j] \u2212\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8\u2032[i, j](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\n(where j \u2208 arg min j\u2032 ku\u2212 u j\u2032k2)\n\nn\n\n\u2264\n\n\u2211\ni=1\n\n\u03c6i(x)(cid:12)(cid:12)(cid:12)\u03b8[i, j] \u2212 \u03b8\u2032[i, j](cid:12)(cid:12)(cid:12)\n\n "}, {"Page_number": 138, "text": "128\n\nchapter 4. fuzzy q-iteration\n\nn\n\n\u03c6i(x)k\u03b8\u2212 \u03b8\u2032k\u221e\n\n\u2211\ni=1\n\n\u2264\n\u2264 k\u03b8\u2212 \u03b8\u2032k\u221e\n\nwhere the last step is true because the sum of the normalized mfs \u03c6i(x) is 1. since p\nconsists of a set of assignments (4.9), it is also a nonexpansion.\n\nadditionally, t is a contraction with factor \u03b3 (see section 2.3), and therefore\n\np\u25e6 t \u25e6 f is also a contraction with factor \u03b3, i.e., for any \u03b8,\u03b8\u2032:\n\nk(p\u25e6 t \u25e6 f)(\u03b8)\u2212 (p\u25e6 t \u25e6 f)(\u03b8\u2032)k\u221e \u2264 \u03b3k\u03b8\u2212 \u03b8\u2032k\u221e\n\nso, p\u25e6 t \u25e6 f has a unique fixed point \u03b8\u2217, and synchronous fuzzy q-iteration con-\nverges to this fixed point as \u2113 \u2192 \u221e.\nin the sequel, a concise notation of asynchronous fuzzy q-iteration is needed.\nrecall that n = nm, and that [i, j] = i + ( j \u2212 1)n, with [i, j] \u2208 {1, . . . , n} for i \u2208\n{1, . . . , n} and j \u2208 {1, . . . , m}. define for all l = 0, . . . , n, recursively, the mappings\nsl : rn \u2192 rn as:\n\n(cid:3)\n\ns0(\u03b8) = \u03b8\n\n[sl(\u03b8)]l\u2032 =([(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8))]l\u2032 if l\u2032 = l\n\n[sl\u22121(\u03b8)]l\u2032\n\nif l\u2032 \u2208 {1, . . . , n}\\{l}\n\nso, sl for l > 0 corresponds to updating the first l parameters using approximate\nasynchronous q-iteration, and sn is a complete iteration of the algorithm.\n\ntheorem 4.2 (convergence of asynchronous fuzzy q-iteration)\nasynchronous\nfuzzy q-iteration (algorithm 4.2) converges to the same fixed point as synchronous\nfuzzy q-iteration.\n\nproof: we first show that sn is a contraction with factor \u03b3 < 1, i.e., that for any \u03b8,\u03b8\u2032:\n\nksn(\u03b8)\u2212 sn(\u03b8\u2032)k\u221e \u2264 \u03b3k\u03b8\u2212 \u03b8\u2032k\u221e\n\nthis is done by induction, element by element. by the definition of sl, the first ele-\nment is only updated by s1:\n\n(cid:12)(cid:12)[sn(\u03b8)]1 \u2212 [sn(\u03b8\u2032)]1(cid:12)(cid:12) =(cid:12)(cid:12)[s1(\u03b8)]1 \u2212 [s1(\u03b8\u2032)]1(cid:12)(cid:12)\n\n=(cid:12)(cid:12)[(p\u25e6 t \u25e6 f)(\u03b8)]1 \u2212 [(p\u25e6 t \u25e6 f)(\u03b8\u2032)]1(cid:12)(cid:12)\n\n\u2264 \u03b3k\u03b8\u2212 \u03b8\u2032k\u221e\n\nthe last step in this equation is true because p\u25e6 t \u25e6 f is a contraction. furthermore,\ns1 is a nonexpansion:\n\nks1(\u03b8)\u2212 s1(\u03b8\u2032)k\u221e = max(cid:8)(cid:12)(cid:12)[(p\u25e6 t \u25e6 f)(\u03b8)]1 \u2212 [(p\u25e6 t \u25e6 f)(\u03b8\u2032)]1(cid:12)(cid:12) ,\n\u2264 max(cid:8)\u03b3k\u03b8\u2212 \u03b8\u2032k\u221e,k\u03b8\u2212 \u03b8\u2032k\u221e, . . . ,k\u03b8\u2212 \u03b8\u2032k\u221e(cid:9)\n\u2264 k\u03b8\u2212 \u03b8\u2032k\u221e\n\n(cid:12)(cid:12)\u03b82 \u2212 \u03b8\u20322(cid:12)(cid:12) , . . . ,(cid:12)(cid:12)\u03b8n \u2212 \u03b8\u2032n(cid:12)(cid:12)(cid:9)\n\n "}, {"Page_number": 139, "text": "4.3. analysis of fuzzy q-iteration\n\n129\n\nwe now prove the lth step, thus completing the induction. assume that for l\u2032 =\n1, . . . , l \u2212 1, the following relationships hold:\n\n(cid:12)(cid:12)[sn(\u03b8)]l\u2032 \u2212 [sn(\u03b8\u2032)]l\u2032(cid:12)(cid:12) =(cid:12)(cid:12)[sl\u2032(\u03b8)]l\u2032 \u2212 [sl\u2032(\u03b8\u2032)]l\u2032(cid:12)(cid:12) \u2264 \u03b3k\u03b8\u2212 \u03b8\u2032k\u221e\n\nksl\u2032 (\u03b8)\u2212 sl\u2032(\u03b8\u2032)k\u221e \u2264 k\u03b8\u2212 \u03b8\u2032k\u221e\n\nthen, the contraction property for the lth element of sn can be proven as follows:\n\n(cid:12)(cid:12)[sn(\u03b8)]l \u2212 [sn(\u03b8\u2032)]l(cid:12)(cid:12) =(cid:12)(cid:12)[sl(\u03b8)]l \u2212 [sl(\u03b8\u2032)]l(cid:12)(cid:12)\n\n=(cid:12)(cid:12)[(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8))]l \u2212 [(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8\u2032))]l(cid:12)(cid:12)\n\n\u2264 \u03b3ksl\u22121(\u03b8)\u2212 sl\u22121(\u03b8\u2032)k\u221e\n\u2264 \u03b3k\u03b8\u2212 \u03b8\u2032k\u221e\n\nfurthermore, the lth intermediate mapping sl is a nonexpansion:\n\nksl(\u03b8)\u2212 sl(\u03b8\u2032)k\u221e = max(cid:8)(cid:12)(cid:12)[s1(\u03b8)]1 \u2212 [s1(\u03b8\u2032)]1(cid:12)(cid:12) , . . . ,(cid:12)(cid:12)[sl\u22121(\u03b8)]l\u22121 \u2212 [sl\u22121(\u03b8\u2032)]l\u22121(cid:12)(cid:12) ,\n(cid:12)(cid:12)[(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8))]l \u2212 [(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8\u2032))]l(cid:12)(cid:12) ,\n(cid:12)(cid:12)\u03b8l+1 \u2212 \u03b8\u2032l+1(cid:12)(cid:12) , . . . ,(cid:12)(cid:12)\u03b8n \u2212 \u03b8\u2032n(cid:12)(cid:12)(cid:9)\n\u2264 max(cid:8)\u03b3k\u03b8\u2212 \u03b8\u2032k\u221e, . . . ,\u03b3k\u03b8\u2212 \u03b8\u2032k\u221e,\n\u03b3k\u03b8\u2212 \u03b8\u2032k\u221e,\nk\u03b8\u2212 \u03b8\u2032k\u221e, . . . ,k\u03b8\u2212 \u03b8\u2032k\u221e(cid:9)\n\u2264 k\u03b8\u2212 \u03b8\u2032k\u221e\n\nso, for any l, |[sn(\u03b8)]l \u2212 [sn(\u03b8\u2032)]l| \u2264 \u03b3k\u03b8\u2212 \u03b8\u2032k\u221e, which means that sn is a contrac-\ntion with factor \u03b3 < 1. therefore, asynchronous fuzzy q-iteration has a unique fixed\npoint.\n\nthe fixed point \u03b8\u2217 of p\u25e6 t \u25e6 f can be shown to be a fixed point of sn using a\nvery similar, element-by-element procedure, which is not given here. since sn has\na unique fixed point, this has to be \u03b8\u2217. therefore, asynchronous fuzzy q-iteration\nasymptotically converges to \u03b8\u2217, and the proof is complete.\n(cid:3)\n\nthis proof is actually more general, showing that approximate asynchronous q-\niteration converges for any approximation mapping f and projection mapping p for\nwhich p \u25e6 t \u25e6 f is a contraction. note that a similar result holds for exact asyn-\nchronous v-iteration (bertsekas, 2007, sec. 1.3.2).\nwe show next that asynchronous fuzzy q-iteration converges at least as fast as the\nsynchronous version, i.e., that in a given number of iterations, the asynchronous algo-\nrithm takes the parameter vector at least as close to the fixed point as the synchronous\none. for that, we first need the following monotonicity lemma. in the sequel, vector\nand vector function inequalities are understood to be satisfied element-wise.\n\nlemma 4.1 (monotonicity) if \u03b8 \u2264 \u03b8\u2032, then (p \u25e6 t \u25e6 f)(\u03b8) \u2264 (p \u25e6 t \u25e6 f)(\u03b8\u2032) and\nsn(\u03b8) \u2264 sn(\u03b8\u2032).\nproof: to show that p\u25e6 t \u25e6 f is monotonic, we will show in turn that (i) p, (ii) t ,\nand (iii) f are monotonic.\n\n "}, {"Page_number": 140, "text": "130\n\nchapter 4. fuzzy q-iteration\n\n(i) given q \u2264 q\u2032, it follows that for all i, j:\n\nq(xi, u j) \u2264 q\u2032(xi, u j)\n\nthis is equivalent to [p(q)][i, j] \u2264 [p(q\u2032)][i, j], so p is monotonic.\n\n(ii) given q \u2264 q\u2032, it follows that, for any state-action pair (x, u):\n\nmax\n\nu\u2032\n\nq( f (x, u), u\u2032) \u2264 max\n\n\u00afu\n\nq\u2032( f (x, u), \u00afu)\n\nby multiplying both sides of the equation by \u03b3 and then adding \u03c1(x, u), we obtain:\n\n\u03c1(x, u) + \u03b3max\n\nu\u2032\n\nq( f (x, u), u\u2032) \u2264 \u03c1(x, u) + \u03b3max\n\n\u00afu\n\nq\u2032( f (x, u), \u00afu)\n\nwhich is equivalent to [t (q)](x, u) \u2264 [t (q\u2032)](x, u), so t is monotonic.\n\n(iii) given \u03b8 \u2264 \u03b8\u2032, it follows that:\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8[i, j] \u2264\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8\u2032[i, j], where j \u2208 arg min\n\nj\u2032\n\nku\u2212 u j\u2032k2\n\nthis is equivalent to [f(\u03b8)](x, u) \u2264 [f(\u03b8\u2032)](x, u), so f is monotonic. because p, t ,\nand f are all monotonic, so is p\u25e6 t \u25e6 f.\nnext, the asynchronous q-iteration mapping sn is shown to be monotonic by\ninduction, using a derivation similar to that in the proof of theorem 4.2. for the first\nelement of sn:\n\n[sn(\u03b8)]1 = [s1(\u03b8)]1 = [(p\u25e6 t \u25e6 f)(\u03b8)]1\n\n\u2264 [(p\u25e6 t \u25e6 f)(\u03b8\u2032)]1 = [s1(\u03b8\u2032)]1 = [sn(\u03b8\u2032)]1\n\nwhere the monotonicity property of p\u25e6 t \u25e6 f was used. the intermediate mapping\ns1 is monotonic:\n\ns1(\u03b8) = [[(p\u25e6 t \u25e6 f)(\u03b8)]1,\u03b82, . . . ,\u03b8n]t\n\n\u2264 [[(p\u25e6 t \u25e6 f)(\u03b8\u2032)]1,\u03b8\u20322, . . . ,\u03b8\u2032n]t = s1(\u03b8\u2032)\n\nwe now prove the lth step, thus completing the induction. assume that for l\u2032 =\n1, . . . , l \u2212 1, the mappings sl\u2032 are monotonic:\n\nsl\u2032(\u03b8) \u2264 sl\u2032(\u03b8\u2032)\n\nthen, the monotonicity property for the lth element of sn can be proven as follows:\n\n[sn(\u03b8)]l = [sl(\u03b8)]l = [(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8))]l\n\n\u2264 [(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8\u2032))]l = [sl(\u03b8\u2032)]l = [sn(\u03b8\u2032)]l\n\nwhere the monotonicity of p\u25e6 t \u25e6 f and sl\u22121 was used. furthermore, the lth inter-\nmediate mapping sl is also monotonic:\nsl(\u03b8) = [[s1(\u03b8)]1 . . . , [sl\u22121(\u03b8)]l\u22121, [(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8))]l,\u03b8l+1, . . . ,\u03b8n]t\n\n\u2264 [[s1(\u03b8\u2032)]1 . . . , [sl\u22121(\u03b8\u2032)]l\u22121, [(p\u25e6 t \u25e6 f)(sl\u22121(\u03b8\u2032))]l,\u03b8\u2032l+1, . . . ,\u03b8\u2032n]t = sl(\u03b8\u2032)\n\n "}, {"Page_number": 141, "text": "4.3. analysis of fuzzy q-iteration\n\n131\n\ntherefore, for any l, [sn(\u03b8)]l \u2264 [sn(\u03b8\u2032)]l, i.e., sn is monotonic, which concludes the\n\n(cid:3)\n\nproof.\n\nasynchronous fuzzy q-iteration converges at least as fast as the synchronous\nalgorithm, in the sense that \u2113 iterations of the asynchronous algorithm take the pa-\nrameter vector at least as close to \u03b8\u2217 as \u2113 iterations of the synchronous algorithm.\nthis is stated formally as follows.\n\ntheorem 4.3 (convergence rate) if a parameter vector \u03b8 satisfies \u03b8 \u2264 (p \u25e6 t \u25e6\nf)(\u03b8) \u2264 \u03b8\u2217, then:\n\n(p\u25e6 t \u25e6 f)\u2113(\u03b8) \u2264 s\u2113\n\nn(\u03b8) \u2264 \u03b8\u2217 \u2200\u2113 \u2265 1\n\nn(\u03b8) denotes the composition of sn(\u03b8) \u2113-times with itself, i.e., s\u2113\n\nwhere s\u2113\nsn \u25e6\u00b7\u00b7\u00b7\u25e6 sn)(\u03b8), and similarly for (p\u25e6 t \u25e6 f)\u2113(\u03b8).\n\nn(\u03b8) = (sn \u25e6\n\nproof: the theorem will be proven by induction on \u2113. first, take \u2113 = 1, for which\n\nthe following must be proven:\n\n(p\u25e6 t \u25e6 f)(\u03b8) \u2264 sn(\u03b8) \u2264 \u03b8\u2217\n\nby assumption, (p\u25e6 t \u25e6 f)(\u03b8) \u2264 \u03b8\u2217. it remains to be shown that:\n\nsn(\u03b8) \u2264 \u03b8\u2217\n\n(p\u25e6 t \u25e6 f)(\u03b8) \u2264 sn(\u03b8)\n\n(4.16)\n\n(4.17)\n\nequation (4.16) follows by applying sn to each side of the inequality \u03b8 \u2264 \u03b8\u2217, which\nis true by assumption. because sn is monotonic, we obtain:\n\nsn(\u03b8) \u2264 sn(\u03b8\u2217) = \u03b8\u2217\n\nwhere the last step is true because \u03b8\u2217 is the fixed point of sn. the inequality (4.17)\ncan be shown element-wise, in a similar way to the proof that sn is monotonic\n(lemma 4.1). so, this part of the proof is omitted.\n\nwe now prove the \u2113th step, thus completing the induction. assuming that:\n\nwe intend to prove:\n\n(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8) \u2264 s\u2113\u22121\n\nn\n\n(\u03b8) \u2264 \u03b8\u2217\n\n(p\u25e6 t \u25e6 f)\u2113(\u03b8) \u2264 s\u2113\n\nn(\u03b8) \u2264 \u03b8\u2217\n\nwhich can be split into three inequalities:\n\n(p\u25e6 t \u25e6 f)\u2113(\u03b8) \u2264 \u03b8\u2217\ns\u2113\nn(\u03b8) \u2264 \u03b8\u2217\n(p\u25e6 t \u25e6 f)\u2113(\u03b8) \u2264 s\u2113\n\nn(\u03b8)\n\n(4.18)\n\n(4.19)\n\n(4.20)\n\nto obtain (4.18), we apply p\u25e6 t \u25e6 f to both sides of the equation (p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8) \u2264\n\u03b8\u2217, and use the fact that p \u25e6 t \u25e6 f is monotonic and has the fixed point \u03b8\u2217. the\n\n "}, {"Page_number": 142, "text": "132\n\nchapter 4. fuzzy q-iteration\n\ninequality (4.19) can be obtained in a similar way, by exploiting the properties of sn.\nto derive (4.20), we start from:\n\nand apply sn to both sides of this equation, using the fact that it is monotonic:\n\n(\u03b8)\n\nn\n\n(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8) \u2264 s\u2113\u22121\nsn(cid:16)(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8)(cid:17) \u2264 sn(cid:16)s\u2113\u22121\nsn(cid:16)(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8)(cid:17) \u2264 s\u2113\n\nn(\u03b8)\n\nn\n\n(\u03b8)(cid:17) ,\n\ni.e.,\n\n(4.21)\n\nnotice that (p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8) satisfies a relationship similar to that assumed for \u03b8 in\n\nthe body of the theorem, namely:\n\n(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8) \u2264 (p\u25e6 t \u25e6 f)(cid:16)(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8)(cid:17) \u2264 \u03b8\u2217\n\nso, by replacing \u03b8 with (p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8) in (4.17), the following (valid) relationship\n\nis obtained:\n\n(p\u25e6 t \u25e6 f)(cid:16)(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8)(cid:17) \u2264 sn(cid:16)(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8)(cid:17) ,\n(p\u25e6 t \u25e6 f)\u2113(\u03b8) \u2264 sn(cid:16)(p\u25e6 t \u25e6 f)\u2113\u22121(\u03b8)(cid:17)\n\ni.e.,\n\ncombining this inequality with (4.21) leads to the desired result (4.20), and the proof\nis complete.\n(cid:3)\n\nnote that a similar result holds in the context of exact (synchronous versus asyn-\nchronous) v-iteration (bertsekas, 2007, sec. 1.3.2). the result of theorem 4.3 will\nnot be needed for the analysis in the sequel.\n\nin the remainder of this section, in addition to examining the asymptotical prop-\nerties of fuzzy q-iteration, we also consider an implementation that stops when\nk\u03b8\u2113+1 \u2212 \u03b8\u2113k\u221e \u2264 \u03b5qi, with a convergence threshold \u03b5qi > 0 (see algorithms 4.1 and\n\n4.2). this implementation returns the solution b\u03b8\u2217 = \u03b8\u2113+1. such an implementation\n\nwas given in algorithm 4.1 for synchronous fuzzy q-iteration, and in algorithm 4.2\nfor asynchronous fuzzy q-iteration.\n\ntheorem 4.4 (finite termination) for any choice of threshold \u03b5qi > 0 and any ini-\ntial parameter vector \u03b80 \u2208 rn, synchronous and asynchronous fuzzy q-iteration stop\n\nin a finite number of iterations.\n\nproof: consider synchronous fuzzy q-iteration. because the mapping p\u25e6 t \u25e6 f\n\nis a contraction with factor \u03b3 < 1 and fixed point \u03b8\u2217, we have:\n\nk\u03b8\u2113+1 \u2212 \u03b8\u2217k\u221e = k(p\u25e6 t \u25e6 f)(\u03b8\u2113)\u2212 (p\u25e6 t \u25e6 f)(\u03b8\u2217)k\u221e\n\n\u2264 \u03b3k\u03b8\u2113 \u2212 \u03b8\u2217k\u221e\n\nby induction, k\u03b8\u2113 \u2212 \u03b8\u2217k\u221e \u2264 \u03b3\u2113k\u03b80 \u2212 \u03b8\u2217k\u221e for any \u2113 > 0. by the banach fixed\npoint theorem (see, e.g., istratescu, 2002, ch. 3), \u03b8\u2217 is bounded. because the ini-\ntial parameter vector \u03b80 is also bounded, k\u03b80 \u2212 \u03b8\u2217k\u221e is bounded. using the notation\n\n "}, {"Page_number": 143, "text": "4.3. analysis of fuzzy q-iteration\n\n133\n\nb0 = k\u03b80 \u2212 \u03b8\u2217k\u221e, it follows that b0 is bounded and that k\u03b8\u2113 \u2212 \u03b8\u2217k\u221e \u2264 \u03b3\u2113b0 for any\n\u2113 > 0. therefore, we have:\n\nk\u03b8\u2113+1 \u2212 \u03b8\u2113k\u221e \u2264 k\u03b8\u2113+1 \u2212 \u03b8\u2217k\u221e +k\u03b8\u2113 \u2212 \u03b8\u2217k\u221e\n\n\u2264 \u03b3\u2113(\u03b3+ 1)b0\n\nusing this inequality, for any \u03b5qi > 0, a number of iterations, l, that guarantees\nk\u03b8l+1 \u2212 \u03b8lk\u221e \u2264 \u03b5qi can be chosen as:\n\nl =(cid:24)log\u03b3\n\n\u03b5qi\n\n(\u03b3+ 1)b0(cid:25)\n\ntherefore, the algorithm stops in at most l iterations. because b0 is bounded, l is\nfinite.\n\nthe proof for asynchronous fuzzy q-iteration proceeds in the same way, because\nthe asynchronous q-iteration mapping sn is also a contraction with factor \u03b3 < 1 and\nfixed point \u03b8\u2217.\n(cid:3)\n\nthe following bounds on the suboptimality of the resulting approximate q-\n\nfunction and policy hold.\n\ntheorem 4.5 (near-optimality) denote by ff\u25e6p \u2282 q the set of fixed points of the\nmapping f \u25e6 p, and define the minimum distance3 between q\u2217 and any fixed point of\nf \u25e6 p: \u03c2\u2217qi = minq\u2032\u2208ff\u25e6p kq\u2217 \u2212 q\u2032k\u221e. the convergence point \u03b8\u2217 of asynchronous and\n\nsynchronous fuzzy q-iteration satisfies:\n\nkq\u2217 \u2212 f(\u03b8\u2217)k\u221e \u2264\n\n2\u03c2\u2217qi\n1\u2212 \u03b3\n\n(4.22)\n\nadditionally, the parameter vector b\u03b8\u2217 obtained by asynchronous or synchronous\n\nfuzzy q-iteration in a finite number of iterations, with a threshold \u03b5qi, satisfies:\n\nkq\u2217 \u2212 f(b\u03b8\u2217)k\u221e \u2264\nkq\u2217 \u2212 qbh\u2217k\u221e \u2264\n\u02c6\u02c6h\u2217k\u221e \u2264\nkq\u2217 \u2212 q\n\n2\u03c2\u2217qi + \u03b3\u03b5qi\n\n1\u2212 \u03b3\n\n4\u03b3\u03c2\u2217qi\n(1\u2212 \u03b3)2\n2\u03b3(2\u03c2\u2217qi + \u03b3\u03b5qi)\n\n(1\u2212 \u03b3)2\n\n(4.23)\n\n(4.24)\n\n(4.25)\n\nfurthermore:\n\nwhere qbh\u2217 is the q-function of a policybh\u2217 that is greedy in f(\u03b8\u2217) (4.14), and q\n\nthe q-function of a policy\n\n\u02c6\u02c6h\u2217 that is greedy in f(b\u03b8\u2217) (4.13).\n\n3for simplicity, we assume that this minimum distance exists. if the minimum does not exist, then \u03c2\u2217qi\n\n\u02c6\u02c6h\u2217 is\n\nshould be taken so that \u2203q\u2032 \u2208 ff\u25e6p with kq\u2032 \u2212 q\u2217k\u221e \u2264 \u03c2\u2217qi.\n\n "}, {"Page_number": 144, "text": "134\n\nchapter 4. fuzzy q-iteration\n\nproof: the bound (4.22) was given in section 3.4.4, and only relies on the prop-\nerties of the fixed point \u03b8\u2217 and mappings f, p, and t , so it applies both to syn-\nchronous and asynchronous fuzzy q-iteration.4\n\nnumber of iterations after which the algorithm stops, which is finite by theorem 4.4.\n\nin order to obtain (4.23), a bound on kb\u03b8\u2217 \u2212 \u03b8\u2217k\u221e is derived first. let l be the\ntherefore,b\u03b8\u2217 = \u03b8l+1. we have:\n\nk\u03b8l \u2212 \u03b8\u2217k\u221e \u2264 k\u03b8l+1 \u2212 \u03b8lk\u221e +k\u03b8l+1 \u2212 \u03b8\u2217k\u221e\n\n\u2264 \u03b5qi + \u03b3k\u03b8l \u2212 \u03b8\u2217k\u221e\n\nwhere the last step follows from the convergence condition k\u03b8l+1 \u2212 \u03b8lk\u221e \u2264 \u03b5qi and\nfrom the contracting nature of the updates (see also the proof of theorem 4.4). from\nthe last inequality, it follows that k\u03b8l \u2212 \u03b8\u2217k\u221e \u2264\n\n1\u2212\u03b3 and therefore that:\n\n\u03b5qi\n\nk\u03b8l+1 \u2212 \u03b8\u2217k\u221e \u2264 \u03b3k\u03b8l \u2212 \u03b8\u2217k\u221e \u2264\n\n\u03b3\u03b5qi\n1\u2212 \u03b3\n\nwhich is equivalent to:\n\nkb\u03b8\u2217 \u2212 \u03b8\u2217k\u221e \u2264\n\n\u03b3\u03b5qi\n1\u2212 \u03b3\n\n(4.26)\n\nusing this inequality, the suboptimality of the q-function f(b\u03b8\u2217) can be bounded\n\nwith:\n\nkq\u2217 \u2212 f(b\u03b8\u2217)k\u221e \u2264 kq\u2217 \u2212 f(\u03b8\u2217)k\u221e +kf(\u03b8\u2217)\u2212 f(b\u03b8\u2217)k\u221e\n\n\u2264 kq\u2217 \u2212 f(\u03b8\u2217)k\u221e +kb\u03b8\u2217 \u2212 \u03b8\u2217k\u221e\n\n+\n\n\u2264\n\n\u2264\n\n2\u03c2\u2217qi\n1\u2212 \u03b3\n2\u03c2\u2217qi + \u03b3\u03b5qi\n\n\u03b3\u03b5qi\n1\u2212 \u03b3\n\n1\u2212 \u03b3\n\nthus obtaining (4.23), where the second step is true because f is a nonexpansion\n(which was shown in the proof of theorem 4.1), and the third step follows from\n(4.22) and (4.26). the bound equally applies to synchronous and asynchronous fuzzy\nq-iteration, because its derivation only relies on the fact that their updates are con-\ntractions with a factor \u03b3.\n\nthe bounds (4.24) and (4.25), which characterize the suboptimality of the poli-\n\ncies resulting from f(\u03b8\u2217) and f(b\u03b8\u2217), follow from the equation (3.25), also given\n\nin section 3.4.4. recall that this equation relates the suboptimality of an arbitrary\nq-function q with the suboptimality of a policy h that is greedy in this q-function:\n\nkq\u2217 \u2212 qhk\u221e \u2264\n\n2\u03b3\n\n(1\u2212 \u03b3)kq\u2217 \u2212 qk\u221e\n\n(4.27)\n\n4the bound was given in section 3.4.4 without a proof. the proof is not difficult to develop, and\nwe refer the reader who wishes to understand how this can be done to (tsitsiklis and van roy, 1996,\nappendices), where a proof is given in the context of v-iteration. the same remark applies to (4.27).\n\n "}, {"Page_number": 145, "text": "4.3. analysis of fuzzy q-iteration\n\n135\n\nto obtain (4.24) and (4.25), this inequality is applied to the q-functions f(\u03b8\u2217) and\n(cid:3)\n\nf(b\u03b8\u2217), using their suboptimality bounds (4.22) and (4.23).\n\nexamining (4.23) and (4.25), it can be seen that the suboptimality of the solution\ncomputed in a finite number of iterations is given by a sum of two terms. the sec-\nond term depends linearly on the precision \u03b5qi with which the solution is computed,\nand is easy to control by setting \u03b5qi as close to 0 as needed. the first term in the\nsum depends linearly on \u03c2\u2217qi, which is in turn related to the accuracy of the fuzzy ap-\nproximator, and is more difficult to control. the \u03c2\u2217qi-dependent term also contributes\nto the suboptimality of the asymptotic solutions (4.22), (4.24). ideally, the optimal\n\nq-function q\u2217 is a fixed point of f \u25e6 p, in which case \u03c2\u2217qi = 0 and fuzzy q-iteration\nasymptotically converges to q\u2217. for instance, q\u2217 is a fixed point of f \u25e6 p if it is\nexactly representable by the chosen approximator, i.e., if for all x, u:\n\nq\u2217(x, u) =\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)q\u2217(xi, u j), where j \u2208 arg min\n\nj\u2032\n\nku\u2212 u j\u2032k2\n\nsection 4.3.2 provides additional insight into the relationship between the subopti-\nmality of the solution and the accuracy of the approximator.\n\n4.3.2 consistency\n\nnext, we analyze the consistency of synchronous and asynchronous fuzzy q-\niteration. it is shown that the approximate solution f(\u03b8\u2217) asymptotically converges\nto the optimal q-function q\u2217, as the largest distance between the cores of adjacent\nfuzzy sets and the largest distance between adjacent discrete actions both decrease to\n0. an explicit relationship between the suboptimality of f(\u03b8\u2217) and the accuracy of\nthe approximator is derived.\n\nthe state resolution step \u03b4x is defined as the largest distance between any point\nin the state space and the nearest mf core. the action resolution step \u03b4u is defined\nsimilarly for the discrete actions. formally:\n\n\u03b4x = sup\nx\u2208x\n\u03b4u = sup\nu\u2208u\n\ni=1,...,nkx\u2212 xik2\nmin\nj=1,...,mku\u2212 u jk2\nmin\n\n(4.28)\n\n(4.29)\n\nwhere xi is the core of the ith mf, and u j is the jth discrete action. smaller values of\n\u03b4x and \u03b4u indicate a higher resolution. the goal is to show that lim\u03b4x\u21920, \u03b4u\u21920 f(\u03b8\u2217) =\nq\u2217.\n\nwe assume that f and \u03c1 are lipschitz continuous, as formalized next.\n\nassumption 4.1 (lipschitz continuity) the dynamics f and the reward function \u03c1\nare lipschitz continuous, i.e., there exist finite constants l f \u2265 0, l\u03c1 \u2265 0 so that:\n\nk f (x, u)\u2212 f ( \u00afx, \u00afu)k2 \u2264 l f (kx\u2212 \u00afxk2 +ku\u2212 \u00afuk2)\n|\u03c1(x, u)\u2212 \u03c1( \u00afx, \u00afu)| \u2264 l\u03c1(kx\u2212 \u00afxk2 +ku\u2212 \u00afuk2)\n\n\u2200x, \u00afx \u2208 x, u, \u00afu \u2208 u\n\n "}, {"Page_number": 146, "text": "136\n\nchapter 4. fuzzy q-iteration\n\nwe also require that the mfs are lipschitz continuous.\n\nrequirement 4.2 every mf \u03c6i is lipschitz continuous, i.e., for every i there exists a\nfinite constant l\u03c6i \u2265 0 so that:\n\nk\u03c6i(x)\u2212 \u03c6i( \u00afx)k2 \u2264 l\u03c6ikx\u2212 \u00afxk2, \u2200x, \u00afx \u2208 x\n\nfinally, the mfs should be local and evenly distributed, in the following sense.\n\nrequirement 4.3 every mf \u03c6i has a bounded support, which is contained in a ball\nwith a radius proportional to \u03b4x. formally, there exists a finite \u03bd > 0 so that:\n\n{x |\u03c6i(x) > 0} \u2282 {x |kx\u2212 xik2 \u2264 \u03bd\u03b4x } , \u2200i\n\nfurthermore, for every x, only a finite number of mfs are nonzero. formally, there\nexists a finite \u03ba > 0 so that:\n\n|{i |\u03c6i(x) > 0}| \u2264 \u03ba, \u2200x\n\nwhere |\u00b7| denotes set cardinality.\n\nlipschitz continuity conditions such as those of assumption 4.1 are typically\nneeded to prove the consistency of algorithms for approximate dp (e.g., gonzalez\nand rofman, 1985; chow and tsitsiklis, 1991). moreover, note that requirement 4.2\nis not restrictive; for instance, triangular mfs (example 4.1) and b-spline mfs are\nlipschitz continuous.\n\nrequirement 4.3 is satisfied in many cases of interest. for instance, it is satisfied\nby convex fuzzy sets with their cores distributed on an (equidistant or irregular) rect-\nangular grid in the state space, such as the triangular partitions of example 4.1. in\nsuch cases, every point x falls inside a hyperbox defined by the two adjacent cores\nthat are closest to xd on each axis d. some points will fall on the boundary of several\nhyperboxes, in which case we can just pick any of these hyperboxes. given require-\nment 4.1 and because the fuzzy sets are convex, only the mfs with the cores in the\ncorners of the hyperbox can take nonzero values in the chosen point. since the num-\nber of corners is 2d, where d is the dimension of x , we have:\n\n|{i |\u03c6i(x) > 0}| \u2264 2d\n\nand a choice \u03ba = 2d satisfies the second part of requirement 4.3. furthermore, along\nany axis of the state space, a given mf will be nonzero over an interval that spans at\nmost two hyperboxes. from the definition of \u03b4x, the largest diagonal of any hyperbox\nis 2\u03b4x, and therefore:\n\n{x |\u03c6i(x) > 0} \u2282 {x |kx\u2212 xik2 \u2264 4\u03b4x }\n\nwhich means that a choice \u03bd = 4 satisfies the first part of requirement 4.3.\n\nthe next lemma bounds the approximation error introduced by every iteration of\nthe synchronous algorithm. since we are ultimately interested in characterizing the\nconvergence point \u03b8\u2217, which is the same for both algorithms, the final consistency\nresult (theorem 4.6) applies to the asynchronous algorithm as well.\n\n "}, {"Page_number": 147, "text": "4.3. analysis of fuzzy q-iteration\n\n137\n\nsatisfies:\n\nlemma 4.2 (bounded error) under assumption 4.1 and if requirements 4.2 and\n4.3 are satisfied, there exists a constant \u03b5\u03b4 \u2265 0, \u03b5\u03b4 = o(\u03b4x) + o(\u03b4u), so that any\n\nsequence of q-functions bq0,bq1,bq2, . . . produced by synchronous fuzzy q-iteration\nproof: since any q-function bq\u2113 in a sequence produced by fuzzy q-iteration is of\nbq of the form f(\u03b8) for some \u03b8 satisfies:\n\nkbq\u2113+1 \u2212 t (bq\u2113)k\u221e \u2264 \u03b5\u03b4, for any \u2113 \u2265 0\n\nthe form f(\u03b8\u2113) for some parameter vector \u03b8\u2113, it suffices to prove that any q-function\n\nfor any pair (x, u):\n\n\u2211\ni=1\n\n  n\n  n\n\n(where j \u2208 arg min j\u2032 ku\u2212 u j\u2032k2)\n\n(cid:12)(cid:12)(cid:12)[(f \u25e6 p\u25e6 t )(bq)](x, u)\u2212 [t (bq)](x, u)(cid:12)(cid:12)(cid:12)\n=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\u2264(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\nk(f \u25e6 p\u25e6 t )(bq)\u2212 t (bq)k\u221e \u2264 \u03b5\u03b4\n\u03c6i(x)[t (bq)](xi, u j)!\u2212 [t (bq)](x, u)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\nu\u2032 bq( f (xi, u j), u\u2032)(cid:21)!\n\u03c6i(x)(cid:20)\u03c1(xi, u j) + \u03b3max\nu\u2032 bq( f (x, u), u\u2032)(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\u2212(cid:20)\u03c1(x, u) + \u03b3max\n\u03c6i(x)\u03c1(xi, u j)!\u2212 \u03c1(x, u)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n+ \u03b3(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n  n\nu\u2032 bq( f (xi, u j), u\u2032)!\u2212 max\n\n  n\n\n\u03c6i(x) max\n\n\u2211\ni=1\n\n\u2211\ni=1\n\n\u2211\ni=1\n\nu\u2032 bq( f (x, u), u\u2032)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\n(4.30)\n\nnote that maxubq(x, u) exists, because bq can take at most m distinct values for any\nfixed x. this is because bq is of the form f(\u03b8) given in (4.3) for some \u03b8, and is\n\ntherefore constant in each set u j, for j = 1, . . . , m. the first term on the right-hand\nside of (4.30) is:\n\nn\n\n\u2211\ni=1\n\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\n\u03c6i(x) [\u03c1(xi, u j)\u2212 \u03c1(x, u)](cid:12)(cid:12)(cid:12)(cid:12)(cid:12) \u2264\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)l\u03c1(kxi \u2212 xk2 +ku j \u2212 uk2)\n\n\u2264 l\u03c1\"ku j \u2212 uk2 +\n\n\u03c6i(x)kxi \u2212 xk2#\n\nn\n\n\u2211\ni=1\n\nwhere the lipschitz continuity of \u03c1 was used, and the last step follows from the\n\n\u2264 l\u03c1(\u03b4u + \u03ba\u03bd\u03b4x)\n\n(4.31)\n\n "}, {"Page_number": 148, "text": "138\n\nchapter 4. fuzzy q-iteration\n\ndefinition of \u03b4u and requirement 4.3. the second term in the right-hand side of\n(4.30) is:\n\n\u03c6i(x)(cid:20)max\n\nn\n\n\u2211\ni=1\n\n\u03b3(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\n\nn\n\n\u2264 \u03b3\n\nu\u2032 bq( f (xi, u j), u\u2032)\u2212 max\n\u03c6i(x)(cid:12)(cid:12)(cid:12)(cid:12)max\n\nu\u2032 bq( f (x, u), u\u2032)(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\nj\u2032 bq( f (x, u), u j\u2032)(cid:12)(cid:12)(cid:12)(cid:12)\nj\u2032 bq( f (xi, u j), u j\u2032)\u2212 max\n(cid:12)(cid:12)(cid:12)bq( f (xi, u j), u j\u2032)\u2212bq( f (x, u), u j\u2032)(cid:12)(cid:12)(cid:12)\n\n\u03c6i(x) max\n\n\u2211\ni=1\n\n\u2211\ni=1\n\n\u2264 \u03b3\n\nj\u2032\n\nn\n\n(4.32)\n\nn\n\n\u2211\n\nsecond step is true because the difference between the maxima of two functions of\nthe same variable is at most the maximum of the difference of the functions. writing\n\nthe first step is true because bq is constant in each set u j, for j = 1, . . . , m. the\nbq explicitly as in (4.3), we have:\n(cid:12)(cid:12)(cid:12)bq( f (xi, u j), u j\u2032)\u2212bq( f (x, u), u j\u2032)(cid:12)(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\ni\u2032=1(cid:2)\u03c6i\u2032( f (xi, u j))\u03b8[i\u2032, j\u2032] \u2212 \u03c6i\u2032( f (x, u))\u03b8[i\u2032, j\u2032](cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\ni\u2032=1(cid:12)(cid:12)\u03c6i\u2032( f (xi, u j))\u2212 \u03c6i\u2032( f (x, u))(cid:12)(cid:12)|\u03b8[i\u2032, j\u2032]|\ndefine i\u2032 =(cid:8)i\u2032(cid:12)(cid:12)\u03c6i\u2032( f (xi, u j)) 6= 0 or \u03c6i\u2032( f (x, u)) 6= 0(cid:9). using requirement 4.3, |i\u2032|\u2264\ni\u2032\u2208i\u2032(cid:12)(cid:12)\u03c6i\u2032( f (xi, u j))\u2212 \u03c6i\u2032( f (x, u))(cid:12)(cid:12)|\u03b8[i\u2032, j\u2032]| \u2264 \u2211\n\n2\u03ba. denote l\u03c6 = maxi l\u03c6i (where requirement 4.2 is employed). then, the right-\nhand side of (4.33) is equal to:\n\nl\u03c6l f (kxi \u2212 xk2 +ku j \u2212 uk2)k\u03b8k\u221e\n\u2264 2\u03bal\u03c6l f (kxi \u2212 xk2 +ku j \u2212 uk2)k\u03b8k\u221e\n\n(4.33)\n\ni\u2032\u2208i\u2032\n\n\u2211\n\n\u2211\n\n\u2264\n\nn\n\n(4.34)\n\nusing (4.33) and (4.34) in (4.32) yields:\n\n\u03b3\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x) max\n\nj\u2032\n\nn\n\n(cid:12)(cid:12)(cid:12)bq( f (xi, u j), u j\u2032)\u2212bq( f (x, u), u j\u2032)(cid:12)(cid:12)(cid:12)\n\u2264 2\u03b3\u03bal\u03c6l fk\u03b8k\u221e\"ku j \u2212 uk2 +\n\n\u03c6i(x) max\n\n\u2264 \u03b3\n\n\u2211\ni=1\n\n\u2211\ni=1\n\nj\u2032\n\nn\n\n2\u03bal\u03c6l f (kxi \u2212 xk2 +ku j \u2212 uk2)k\u03b8k\u221e\n\n\u03c6i(x)kxi \u2212 xk2#\n\n\u2264 2\u03b3\u03bal\u03c6l fk\u03b8k\u221e(\u03b4u + \u03ba\u03bd\u03b4x)\n\n(4.35)\n\nwhere the last step follows from the definition of \u03b4u and requirement 4.3. finally,\nsubstituting (4.31) and (4.35) into (4.30) yields:\n\n(cid:12)(cid:12)(cid:12)[(f \u25e6 p\u25e6 t )(bq)](x, u)\u2212 [t (bq)](x, u)(cid:12)(cid:12)(cid:12) \u2264 (l\u03c1 + 2\u03b3\u03bal\u03c6l fk\u03b8k\u221e)(\u03b4u + \u03ba\u03bd\u03b4x) (4.36)\n\n "}, {"Page_number": 149, "text": "4.3. analysis of fuzzy q-iteration\n\n139\n\ngiven a bounded initial parameter vector \u03b80, all the parameter vectors considered\nby the algorithm are bounded, which can be shown as follows. by the banach fixed\n\npoint theorem, the optimal parameter vector \u03b8\u2217 (the unique fixed point of p\u25e6 t \u25e6 f)\nis finite. also, we have k\u03b8\u2113 \u2212 \u03b8\u2217k\u221e \u2264 \u03b3\u2113k\u03b80 \u2212 \u03b8\u2217k\u221e (see the proof of theorem 4.4).\nsince k\u03b80 \u2212\u03b8\u2217k\u221e is bounded, all the other distances are bounded, and all the param-\neter vectors \u03b8\u2113 are bounded. let b\u03b8 = max\u2113\u22650k\u03b8\u2113k\u221e, which is bounded. therefore,\nk\u03b8k\u221e \u2264 b\u03b8 in (4.36), and the proof is complete with:\n\n\u03b5\u03b4 = (l\u03c1 + 2\u03b3\u03bal\u03c6l f b\u03b8)(\u03b4u + \u03ba\u03bd\u03b4x) = o(\u03b4x) + o(\u03b4u)\n\n(cid:3)\n\ntheorem 4.6 (consistency) under assumption 4.1 and if requirements 4.2 and 4.3\nare satisfied, synchronous and asynchronous fuzzy q-iteration are consistent:\n\nf(\u03b8\u2217) = q\u2217\n\nlim\n\n\u03b4x\u21920,\u03b4u\u21920\n\nfurthermore, the suboptimality of the approximate q-function satisfies:\n\nkf(\u03b8\u2217)\u2212 q\u2217k\u221e = o(\u03b4x) + o(\u03b4u)\nproof: first, it will be shown that kf(\u03b8\u2217)\u2212 q\u2217k\u221e \u2264 \u03b5\u03b4\nq-functions bq0,bq1,bq2, . . . produced by synchronous fuzzy q-iteration, and let us\nestablish by induction a bound on kbq\u2113 \u2212 t \u2113(bq0)k\u221e for \u2113 \u2265 1. by lemma 4.2, we\n\n1\u2212\u03b3. consider a sequence of\n\nhave:\n\nassume that for some \u2113 \u2265 1, we have:\n\nkbq1 \u2212 t (bq0)k\u221e \u2264 \u03b5\u03b4\n\nthen, for \u2113 + 1:\n\nkbq\u2113 \u2212 t \u2113(bq0)k\u221e \u2264 \u03b5\u03b4(1 + \u03b3+\u00b7\u00b7\u00b7 + \u03b3\u2113\u22121)\n\n(4.37)\n\nkbq\u2113+1 \u2212 t \u2113+1(bq0)k\u221e \u2264 kbq\u2113+1 \u2212 t (bq\u2113)k\u221e +kt (bq\u2113)\u2212 t \u2113+1(bq0)k\u221e\n\n\u2264 \u03b5\u03b4 + \u03b3kbq\u2113 \u2212 t \u2113(bq0)k\u221e\n\u2264 \u03b5\u03b4 + \u03b3\u03b5\u03b4(1 + \u03b3+\u00b7\u00b7\u00b7 + \u03b3\u2113\u22121)\n\u2264 \u03b5\u03b4(1 + \u03b3+\u00b7\u00b7\u00b7 + \u03b3\u2113)\n\nwhere in the second step we used lemma 4.2 and the contraction property of t . the\ninduction is therefore complete, and (4.37) is true for any \u2113 \u2265 1.\nthe inequality (4.37) means that for any pair (x, u), we have:\n\n[t \u2113(bq0)](x, u)\u2212 \u03b5\u03b4(1 + \u03b3+\u00b7\u00b7\u00b7 + \u03b3\u2113\u22121) \u2264 bq\u2113(x, u)\n\nwe take the limit of this pair of inequalities as \u2113 \u2192 \u221e, and use the facts that\n\n\u2264 [t \u2113(bq0)](x, u) + \u03b5\u03b4(1 + \u03b3+\u00b7\u00b7\u00b7 + \u03b3\u2113\u22121)\n\n "}, {"Page_number": 150, "text": "140\n\nchapter 4. fuzzy q-iteration\n\nlim\u2113\u2192\u221e t \u2113(bq0) = q\u2217 and lim\u2113\u2192\u221ebq\u2113 = f(\u03b8\u2217) (recall that bq\u2113 is the \u2113th q-function\n\nproduced by synchronous fuzzy q-iteration, which converges to f(\u03b8\u2217)) to obtain:\n\n\u03b5\u03b4\n\nq\u2217(x, u)\u2212\n\n1\u2212 \u03b3 \u2264 [f(\u03b8\u2217)](x, u) \u2264 q\u2217(x, u) +\n\n\u03b5\u03b4\n1\u2212 \u03b3\n\nfor any (x, u), which means that kf(\u03b8\u2217)\u2212 q\u2217k\u221e \u2264 \u03b5\u03b4\n\n1\u2212\u03b3. note that a related bound for\napproximate v-iteration was proven along similar lines by bertsekas and tsitsiklis\n(1996, sec. 6.5.3).\n\nnow, using the explicit formula for \u03b5\u03b4 found in the proof of lemma 4.2:\n\nlim\n\n\u03b4x\u21920,\u03b4u\u21920kf(\u03b8\u2217)\u2212 q\u2217k\u221e = lim\n\n\u03b4x\u21920,\u03b4u\u21920\n\n= lim\n\n\u03b4x\u21920,\u03b4u\u21920\n\n= 0\n\n\u03b5\u03b4\n1\u2212 \u03b3\n(l\u03c1 + 2\u03b3\u03bal\u03c6l f b\u03b8)(\u03b4u + \u03ba\u03bd\u03b4x)\n\n1\u2212 \u03b3\n\nand the first result of the theorem is proven. furthermore, using the same lemma,\n\u03b5\u03b4\n\n1\u2212\u03b3 = o(\u03b4x) + o(\u03b4u), which implies kf(\u03b8\u2217)\u2212 q\u2217k\u221e = o(\u03b4x) + o(\u03b4u), thus com-\n\n(cid:3)\n\npleting the proof.\n\nin addition to guaranteeing consistency, theorem 4.6 also relates the subopti-\nmality of the q-function f(\u03b8\u2217) to the accuracy of the fuzzy approximator. using\ntheorem 4.5, the accuracy can be further related to the suboptimality of the policy\n\nbh\u2217 greedy in f(\u03b8\u2217), and to the suboptimality of the solution (q-function f(b\u03b8\u2217) and\n\n\u02c6\u02c6h\u2217) obtained after a finite number of iterations.\n\ncorresponding policy\n\n4.3.3 computational complexity\n\nin this section, the time and memory complexity of fuzzy q-iteration are examined.\nit is easy to see that each iteration of the synchronous and asynchronous fuzzy q-\niteration (algorithms 4.1 and 4.2) requires o(n2m) time to run. here, n is the num-\nber of mfs and m is the number of discrete actions, leading to a parameter vec-\ntor of length nm. the complete algorithms consist of l iterations and thus require\no(ln2m) computation. fuzzy q-iteration requires o(nm) memory. the memory\ncomplexity is not proportional to l because, in practice, any \u03b8\u2113\u2032 for which \u2113\u2032 < \u2113 can\nbe discarded.\n\nexample 4.3 comparison with least-squares policy iteration. as an example, we\ncompare the complexity of fuzzy q-iteration with that of a representative algorithm\nfrom the approximate policy iteration class, namely least-squares policy iteration\n(lspi) (algorithm 3.11). we focus on the case in which both algorithms compute the\nsamenumberofparameters. at each iteration, lspi performs policy evaluation (with\nthe least-squares temporal difference for q-functions, algorithm 3.8) and policy im-\nprovement. to approximate the q-function, lspi typically employs discrete-action\napproximators (example 3.1), which consist of n state-dependent basis functions\n\n "}, {"Page_number": 151, "text": "4.4. optimizing the membership functions\n\n141\n\nand m discrete actions, and have nm parameters. the time complexity of each policy\nevaluation is o(n3m3) if \u201cnaive\u201d matrix inversion is used to solve the linear system\nof size nm. more efficient algorithms than matrix inversion can be obtained, e.g.,\nby incrementally computing the inverse, but the time complexity will still be larger\nthan o(n2m2). the memory complexity is o(n2m2). therefore, the asymptotic up-\nper bounds on the time complexity per iteration and on the memory complexity are\nworse (larger) for lspi than for fuzzy q-iteration.\n\nthis comparison should be considered in light of some important differences be-\ntween fuzzy q-iteration and lspi. the fact that both algorithms employ the same\nnumber of parameters means they employ similarly, but not identically powerful\napproximators: due to requirements 4.1, 4.2, and 4.3, the class of approximators\nconsidered by fuzzy q-iteration is smaller, and therefore less powerful. these re-\nquirements also enable fuzzy q-iteration to perform more computationally efficient\nparameter updates, e.g., because the projection is reduced to an update (4.9).\n(cid:3)\n\n4.4 optimizing the membership functions\n\nthe accuracy of the solution found by fuzzy q-iteration crucially depends on the\nquality of the fuzzy approximator, which in turn is determined by the mfs and by\nthe action discretization. we focus here on the problem of obtaining good mfs, and\nassume the action discretization is fixed. the mfs can be designed beforehand, in\nwhich case two possibilities arise. if prior knowledge about the shape of the optimal\nq-function is available to design the mfs, then a moderate number of mfs may be\nsufficient to achieve a good approximator. however, such prior knowledge is often\ndifficult to obtain without actually computing the optimal q-function. when prior\nknowledge is not available, a large number of mfs must be defined to provide a good\ncoverage and resolution over the entire state space, even in areas that will eventually\nbe irrelevant to the policy.\n\nin this section, we consider a different method, which does not require the mfs to\nbe designed in advance. in this method, parameters encoding the location and shape\nof the mfs are optimized, while the number of mfs is kept constant. the goal is to\nobtain a set of mfs that are near optimal for the problem at hand. since the mfs\ncan be regarded as basis functions, this approach may be regarded as a basis function\noptimization technique, such as those introduced in section 3.6.1. mf optimization\nis useful when prior knowledge about the shape of the optimal q-function is not\navailable and the number of mfs is limited.\n\n4.4.1 a general approach to membership function optimization\n\nlet the (normalized) mfs be parameterized by a vector \u03be \u2208 \u03be. typically, the param-\neter vector \u03be includes information about the location and shape of the mfs. denote\nthe mfs by \u03c6i(\u00b7;\u03be) : x \u2192 r, i = 1, . . . , n, to highlight their dependence on \u03be (where\n\n "}, {"Page_number": 152, "text": "142\n\nchapter 4. fuzzy q-iteration\n\nthe dot stands for the argument x). the goal is to find a parameter vector that leads to\ngood mfs. in the suboptimality bounds provided by theorem 4.5, the quality of the\napproximator (and thereby the quality of the mfs) is indirectly represented by the\n\nminimum distance \u03c2\u2217qi between q\u2217 and any fixed point of the mapping f \u25e6 p. how-\never, since q\u2217 is not available, \u03c2\u2217qi cannot be directly computed nor used to evaluate\nthe mfs.\n\ninstead, we propose an score function (optimization criterion) that is directly\nrelated to the performance of the policy obtained. specifically, we aim to find an\noptimal parameter vector \u03be\u2217 that maximizes the weighted sum of the returns from a\nfinite set x0 of representative states:\n\ns(\u03be) = \u2211\nx0\u2208x0\n\nw(x0)rh(x0)\n\n(4.38)\n\nthe policy h is computed by running synchronous or asynchronous fuzzy q-iteration\nto (near-)convergence with the mfs specified by the parameters \u03be. the representative\nstates are weighted by the function w : x0 \u2192 (0, 1]. this score function was discussed\nbefore in the context of basis function optimization, see (3.55) in section 3.6.1.\n\nthe infinite-horizon return from each representative state x0 is estimated using a\n\nsimulated trajectory of length k:\n\nrh(x0) =\n\nk\n\n\u2211\nk=0\n\n\u03b3k\u03c1(xk, h(xk))\n\n(4.39)\n\nin this trajectory, xk+1 = f (xk, h(xk)) for k \u2265 0. this simulation procedure is a vari-\nant of (3.64), specialized for the deterministic case considered in this chapter. by\nchoosing the length k of the trajectory using (3.65), a desired precision \u03b5mc can be\nguaranteed for the return estimate.\n\nthe set x0, together with the weight function w, determines the performance of\nthe resulting policy. a good choice of x0 and w will depend on the problem at hand.\nfor instance, if the process only needs to be controlled starting from a known set of\ninitial states, then x0 should be equal to (or included in) this set. initial states that are\ndeemed more important can be assigned larger weights.\n\nbecause each policy is computed by running fuzzy q-iteration with fixed mf\nparameters, this technique does not suffer from the convergence problems associated\nwith the adaptation of the approximator while running the dp/rl algorithm (see\nsection 3.6.3).\n\nthis technique is not restricted to a particular optimization algorithm to search\nfor \u03be\u2217. however, the score (4.38) can generally be a nondifferentiable function of\n\u03be, with multiple local optima, so a global, gradient-free optimization technique is\npreferable. in section 4.4.3, an algorithm will be given to optimize the locations\nof triangular mfs using the cross-entropy (ce) method for optimization. first, the\nnecessary background on ce optimization is outlined in the next section.\n\n "}, {"Page_number": 153, "text": "4.4. optimizing the membership functions\n\n143\n\n4.4.2 cross-entropy optimization\n\nthis section provides a brief introduction to the ce method for optimization (ru-\nbinstein and kroese, 2004). in this introduction, the information presented and the\nnotation employed are specialized to the application of ce optimization for finding\nmfs (to be used in fuzzy q-iteration). for a detailed, general description of the ce\nmethod, see appendix b.\n\nconsider the following optimization problem:\n\ns(\u03be)\n\nmax\n\u03be\u2208\u03be\n\n(4.40)\n\nwhere s : \u03be \u2192 r is the score function (optimization criterion) to maximize, and the\nparameters \u03be take values in the domain \u03be. denote the maximum by s\u2217. the ce\nmethod maintains a probability density with support \u03be. at each iteration, a number\nof samples are drawn from this density and the score values of these samples are\ncomputed. a (smaller) number of samples that have the best scores are kept, and the\nremaining samples are discarded. the probability density is then updated using the\nselected samples, such that at the next iteration the probability of drawing better sam-\nples is increased. the algorithm stops when the score of the worst selected sample\nno longer improves significantly.\n\nformally, a family of probability densities {p(\u00b7; v)} must be chosen, where the\ndot stands for the random variable \u03be. this family has support \u03be and is parameterized\nby v. at each iteration \u03c4 of the ce algorithm, a number nce of samples are drawn\nfrom the density p(\u00b7; v\u03c4\u22121), their scores are computed, and the (1\u2212 \u03c1ce) quantile5\n\u03bb\u03c4 of the sample scores is determined, with \u03c1ce \u2208 (0, 1). then, a so-called associated\nstochastic problem is defined, which involves estimating the probability that the score\nof a sample drawn from p(\u00b7; v\u03c4\u22121) is at least \u03bb\u03c4:\n\np\u03be\u223cp(\u00b7;v\u03c4\u22121)(s(\u03be) \u2265 \u03bb\u03c4) = e\u03be\u223cp(\u00b7;v\u03c4\u22121){i(s(\u03be) \u2265 \u03bb\u03c4)}\n\n(4.41)\n\nwhere i is the indicator function, equal to 1 whenever its argument is true, and 0\notherwise.\n\nthe probability (4.41) can be estimated by importance sampling. for this prob-\nlem, an importance sampling density is one that increases the probability of the in-\nteresting event s(\u03be) \u2265 \u03bb\u03c4. an optimal importance sampling density in the family\n{p(\u00b7; v)}, in the smallest cross-entropy (kullback\u2013leibler divergence) sense, is given\nby a solution of:\n\narg max\n\nv\n\ne\u03be\u223cp(\u00b7;v\u03c4\u22121){i(s(\u03be) \u2265 \u03bb\u03c4) ln p(\u03be; v)}\n\n(4.42)\n\nan approximate solution v\u03c4 of (4.42) is computed using:\n\nv\u03c4 = v\u2021\n\n\u03c4, where v\u2021\n\n\u03c4 \u2208 arg max\n\nv\n\n1\nnce\n\nnce\n\u2211\nis=1\n\ni(s(\u03beis) \u2265 \u03bb\u03c4) ln p(\u03beis ; v)\n\n(4.43)\n\n5if the score values of the samples are ordered increasingly and indexed such that s1 \u2264 \u00b7\u00b7\u00b7 \u2264 snce , then\n\nthe (1\u2212 \u03c1ce) quantile is \u03bb\u03c4 = s\u2308(1\u2212\u03c1ce)nce\u2309.\n\n "}, {"Page_number": 154, "text": "144\n\nchapter 4. fuzzy q-iteration\n\nonly the samples that satisfy s(ais) \u2265 \u03bb\u03c4 contribute to this formula, since the contri-\nbutions of the other samples are made to be zero by the product with the indicator\nfunction. in this sense, the updated density parameter only depends on these best\nsamples, and the other samples are discarded.\n\nce optimization proceeds with the next iteration using the new density parame-\nter v\u03c4 (the probability (4.41) is never actually computed). the updated density aims\nat generating good samples with higher probability than the old density, thus bring-\ning \u03bb\u03c4+1 closer to the optimum s\u2217. the goal is to eventually converge to a density\nthat generates samples close to optimal value(s) of \u03be with very high probability. the\nalgorithm can be stopped when the (1 \u2212 \u03c1ce)-quantile of the sample performance\nimproves for dce > 1 consecutive iterations, but these improvements do not exceed\n\u03b5ce; alternatively, the algorithm stops when a maximum number of iterations \u03c4max is\nreached. then, the largest score among the samples generated in all the iterations is\ntaken as the approximate solution of the optimization problem, and the correspond-\ning sample as an approximate location of the optimum. note that ce optimization\ncan also use a so-called smoothing procedure to incrementally update the density\nparameters, but we do not employ such a procedure in this chapter (see appendix b\nfor details on this procedure).\n\n4.4.3 fuzzy q-iteration with cross-entropy optimization of the mem-\n\nbership functions\n\nin this section, a complete algorithm is given for finding optimal mfs to be used\nin fuzzy q-iteration. this algorithm employs the ce method to optimize the cores\nof triangular mfs (example 4.1). triangular mfs are chosen because they are the\nsimplest mfs that ensure the convergence of fuzzy q-iteration. ce optimization is\nchosen as an illustrative example of a global optimization technique that can be used\nfor this problem. many other optimization algorithms could be applied to optimize\nthe mfs, e.g., genetic algorithms (goldberg, 1989), tabu search (glover and laguna,\n1997), pattern search (torczon, 1997; lewis and torczon, 2000), etc.\n\nrecall that the state space dimension is denoted by d. in this section, it is as-\n\nsumed that the state space is a hyperbox centered on the origin:\n\nx = [\u2212xmax,1, xmax,1]\u00d7\u00b7\u00b7\u00b7\u00d7 [\u2212xmax,d, xmax,d]\n\nwhere xmax,d \u2208 (0, \u221e), d = 1, . . . , d. separately for each state variable xd, a trian-\ngular fuzzy partition is defined with core values cd,1 < \u00b7\u00b7\u00b7 < cd,nd , which give nd\ntriangular mfs. the product of each combination of single-dimensional mfs gives\na pyramid-shaped d-dimensional mf in the fuzzy partition of x . the parameters to\nbe optimized are the (scalar) free cores on each axis. the first and last core values on\neach axis are not free, but are always equal to the limits of the domain: cd,1 = \u2212xmax,d\nand cd,nd = xmax,d, hence, the number of free cores is only n\u03be = \u2211d\nd=1(nd \u2212 2). the\nparameter vector \u03be can be obtained by collecting the free cores:\n\n\u03be = [c1,2, . . . , c1,n1\u22121, . . . . . . , cd,2, . . . , cd,nd\u22121]t\n\n "}, {"Page_number": 155, "text": "4.5. experimental study\n\n145\n\nand has the domain:\n\n\u03be = (\u2212xmax,1, xmax,1)n1\u22122 \u00d7\u00b7\u00b7\u00b7\u00d7 (\u2212xmax,d, xmax,d)nd\u22122\n\nthe goal is to find a parameter vector \u03be\u2217 that maximizes the score function (4.38).\n\nto apply ce optimization, we choose a family of densities with independent\n(univariate) gaussian components for each of the n\u03be parameters.the gaussian den-\nsity for the ith parameter is determined by its mean \u03b7i and standard deviation \u03c3i.\nusing gaussian densities has the advantage that (4.43) has a closed-form solution\n(rubinstein and kroese, 2004), given by the mean and standard deviation of the best\nsamples. by exploiting this property, simple update rules can be obtained for the\ndensity parameters, see, e.g., line 13 of the upcoming algorithm 4.3. note also that\nwhen the gaussian family is used, the ce method can actually converge to a pre-\ncise optimum location \u03be\u2217, by letting each univariate gaussian density converge to a\ndegenerate (dirac) distribution that assigns all the probability mass to the value \u03be\u2217i .\nthis degenerate distribution is obtained for \u03b7i = \u03be\u2217i and \u03c3i \u2192 0.\nbecause the support of the chosen density is rn\u03be , which is larger than \u03be, samples\nthat do not belong to \u03be are rejected and generated again. the density parameter vector\nv consists of the vector of means \u03b7 and the vector of standard deviations \u03c3, each of\nthem containing n\u03be elements. the vectors \u03b7 and \u03c3 are initialized using:\n\n\u03b70 = 0, \u03c30 = [xmax,1, . . . , xmax,1, . . . . . . , xmax,d, . . . , xmax,d]t\n\nwhere each bound xmax,d is replicated nd \u2212 2 times, for d = 1, . . . , d. these values\nensure that the samples cover the state space well in the first iteration of the ce\nmethod.\n\nalgorithm 4.3 summarizes fuzzy q-iteration with ce optimization of the mfs.\neither the synchronous or the asynchronous variant of fuzzy q-iteration could be\nused at line 7 of this algorithm, but the variant should not be changed during the\nce procedure, since the two variants can produce different solutions for the same\nmfs and convergence threshold. at line 10 of algorithm 4.3, the samples are sorted\nin an ascending order of their scores, to simplify the subsequent formulas. at line\n13, the closed-form update of the gaussian density parameters is employed. in these\nupdates, the mathematical operators (e.g., division by a constant, square root) should\nbe understood as working element-wise, separately for each element of the vectors\nconsidered. for a description of the stopping condition and parameters of the ce\nmethod, see again section 4.4.2 or, for more details, appendix b.\n\n4.5 experimental study\n\nwe dedicate the remainder of this chapter to an extensive experimental study of fuzzy\nq-iteration. this study is organized in four parts, each focusing on different aspects\nrelevant to the practical application of the algorithm. the first example illustrates the\n\n "}, {"Page_number": 156, "text": "146\n\nchapter 4. fuzzy q-iteration\n\nalgorithm 4.3 fuzzy q-iteration with cross-entropy mf optimization.\ninput: dynamics f , reward function \u03c1, discount factor \u03b3,\n\nset of discrete actions ud, fuzzy q-iteration convergence threshold \u03b5qi,\nrepresentative states x0, weight function w,\nce parameters \u03c1ce \u2208 (0, 1), nce \u2265 2, dce \u2265 2, \u03c4max \u2265 2, \u03b5ce \u2265 0\n\n1: \u03c4 \u2190 0\n2: \u03b70 \u2190 0, \u03c30 \u2190 [xmax,1, . . . , xmax,1, . . . . . . , xmax,d, . . . , xmax,d]t\n3: repeat\n4:\n\n\u03c4 \u2190 \u03c4+ 1\ngenerate samples \u03be1, . . . ,\u03bence from gaussians given by \u03b7\u03c4\u22121 and \u03c3\u03c4\u22121\nfor is = 1, . . . , nce do\n\nrun fuzzy q-iteration with mfs \u03c6i(x;\u03beis), actions ud, and threshold \u03b5qi\ncompute score s(\u03beis) of resulting policy h, using (4.38)\n\nend for\nreorder and reindex samples s.t. s1 \u2264 \u00b7\u00b7\u00b7 \u2264 snce\n\u03bb\u03c4 \u2190 s\u2308(1\u2212\u03c1ce)nce\u2309, the (1\u2212 \u03c1ce) quantile of the sample scores\ni\u03c4 \u2190 \u2308(1\u2212 \u03c1ce)nce\u2309, index of the first of the best samples\nis=i\u03c4 (\u03beis \u2212 \u03b7\u03c4)2\n\u03b7\u03c4 \u2190 1\n\nnce\u2212i\u03c4+1 \u2211nce\n\nnce\u2212i\u03c4+1 \u2211nce\n\n\u03beis ; \u03c3\u03c4 \u2190q 1\n\nis=i\u03c4\n\n14: until (\u03c4 > dce and |\u03bb\u03c4\u2212\u03c4\u2032 \u2212 \u03bb\u03c4\u2212\u03c4\u2032\u22121| \u2264 \u03b5ce, for \u03c4\u2032 = 0, . . . , dce \u2212 1) or \u03c4 = \u03c4max\noutput: best sampleb\u03be\u2217, its score, and corresponding fuzzy q-iteration solution\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\nconvergence and consistency of fuzzy q-iteration, using a dc motor problem. the\nsecond example employs a two-link manipulator to demonstrate the effects of action\ninterpolation, and also to compare fuzzy q-iteration with fitted q-iteration. in the\nthird example, the real-time control performance of fuzzy q-iteration is illustrated\nusing an inverted pendulum swing-up problem. for these three examples, the mfs\nare designed in advance. in the fourth and final example, the effects of optimizing the\nmfs (with the ce approach of section 4.4) are studied in the classical car-on-the-hill\nbenchmark.\n\n4.5.1 dc motor: convergence and consistency study\n\nthis section illustrates the practical impact of the convergence and consistency prop-\nerties of fuzzy q-iteration, using the dc motor problem introduced in section 3.4.5.\nthe dc motor system is chosen because its simplicity allows extensive simulations\nto be performed with reasonable computational costs. first, the convergence rates of\nsynchronous and asynchronous fuzzy q-iteration are empirically compared. then,\nthe change in solution quality as the approximation power increases is investigated,\nto illustrate how the consistency properties of fuzzy q-iteration influence its behav-\nior in practice. recall that consistency was proven under the condition of lipschitz\ncontinuous dynamics and rewards (assumption 4.1). to examine the impact of vio-\n\n "}, {"Page_number": 157, "text": "4.5. experimental study\n\n147\n\nlating this condition, we introduce discontinuities in the reward function, and repeat\nthe consistency study.\n\ndc motor problem\n\nconsider the second-order discrete-time model of the dc motor:\n\nf (x, u) = ax + bu\n\na =(cid:20)1\n\n0\n\n0.0049\n\n0.9540(cid:21) , b =(cid:20)0.0021\n0.8505(cid:21)\n\n(4.44)\n\nwhere x1 = \u03b1 \u2208 [\u2212\u03c0,\u03c0] rad is the shaft angle, x2 = \u02d9\u03b1 \u2208 [\u221216\u03c0, 16\u03c0] rad/s is the\nangular velocity, and u\u2208 [\u221210, 10] v is the control input (voltage). the state variables\nare restricted to their domains using saturation. the control goal is to stabilize the\nsystem around x = 0, and is described by the quadratic reward function:\n\n\u03c1(x, u) = \u2212xtqrewx\u2212 rrewu2\nqrew =(cid:20)5\n\n0.01(cid:21) , rrew = 0.01\n\n0\n\n0\n\n(4.45)\n\nwith discount factor \u03b3 = 0.95. this reward function is shown in figure 4.3.\n\n0\n\n)\n0\n\n\u221220\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\n\u03c1\n\n\u221240\n\n\u221260\n\n\u221280\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\nfigure 4.3\na state-dependent slice through the reward function (4.45), for u = 0. reproduced with per-\nmission from (bus\u00b8oniu et al., 2008b), c(cid:13) 2008 ieee.\n\nsynchronous and asynchronous convergence\n\nfirst, we compare the convergence rates of synchronous and asynchronous fuzzy q-\niteration. a triangular fuzzy partition with n\u2032 = 41 equidistant cores for each state\nvariable is defined, leading to n = 412 fuzzy sets in the two-dimensional partition\nof x . the action space is discretized into 15 equidistant values. first, synchronous\nfuzzy q-iteration is run with a very small threshold \u03b5qi = 10\u22128, to obtain an accu-\n\nrate approximationb\u03b8\u2217 of the optimal parameter vector \u03b8\u2217. then, in order to compare\nhow synchronous and asynchronous fuzzy q-iteration approach b\u03b8\u2217, the two algo-\nrithms are run until their parameter vectors are closer than 10\u22125 tob\u03b8\u2217 in the infinity\nnorm, i.e., until k\u03b8\u2113 \u2212b\u03b8\u2217k\u221e \u2264 10\u22125. for these experiments, as well as throughout\n\n "}, {"Page_number": 158, "text": "148\n\nchapter 4. fuzzy q-iteration\n\nthe remaining examples of this chapter, the parameter vector of fuzzy q-iteration is\ninitialized to zero.\n\nnumber of iterations \u2113, for both variants of fuzzy q-iteration. the asynchronous algo-\n\nfigure 4.4 presents the evolution of the distance between \u03b8\u2113 and b\u03b8\u2217 with the\nrithm approachesb\u03b8\u2217 faster than the synchronous one, and gets within a 10\u22125 distance\n\n20 iterations earlier (in 112 iterations, whereas the synchronous algorithm requires\n132). because the time complexity of one iteration is nearly the same for the two\nalgorithms, this generally translates into computational savings for the asynchronous\nversion.\n\ns\nr\ne\n\nt\n\ne\nm\na\nr\na\np\n\n \nl\n\na\nm\n\ni\nt\n\np\no\n\u2212\nr\na\ne\nn\n\n \n\no\n\nt\n \n\ne\nc\nn\na\n\nt\ns\nd\n\ni\n\n3\n10\n\n1\n10\n\n\u22121\n\n10\n\n\u22123\n\n10\n\n\u22125\n\n10\n\n \n0\n\n \n\nsynchronous\nasynchronous\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n140\n\niteration number\n\nfigure 4.4 convergence of synchronous and asynchronous fuzzy q-iteration.\n\nfigure 4.5 shows a representative fuzzy q-iteration solution for the dc motor\n\n(specifically the solution corresponding to b\u03b8\u2217). a state-dependent slice through the\n\napproximate q-function is shown (obtained by setting the action u to 0), together\nwith a greedy policy resulting from this q-function and a controlled trajectory.\n\nfor the experiments in the remainder of this chapter, we will always employ syn-\nchronous fuzzy q-iteration,6 while referring to it simply as \u201cfuzzy q-iteration,\u201d for\nthe sake of conciseness. because small convergence thresholds \u03b5qi will be imposed,\nthe parameter vectors obtained will always be near optimal, and therefore near to\nthose that would be obtained by the asynchronous algorithm. hence, the conclusions\nof the experiments also apply to asynchronous fuzzy q-iteration.\n\nconsistency and the effect of discontinuous rewards\n\nnext, we investigate how the quality of the fuzzy q-iteration solution changes as the\napproximation power increases, to illustrate the practical impact of the consistency\nproperties of the algorithm. a triangular fuzzy partition with n\u2032 equidistant cores for\neach state variable is defined, leading to a total number of n = n\u20322 fuzzy sets. the\nvalue of n\u2032 is gradually increased from 3 to 41. similarly, the action is discretized\n\n6the reason for this choice is implementation-specific. namely, each synchronous iteration can be\nrewritten as a matrix multiplication, which in our matlab r(cid:13) implementation is executed using highly\nefficient low-level routines. the matrix implementation of synchronous fuzzy q-iteration therefore runs\nmuch faster than the element-by-element updates of asynchronous fuzzy q-iteration. if a specialized li-\nbrary for linear algebra were not available, the asynchronous algorithm would have the same cost per\niteration as the synchronous one, and would be preferable because it can converge in fewer iterations, as\npredicted by the theory.\n\n "}, {"Page_number": 159, "text": "4.5. experimental study\n\n149\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212200\n\n\u2212400\n\n\u2212600\n\n\u2212800\n50\n\n50\n\n0\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n\u221250\n\n \n\n\u22122\n\n(a) slice through q-function for u = 0.\n\n0\n\n\u03b1 [rad]\n\n2\n\n(b) policy.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n(c) controlled trajectory from x0 = [\u2212\u03c0, 0]t.\n\nfigure 4.5 a fuzzy q-iteration solution for the dc motor.\n\ninto m equidistant values, with m \u2208 {3, 5, . . . , 15} (only odd values are used because\nthe 0 action is necessary to avoid chattering). the convergence threshold is set to\n\u03b5qi = 10\u22125 to ensure that the obtained parameter vector is close to the fixed point of\nthe algorithm.\n\nin a first set of experiments, fuzzy q-iteration is run for each combination of n\nand m, with the original reward function (4.45). recall that the consistency of fuzzy\nq-iteration was proven under lipschitz continuity assumptions on the dynamics and\nrewards (assumption 4.1). indeed, the transition function (4.44) of the dc motor is\nlipschitz continuous with a lipschitz constant l f \u2264 max{kak2,kbk2} (this bound\nfor l f holds for any system with linear dynamics), and the reward function (4.45) is\nalso lipschitz continuous, since it is smooth and has bounded support. so, for this\nfirst set of experiments, the consistency of fuzzy q-iteration is guaranteed, and its\nsolutions are expected to improve as n and m increase.\n\nthe aim of a second set of experiments is to study the practical effect of violating\nlipschitz continuity, by adding discontinuities to the reward function. discontinuous\nrewards are common practice due to the origins of reinforcement learning (rl) in\n\n "}, {"Page_number": 160, "text": "150\n\nchapter 4. fuzzy q-iteration\n\nartificial intelligence, where discrete-valued tasks are often considered. in our exper-\niments, the choice of the discontinuous reward function cannot be arbitrary. instead,\nto ensure a meaningful comparison between the solutions obtained with the original\nreward function (4.45) and those obtained with the new reward function, the quality\nof the policies must be preserved. one way to do this is to add a term of the form\n\u03b3\u03c8( f (x, u))\u2212\u03c8(x) to each reward \u03c1(x, u), where \u03c8 : x \u2192 r is an arbitrary bounded\nfunction (ng et al., 1999):\n\n\u03c1\u2032(x, u) = \u03c1(x, u) + \u03b3\u03c8( f (x, u))\u2212 \u03c8(x)\n\n(4.46)\n\n\u03c1\u2032 \u2212 q\u2217\u03c1\u2032 = qh\n\nthe quality of the policies is preserved by reward modifications of this form, in the\n\u03c1 \u2212 q\u2217\u03c1, where q\u03c1 denotes a q-function\nsense that for any policy h, qh\nunder the reward function \u03c1, and q\u03c1\u2032 a q-function under \u03c1\u2032. indeed, it is easy to\nshow by replacing \u03c1\u2032 in the expression (2.2) for the q-function, that for any policy\nh, including any optimal policy, we have that qh\n\u03c1(x, u)\u2212 \u03c8(x) \u2200x, u (ng\net al., 1999). in particular, a policy is optimal for \u03c1\u2032 if and only if it is optimal for \u03c1.\nwe choose a discontinuous function \u03c8, which is positive only in a rectangular\n\n\u03c1\u2032 (x, u) = qh\n\nregion around the origin:\n\n\u03c8(x) =(10\n\n0\n\nif |x1| \u2264 \u03c0/4 and |x2| \u2264 4\u03c0\notherwise\n\n(4.47)\n\nwith this form of \u03c8, the newly added term in (4.46) rewards transitions that take the\nstate inside the rectangular region, and penalizes transitions that take it outside. a\nrepresentative slice through the resulting reward function is presented in figure 4.6\n(compare with figure 4.3). the additional positive rewards are visible as crests above\nthe quadratic surface. the penalties are not visible in the figure, because their corre-\nsponding, downward-oriented crests are situated under the surface.\n\n0\n\n)\n0\n\n\u221220\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\n\u2019\n\u03c1\n\n\u221240\n\n\u221260\n\n\u221280\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\nfigure 4.6\na slice through the modified reward function (4.46), for u = 0. reproduced with permission\nfrom (bus\u00b8oniu et al., 2008b), c(cid:13) 2008 ieee.\n\nthe performance (score) of the policies obtained with fuzzy q-iteration is given\nin figure 4.7. each point in these graphs corresponds to the return of the policy\nobtained, averaged over the grid of initial states:\n\nx0 = {\u2212\u03c0,\u22125\u03c0/6,\u22124\u03c0/6, . . . ,\u03c0}\u00d7{\u221216\u03c0,\u221214\u03c0, . . . , 16\u03c0}\n\n(4.48)\n\n "}, {"Page_number": 161, "text": "4.5. experimental study\n\n151\n\nthe returns are evaluated using simulation (4.39), with a precision of \u03b5mc = 0.1.\nwhile the reward functions used for q-iteration are different, the performance eval-\nuation is always done with the reward (4.45), to allow an easier comparison. as\nalready explained, the change in the reward function preserves the quality of the poli-\ncies, so comparing policies in this way is meaningful. the qualitative evolution of the\nperformance is similar when evaluated with the modified reward function (4.46) with\n\u03c8 as in (4.47).\n\ne\nr\no\nc\ns\n\n\u2212200\n\n\u2212250\n\n\u2212300\n\n\u2212350\n\n\u2212400\n15\n\n13\n\n11\n\n9\n\nm\n\n7\n\n5\n\n3\n\n3\n\n8\n\n13 18 23 28 33 3841\n\nn\u2019\n\ne\nr\no\nc\ns\n\n\u2212200\n\n\u2212250\n\n\u2212300\n\n\u2212350\n\n\u2212400\n15\n\n13\n\n11\n\n9\n\nm\n\n7\n\n5\n\n3\n\n3\n\n8\n\n13 18 23 28 33 3841\n\nn\u2019\n\n(a) quadratic reward (4.45).\n\n(b) discontinuous reward (4.46); evaluation with\nquadratic reward.\n\ne\nr\no\nc\ns\n\n\u2212203\n\n\u2212204\n\n\u2212205\n\n\u2212206\n\n\u2212207\n15\n\n13\n\n11\n\n9\n\nm\n\n7\n\n5\n\n3\n\n10 14 18 22 26 30 34 38 41\n\nn\u2019\n\n\u2212200\n\n\u2212250\n\ne\nr\no\nc\ns\n\n\u2212300\n\n\u2212350\n\n\u2212400\n\n \n\n \n\nscore, quadratic reward\nscore, discontinuous reward\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\nn\u2019\n\n(c) quadratic reward, detail.\n\n(d) average performance over m, for varying n\u2032.\n\nfigure 4.7\nthe performance of fuzzy q-iteration as a function of n and m, for quadratic and discontin-\nuous rewards in the dc motor problem. reproduced with permission from (bus\u00b8oniu et al.,\n2008b), c(cid:13) 2008 ieee.\n\nwhen the continuous reward is used, the performance of fuzzy q-iteration is\n\nures 4.7(a) and 4.7(c). also, the influence of the number of discrete actions is small\n\nalready near optimal for n\u2032 = 20 and is relatively smooth for n\u2032 \u2265 20, see fig-\nfor n\u2032 \u2265 4. so, the consistency properties of fuzzy q-iteration have a clear beneficial\n\neffect. however, when the reward is changed to the discontinuous function (4.46),\nthus violating the assumptions for consistency, the performance indeed varies signif-\nicantly as n\u2032 increases, see figure 4.7(b). for many values of n\u2032, the influence of\nm also becomes significant. additionally, for many values of n\u2032 the performance is\nworse than with the continuous reward function, see figure 4.7(d).\n\nan interesting and somewhat counterintuitive fact is that the performance is not\nmonotonic in n\u2032 and m. for a given value of n\u2032, the performance sometimes de-\n\n "}, {"Page_number": 162, "text": "152\n\nchapter 4. fuzzy q-iteration\n\ncreases as m increases. a similar effect occurs as m is kept fixed and n\u2032 varies. this\neffect is present with both reward functions, but is more pronounced in figure 4.7(b)\nthan in figures 4.7(a) and 4.7(c). the magnitude of the changes decreases signifi-\ncantly as n\u2032 and m become large in figures 4.7(a) and 4.7(c); this is not the case in\nfigure 4.7(b).\n\nthe negative effect of reward discontinuities on the consistency of the algorithm\ncan be intuitively explained as follows. the discontinuous reward function (4.46)\nleads to discontinuities in the optimal q-function. as the placement of the mfs\nchanges with increasing n\u2032, the accuracy with which the fuzzy approximator captures\nthese discontinuities changes as well. this accuracy depends less on the number of\nmfs, than on their positions (mfs should be ideally concentrated around the discon-\ntinuities). so, it may happen that for a certain, smaller value of n\u2032 the performance\nis better than for another, larger value. in contrast, the smoother optimal q-function\nresulting from the continuous reward function (4.45) is easier to approximate using\ntriangular mfs.\n\na similar behavior of fuzzy q-iteration was observed in additional experiments\nwith other discontinuous reward functions. in particular, adding more discontinu-\nities similar to those in (4.46) does not significantly influence the evolution of the\nperformance in figure 4.7(b). decreasing the magnitude of the discontinuities (e.g.,\nreplacing the value 10 by 1 in (4.46)) decreases the magnitude of the performance\nvariations, but they are still present and they do not decrease as n and m increase.\n\n4.5.2 two-link manipulator: effects of action interpolation, and com-\n\nparison with fitted q-iteration\n\nin this section, fuzzy q-iteration is applied to stabilize a two-link manipulator op-\nerating in a horizontal plane. using this problem, the effects of employing the\ncontinuous-action, interpolated policy (4.15) are investigated, and fuzzy q-iteration\nis compared with fitted q-iteration (algorithm 3.4 of section 3.4.3). the two-link\nmanipulator example also illustrates that fuzzy q-iteration works well in problems\nhaving a higher dimensionality than the dc motor of section 4.5.1; the two-link\nmanipulator has four state variables and two action variables.\n\ntwo-link manipulator problem\n\nthe two-link manipulator, depicted in figure 4.8, is described by the fourth-order,\ncontinuous-time nonlinear model:\n\nm(\u03b1) \u00a8\u03b1+ c(\u03b1, \u02d9\u03b1) \u02d9\u03b1 = \u03c4\n\n(4.49)\n\nwhere \u03b1 = [\u03b11,\u03b12]t contains the angular positions of the two links, \u03c4 = [\u03c41,\u03c42]t\ncontains the torques of the two motors, m(\u03b1) is the mass matrix, and c(\u03b1, \u02d9\u03b1) is the\ncoriolis and centrifugal forces matrix. the state signal contains the angles and angu-\nlar velocities: x = [\u03b11, \u02d9\u03b11,\u03b12, \u02d9\u03b12]t, and the control signal is u = \u03c4. the angles \u03b11,\u03b12\nvary in the interval [\u2212\u03c0,\u03c0) rad, and \u201cwrap around\u201d so that, e.g., a rotation of 3\u03c0/2\nfor the first link corresponds to a value \u03b11 = \u2212\u03c0/2. the angular velocities \u02d9\u03b11, \u02d9\u03b12\n\n "}, {"Page_number": 163, "text": "4.5. experimental study\n\n153\n\nm\n\n2\n\nl\n2\n\n\u03b1\n2\n\nmotor\n\n2\n\nm\n\n1\n\n\u03b1\n1\n\nl\n1\n\nmotor\n\n1\n\nfigure 4.8 schematic representation of the two-link manipulator.\n\nare restricted to the interval [\u22122\u03c0, 2\u03c0] rad/s using saturation, while the torques are\nconstrained as follows: \u03c41 \u2208 [\u22121.5, 1.5] nm, \u03c42 \u2208 [\u22121, 1] nm. the discrete time step\nis set to ts = 0.05 s, and the discrete-time dynamics f are obtained by numerically\nintegrating (4.49) between consecutive time steps.\n\nthe matrices m(\u03b1) and c(\u03b1, \u02d9\u03b1) have the following form:\n\nm(\u03b1) =(cid:20)p1 + p2 + 2p3 cos\u03b12 p2 + p3 cos\u03b12\n(cid:21)\nc(\u03b1, \u02d9\u03b1) =(cid:20)b1 \u2212 p3 \u02d9\u03b12 sin\u03b12 \u2212p3( \u02d9\u03b11 + \u02d9\u03b12) sin\u03b12\n\np2 + p3 cos\u03b12\n\np3 \u02d9\u03b11 sin\u03b12\n\nb2\n\np2\n\n(4.50)\n\n(cid:21)\n\nthe meaning and values of the physical variables in the system are given in table 4.1.\nusing these, the rest of the parameters in (4.50) can be computed as follows: p1 =\nm1c2\n\n2 + i2, and p3 = m2l1c2.\n\n1 + i1, p2 = m2c2\n\n1 + m2l2\n\ntable 4.1 parameters of the two-link manipulator.\n\nsymbol value\n\nunits meaning\n\nl1; l2\nm1; m2\ni1; i2\nc1; c2\nb1; b2\n\n0.4; 0.4\n1.25; 0.8\n0.066; 0.043\n0.2; 0.2\n0.08; 0.02\n\nm\nkg\nkg m2\nm\nkg/s\n\nlink lengths\nlink masses\nlink inertias\ncenter of mass coordinates\ndamping in the joints\n\nthe control goal is the stabilization of the system around \u03b1 = \u02d9\u03b1 = 0, and is\n\nexpressed by the quadratic reward function:\n\n\u03c1(x, u) = \u2212xtqrewx, with qrew = diag[1, 0.05, 1, 0.05]\n\n(4.51)\n\nthe discount factor is set to \u03b3 = 0.98, which is large enough to allow rewards around\nthe goal state to influence the values of states early in the trajectories, leading to an\noptimal policy that successfully stabilizes the manipulator.\n\n "}, {"Page_number": 164, "text": "154\n\nchapter 4. fuzzy q-iteration\n\nresults of fuzzy q-iteration, and effects of using interpolated actions\n\nto apply fuzzy q-iteration, triangular fuzzy partitions are defined for every state\nvariable and then combined, as in example 4.1. for the angles, a core is placed\nin the origin, and 6 logarithmically-spaced cores are placed on each side of the\norigin. for the velocities, a core is placed in the origin, and 3 logarithmically-\nspaced cores are used on each side of the origin. this leads to a total number of\n\n(2\u00b7 6 + 1)2 \u00b7 (2\u00b7 3 + 1)2 = 8281 mfs. the cores are spaced logarithmically to ensure\n\na higher accuracy of the solution around the origin, while using only a limited number\nof mfs. this represents a mild form of prior knowledge about the importance of the\nstate space region close to the origin. each torque variable is discretized using 5 val-\nues: \u03c41 \u2208 {\u22121.5,\u22120.36, 0, 0.36, 1.5} and \u03c42 \u2208 {\u22121,\u22120.24, 0, 0.24, 1}. these values\nare logarithmically spaced along the two axes of the action space. the convergence\nthreshold is set to \u03b5qi = 10\u22125.\n\nwith these values, fuzzy q-iteration converged after 426 iterations. figure 4.9\ncompares the discrete-action results with the corresponding continuous-action re-\nsults. in particular, figure 4.9(a) depicts the discrete-action policy given by (4.13),\nwhile figure 4.9(b) depicts the interpolated, continuous-action policy computed with\n(4.15). the continuous-action policy is, of course, smoother than the discrete-action\npolicy. figures 4.9(c) and 4.9(d) show two representative trajectories of the two-link\nmanipulator, controlled by the discrete-action and continuous-action policies, respec-\ntively. both policies are able to stabilize the system after about 2 s. however, the\ndiscrete-action policy leads to more chattering of the control action and to a steady-\nstate error for the angle of the second link, whereas the continuous-action policy\nalleviates these problems.\n\ncompared to the dc motor, a larger number of triangular mfs and discrete ac-\ntions are generally required to represent the q-function for the manipulator problem,\nand the computational and memory demands of fuzzy q-iteration increase accord-\ningly. in fact, they increase exponentially with the number of state-action variables.\nfor concreteness, assume that for a general problem with d state variables and c ac-\ntion variables, n\u2032 triangular mfs are defined along each state dimension, and each ac-\ntion dimension is discretized into m\u2032 actions. then n\u2032dm\u2032c parameters are required,\nleading to a time complexity per iteration of o(n\u20322dm\u2032) and to a memory complexity\nof o(n\u2032dm\u2032) (see also section 4.3.3). the dc motor therefore requires n\u20322m\u2032 pa-\nrameters, o(n4m\u2032) computations per iteration, and o(n2m\u2032) memory, whereas the\nmanipulator requires n\u20324m\u20322 parameters (i.e., n\u20322m\u2032 times more than the dc motor),\no(n8m\u20322) computations per iteration, and o(n4m\u20322) memory.\n\ncomparison with fitted q-iteration\n\nnext, we compare the solution of fuzzy q-iteration with a solution obtained by fit-\nted q-iteration with a nonparametric approximator (algorithm 3.4). even though\nfitted q-iteration is a model-free, sample-based algorithm, it can easily be adapted\nto the model-based setting considered in this chapter by using the model to gener-\nate the samples. in order to make the comparison between the two algorithms more\nmeaningful, fitted q-iteration is supplied with the same state-action samples as those\n\n "}, {"Page_number": 165, "text": "4.5. experimental study\n\n155\n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n1\n\n \n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n1\n\n \n\n]\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n]\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n2\n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n\u22121.5\n\n]\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n \n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n]\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n2\n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n\u22121.5\n\n \n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n(a) a slice through the discrete-action policy, for\n\u02d9\u03b11 = \u02d9\u03b12 = 0 and parallel to the plane (\u03b11,\u03b12).\nthe fuzzy cores for the angle variables are rep-\nresented as small white disks with dark edges.\n\n(b) a similarly obtained slice through the\ncontinuous-action policy.\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n \n,\n\n\u03b1\n\n2\n\n1\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03b1\n\n \n,\n\n\u2019\n\n\u03b1\n\n2\n\n1\n\n]\n\nm\nn\n\n[\n \n\n\u03c4\n \n,\n\n\u03c4\n\n2\n\n1\n\n2\n\n0\n\n\u22122\n\n0\n\n5\n\n0\n\n\u22125\n\n0\n\n1\n\n0\n\n\u22121\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0\n\n\u221210\n\n\u221220\n\n0\n\n1\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2\n\n2\n\n3\n\n3\n\n3\n\n3\n\n4\n\n4\n\n4\n\n4\n\n5\n\n5\n\n5\n\n5\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n \n,\n\n\u03b1\n\n2\n\n1\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03b1\n\n \n,\n\n\u2019\n\n\u03b1\n\n2\n\n1\n\n]\n\nm\nn\n\n[\n \n\n\u03c4\n \n,\n\n\u03c4\n\n2\n\n1\n\n2\n\n0\n\n\u22122\n\n0\n\n5\n\n0\n\n\u22125\n\n0\n\n1\n\n0\n\n\u22121\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0\n\n\u221210\n\n\u221220\n\n0\n\n1\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2\n\n2\n\n3\n\n3\n\n3\n\n3\n\n4\n\n4\n\n4\n\n4\n\n5\n\n5\n\n5\n\n5\n\nt [s]\n\nt [s]\n\n(c) a trajectory controlled by the discrete-action\npolicy (thin black line \u2013 link 1, thick gray line \u2013\n\n(d) a trajectory from x0 = [\u2212\u03c0, 0,\u2212\u03c0, 0]t, con-\n\ntrolled by the continuous-action policy.\n\nlink 2). the initial state is x0 = [\u2212\u03c0, 0,\u2212\u03c0, 0]t.\nfigure 4.9\nresults of fuzzy q-iteration for the two-link manipulator. the discrete-action results are shown\non the left-hand side of the figure, and the continuous-action results on the right-hand side.\n\n "}, {"Page_number": 166, "text": "156\n\nchapter 4. fuzzy q-iteration\n\nemployed by fuzzy q-iteration, namely the cross-product of the 8281 mf cores and\nthe 25 discrete actions, leading to a total number of 207 025 samples.\n\nto apply fitted q-iteration, we choose a nonparametric approximator that com-\nbines a discretization of the action space with ensembles of extremely randomized\ntrees (extra-trees) (geurts et al., 2006) to approximate over the state space. a distinct\nensemble is used for each of the discrete actions, in analogy to the fuzzy approxima-\ntor. the discrete actions are the same as for fuzzy q-iteration above. each ensemble\nconsists of ntr = 50 extremely randomized trees, and the tree construction parame-\nters are set to their default values, as described next. the first parameter, ktr, is the\nnumber of cut directions evaluated when splitting a node, and is set equal to 4, which\nis the dimensionality of the input to the regression trees (the 4-dimensional state vari-\nable). the second parameter, nmin\n, is the minimum number of samples that has to be\nassociated with a node in order to split that node further, and is set equal to 2, so\nthe trees are fully developed. for a more detailed description of the ensembles of\nextremely randomized trees, see appendix a. note that a similar q-function approx-\nimator was used in our application of fitted q-iteration to the dc motor, discussed in\nsection 3.4.5.\n\ntr\n\nfitted q-iteration is run for a predefined number of 400 iterations, and the\nq-function found after the 400th iteration is considered satisfactory. figure 4.10\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n1\n\n \n\n1.5\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n\u22121.5\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n \n,\n\n\u03b1\n\n2\n\n1\n\n2\n\n0\n\n\u22122\n\n0\n\n5\n\n0\n\n\u22125\n\n0\n\n1\n\n0\n\n\u22121\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03b1\n\n \n,\n\n\u2019\n\n\u03b1\n\n2\n\n1\n\n]\n\nm\nn\n\n[\n \n\n\u03c4\n \n,\n\n\u03c4\n\n2\n\n1\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0\n\n\u221210\n\n\u221220\n\n0\n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n2\n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n \n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n1\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2\n\n2\n\n3\n\n3\n\n3\n\n3\n\n4\n\n4\n\n4\n\n4\n\n5\n\n5\n\n5\n\n5\n\n(a) a slice through the policy for \u02d9\u03b11 = \u02d9\u03b12 = 0.\n\nt [s]\n\n(b) a controlled\n\ntrajectory\n\n[\u2212\u03c0, 0,\u2212\u03c0, 0]t.\n\nfrom x0 =\n\nfigure 4.10 results of fitted q-iteration for the two-link manipulator.\n\n "}, {"Page_number": 167, "text": "4.5. experimental study\n\n157\n\npresents a greedy policy resulting from this q-function, together with a representative\ncontrolled trajectory. although it roughly resembles the fuzzy q-iteration policies of\nfigures 4.9(a) and 4.9(b), the fitted q-iteration policy of figure 4.10(a) contains spu-\nrious (and probably incorrect) actions for many states. the policy obtained by fitted\nq-iteration stabilizes the system more poorly in figure 4.10(b), than the solution of\nfuzzy q-iteration in figures 4.9(c) and 4.9(d). so, in this case, fuzzy q-iteration with\ntriangular mfs outperforms fitted q-iteration with extra-trees approximation.\n\nnote that instead of building a distinct ensemble of extra-trees for each of the dis-\ncrete actions, fitted q-iteration could also work with a single ensemble of trees that\ntake continuous state-continuous action pairs as inputs. this might lead to a better\nperformance, as it would allow the algorithm to identify structure along the action\ndimensions of the q-functions. however, it would also make the results less com-\nparable with those of fuzzy q-iteration, which always requires action discretization,\nand for this reason we do not adopt this solution here.\n\n4.5.3 inverted pendulum: real-time control\n\nnext, fuzzy q-iteration is used to swing up and to stabilize a real-life underactu-\nated inverted pendulum. this application illustrates the performance of the fuzzy\nq-iteration solutions in real-time control.\n\ninverted pendulum problem\n\nthe inverted pendulum is obtained by placing a mass off-center on a disk that rotates\nin a vertical plane and is driven by a dc motor (figure 4.11).7 note that this dc\nmotor is the same system which was modeled for use in simulations in section 4.5.1,\nand earlier throughout the examples of chapter 3. the control voltage is limited so\nthat the motor does not provide enough power to push the pendulum up in a single\nrotation. instead, the pendulum needs to be swung back and forth (destabilized) to\ngather energy, prior to being pushed up and stabilized. this creates a difficult, highly\nnonlinear control problem.\n\nthe continuous-time dynamics of the inverted pendulum are:\n\n\u00a8\u03b1 =\n\n1\n\nj(cid:18)mgl sin(\u03b1)\u2212 b \u02d9\u03b1\u2212\n\nk2\nr\n\n\u02d9\u03b1+\n\nk\n\nr\n\nu(cid:19)\n\n(4.52)\n\ntable 4.2 shows the meanings and values of the parameters appearing in this equa-\ntion. note that some of these parameters (e.g., j and m) are rough estimates, and that\nthe real system exhibits unmodeled dynamics such as static friction. the state signal\nconsists of the angle and the angular velocity of the pendulum, i.e., x = [\u03b1, \u02d9\u03b1]t. the\nangle \u03b1 \u201cwraps around\u201d in the interval [\u2212\u03c0,\u03c0) rad, where \u03b1 = \u2212\u03c0 corresponds to\npointing down and \u03b1 = 0 corresponds to pointing up. the velocity \u02d9\u03b1 is restricted to\n\n7this is different from the classical cart-pendulum system, in which the pendulum is attached to a\ncart and is indirectly actuated via the acceleration of the cart (e.g., doya, 2000; riedmiller et al., 2007).\nhere, the pendulum is actuated directly, and the system only has two state variables, as opposed to the\ncart-pendulum, which has four.\n\n "}, {"Page_number": 168, "text": "158\n\nchapter 4. fuzzy q-iteration\n\nm\n\n\u03b1\n\nl\n\nmotor\n\n(a) the real inverted pendulum system.\n\n(b) a schematic representation.\n\nfigure 4.11 the inverted pendulum.\n\nthe interval [\u221215\u03c0, 15\u03c0] rad/s using saturation, and the control action (voltage) u is\nconstrained to [\u22123, 3] v. the sample time ts is chosen to be 0.005 s, and the discrete-\ntime dynamics f are obtained by numerically integrating (4.52) between consecutive\ntime steps.\n\ntable 4.2 parameters of the inverted pendulum.\n\nsymbol value\n\nunits\n\nmeaning\n\nm\ng\nl\nj\nb\nk\nr\n\n0.055\n9.81\n0.042\n1.91\u00b7 10\u22124\n3\u00b7 10\u22126\n0.0536\n9.5\n\nmass\nkg\nm/s2\ngravitational acceleration\ndistance from center of disk to mass\nm\nkg m2\nmoment of inertia\nnms/rad viscous damping\nnm/a\n\u03c9\n\ntorque constant\nrotor resistance\n\nthe goal is to stabilize the pendulum in the unstable equilibrium x = 0 (pointing\n\nup). the following quadratic reward function is chosen to express this goal:\n\n\u03c1(x, u) = \u2212xtqrewx\u2212 rrewu2\nqrew =(cid:20)5\n0 0.1(cid:21) , rrew = 1\n\n0\n\n(4.53)\n\nthe discount factor is \u03b3 = 0.98. this discount factor is large so that rewards around\nthe goal state (pointing up) influence the values of states early in the trajectories. this\nleads to an optimal policy that successfully swings up and stabilizes the pendulum.\n\nresults of fuzzy q-iteration\n\ntriangular fuzzy partitions with 19 equidistant cores are defined for both state vari-\nables, and then combined as in example 4.1. this relatively large number of mfs is\n\n "}, {"Page_number": 169, "text": "4.5. experimental study\n\n159\n\nchosen to ensure a good accuracy of the solution. the control action is discretized\nusing 5 equidistant values, and the convergence threshold is set to \u03b5qi = 10\u22125.\n\nwith these settings, fuzzy q-iteration converged after 659 iterations. figure 4.12\nshows the solution obtained, together with controlled trajectories (swing-ups) of the\n\nsimulated and real-life pendulum, starting from the stable equilibrium x0 = [\u2212\u03c0, 0]t\n\n(pointing down). in particular, figure 4.12(c) is the trajectory of the simulation model\n(4.52), while figure 4.12(d) is a trajectory of the real system. for the real system,\nonly the angle is measured, and the angular velocity is estimated using a discrete dif-\nference, which results in a noisy signal. even though the model is simplified and does\nnot include effects such as measurement noise and static friction, the policy result-\ning from fuzzy q-iteration performs well: it stabilizes the real system in about 1.5 s,\naround 0.25 s longer than in simulation. this discrepancy is due to the differences\nbetween the model and the real system. note that, because only discrete actions are\navailable, the control action chatters.\n\n0\n\n)\n0\n\n\u22122000\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u22124000\n\n\u22126000\n\n40\n\n20\n\n0\n\n\u221220\n\n\u03b1\u2019 [rad/s]\n\n\u221240\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n(a) slice through the q-function for u = 0.\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n2\n\n2\n\n2\n\n2\n\n2\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(b) policy.\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n2\n\n2\n\n2\n\n2\n\n(c) swing-up of the simulated system.\n\n(d) swing-up of the real system.\n\nfigure 4.12 results of fuzzy q-iteration for the inverted pendulum.\n\n "}, {"Page_number": 170, "text": "160\n\nchapter 4. fuzzy q-iteration\n\n4.5.4 car on the hill: effects of membership function optimization\n\nin this section, we study empirically the performance of fuzzy q-iteration with ce\noptimization of the mfs (algorithm 4.3). to this end, we apply fuzzy q-iteration\nwith optimized mfs to the car-on-the-hill problem (moore and atkeson, 1995), and\ncompare the results with those of fuzzy q-iteration with equidistant mfs.\n\ncar-on-the-hill problem\n\nthe car on the hill is widely used as a benchmark in approximate dp/rl. it was first\ndescribed by moore and atkeson (1995), and was used, e.g., by munos and moore\n(2002) as a primary benchmark for v-iteration with resolution refinement, and by\nernst et al. (2005) to validate fitted q-iteration. in the car-on-the-hill problem, a\npoint mass (the \u201ccar\u201d) must be driven past the top of a frictionless hill by applying\na horizontal force, see figure 4.13. for some initial states, the maximum available\nforce is not sufficient to drive the car directly up the hill. instead, it has to be driven\nup the opposite slope (left) and gather energy prior to accelerating towards the goal\n(right). this problem is roughly similar to the inverted pendulum of section 4.5.3;\nthere, the pendulum had to be swung back and forth to gather energy, which here\ncorresponds to driving the car left and then right. an important difference is that the\npendulum had to be stabilized, whereas the car only has to be driven past the top,\nwhich is easier to do.\n\n0.5\n\n)\np\n(\nh\n\n0\n\nu\n\nmg\n\n\u22120.5\n\n\u22121\n\n\u22120.5\n\n0\np\n\n0.5\n\n1\n\nfigure 4.13\nthe car on the hill. the \u201ccar\u201d is represented as a black bullet, and its goal is to drive out of the\nfigure to the right.\n\nthe continuous-time dynamics of the car are (moore and atkeson, 1995; ernst\n\net al., 2005):\n\n\u00a8p =\n\n1\n\ndp (cid:17)2(cid:18)u\u2212 g\n\n1 +(cid:16) dh(p)\n\ndh(p)\n\ndp \u2212 \u02d9p2 dh(p)\n\ndp\n\nd2h(p)\n\nd2 p (cid:19)\n\n(4.54)\n\nwhere p \u2208 [\u22121, 1] m is the horizontal position of the car, \u02d9p \u2208 [\u22123, 3] m/s is its veloc-\nity, u \u2208 [\u22124, 4] n is the horizontal force applied, g = 9.81 m/s2 is the gravitational\nacceleration, and h denotes the shape of the hill, which is given by:\n\nh(p) =(p2 + p\n\np\u221a1+5p2\n\nif p < 0\nif p \u2265 0\n\n "}, {"Page_number": 171, "text": "4.5. experimental study\n\n161\n\nfurthermore, a unity mass of the car is assumed. the discrete time step is set to\nts = 0.1 s, and the discrete-time dynamics f are obtained by numerically integrating\n(4.54) between consecutive time steps.\n\nthe state signal consists of the position and velocity of the car, x = [p, \u02d9p]t, while\nthe control action u is the applied force. the state space is x = [\u22121, 1]\u00d7[\u22123, 3] plus a\nterminal state (see below), and the action space is u = [\u22124, 4]. whenever the position\nor velocity exceed the bounds, the car reaches the terminal state, from which it can\nno longer escape, and the trial terminates. throughout the remainder of this example,\nthe action space is discretized into ud = {\u22124, 4}. these two values are sufficient to\nobtain a good solution, given that the car does not have to be stabilized, but only\ndriven past the top of the hill, which only requires it to be fully accelerated towards\nthe left and right.\n\nthe goal is to drive past the top of the hill to the right with a speed within the\nallowed limits. reaching a terminal state in any other way is considered a failure.\nthe reward function chosen to express this goal is:\n\nrk+1 = \u03c1(xk, uk) =\uf8f1\uf8f4\uf8f2\uf8f4\uf8f3\n\n1\n\n\u22121 if x1,k+1 < \u22121 or(cid:12)(cid:12)x2,k+1(cid:12)(cid:12) > 3\nif x1,k+1 > 1 and(cid:12)(cid:12)x2,k+1(cid:12)(cid:12) \u2264 3\n\notherwise\n\n0\n\nthe discount factor is \u03b3 = 0.95.\n\n(4.55)\n\nthis reward function is represented in figure 4.14(a). it is chosen to be discontin-\nuous to provide a challenging problem for the mf optimization algorithm, by making\nthe q-function difficult to approximate. to illustrate this difficulty, figure 4.14(b) de-\npicts an approximately optimal q-function. this q-function was obtained with fuzzy\nq-iteration using a very fine fuzzy partition, which contains 401\u00d7 301 mfs. (even\nthough the consistency of fuzzy q-iteration is not guaranteed since the reward is dis-\ncontinuous, this fine partition should at least lead to a rough approximation of the\noptimal q-function.) clearly, the large number of discontinuities appearing in this\nq-function make it difficult to approximate.\n\n)\n4\n\u2212\n\n \n,\n\u2019\n\np\n\n,\n\np\n(\n\u03c1\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n20\n\n15\n\n10\n\n5\n\n)\n4\n\u2212\n\n \n,\n\u2019\n\n,\n\np\np\n(\nq\n\n2\n\n0\n\np\u2019\n\n\u22122\n\n\u22121\n\n0\n\n\u22120.5\n\np\n\n0.5\n\n1\n\n2\n\n0\n\np\u2019\n\n\u22122\n\n\u22121\n\n0\n\n\u22120.5\n\np\n\n0.5\n\n1\n\n(a) a slice through \u03c1 for u = \u22124.\n\n(b) a slice through an approximately optimal q-\nfunction, also for u = \u22124.\n\nfigure 4.14\nreward function and an approximately optimal q-function for the car on the hill.\n\n "}, {"Page_number": 172, "text": "162\n\nchapter 4. fuzzy q-iteration\n\nresults of fuzzy q-iteration with mf optimization\n\nto apply fuzzy q-iteration with ce optimization of the mfs, triangular fuzzy parti-\ntions are defined for both state variables and then combined as in example 4.1. the\nnumber of mfs is chosen to be the same for each of the two variables, and is denoted\nby n\u2032. this number is gradually increased from 3 to 20.8 a given value of n\u2032 cor-\nresponds to a total number of n = n\u20322 mfs in the fuzzy partition of x . to compute\nthe score function (optimization criterion) (4.38), the following equidistant grid of\nrepresentative states is chosen:\n\nx0 = {\u22121,\u22120.75,\u22120.5, . . . , 1}\u00d7{\u22123,\u22122,\u22121, . . . , 3}\n\nand each point is weighted by 1\n. since the representative states are uniformly dis-\n|x0|\ntributed and equally weighted, the algorithm is expected to lead to a uniformly good\nperformance across the state space. the parameters of the ce optimization method\nare set to typical values, as follows: nce = 5\u00b7 2\u00b7 n\u03be, \u03c1ce = 0.05, and dce = 5. the\nnumber of samples nce is set to be 5 times the number of parameters needed to de-\nscribe the probability density used in ce optimization. recall that one mean and one\nstandard deviation are needed to describe the gaussian density for each of the n\u03be mf\nparameters. in turn, n\u03be = (n\u2032 \u2212 2)2, because there are n\u2032 \u2212 2 free cores to optimize\nalong each of the two axes of x . additionally, the maximum number of ce iterations\nis set to \u03c4max = 50, and the same value 10\u22123 is used as admissible error \u03b5mc in the\nreturn estimation, as fuzzy q-iteration convergence threshold \u03b5qi, and as ce conver-\ngence threshold \u03b5ce. with these settings, 10 independent runs are performed for each\nvalue of n\u2032.\n\nfigure 4.15 compares the results obtained using optimized mfs with those ob-\ntained using the same number of equidistant mfs. in particular, figure 4.15(a) shows\nthe mean score across the 10 independent runs of the mf optimization algorithm, to-\ngether with 95% confidence intervals on this mean. this figure also includes the\nperformance with equidistant mfs, and the best possible performance that can be\nobtained with the two discrete actions.9 the optimized mfs reliably provide a bet-\nter performance than the same number of equidistant mfs. for n\u2032 \u2265 12, they lead\n\nto a nearly optimal performance. as also observed in the consistency study of sec-\ntion 4.5.1, the discontinuous reward function leads to unpredictable variations of the\nperformance as the number of equidistant mfs is increased. optimizing the mfs\nrecovers a more predictable performance increase, because the mfs are adjusted to\nbetter represent the discontinuities of the q-function. figure 4.15(b) shows the com-\nputational cost of fuzzy q-iteration with mf optimization and with equidistant mfs.\n\n8the experiments stop at 20 mfs to limit the computation time per experiment in the order of hours.\nto run the experiments, we used matlab 7 on a pc with an intel t2400 1.83 ghz cpu and 2 gb ram.\n9this optimal performance is obtained using the following brute-force procedure. all the possible\nsequences of actions of a sufficient length k are generated, and the system is controlled in an open-\nloop fashion with all these sequences, starting from every state x0 in x0. for a given state x0, the largest\ndiscounted return obtained in this way is optimal under the action discretization ud. the length k is\nsufficient if, from any initial state in x0, an optimal trajectory leads to a terminal state after at most k\nsteps.\n\n "}, {"Page_number": 173, "text": "4.5. experimental study\n\n163\n\nthe performance gained by optimizing the mfs comes at a large computational cost,\nseveral orders of magnitude higher than the cost incurred by the equidistant mfs.\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\ne\nr\no\nc\ns\n\n \n\noptimized mfs, mean score\noptimized mfs, 95% confidence bounds\noptimal score\nequidistant mfs, score\n\n\u22120.1\n\n \n3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n\nn\u2019\n\n(a) performance.\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n6\n10\n\n4\n10\n\n2\n10\n\n0\n10\n\n\u22122\n\n10\n\n \n\noptimized mfs, mean execution time\noptimized mfs, 95% confidence bounds\nequidistant mfs, execution time\n\n \n3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n\nn\u2019\n\n(b) execution time.\n\nfigure 4.15\ncomparison between fuzzy q-iteration with optimized and equidistant mfs for the car on the\nhill.\n\nfigure 4.16 presents a representative set of final, optimized mfs. in this figure,\nthe number of mfs on each axis is n\u2032 = 10. to better understand this placement\nof the mfs, see again the approximately optimal q-function of figure 4.14(b). it is\nimpossible to capture all the discontinuities of this q-function with only 10 mfs on\neach axis. instead, the mf optimization algorithm concentrates most of the mfs in\nthe region of the state space where p \u2248 \u22120.8. in this region the car, having accu-\nmulated sufficient energy, has to stop moving left and accelerate toward the right;\nthis is a critical control decision. therefore, this placement of mfs illustrates that,\nwhen the number of mfs is insufficient to accurately represent the q-function over\nthe entire state space, the optimization algorithm focuses the approximator on the\nregions that are most important for the performance. the mfs on the velocity axis \u02d9p\nare concentrated towards large values, possibly in order to represent more accurately\nthe top-left region of the q-function, which is most irregular in the neighborhood of\np = \u22120.8.\n\n)\n\u2019\np\n\n \n,\n\np\n(\n \n\u03c6\n\n1\n\n0.5\n\n0\n3\n\n2\n\n1\n\n0\n\np\u2019\n\n\u22121\n\n\u22122\n\n\u22123\n\n\u22121\n\n0\n\n\u22120.5\n\np\n\n1\n\n0.5\n\nfigure 4.16 a representative set of optimized mfs for the car on the hill.\n\n "}, {"Page_number": 174, "text": "164\n\nchapter 4. fuzzy q-iteration\n\n4.6 summary and discussion\n\nin this chapter, we have considered fuzzy q-iteration, an algorithm for approximate\nvalue iteration that represents q-functions using a fuzzy partition of the state space\nand a discretization of the action space. fuzzy q-iteration was shown to be conver-\ngent to a near-optimal solution, and consistent under continuity assumptions on the\ndynamics and the reward function. a version of the algorithm where parameters are\nupdated in an asynchronous fashion was proven to converge at least as fast as the\nsynchronous variant. as an alternative to designing the mfs in advance, we have\ndeveloped a method to optimize the parameters of a constant number of mfs. a\ndetailed experimental study of fuzzy q-iteration was also performed, which led to\nthe following important conclusions: discontinuous reward functions can harm the\nperformance of fuzzy q-iteration; in certain problems, fuzzy q-iteration can outper-\nform fitted q-iteration with nonparametric approximation; and mf optimization is\nbeneficial for performance but computationally intensive.\n\nwhile fuzzy q-iteration has been presented in this chapter as an algorithm for\nsolving problems with deterministic dynamics, it can also be extended to the stochas-\ntic case. consider, for instance, asynchronous fuzzy q-iteration, given in algo-\nrithm 4.2. in the stochastic case, the parameter update at line 5 of this algorithm\nwould become:\n\n\u03b8[i, j] \u2190 ex\u2032\u223c \u02dcf (xi,u j,\u00b7)( \u02dc\u03c1(xi, u j, x\u2032) + \u03b3max\n\nj\u2032\n\n\u03c6i\u2032(x\u2032)\u03b8[i\u2032, j\u2032])\n\nn\n\n\u2211\ni\u2032=1\n\nwhere x\u2032 is sampled using the probability density function \u02dcf (xi, u j,\u00b7) of the next state\nx\u2032, given xi and u j. in general, the expectation in this update cannot be computed ex-\nactly, but must be estimated from a finite number of samples. in this case, our analy-\nsis does not apply directly, but the finite-sample results outlined in section 3.4.4 may\nhelp to analyze the effect of the finite-sampling errors. moreover, in some special\ncases, e.g., when there is a finite number of possible successor states, the expectation\ncan be computed exactly, in which case the theoretical analysis of this chapter applies\nafter some minor changes.\n\nmodel-free, rl algorithms with fuzzy approximation can be derived similarly to\nthe rl algorithms of section 3.4.2. for instance, the fuzzy approximator can easily\nbe employed in gradient-based q-learning (algorithm 3.3), leading to a fuzzy q-\nlearning algorithm. the theoretical properties of such model-free algorithms can be\ninvestigated using the framework of nonexpansive approximators (section 3.4.4).\nanother possibility is to learn a model of the mdp from the data (transition samples),\nand then apply fuzzy q-iteration to this model. to this end, it suffices to learn, for\nall the discrete actions, the next states reached from the mf cores and the resulting\nrewards. since it is unlikely that any transition samples will be located precisely at\nthe mf cores, the algorithm must learn from samples located nearby, which requires\nsmoothness assumptions on the dynamics and reward function (such as the lipschitz\ncontinuity already assumed for the consistency analysis).\n\n "}, {"Page_number": 175, "text": "4.6. summary and discussion\n\n165\n\nto improve the scalability of the fuzzy approximator to high-dimensional prob-\nlems, mfs that lead to subexponential complexity should be used (e.g., gaussians),\nin combination with techniques to find good mfs automatically, such as the mf opti-\nmization technique of section 4.4. if the computational demands of mf optimization\nbecome prohibitive, other approaches for finding the mfs must be explored, such as\nresolution refinement (section 3.6.2). furthermore, action-space approximators more\npowerful than discretization could be studied, e.g., using a fuzzy partition of the ac-\ntion space. such approximators may naturally lead to continuous-action policies.\n\nthe extensive analysis and experimentation presented in this chapter serve to\nstrengthen the knowledge about approximate value iteration developed in chapter 3.\ncontinuing along similar lines, the next two chapters consider in detail, respectively,\nalgorithms for approximate policy iteration and for approximate policy search.\n\nbibliographical notes\n\nthis chapter integrates and extends the authors\u2019 earlier work on fuzzy q-iteration\n(bus\u00b8oniu et al., 2008c, 2007, 2008b,d). for the theoretical analysis, certain limiting\nassumptions made in some of this work were removed, such as an originally discrete\naction space in (bus\u00b8oniu et al., 2008c, 2007), and a restrictive bound on the lipschitz\nconstant of the process dynamics in (bus\u00b8oniu et al., 2008b). the mf optimization\napproach of section 4.4 was proposed in (bus\u00b8oniu et al., 2008d).\n\nfuzzy approximators have typically been used in model-free (rl) techniques\nsuch as q-learning (horiuchi et al., 1996; jouffe, 1998; glorennec, 2000) and actor-\ncritic algorithms (berenji and vengerov, 2003; lin, 2003). a method that shares\nimportant similarities with our mf optimization approach was proposed by menache\net al. (2005). they applied the ce method to optimize the locations and shapes of\na constant number of basis functions for approximate policy evaluation, using the\nbellman error as an optimization criterion.\n\n "}, {"Page_number": 176, "text": "5\n\napproximate policy iteration for online\nlearning and continuous-action control\n\nthis chapter considers a model-free, least-squares algorithm for approximate pol-\nicy iteration. an online variant of this algorithm is developed, and some important\nissues that appear in online reinforcement learning are emphasized along the way.\nadditionally, a procedure to integrate prior knowledge about the policy in this on-\nline variant is described, and a continuous-action approximator for the offline variant\nis introduced. these developments are experimentally evaluated for several control\nproblems.\n\n5.1 introduction\n\nwhereas chapter 4 focused on an algorithm for approximate value iteration, the\npresent chapter concerns the second major class of techniques for approximate dp/\nrl: approximate policy iteration (pi). in pi, policies are evaluated by constructing\ntheir value functions, which are then used to find new, improved policies. approx-\nimate pi was reviewed in section 3.5, and this chapter builds and expands on the\nfoundation given there. in particular, the least-squares policy iteration (lspi) algo-\nrithm (lagoudakis and parr, 2003a) is selected, and three extensions to it are intro-\nduced: an online variant, an approach to integrate prior knowledge in this variant,\nand a continuous-action approximator for (offline) lspi.\n\nthe first topic of this chapter is therefore the development of an online variant\nof lspi. in online reinforcement learning (rl), a solution is learned from data col-\nlected by interacting with the controlled system. a suitable online algorithm, in the\nfirst place, must quickly provide a good performance, instead of only at the end of\nthe learning process, as is the case in offline rl. second, it must explore novel action\nchoices, even at the risk of a temporarily reduced performance, in order to avoid lo-\ncal optima and to eventually achieve a (near-)optimal performance. lspi is originally\noffline: it improves the policy only after an accurate q-function has been computed\nfrom a large batch of samples. in order to transform it into a good online algorithm,\nthe two requirements above must be satisfied. to quickly obtain a good performance,\npolicy improvements are performed once every few transitions, before an accurate\nevaluation of the current policy can be completed. such policy improvements are\n\n167\n\n "}, {"Page_number": 177, "text": "168\n\nchapter 5. online and continuous-action lspi\n\nsometimes called \u201coptimistic\u201d (sutton, 1988; bertsekas, 2007). to satisfy the ex-\nploration requirement, online lspi must sometimes try actions different from those\ngiven by the current policy. online lspi can be combined with many exploration\nprocedures, and in this chapter, the classical, so-called \u03b5-greedy exploration (sutton\nand barto, 1998) is applied: at every step, an exploratory, uniformly distributed ran-\ndom action is applied with probability \u03b5, and the action given by the current policy\nis applied with probability 1\u2212 \u03b5.\nrl is usually envisioned as working in a purely model-free fashion, without any\nprior knowledge about the problem. however, using prior knowledge can be highly\nbeneficial if it is available. in the second topic of this chapter, we illustrate how prior\nknowledge about the policy can be exploited to increase the learning rate of online\nlspi. in particular, policies that are monotonic in the state variables are considered.\nsuch policies are suitable for controlling, e.g., (nearly) linear systems, or systems\nthat are (nearly) linear and have monotonic input nonlinearities (such as saturation\nor dead-zone nonlinearities). a speedup of the learning process is then expected, be-\ncause the online lspi algorithm restricts its focus to the class of monotonic policies,\nand no longer invests valuable learning time in trying other, unsuitable policies.\n\na third important development in this chapter is a continuous-action q-function\napproximator for offline lspi, which combines state-dependent basis functions with\northogonal polynomial approximation over the action space. continuous actions are\nuseful in many classes of control problems. for instance, when a system must be\nstabilized around an unstable equilibrium, any discrete-action policy will lead to un-\ndesirable chattering of the control action.\n\nthese developments are empirically studied in three problems that were also em-\nployed in earlier chapters: the inverted pendulum, the two-link manipulator, and the\ndc motor. in particular, online lspi is evaluated for inverted pendulum swing-up\n(for which it is compared with offline lspi and with another online pi algorithm,\nand real-time learning results are given), as well as for two-link manipulator stabi-\nlization. the effects of using prior knowledge in online lspi are then investigated\nfor dc motor stabilization. we finally return to the inverted pendulum to examine\nthe effects of continuous-action, polynomial approximation.\n\nthe remainder of this chapter starts by briefly revisiting lspi, in section 5.2.\nthen, online lspi is developed in section 5.3, the procedure to integrate prior knowl-\nedge about the policy is presented in section 5.4, and the continuous-action, poly-\nnomial q-function approximator is explained in section 5.5. section 5.6 provides\nthe empirical evaluation of these three techniques, and section 5.7 closes the chapter\nwith a summary and discussion.\n\n5.2 a recapitulation of least-squares policy iteration\n\nlspi is an offline algorithm for approximate policy iteration that evaluates policies\nusing the least-squares temporal difference for q-functions (lstd-q) and performs\n\n "}, {"Page_number": 178, "text": "5.2. a recapitulation of least-squares policy iteration\n\n169\n\nexact policy improvements. lstd-q was described in detail in section 3.5.2 and\nwas presented in a procedural form in algorithm 3.8, while lspi was discussed in\nsection 3.5.5 and summarized in algorithm 3.11. here, we only provide a summary\nof these results, and make some additional remarks regarding the practical imple-\nmentation of the algorithm.\n\nin lspi, q-functions are approximated using a linear parametrization:\n\nbq(x, u) = \u03c6t(x, u)\u03b8\n\nwhere \u03c6(x, u) = [\u03c61(x, u), . . . ,\u03c6n(x, u)]t is a vector of n basis functions (bfs), and\n\u03b8 \u2208 rn is a parameter vector. to find the approximate q-function of the current\n\npolicy, the parameter vector is computed from a batch of transition samples, using\nlstd-q. then, an improved, greedy policy in this q-function is determined, the\napproximate q-function of this improved policy is found, and so on.\n\nalgorithm 5.1 presents lspi integrated with an explicit description of the lstd-\nq policy evaluation step. this explicit form makes it easier to compare offline lspi\nwith the online variant that will be introduced later.\n\nalgorithm 5.1 offline least-squares policy iteration.\ninput: discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r, samples {(xls, uls, x\u2032ls\n\n, rls)| ls = 1, . . . , ns}\n\n1: initialize policy h0\n2: repeat at every iteration \u2113 = 0, 1, 2, . . .\n3:\n\n\u03b30 \u2190 0, \u03bb0 \u2190 0, z0 \u2190 0\nfor ls = 1, . . . , ns do\n\n4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n\u03b3ls \u2190 \u03b3ls\u22121 + \u03c6(xls, uls)\u03c6t(xls, uls)\n\u03bbls \u2190 \u03bbls\u22121 + \u03c6(xls, uls)\u03c6t(x\u2032ls\n, h(x\u2032ls\nzls \u2190 zls\u22121 + \u03c6(xls, uls)rls\n\u03bbns\u03b8\u2113 + 1\nns\n\nend for\nsolve 1\nns\nh\u2113+1(x) \u2190 u where u \u2208 arg max \u00afu \u03c6t(x, \u00afu)\u03b8\u2113, \u2200x\n\n\u03b3ns\u03b8\u2113 = \u03b3 1\nns\n\nzns\n\n10:\n11: until h\u2113+1 is satisfactory\n\n\u22b2 start lstd-q policy evaluation\n\n))\n\n\u22b2 finalize policy evaluation\n\u22b2 policy improvement\n\noutput: bh\u2217 = h\u2113+1\napproximate q-function bq\u2113(x, u) = \u03c6t(x, u)\u03b8\u2113, which has a precise formal meaning,\n\nas explained next. the linear system at line 9 approximates the projected bellman\nequation given in matrix form by (3.38), and repeated here for easy reference:\n\nthe parameter \u03b8\u2113 obtained by lstd-q at line 9 of algorithm 5.1 leads to an\n\n\u03b3\u03b8h\u2113 = \u03b3\u03bb\u03b8h\u2113 + z\n\n(5.1)\n\nthe approximation is obtained by replacing the matrices \u03b3, \u03bb, and the vector z with\nestimates derived from the samples. the matrix equation (5.1) is in turn equivalent\nto the original projected bellman equation (3.34):\n\nbqh\u2113 = (pw \u25e6 t h)(bqh\u2113)\n\n "}, {"Page_number": 179, "text": "170\n\nchapter 5. online and continuous-action lspi\n\nsquares projection onto the space spanned by the bfs. the weight function is identi-\n\nwhere bqh\u2113(x, u) = \u03c6t(x, u)\u03b8h\u2113 , and the mapping pw performs a weighted least-\ncal to the distribution of the state-action samples used in lstd-q. the q-function bq\u2113\nobtained by lstd-q is thus an estimate of the solution bqh\u2113 to the projected bellman\n\nnote that, because \u03b8\u2113 appears on both sides of the equation at line 9, this equation\n\nequation.\n\ncan be simplified to:\n\n1\nns\n\n(\u03b3ns \u2212 \u03b3\u03bbns)\u03b8\u2113 =\n\n1\nns\n\nzns\n\nand therefore the matrices \u03b3 and \u03bb do not have to be estimated separately. instead,\nthe combined matrix \u03b3\u2212\u03b3\u03bb can be estimated as a single object, thereby reducing the\nmemory demands of lspi.\nat line 10 of algorithm 5.1, an improved policy is found that is greedy in the ap-\n\nproximate q-function bq\u2113. in practice, improved policies do not have to be explicitly\n\ncomputed and stored. instead, for any given state x, improved (greedy) actions can\nbe computed on-demand, by using:\n\nh\u2113+1(x) = u, where u \u2208 arg max\n\n\u00afu\n\n\u03c6t(x, \u00afu)\u03b8\u2113\n\n(5.2)\n\nthe maximization in this equation must be solved efficiently, because at every policy\nevaluation, a greedy action has to be computed for each of the ns samples (see line 6\nof algorithm 5.1). efficient maximization is possible when a suitable approximator\nis chosen (i.e., when suitable bfs are defined). for instance, with a discrete-action\napproximator of the type introduced in example 3.1, the maximization can be solved\nby enumeration over the set of discrete actions.\n\nas long as the policy evaluation error is bounded, lspi eventually produces poli-\ncies with a bounded suboptimality (see section 3.5.6). it is not, however, guaranteed\nto converge to a fixed policy \u2013 although it often does in practice. for instance, the\nvalue function parameters might converge to limit cycles, so that every point on the\ncycle yields a near-optimal policy.\n\n5.3 online least-squares policy iteration\n\nlspi is an offline rl algorithm: it employs data collected in advance to learn a\npolicy that should perform well at the end of the learning process. however, one of\nthe main goals of rl is to develop algorithms that learn online, by interacting with\nthe controlled system. therefore, in this section we extend lspi to online learning.\na good online algorithm must satisfy two requirements. first, by exploiting the\ndata collected by interaction, it must quickly provide a good performance, instead\nof only at the end of the learning process. second, it must also eventually achieve a\n(near-)optimal performance, without getting stuck in a local optimum. to this end,\nactions different from those indicated by the current policy must be explored, even\n\n "}, {"Page_number": 180, "text": "5.3. online least-squares policy iteration\n\n171\n\nat the risk of a temporarily reduced performance. hence, this second requirement is\npartly in conflict with the first, and the combination of the two is traditionally called\nthe exploration-exploitation trade-off in the rl literature (thrun, 1992; kaelbling,\n1993; sutton and barto, 1998). one way to formalize this trade-off is to use the\nnotion of regret, which roughly speaking is the cumulative difference between the\noptimal returns and the returns actually obtained over the learning process (see, e.g.,\nauer et al., 2002; audibert et al., 2007; auer et al., 2009; bubeck et al., 2009).\nminimizing the regret leads to fast learning and efficient exploration, by requiring\nthat the performance becomes near optimal (which ensures exploration is applied),\nand that this happens as soon as possible (which ensures that learning is fast and is\ndelayed by exploration only as much as necessary).\n\nto ensure that our online variant of lspi learns quickly (thereby satisfying the\nfirst requirement above), policy improvements must be performed once every few\ntransitions, before an accurate evaluation of the current policy can be completed.\nthis is a crucial difference with offline lspi, which improves the policy only after\nan accurate q-function has been obtained by running lstd-q on a large batch of\nsamples. in the extreme case, online lspi improves the policy after every transition,\nand applies the improved policy to obtain a new transition sample. then, another\npolicy improvement takes place, and the cycle repeats. such a variant of pi is called\nfully optimistic (sutton, 1988; bertsekas, 2007). in general, online lspi improves\nthe policy once every several (but not too many) transitions; this variant is called\npartially optimistic.\n\nto satisfy the second requirement, online lspi must explore, by trying other ac-\ntions than those given by the current policy. without exploration, only the actions\ndictated by the current policy would be performed in every state, and samples of\nthe other actions in that state would not be available. this would lead to a poor es-\ntimation of the q-values of these other actions, and the resulting q-function would\nnot be reliable for policy improvement. furthermore, exploration helps obtain data\nfrom regions of the state space that would not be reached using only the greedy pol-\nicy. in this chapter, the classical, \u03b5-greedy exploration (sutton and barto, 1998) is\nused: at every step k, a uniform random exploratory action is applied with probabil-\nity \u03b5k \u2208 [0, 1], and the greedy (maximizing) action with probability 1\u2212\u03b5k. typically,\n\u03b5k decreases over time (as k increases), so that the algorithm increasingly exploits\nthe current policy, as this policy (expectedly) approaches the optimal one. other ex-\nploration procedures are possible, see, e.g., (li et al., 2009) for a comparison in the\ncontext of lspi with online sample collection.\n\nalgorithm 5.2 presents online lspi with \u03b5-greedy exploration. the differences\nwith offline lspi are clearly visible in a comparison with algorithm 5.1. in partic-\nular, online lspi collects its own samples by interacting with the system (line 6),\nduring which it employs exploration (line 5). also, instead of waiting until many\nsamples have been processed to perform policy improvements, online lspi solves\nfor the q-function parameters and improves the policy at short intervals, using the\ncurrently available values of \u03b3, \u03bb, and z (lines 11\u201312).\n\nonline lspi uses two new, essential parameters that are not present in offline\nlspi: the number of transitions k\u03b8 > 0 between consecutive policy improvements,\n\n "}, {"Page_number": 181, "text": "172\n\nchapter 5. online and continuous-action lspi\n\nalgorithm 5.2 online least-squares policy iteration with \u03b5-greedy exploration.\ninput: discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r,\npolicy improvement interval k\u03b8, exploration schedule {\u03b5k}\u221e\na small constant \u03b2\u03b3 > 0\n1: \u2113 \u2190 0, initialize policy h0\n2: \u03b30 \u2190 \u03b2\u03b3in\u00d7n, \u03bb0 \u2190 0, z0 \u2190 0\n3: measure initial state x0\n4: for every time step k = 0, 1, 2, . . . do\n\nk=0,\n\na uniform random action in u with probability \u03b5k (explore)\n\nwith probability 1\u2212 \u03b5k (exploit)\n\nuk \u2190(h\u2113(xk)\napply uk, measure next state xk+1 and reward rk+1\n\u03b3k+1 \u2190 \u03b3k + \u03c6(xk, uk)\u03c6t(xk, uk)\n\u03bbk+1 \u2190 \u03bbk + \u03c6(xk, uk)\u03c6t(xk+1, h\u2113(xk+1))\nzk+1 \u2190 zk + \u03c6(xk, uk) rk+1\nif k = (\u2113 + 1)k\u03b8 then\nk+1 \u03b3k+1\u03b8\u2113 = 1\n\nsolve 1\nk+1 \u03bbk+1\u03b8\u2113 + 1\nh\u2113+1(x) \u2190 arg maxu \u03c6t(x, u)\u03b8\u2113, \u2200x\n\u2113 \u2190 \u2113 + 1\n\nk+1 zk+1\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\nend if\n\n14:\n15: end for\n\n\u22b2 finalize policy evaluation\n\u22b2 policy improvement\n\nand the exploration schedule {\u03b5k}\u221e\nk=0. when k\u03b8 = 1, the policy is updated after every\nsample and online lspi is fully optimistic. when k\u03b8 > 1, the algorithm is partially\noptimistic. note that the number k\u03b8 should not be chosen too large, and a signifi-\ncant amount of exploration is recommended, i.e., \u03b5k should not approach 0 too fast.\nin this chapter, the exploration probability is initially set to a value \u03b50, and decays\nexponentially1 once every second with a decay rate of \u03b5d \u2208 (0, 1):2\n\n\u03b5k = \u03b50 \u03b5\u230akts\u230b\n\nd\n\n(5.3)\n\nwhere ts is the sampling time of the controlled system, and \u230a\u00b7\u230b denotes the largest\ninteger smaller than or equal to the argument (floor). like in the offline case, im-\nproved policies do not have to be explicitly computed; instead, improved actions can\nbe computed on-demand. to ensure its invertibility, \u03b3 is initialized to a small multiple\n\u03b2\u03b3 > 0 of the identity matrix.\n\noffline lspi rebuilds \u03b3, \u03bb, and z from scratch before every policy improvement.\n\n1an exponential decay does not asymptotically lead to infinite exploration, which is required by some\nonline rl algorithms (section 2.3.2). nevertheless, for an experiment having a finite duration, \u03b5d can be\nchosen large enough to provide any desired amount of exploration.\n\n2the exploration probability \u03b5k decays once every second, instead of once every time step (sampling\nperiod), in order to ensure that exploration schedules are comparable even among systems with different\nsampling times. of course, a very similar effect can be obtained by decaying \u03b5k once every time step, with\na larger \u03b5d when ts < 1, or a smaller \u03b5d when ts > 1.\n\n "}, {"Page_number": 182, "text": "5.4. online lspi with prior knowledge\n\n173\n\nonline lspi cannot do this, because the few samples that arrive before the next policy\nimprovement are not sufficient to construct informative new estimates of \u03b3, \u03bb, and z.\ninstead, these estimates are continuously updated. the underlying assumption is that\nthe q-functions of subsequent policies are similar, which means that the previous\nvalues of \u03b3, \u03bb, and z are also representative of the improved policy.\n\nan alternative would be to store the samples and use them to rebuild \u03b3, \u03bb, and z\nbefore each policy improvement. this would incur larger computational costs, which\nwould also increase with the number of samples observed, and might therefore make\nthe algorithm too slow for online real-time learning after many samples have been\nobserved. such difficulties often appear when batch rl algorithms like lspi must\nbe applied in real time, and some general ways to address them are discussed, e.g.,\nby ernst et al. (2006a, section 5). in algorithm 5.2, we ensure that the computa-\ntional and memory demands are independent of the number of samples observed, by\nexploiting optimistic policy updates and reusing \u03b3, \u03bb, and z.\n\nmore specifically, the time complexity per step of online lspi is o(n3). the cost\nis the largest at time steps where policy improvements are performed, because this\ninvolves solving the linear system at line 11. this cost can be reduced by using com-\nputationally efficient methods to solve the system, but will still be larger than o(n2).\nthe memory required to store \u03b3, \u03bb, and z is o(n2). like offline lspi, the online\nalgorithm can estimate the combined matrix \u03b3\u2212 \u03b3\u03bb instead of \u03b3 and \u03bb separately,\nthereby reducing its memory requirements.\nbefore closing this section, we discuss the separation of the learning process into\ndistinct trials. as previously explained in chapter 2, trials arise naturally in problems\nwith terminal states, in which a trial is defined as a trajectory starting from some ini-\ntial state and ending in a terminal state. a terminal state, once reached, can no longer\nbe exited. so, the system must be reset in some fashion to an initial state, thereby\nstarting a new trial. for instance, many robot manipulators have safeguards that stop\nthe robot\u2019s motion if its pose gets outside the operating range (reaches a terminal\nstate), after which human intervention is required to reposition the robot (start a new\ntrial). if the problem does not have terminal states, it is possible to learn from a sin-\ngle, long trial. however, even in this case, it may still be beneficial for learning to\nterminate trials artificially. for instance, when learning a stabilizing control law, if\nthe system has been successfully stabilized and exploration is insufficient to drive the\nstate away from the equilibrium, there is little more to be learned from that trial, and\na new trial starting from a new state will be more useful. in the sequel, we denote by\nttrial the duration of such an artificially terminated trial.\n\n5.4 online lspi with prior knowledge\n\nrl is typically envisioned as working without any prior knowledge about the con-\ntrolled system or about the optimal solution. however, in practice, a certain amount\nof prior knowledge is often available, and using this prior knowledge can be very\n\n "}, {"Page_number": 183, "text": "174\n\nchapter 5. online and continuous-action lspi\n\nbeneficial. prior knowledge can refer, e.g., to the policy, to the value function, or to\nthe system dynamics. we focus on using prior knowledge about the optimal policy,\nor more generally about good policies that are not necessarily optimal. this focus is\nmotivated by the fact that it is often easier to obtain knowledge about the policy than\nabout the value function.\n\na general way of specifying knowledge about the policy is by defining con-\nstraints. for instance, one might know, and therefore require, that the policy is (glob-\nally or piecewise) monotonic in the state variables, or inequality constraints might\nbe available on the state and action variables. the main benefit of constraining poli-\ncies is a speedup of the learning process, which is expected because the algorithm\nrestricts its focus to the constrained class of policies, and no longer invests valuable\nlearning time in trying other, unsuitable policies. this speedup is especially relevant\nin online learning, although it may help reduce the computational demands of offline\nlearning as well.\n\nthe original (online or offline) lspi does not explicitly represent policies, but\ncomputes them on demand by using (5.2). therefore, the policy is implicitly defined\nby the q-function. in principle, it is possible to use the constraints on the policy in\norder to derive corresponding constraints on the q-function. however, this derivation\nis very hard to perform in general, due to the complex relationship between a policy\nand its q-function. a simpler solution is to represent the policy explicitly (and, in\ngeneral, approximately), and to enforce the constraints in the policy improvement\nstep. this is the solution adopted in the sequel.\n\nin the remainder of this section, we develop an online lspi algorithm for ex-\nplicitly parameterized, globally monotonic policies. such a policy is monotonic with\nrespect to any state variable, if the other state variables are held constant. mono-\ntonic policies are suitable for controlling important classes of systems. for instance,\na monotonic policy is appropriate for a (nearly) linear system, or a nonlinear system\nin a neighborhood of an equilibrium where it is nearly linear. this is because linear\npolicies, which work well for controlling linear systems, are monotonic. monotonic\npolicies also work well for some linear systems with monotonic input nonlinearities\n(such as saturation or dead-zone nonlinearities). in such cases, the policy may be\nstrongly nonlinear, but still monotonic. of course, in general, the global monotonic-\nity requirement is restrictive. it can be made less restrictive, e.g., by requiring that\nthe policy is monotonic only over a subregion of the state space, such as in a neigh-\nborhood of an equilibrium. multiple monotonicity regions can also be considered.\n\n5.4.1 online lspi with policy approximation\n\npolicy iteration with explicit policy approximation was described in section 3.5.5.\nhere, we specialize this discussion for online lspi. consider a linearly parameter-\nized policy representation of the form (3.12), repeated here:\n\nwhere \u03d5(x) = [\u03d51(x), . . . ,\u03d5n (x)]t is a vector containing n state-dependent bfs,\nand \u03d1 is the policy parameter vector. a scalar action is assumed, but the parametriza-\n\nbh(x) = \u03d5t(x)\u03d1\n\n(5.4)\n\n "}, {"Page_number": 184, "text": "5.4. online lspi with prior knowledge\n\n175\n\ntion can easily be extended to multiple action variables. when no prior knowledge\nabout the policy is available, approximate policy improvement can be performed by\nsolving the unconstrained linear least-squares problem (3.47), which for linearly pa-\nrameterized q-functions and scalar actions becomes:\n\n\u03d1\u2113+1 = \u03d1\u2021, where \u03d1\u2021 \u2208 arg min\nand uis \u2208 arg max\n\n\u03d1\n\nu\n\nns\n\u2211\n\nis=1(cid:0)\u03d5t(xis)\u03d1\u2212 uis(cid:1)2\n\n\u03c6t(xis, u)\u03b8\u2113\n\n(5.5)\n\nhere, the parameter vector \u03d1\u2113+1 leads to an improved policy, and {x1, . . . , xns} is a\nset of samples to be used for policy improvement.\nto obtain online lspi with parameterized policies, the exact policy improvement\nat line 12 of algorithm 5.2 is replaced by (5.5). moreover, the parameterized policy\n(5.4) is used to choose actions at line 5, and in the updates of \u03bb at line 8.\n\nan added benefit of the approximate policy (5.4) is that it produces continuous\nactions. note however that, if a discrete-action q-function approximator is employed,\nthe continuous actions given by the policy must be quantized during learning into ac-\ntions belonging to a discrete set ud. in this case, the policy evaluation step actually\nestimates the q-function of a quantized version of the policy. the quantization func-\ntion qd : u \u2192 ud is used, given by:\n\nqd(u) = u\u2021, where u\u2021 \u2208 arg min\n\n(5.6)\n\nu j\u2208ud (cid:12)(cid:12)u\u2212 u j(cid:12)(cid:12)\n\n5.4.2 online lspi with monotonic policies\nconsider a problem with a d-dimensional state space x \u2286 rd. in this section, it is\n\nassumed that x is a hyperbox:\n\nx = [xmin,1, xmax,1]\u00d7\u00b7\u00b7\u00b7\u00d7 [xmin,d, xmax,d]\n\nwe say that a policy h is monotonic along the dth dimension of the state space if\n\nwhere xmin,d \u2208 r, xmax,d \u2208 r, and xmin,d < xmax,d, for d = 1, . . . , d.\nand only if, for any pair x \u2208 x, \u00afx \u2208 x of states that fulfill:\n\nxd \u2264 \u00afxd\nxd\u2032 = \u00afxd\u2032 \u2200d\u2032 6= d\n\nthe policy satisfies:\n\n\u03b4mon,d \u00b7 h(x) \u2264 \u03b4mon,d \u00b7 h( \u00afx)\n\n(5.7)\nwhere the scalar \u03b4mon,d \u2208 {\u22121, 1} specifies the monotonicity direction: if \u03b4mon,d is\n\u22121 then h is decreasing along dimension d, whereas if \u03b4mon,d is 1 then h is increasing.\nwe say that a policy is (fully) monotonic if it is monotonic along every dimension\nof the state space. in this case, the monotonicity directions are collected in a vector\n\n\u03b4mon = [\u03b4mon,1, . . . ,\u03b4mon,d]t \u2208 {\u22121, 1}d.\n\n "}, {"Page_number": 185, "text": "176\n\nchapter 5. online and continuous-action lspi\n\nin this chapter, policies are approximated using axis-aligned, normalized radial\nbasis functions (rbfs) (see example 3.1) that are distributed on a grid with n1 \u00d7\n\u00b7\u00b7\u00b7\u00d7 nd elements. the grid spacing is equidistant along each dimension, and all the\nrbfs have identical widths. before examining how (5.7) can be satisfied when using\nsuch rbfs, a notation is required to relate the d-dimensional position of an rbf on\nthe grid with its scalar index in the vector \u03d5. consider the rbf located at indices\nid along every dimension d = 1, . . . , d, where id \u2208 {1, . . . , nd}. the position of this\nrbf is therefore described by the d-dimensional index (i1, . . . , id). we introduce\nthe notation [i1, . . . , id] for the corresponding scalar index of the rbf in \u03d5, which is\ncomputed as follows:\n\n[i1, . . . , id] = i1 + (i2 \u2212 1)n1 + (i3 \u2212 1)n1n2 +\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 + (id \u2212 1)n1n2\u00b7\u00b7\u00b7 nd\u22121\nthis formula can be understood more easily as a generalization of the two-\ndimensional case, for which [i1, i2] = i1 + (i2 \u2212 1)n1. in this two-dimensional case,\nthe grid (matrix) of rbfs has n1 \u00d7 n2 elements, and the vector \u03d5 is obtained by\nfirst taking the left-most column of the grid (matrix), which contains n1 elements,\nthen appending the second column from the left (also with n1 elements), and so on.\nthus, the rbf at position (i1, . . . , id) on the grid sits at index i = [i1, . . . , id]\nin the vector \u03d5, and is multiplied by the policy parameter \u03d1i = \u03d1[i1,...,id] when the\napproximate policy (5.4) is computed. the d-dimensional center of this rbf is\n[c1,i1, . . . , cd,id]t, where cd,id denotes the idth grid coordinate along dimension d.\nwithout any loss of generality, the coordinates are assumed to increase monotoni-\ncally along each dimension d:\n\ncd,1 < \u00b7\u00b7\u00b7 < cd,nd\n\nfurthermore, we impose that the first and last grid elements are placed at the limits\nof the domain: cd,1 = xmin,d and cd,nd = xmax,d.\n\nwith these conditions, and also because the normalized rbfs are equidistant and\nidentically shaped, we conjecture that in order to satisfy (5.7) it suffices to properly\norder the parameters corresponding to each sequence of rbfs along all the grid lines,\nand in every dimension of the state space.3 an example of this ordering relationship,\nfor a 3\u00d7 3 grid of rbfs, is:\n\n\u03d1[1,1] \u2264 \u03d1[1,2] \u2264 \u03d1[1,3]\n\u2265\n\u2265\n\u03d1[2,1] \u2264 \u03d1[2,2] \u2264 \u03d1[2,3]\n\u2265\n\u2265\n\u03d1[3,1] \u2264 \u03d1[3,2] \u2264 \u03d1[3,3]\n\n\u2265\n\u2265\n\n(5.8)\n\nin this case, the policy is decreasing along the first dimension of x (vertically in the\nequation), and increasing along the second dimension (horizontally in the equation).\n\n3to our knowledge, this monotonicity property has not yet been formally proven; however, it can be\n\nverified empirically for many rbf configurations.\n\n "}, {"Page_number": 186, "text": "5.5. lspi with continuous-action, polynomial approximation\n\n177\n\nfor a general grid in d dimensions, the monotonicity conditions can be written:\n\n\u03b4mon,1 \u00b7 \u03d1[1,i2,i3,...,id] \u2264\u03b4mon,1 \u00b7 \u03d1[2,i2,i3,...,id] \u2264 . . . \u2264 \u03b4mon,1 \u00b7 \u03d1[n1,i2,...,id]\n\nfor all i2, i3, . . . , id,\n\u03b4mon,2 \u00b7 \u03d1[i1,1,i3,...,id] \u2264\u03b4mon,2 \u00b7 \u03d1[i1,2,i3,...,id] \u2264 . . . \u2264 \u03b4mon,2 \u00b7 \u03d1[i1,n2,i3,...,id]\nfor all i1, i3, . . . , id,\n\u00b7\u00b7\u00b7\n\n\u03b4mon,d \u00b7 \u03d1[i1,i2,i3,...,1] \u2264\u03b4mon,d \u00b7 \u03d1[i1,i2,i3,...,2] \u2264 . . . \u2264 \u03b4mon,d \u00b7 \u03d1[i1,i2,i3,...,nd]\nfor all i1, i2, . . . , id\u22121\n\n\u00b7\u00b7\u00b7\n\n\u00b7\u00b7\u00b7\n\n(5.9)\n\nthe total number of inequalities in this equation is:\n\nd\n\nd=1 (nd \u2212 1)\n\n\u2211\n\nnd\u2032!\n\nd\n\n\u220f\n\nd\u2032=1, d\u20326=d\n\nthe monotonicity of the policy is enforced in online lspi by replacing the un-\n\nconstrained policy improvement with the constrained least-squares problem:\n\n\u03d1\u2113+1 = \u03d1\u2021, where \u03d1\u2021 \u2208\n\narg min\n\n\u03d1 satisfying (5.9)\n\nand uis \u2208 arg max\n\nu\n\n\u03c6t(xis, u)\u03b8\u2113\n\nns\n\u2211\n\nis=1(cid:0)\u03d5t(xis)\u03d1\u2212 uis(cid:1)2\n\n(5.10)\n\nand then using the approximate, monotonic policybh\u2113+1(x) = \u03d5t(x)\u03d1\u2113+1. the prob-\n\nlem (5.10) is solved using quadratic programming (see, e.g., nocedal and wright,\n2006).\n\nfor clarity, algorithm 5.3 summarizes online lspi incorporating monotonic poli-\ncies, a general linear parametrization of the q-function, and \u03b5-greedy exploration. if\na discrete-action q-function approximator is used in this algorithm, the approximate\naction must additionally be quantized with (5.6) at lines 5 and 8.\n\nthis framework can easily be generalized to multiple action variables, in which\ncase a distinct policy parameter vector can be used for every action variable, and\nthe monotonicity constraints can be enforced separately, on each of these parameter\nvectors. this also means that different monotonicity directions can be imposed for\ndifferent action variables.\n\n5.5 lspi with continuous-action, polynomial approximation\n\nmost versions of lspi from the literature employ discrete actions (lagoudakis et al.,\n2002; lagoudakis and parr, 2003a; mahadevan and maggioni, 2007). usually, a\nnumber n of bfs are defined over the state space only, and are replicated for each\n\n "}, {"Page_number": 187, "text": "178\n\nchapter 5. online and continuous-action lspi\n\nalgorithm 5.3 online least-squares policy iteration with monotonic policies.\ninput: discount factor \u03b3,\n\nq-function bfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r, policy bfs \u03d51, . . . ,\u03d5n : x \u2192 r\npolicy improvement interval k\u03b8, exploration schedule {\u03b5k}\u221e\na small constant \u03b2\u03b3 > 0\n\nk=0,\n\na uniform random action in u with probability \u03b5k (explore)\n\nwith probability 1\u2212 \u03b5k (exploit)\n\n1: \u2113 \u2190 0, initialize policy parameter \u03d10\n2: \u03b30 \u2190 \u03b2\u03b3in\u00d7n, \u03bb0 \u2190 0, z0 \u2190 0\n3: measure initial state x0\n4: for every time step k = 0, 1, 2, . . . do\n\nuk \u2190(\u03d5t(xk)\u03d1\u2113\napply uk, measure next state xk+1 and reward rk+1\n\u03b3k+1 \u2190 \u03b3k + \u03c6(xk, uk)\u03c6t(xk, uk)\n\u03bbk+1 \u2190 \u03bbk + \u03c6(xk, uk)\u03c6t(xk+1,\u03d5t(xk+1)\u03d1\u2113)\nzk+1 \u2190 zk + \u03c6(xk, uk) rk+1\nif k = (\u2113 + 1)k\u03b8 then\nk+1 \u03b3k+1\u03b8\u2113 = 1\n\u03d1\u2113+1\u2190\u03d1\u2021\u2208 arg min\n\u2113 \u2190 \u2113 + 1\n\nk+1 \u03bbk+1\u03b8\u2113 + 1\n\n\u03d1 satisfying (5.9)\n\nk+1 zk+1\n\nsolve 1\n\nns\n\u2211\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\nend if\n\n14:\n15: end for\n\nis=1(cid:0)\u03d5t(xis)\u03d1\u2212 uis(cid:1)2 , uis \u2208 arg max\n\nu\n\n\u03c6t(xis, u)\u03b8\u2113\n\nof the m discrete actions, leading to a total number n = nm of bfs and parameters.\nsuch approximators were discussed in example 3.1. however, there exist important\nclasses of control problems in which continuous actions are required. for instance,\nwhen a system must be stabilized around an unstable equilibrium, any discrete-action\npolicy will lead to undesirable chattering of the control action and to limit cycles.\n\ntherefore, in this section we introduce a continuous-action q-function approxi-\nmator for lspi. this approximator works for problems with scalar control actions.\nit uses state-dependent bfs and orthogonal polynomials of the action variable, thus\nseparating approximation over the state space from approximation over the action\nspace. note that because the action that maximizes the q-function for a given state\nis not restricted to discrete values in (5.2), this approximator produces continuous-\naction policies. a polynomial approximator is chosen because it allows one to ef-\nficiently solve maximization problems over the action variable (and thus perform\npolicy improvements), by computing the roots of the polynomial\u2019s derivative. more-\nover, orthogonal polynomials are preferred to plain polynomials, because they lead to\nnumerically better conditioned regression problems at the policy improvement step.\nsimilarly to the case of discrete-action approximators, a set of n state-dependent\n\nbfs is defined: \u00af\u03c6i : x \u2192 r, i = 1, . . . , n. only scalar control actions u are consid-\nered, bounded to an interval u = [ul, uh]. to approximate over the action dimension\nof the state-action space, chebyshev polynomials of the first kind are chosen as an\n\n "}, {"Page_number": 188, "text": "5.5. lspi with continuous-action, polynomial approximation\n\n179\n\nillustrative example of orthogonal polynomials, but many other types of orthogonal\npolynomials can alternatively be used. chebyshev polynomials of the first kind are\ndefined by the recurrence relation:\n\n\u03c80( \u00afu) = 1\n\u03c81( \u00afu) = \u00afu\n\n\u03c8j+1( \u00afu) = 2 \u00afu\u03c8j( \u00afu)\u2212 \u03c8j\u22121( \u00afu)\n\nthey are orthogonal to each other on the interval [\u22121, 1] relative to the weight func-\ntion 1/\u221a1\u2212 \u00afu2, i.e., they satisfy:\n\n\u03c8j( \u00afu)\u03c8j\u2032 ( \u00afu)d \u00afu = 0,\n\nj = 0, 1, 2, . . . , j\u2032 = 0, 1, 2, . . . , j\u2032 6= j\n\nz 1\n\n\u22121\n\n1\n\n\u221a1\u2212 \u00afu2\n\nin order to take advantage of the orthogonality property, the action space u must be\nscaled and translated into the interval [\u22121, 1]. this is simply accomplished using the\naffine transformation:\n\nthe approximate q-values for an orthogonal polynomial approximator of degree\n\n\u00afu = \u22121 + 2\n\nu\u2212 ul\nuh \u2212 ul\n\nmp are computed as follows:\n\nmp\n\u2211\nj=0\n\n\u03c8j( \u00afu)\n\nn\n\n\u2211\ni=1\n\n\u00af\u03c6i(x)\u03b8[i, j+1]\n\nbq(x, u) =\n\nthis can be written as bq(x, u) = \u03c6t(x, \u00afu)\u03b8 for the state-action bf vector:\n\n\u03c6(x, \u00afu) = [ \u00af\u03c61(x)\u03c80( \u00afu), . . . , \u00af\u03c6n(x)\u03c80( \u00afu),\n\u00af\u03c61(x)\u03c81( \u00afu), . . . , \u00af\u03c6n(x)\u03c81( \u00afu),\n. . . ,\n\u00af\u03c61(x)\u03c8mp ( \u00afu), . . . , \u00af\u03c6n(x)\u03c8mp ( \u00afu)]t\n\n(5.11)\n\n(5.12)\n\n(5.13)\n\nthe total number of state-action bfs (and therefore the total number of parame-\nters) is n = n(mp + 1). so, given the same number n of state bfs, a polynomial\napproximator of degree mp has the same number of parameters as a discrete-action\napproximator with m = mp + 1 discrete actions.\n\nto find the greedy action (5.2) for a given state x, the approximate q-function\n(5.12) for that value of x is first computed, which yields a polynomial in \u00afu. then, the\nroots of the derivative of this polynomial that lie in the interval (\u22121, 1) are found,\nthe approximate q-values are computed for each root and also for \u22121 and 1, and the\naction that corresponds to the largest q-value is chosen. this action is then translated\nback into u = [ul, uh]:\n\nh(x) = ul + (uh \u2212 ul)\n\n\u00afu(x) = {\u22121, 1}\u222a(cid:26) \u00afu\u2032 \u2208 (\u22121, 1)(cid:12)(cid:12)(cid:12)(cid:12)\n\n1 + arg max \u00afu\u2208 \u00afu(x) \u03c6t(x, \u00afu)\u03b8\n\n, where:\n\n2\n\nd\u03c8( \u00afu\u2032)\n\nd \u00afu\n\n= 0, where \u03c8( \u00afu) = \u03c6t(x, \u00afu)\u03b8(cid:27) (5.14)\n\n "}, {"Page_number": 189, "text": "180\n\nchapter 5. online and continuous-action lspi\n\nd \u00afu\n\ndenotes the derivative d\u03c8( \u00afu)\nd \u00afu\n\nin this equation, d\u03c8( \u00afu\u2032)\nevaluated at \u00afu\u2032. in some cases,\nthe polynomial will attain its maximum inside the interval (\u22121, 1), but in other cases,\nit may not, which is why the boundaries {\u22121, 1} must also be tested. efficient algo-\nrithms can be used to compute the polynomial roots with high accuracy.4 therefore,\nthe proposed parametrization allows the maximization problems in the policy im-\nprovement (5.2) to be solved efficiently and with high accuracy, and is well-suited to\nthe use with lspi.\n\npolynomial approximation can be used in offline, as well as in online lspi. in\n\nthis chapter, we will evaluate it for the offline case.\n\n5.6 experimental study\n\nin this section, we experimentally evaluate the extensions to the lspi algorithm that\nwere introduced above. first, an extensive empirical study of online lspi is per-\nformed in sections 5.6.1 and 5.6.2, using respectively an inverted pendulum and\na robotic manipulator problem. then, in section 5.6.3, the benefits of using prior\nknowledge in online lspi are investigated, for a dc motor example. finally, in sec-\ntion 5.6.4, we return to the inverted pendulum problem to examine the effects of\ncontinuous-action polynomial approximation.\n\n5.6.1 online lspi for the inverted pendulum\n\nthe inverted pendulum swing-up problem is challenging and highly nonlinear, but\nlow-dimensional, which means extensive experiments can be performed with reason-\nable computational costs. using this problem, we study the effects of the exploration\ndecay rate, of the policy improvement interval, and of the trial length on the per-\nformance of online lspi. then, we compare the final performance of online lspi\nwith the performance of offline lspi; and we compare online lspi with an online\npi algorithm that uses, instead of lstd-q, the least-squares policy evaluation for q-\nfunctions (lspe-q) (algorithm 3.9). finally, we provide real-time learning results\nfor the inverted pendulum system.\n\ninverted pendulum problem\n\nthe inverted pendulum problem was introduced in section 4.5.3, and is described\nhere only briefly. the pendulum consists of a weight attached to a rod that is actuated\nby a dc motor and rotates in a vertical plane (see figure 4.11 on page 158). the goal\nis to stabilize the pendulum in the pointing up position. due to the limited torque\nof the dc motor, from certain states (e.g., pointing down) the pendulum cannot be\n\n4in our implementation, the roots are computed as the eigenvalues of the companion matrix of the\n\npolynomial d\u03c8( \u00afu)\n\nd \u00afu , using the \u201croots\u201d function of matlab r(cid:13) (see, e.g., edelman and murakami, 1995).\n\n "}, {"Page_number": 190, "text": "5.6. experimental study\n\n181\n\npushed up in a single rotation, but must be swung back and forth to gather energy\nprior to being pushed up and stabilized.\n\na continuous-time model of the pendulum dynamics is:\n\n\u00a8\u03b1 =\n\n1\n\nj(cid:18)mgl sin(\u03b1)\u2212 b \u02d9\u03b1\u2212\n\nk2\nr\n\n\u02d9\u03b1+\n\nk\n\nr\n\nu(cid:19)\n\nwhere j = 1.91 \u00b7 10\u22124 kgm2, m = 0.055 kg, g = 9.81 m/s2, l = 0.042 m, b = 3 \u00b7\n10\u22126 nms/rad, k = 0.0536 nm/a, r = 9.5 \u03c9. the angle \u03b1 \u201cwraps around\u201d in\nthe interval [\u2212\u03c0,\u03c0) rad, so that, e.g., a rotation of 3\u03c0/2 corresponds to a value\n\u03b1 = \u2212\u03c0/2. when \u03b1 = 0, the pendulum is pointing up. the velocity \u02d9\u03b1 is restricted\nto [\u221215\u03c0, 15\u03c0] rad/s, using saturation, and the control action u is constrained to\n[\u22123, 3] v. the state vector is x = [\u03b1, \u02d9\u03b1]t. the sampling time is ts = 0.005 s, and the\ndiscrete-time transitions are obtained by numerically integrating the continuous-time\ndynamics. the stabilization goal is expressed by the reward function:\n\n\u03c1(x, u) = \u2212xtqrewx\u2212 rrewu2\n0 0.1(cid:21) , rrew = 1\nqrew =(cid:20)5\n\n0\n\nwith discount factor \u03b3 = 0.98. this discount factor is large so that rewards around the\ngoal state (pointing up) influence the values of states early in the trajectories. this\nleads to an optimal policy that successfully swings up and stabilizes the pendulum.\n\na near-optimal solution (q-function and policy) for this problem is given in fig-\nure 5.1. this solution was computed with fuzzy q-iteration (chapter 4) using a fine\ngrid of membership functions in the state space, and a fine discretization of the action\nspace.\n\n0\n\n)\n0\n\n\u22122000\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u22124000\n\n\u22126000\n\n40\n\n20\n\n0\n\n\u221220\n\n\u03b1\u2019 [rad/s]\n\n\u221240\n\n\u22122\n\n2\n\n0\n\n\u03b1 [rad]\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n(a) slice through the q-function for u = 0.\n\n(b) policy.\n\nfigure 5.1 a near-optimal solution for the inverted pendulum.\n\napproximator and performance criteria\n\nto approximate the q-function, a discrete-action approximator of the type introduced\nin example 3.1 of chapter 3 is employed. recall that for such an approximator, n\n\nstate-dependent bfs \u00af\u03c61, . . . , \u00af\u03c6n : x \u2192 r are defined and replicated for every action\n\n "}, {"Page_number": 191, "text": "182\n\nchapter 5. online and continuous-action lspi\n\nin a discretized set ud = {u1, . . . , um}. approximate q-values can be computed for\nany state-discrete action pair with:\n\nwhere, in the state-action bf vector \u03c6(x, u j), the bfs not corresponding to the current\ndiscrete action are taken to be equal to 0:\n\nbq(x, u j) = \u03c6t(x, u j)\u03b8\n\n\u03c6(x, u j) = [0, . . . , 0\n\n, . . . , 0, \u00af\u03c61(x), . . . , \u00af\u03c6n(x)\n\n, 0, . . . , 0, . . . , 0\n\nu1\n\n| {z }\n\n|\n\nu j\n\n{z\n\n}\n\num\n\n| {z }\n\n]t \u2208 rnm\n\n(5.15)\n\nan equidistant 11\u00d7 11 grid of normalized gaussian rbfs (3.6) is used to approx-\nimate the q-function over the state space, and the action space is discretized into 3\ndiscrete values: ud = {\u22123, 0, 3}. this leads to a total number of n = 112 \u00b7 3 = 363\nstate-action bfs. the rbfs are axis-aligned and identical in shape, and their width bd\n2/2, where b\u2032d is the distance between adjacent\nalong each dimension d is equal to b\u2032d\nrbfs along that dimension (the grid step). these rbfs yield a smooth interpolation\nof the q-function over the state space. recalling that the angle spans a domain of\nsize 2\u03c0 and that the angular velocity spans 30\u03c0, we obtain b\u20321 = 2\u03c0\n11\u22121 \u2248 0.63 and\nb\u20322 = 30\u03c0\n11\u22121 \u2248 9.42, which lead to b1 \u2248 0.20 and b2 \u2248 44.41.\nwe first define a performance measure that evaluates the quality of the policy\ncomputed online for a set of representative initial states spanning the entire state\nspace. in particular, after each online lspi experiment is completed, snapshots of\nthe current policy at increasing moments of time are evaluated by estimating, with\nprecision \u03b5mc = 0.1, their average return (score) over the grid of initial states:\n\nx0 = {\u2212\u03c0,\u2212\u03c0/2, 0,\u03c0/2}\u00d7{\u221210\u03c0,\u22123\u03c0,\u2212\u03c0, 0,\u03c0, 3\u03c0, 10\u03c0}\n\n(5.16)\n\nduring performance evaluation, learning and exploration are turned off. this pro-\nduces a curve recording the control performance of the policy over time. the return\nfor each initial state is estimated by simulating only the first k steps of the trajectory,\nwith k given by (2.41).\n\nthe performance measure above is computed in the absence of exploration.\ntherefore, when evaluating the effects of exploration, an additional measure is re-\nquired that does take these effects into account. to obtain this measure, the system is\n\nperiodically reset during the learning process to the initial state x0 = [\u2212\u03c0, 0]t (point-\n\ning down), and the empirical return obtained along a learning trial starting from this\nstate is recorded, without turning off exploration and learning. repeating this evalu-\nation multiple times over the learning process gives a curve indicating the evolution\nof the return obtained while using exploration.\n\nthe two performance measures are respectively called \u201cscore\u201d and \u201creturn with\nexploration\u201d in the sequel. they are not directly comparable, first because they han-\ndle exploration differently, and second because the score evaluates the performance\nover x0, whereas the return with exploration only considers the single state x0. nev-\nertheless, if two experiments employ the same exploration schedule, comparing their\nscore also gives a good idea about how they compare qualitatively in terms of return\n\n "}, {"Page_number": 192, "text": "5.6. experimental study\n\n183\n\nwith exploration. because of this, and also to preserve consistency with the perfor-\nmance criteria used in other chapters of this book, we will rely on the score as a pri-\nmary performance measure, and we will use the return with exploration only when\nthe effect of different exploration schedules must be assessed. note that, because the\nreward function is negative, both performance measures will be negative, and smaller\nabsolute values for these measures correspond to better performance.\n\neffects of the tuning parameters\n\nin this section, we study the effects of varying the tuning parameters of online lspi.\nin particular, we change the exploration decay rate \u03b5d, the number of transitions\nbetween consecutive policy improvements k\u03b8, and the trial length ttrial. each online\nexperiment is run for 600 s, and is divided into trials having the length ttrial. the\ndecaying exploration schedule (5.3) is used, with the initial exploration probability\n\u03b50 = 1, so that a fully random policy is used at first. the parameter \u03b2\u03b3 is set to\n0.001. furthermore, 20 independent runs of each experiment are performed, in order\nto obtain statistically significant results.\n\nto study the influence of \u03b5d,\n\nthe following values are chosen, \u03b5d =\n0.8913, 0.9550, 0.9772, 0.9886, 0.9924, and 0.9962, so that \u03b5k becomes 0.1 after re-\nspectively 20, 50, 100, 200, 300, and 600 s. larger values of \u03b5d correspond to more\nexploration. the policy is improved once every k\u03b8 = 10 transitions, and the trial\nlength is ttrial = 1.5 s, which is sufficient to swing up and stabilize the inverted pen-\ndulum. the initial state of each trial is drawn from a uniform random distribution\nover x . figure 5.2 shows how the score (average return over x0) of the policies\nlearned by online lspi evolves. in particular, the curves in figure 5.2(a) represent\nthe mean performance across the 20 runs, while figure 5.2(b) additionally shows\n95% confidence intervals, but only considers the extreme values of \u03b5d, in order to\navoid cluttering. the score converges in around 120 s of simulated time. the final\nscore improves with more exploration, and the difference between the score with\nlarge and small exploration schedules is statistically significant, as illustrated in fig-\nure 5.2(b). these results are not surprising, since the considerations in section 5.3\nalready indicated that online lspi requires significant exploration.\n\nhowever, too much exploration may decrease the control performance obtained\nduring learning. since exploration is turned off while computing the score, this effect\nis not visible in figure 5.2. to examine the effects of exploration, the experiments\n\nabove are repeated, but this time resetting the system to the initial state x0 = [\u2212\u03c0, 0]t\n\nin one out of every 10 trials (i.e., once every 15 s), and recording the return with\nexploration during every such trial. figure 5.3 presents the results, from which it\nappears that too large an exploration schedule negatively affects the rate of im-\nprovement of this performance measure. the differences between small and large\nexploration schedules are statistically significant, as illustrated in figure 5.3(b). this\nmeans that, when selecting the exploration schedule, a trade-off between the score\nof the policy and the return with exploration must be resolved. for this example,\nan acceptable compromise between the two performance measures is obtained for\n\u03b5d = 0.9886, corresponding to \u03b5k = 0.1 after 200 s. we therefore choose this explo-\n\n "}, {"Page_number": 193, "text": "184\n\nchapter 5. online and continuous-action lspi\n\n\u22121400\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\ne\nr\no\nc\ns\n\n \n\n\u22121400\n\n \n\n \u03b5\n=0.8913\nd\n \u03b5\n=0.9550\nd\n \u03b5\n=0.9772\nd\n \u03b5\n=0.9886\nd\n \u03b5\n=0.9924\nd\n \u03b5\n=0.9962\nd\n\ne\nr\no\nc\ns\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\n \u03b5\n=0.8913, mean\nd\n\n95% confidence bounds\n\n \u03b5\n=0.9962, mean\nd\n\n95% confidence bounds\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n(a) mean score for all the experiments.\n\n(b) mean score with 95% confidence intervals,\nfor the smallest and largest values of \u03b5d.\n\nfigure 5.2\nscore of online lspi for varying \u03b5d in the inverted pendulum problem. the marker locations\nindicate the moments in time when the policies were evaluated.\n\nn\no\n\ni\nt\n\nl\n\na\nr\no\np\nx\ne\nh\n\n \n\nt\ni\n\n \n\nw\nn\nr\nu\ne\nr\n\nt\n\n\u22122000\n\n\u22122100\n\n\u22122200\n\n\u22122300\n\n\u22122400\n\n\u22122500\n\n\u22122600\n\n\u22122700\n\n \u03b5\n=0.8913\nd\n \u03b5\n=0.9550\nd\n \u03b5\n=0.9772\nd\n \u03b5\n=0.9886\nd\n \u03b5\n=0.9924\nd\n \u03b5\n=0.9962\nd\n\n \n\nn\no\n\ni\nt\n\nl\n\na\nr\no\np\nx\ne\nh\n\n \n\nt\ni\n\n \n\nw\nn\nr\nu\ne\nr\n\nt\n\n\u22122000\n\n\u22122100\n\n\u22122200\n\n\u22122300\n\n\u22122400\n\n\u22122500\n\n\u22122600\n\n\u22122700\n\n \n\n \u03b5\n=0.8913, mean\nd\n\n95% confidence bounds\n\n \u03b5\n=0.9962, mean\nd\n\n95% confidence bounds\n\n\u22122800\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n\u22122800\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n(a) mean return with exploration for all the\nexperiments.\n\n(b) mean and 95% confidence intervals for the\nreturn with exploration obtained using the small-\nest and largest values of \u03b5d.\n\nfigure 5.3\nreturn with exploration obtained by online lspi for varying \u03b5d, in the inverted pendulum\nproblem.\n\nration schedule for all the upcoming simulation experiments with online lspi for the\ninverted pendulum.\n\nto study the influence of the number k\u03b8 of transitions between policy improve-\nments, the following values are used: k\u03b8 = 1, 10, 100, 1000, and 5000. when k\u03b8 = 1,\nthe algorithm is fully optimistic: the policy is improved after every sample. the\ntrial length is ttrial = 1.5 s, and the initial state of each trial is drawn from a uni-\nform random distribution over x . as already mentioned, the exploration decay rate\nis \u03b5d = 0.9886. figure 5.4 shows how the performance of the policies learned by\nonline lspi evolves.5 in figure 5.4(a) all the values of k\u03b8 lead to a similar per-\nformance except the cases in which the policy is updated very rarely. for instance,\n\n5in this figure, the performance is measured using the score, which is representative because all the ex-\nperiments use the same exploration schedule. for similar reasons, the score is employed as a performance\nmeasure throughout the remainder of this chapter.\n\n "}, {"Page_number": 194, "text": "5.6. experimental study\n\n185\n\nwhen k\u03b8 = 5000, the performance is worse, and the difference with the performance\nof smaller k\u03b8 is statistically significant, as illustrated in figure 5.4(b). this indicates\nthat policy improvements should not be performed too rarely in online lspi.\n\n\u22121400\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\ne\nr\no\nc\ns\n\n \n\n\u22121400\n\n \n\ne\nr\no\nc\ns\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\n k\n=1\n\u03b8\n k\n=10\n\u03b8\n k\n=100\n\u03b8\n k\n=1000\n\u03b8\n k\n=5000\n\u03b8\n\n k\n=1, mean\n\u03b8\n\n95% confidence bounds\n k\n=5000, mean\n\u03b8\n\n95% confidence bounds\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n(a) mean score for all the experiments.\n\n(b) mean score with 95% confidence intervals,\nfor the smallest and largest values of k\u03b8.\n\nfigure 5.4\nperformance of online lspi for varying k\u03b8 in the inverted pendulum problem.\n\nto study the effects of the trial length, the values ttrial = 0.75, 1.5, 3, 6, and 12 s\nare used, corresponding to, respectively, 800, 400, 200, 100, and 50 learning trials in\nthe 600 s of learning. the initial state of each trial is drawn from a uniform random\ndistribution over x , the policy is improved once every k\u03b8 = 10 samples, and the\nexploration decay rate is \u03b5d = 0.9886. these settings gave a good performance in the\nexperiments above. figure 5.5 reports the performance of online lspi. long trials (6\nand 12 s) are detrimental to the learning rate, as well as to the final performance. short\ntrials are beneficial for the performance because the more frequent re-initializations\nto random states provide more information to the learning algorithm. this difference\nbetween the performance with short and long trials is statistically significant, see\nfigure 5.5(b).\n\n \n\n\u22121400\n\n \n\n\u22121400\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\ne\nr\no\nc\ns\n\ne\nr\no\nc\ns\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\n t\n\ntrial\n\n t\n\ntrial\n\n t\n\ntrial\n\n t\n\ntrial\n\n t\n\ntrial\n\n=0.75s\n\n=1.50s\n\n=3.00s\n\n=6.00s\n\n=12.00s\n\n t\n\ntrial\n\n=0.75s, mean\n\n95% confidence bounds\n t\n\n=12.00s, mean\n\ntrial\n\n95% confidence bounds\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n(a) mean score for all the experiments.\n\n(b) mean score with 95% confidence intervals,\nfor the smallest and largest values of ttrial.\n\nfigure 5.5\nperformance of online lspi for varying ttrial in the inverted pendulum problem.\n\n "}, {"Page_number": 195, "text": "186\n\nchapter 5. online and continuous-action lspi\n\nfigure 5.6 shows the mean execution time for varying \u03b5d, k\u03b8, and ttrial, taken\nacross the 20 independent runs of each experiment.6 the 95% confidence intervals\nare too small to be visible at the scale of the figure, so they are left out. the execution\ntime is larger for smaller k\u03b8, because the most computationally expensive operation\nis solving the linear system at line 11 of algorithm 5.2, which must be done once\nevery k\u03b8 steps. the execution time does not change significantly with the exploration\nschedule or with the trial length, since choosing random actions and re-initializing\nthe state are computationally cheap operations.\n\n4\n10\n\n3\n10\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n \n\n4\n10\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n3\n10\n\n \n\n2\n10\n\n \n\n0.9\n\n0.92\n\n0.96\n\n0.98\n\n1\n\n0.94\n\u03b5\nd\n\n2\n10\n\n \n0\n10\n\n1\n10\n\n2\n10\nk\n\u03b8\n\n3\n10\n\n4\n10\n\n(a) varying \u03b5d.\n\n4\n10\n\n3\n10\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n(b) varying k\u03b8.\n\n \n\n2\n10\n\n \n\n2\n\n4\n\n6\nt\n\ntrial\n\n [s]\n\n8\n\n10\n\n12\n\n(c) varying ttrial.\n\nfigure 5.6 mean execution time of online lspi for the inverted pendulum.\n\nnote that for fully optimistic updates, k\u03b8 = 1, the execution time is around\n2430 s, longer than the length of 600 s for the simulated experiment, and therefore\nonline lspi cannot be run in real time for this value of k\u03b8. some possible ways to\naddress this problem will be discussed in section 5.6.2.\n\ncomparison of online lspi and offline lspi\n\nin this section, online lspi is compared with the original, offline lspi algorithm.\nthe online experiments described above are reused for this comparison. to ap-\nply offline lspi, the same approximator is employed as in the online case. while\nthe online algorithm generates its own samples during learning, a number of ns =\n20000 pregenerated random samples are used for offline lspi, uniformly distributed\nthroughout the state-discrete action space x \u00d7 ud. offline lspi is run 20 times with\n6all the execution times reported in this chapter were recorded while running the algorithms in\n\nmatlab 7 on a pc with an intel core 2 duo e6550 2.33 ghz cpu and with 3 gb ram.\n\n "}, {"Page_number": 196, "text": "5.6. experimental study\n\n187\n\nindependent sets of samples. table 5.1 compares the score (average return over x0)\nof the policies found offline with that of the final policies found at the end of the\nonline experiments, as well as the execution time of offline and online lspi. two\nrepresentative online experiments from the study of \u03b5d are selected for comparison:\nthe experiment with the best mean performance, and the experiment with the worst\nmean performance. representative experiments from the study of k\u03b8 and ttrial are\nselected in a similar way.\n\ntable 5.1 comparison of offline and online lspi (mean; 95% confidence interval).\n\noffline\n\nexperiment\n\nperformance (score)\n\u22121496.8; [\u22121503.6,\u22121490.0]\n\u03b5d = 0.9962 (best)\n\u22121479.0; [\u22121482.3,\u22121475.7]\n\u03b5d = 0.8913 (worst) \u22121534.0; [\u22121546.9,\u22121521.1]\nk\u03b8 = 1 (best)\n\u22121494.3; [\u22121501.5,\u22121487.2]\nk\u03b8 = 5000 (worst) \u22121597.8; [\u22121618.1,\u22121577.4]\nttrial = 0.75 s (best) \u22121486.8; [\u22121492.4,\u22121481.2]\nttrial = 12 s (worst) \u22121598.5; [\u22121664.2,\u22121532.8]\n\nexecution time [s]\n\n82.7; [79.6, 85.8]\n\n335.9; [332.8, 339.1]\n333.6; [331.6, 335.7]\n\n2429.9; [2426.2, 2433.5]\n114.0; [113.7, 114.2]\n\n346.3; [345.1, 347.5]\n346.6; [345.5, 347.7]\n\nthe table indicates that the final performance of online lspi is comparable with\nthe performance of its offline counterpart. on the other hand, online lspi is more\ncomputationally expensive, because it performs more policy improvements. the of-\nfline algorithm employs 20000 samples, whereas online lspi processes the same\nnumber of samples in 100 s of simulated time, and 120000 samples during the entire\nlearning process. nevertheless, as indicated, e.g., in figure 5.2(a), for reasonable pa-\nrameter settings, the score of the policy found by online lspi is already good after\n120 s, i.e., after processing 24000 samples. therefore, online lspi is also compa-\nrable with offline lspi in the number of samples that are sufficient to find a good\npolicy. note that online lspi processes samples only once, whereas the offline algo-\nrithm loops through the samples once at every iteration.\n\nfigure 5.7 presents a representative final solution of online lspi, computed with\nk\u03b8 = 10, \u03b5d = 0.9886, and ttrial = 1.5 s, in comparison to a representative solution\nof the offline algorithm. the two policies, shown in figures 5.7(a) and 5.7(b), have a\nsimilar large-scale structure but differ in some regions. the q-function found online\n(figure 5.7(c)) resembles the near-optimal q-function of figure 5.1(a) more closely\nthan the q-function found offline (figure 5.7(d)), which has an extra peak in the\norigin of the state space. the swing-up trajectories, shown in figures 5.7(e) and\n5.7(f), are similar, but the online solution leads to more chattering.\n\ncomparison of online lspi and online pi with lspe-q\n\nin this section, we consider an online pi algorithm that evaluates policies with lspe-\nq (algorithm 3.9), rather than with lstd-q, as online lspi does. recall that lspe-\nq updates the matrices \u03b3, \u03bb, and the vector z in the same way as lstd-q. however,\nunlike lstd-q, which finds the parameter vector \u03b8 by solving a one-shot linear\n\n "}, {"Page_number": 197, "text": "188\n\nchapter 5. online and continuous-action lspi\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\n0\n\n)\n0\n\n\u22122000\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n(a) policy found online.\n\n(b) policy found offline.\n\n0\n\n)\n0\n\n\u22122000\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u22124000\n\n\u22126000\n\n40\n\n20\n\n0\n\n\u221220\n\n\u03b1\u2019 [rad/s]\n\n\u221240\n\n\u22122\n\n2\n\n0\n\n\u03b1 [rad]\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u22124000\n\n\u22126000\n\n40\n\n20\n\n0\n\n\u221220\n\n\u03b1\u2019 [rad/s]\n\n\u221240\n\n\u22122\n\n2\n\n0\n\n\u03b1 [rad]\n\n(c) slice through the q-function found online,\nfor u = 0.\n\n(d) slice through the q-function found offline,\nfor u = 0.\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n2\n\n2\n\n2\n\n2\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n2\n\n2\n\n2\n\n2\n\n(e) swing-up using the policy found online.\n\n(f) swing-up using the policy found offline.\n\nfigure 5.7\nrepresentative solutions found with online lspi (left) and with offline lspi (right) for the\ninverted pendulum.\n\n "}, {"Page_number": 198, "text": "5.6. experimental study\n\n189\n\nproblem, lspe-q updates the parameter vector incrementally, after every sample. in\nthe online context, the update at step k has the form:\n\n\u03b8k+1 = \u03b8k + \u03b1lspe(\u03b8\u2021\n\nk+1 \u2212 \u03b8k), where:\n\n1\n\nk + 1\n\n\u03b3k+1\u03b8\u2021\n\nk+1 = \u03b3\n\n1\n\nk + 1\n\n\u03bbk+1\u03b8k +\n\n1\n\nk + 1\n\nzk+1\n\n(5.17)\n\nwhere \u03b1lspe is a step size parameter. the policy is improved optimistically, once\nevery k\u03b8 transitions. algorithm 5.4 shows online pi with lspe-q, in a variant that\nemploys \u03b5-greedy exploration; this variant will be used in the sequel.\n\nalgorithm 5.4 online policy iteration with lspe-q and \u03b5-greedy exploration.\ninput: discount factor \u03b3,\n\nbfs \u03c61, . . . ,\u03c6n : x \u00d7 u \u2192 r,\npolicy improvement interval k\u03b8, exploration schedule {\u03b5k}\u221e\nstep size \u03b1lspe > 0, a small constant \u03b2\u03b3 > 0\n\nk=0,\n\na uniform random action in u with probability \u03b5k (explore)\n\nwith probability 1\u2212 \u03b5k (exploit)\n\n1: \u2113 \u2190 0, initialize policy h0, initialize parameters \u03b80\n2: \u03b30 \u2190 \u03b2\u03b3in\u00d7n, \u03bb0 \u2190 0, z0 \u2190 0\n3: measure initial state x0\n4: for every time step k = 0, 1, 2, . . . do\n\nuk \u2190(h\u2113(xk)\napply uk, measure next state xk+1 and reward rk+1\n\u03b3k+1 \u2190 \u03b3k + \u03c6(xk, uk)\u03c6t(xk, uk)\n\u03bbk+1 \u2190 \u03bbk + \u03c6(xk, uk)\u03c6t(xk+1, h\u2113(xk+1))\nzk+1 \u2190 zk + \u03c6(xk, uk) rk+1\n\u03b8k+1 \u2190 \u03b8k +\u03b1lspe(\u03b8\u2021\nk+1\u2212\u03b8k), where 1\nif k = (\u2113 + 1)k\u03b8 then\n\nk+1 \u03b3k+1\u03b8\u2021\n\nh\u2113+1(x) \u2190 arg maxu \u03c6t(x, u)\u03b8k+1, \u2200x\n\u2113 \u2190 \u2113 + 1\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\nend if\n\n14:\n15: end for\n\nk+1 = \u03b3 1\n\nk+1 \u03bbk+1\u03b8k + 1\n\nk+1 zk+1\n\nthe fully optimistic variant of online pi with lspe-q (for k\u03b8 = 1) was studied,\ne.g., by jung and polani (2007a). bertsekas (2007) and jung and polani (2007a)\nconjectured that lspe-q is more promising for online pi than lstd-q, due to its\nincremental nature.\n\nnext, we apply online pi with lspe-q to the swing-up problem. we do not\nstudy the influence of all the parameters, but only focus on parameter k\u03b8, by running\na set of experiments that parallels the study of k\u03b8 for online lspi. the approximator,\nexploration schedule, and trial length are the same as in that experiment, and the same\nk\u03b8 values are used. the matrix \u03b3 is initialized to 0.001\u00b7 in\u00d7n. online pi with lspe-q\nhas an additional step size parameter, \u03b1lspe, which was not present in online lspi.\nin order to choose \u03b1lspe, preliminary experiments were performed for each value\n\n "}, {"Page_number": 199, "text": "190\n\nchapter 5. online and continuous-action lspi\n\nof k\u03b8, using several values of \u03b1lspe: 0.001, 0.01, 0.1, and 1. in these experiments,\nthe following values of \u03b1lspe performed reasonably: 0.001, 0.01, 0.001, 0.01, and\n0.1, for, respectively, k\u03b8 = 1, 10, 100, 1000, and 5000. with these values of \u03b1lspe, 20\nindependent runs are performed for every k\u03b8.\n\nfigure 5.8 presents the performance of online pi with lspe-q across these 20\nruns; compare with figure 5.4. a reliably improving performance is only obtained\nfor k\u03b8 = 1 and \u03b1lspe = 0.001. in this experiment, due to the very small step size,\nlearning is slower than for online lspi in figure 5.4. for all the other experiments,\nonline pi with lspe-q is less reliable than online lspi: there is a larger variation in\nperformance across the 20 runs, as illustrated by the larger 95% confidence intervals\nfor k\u03b8 = 5000 in figure 5.8(b); the experiments for which confidence intervals are\nnot shown have a similar character. to explain why this is the case, recall from sec-\ntion 3.5.2 that in order to guarantee the convergence of lspe-q, it is required that\nstate-action samples are generated according to their steady-state probabilities under\nthe current policy. in online pi, the policy is changed often and many exploratory\nactions are taken, which severely violates this requirement, destabilizing the update\n(5.17). while online lspi is also affected by imprecision in the values of \u03b3, \u03bb, and\nz, it may be more stable because it only uses them to compute one-shot solutions,\nrather than updating the parameter vector recursively, like online pi with lspe-q.\neven though a very small step size may recover a stable performance improvement\nfor online pi with lspe-q (as illustrated for k\u03b8 = 1 with \u03b1lspe = 0.001), this is\nnot guaranteed (as illustrated for k\u03b8 = 100, which uses the same value of \u03b1lspe but\nnevertheless remains unstable).\n\n\u22121400\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\ne\nr\no\nc\ns\n\n \n\n\u22121400\n\n \n\ne\nr\no\nc\ns\n\n\u22121600\n\n\u22121800\n\n\u22122000\n\n\u22122200\n\n k\n=1\n\u03b8\n k\n=10\n\u03b8\n k\n=100\n\u03b8\n k\n=1000\n\u03b8\n k\n=5000\n\u03b8\n\n k\n=1, mean\n\u03b8\n\n95% confidence bounds\n k\n=5000, mean\n\u03b8\n\n95% confidence bounds\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n\u22122400\n\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\n(a) mean score for all the experiments.\n\n(b) mean score with 95% confidence intervals,\nfor the extreme values of k\u03b8.\n\nfigure 5.8\nperformance of online pi with lspe-q for varying k\u03b8 in the inverted pendulum problem.\n\nfigure 5.9 presents the mean execution time of online pi with lspe-q, and re-\npeats the execution time of online lspi from figure 5.6(b), for an easy comparison.\nonline pi with lspe-q solves a linear system at every step, so, for k\u03b8 > 1, it is more\ncomputationally expensive than online lspi, which solves a linear system only be-\nfore policy improvements.\n\n "}, {"Page_number": 200, "text": "5.6. experimental study\n\n191\n\nonline lspi,  mean execution time\nonline pi with lspe\u2212q,  mean execution time\n\n \n\n4\n10\n\n3\n10\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n2\n10\n\n \n0\n10\n\n1\n10\n\n2\n10\nk\n\u03b8\n\n3\n10\n\n4\n10\n\nfigure 5.9\nexecution time of online pi with lspe-q for varying k\u03b8, compared with the execution time\nof online lspi, in the inverted pendulum problem.\n\nonline lspi for the real inverted pendulum\n\nnext, online lspi is used to control the inverted pendulum system in real time, rather\nthan in simulation as in the earlier sections. to make the problem slightly easier for\nthe learning controller, the sampling time is increased to ts = 0.02 s (from 0.005 s),\nand the maximum available control is increased to 3.2 v (from 3 v); even so, the\npendulum must still be swung back and forth to gather energy before it can be turned\nupright. the same approximator is used as in the simulation experiments, and online\nlspi is run for 300 s, divided into 2 s long trials. half of the trials start in the stable\nequilibrium (pointing down), and half in a random initial state obtained by applying\na sequence of random actions. the initial exploration probability is \u03b50 = 1 and de-\ncays with \u03b5d = 0.9848, which leads to a final exploration probability of 0.01. policy\nimprovements are performed only after each trial, because solving the linear system\nat line 11 of algorithm 5.2 at arbitrary time steps may take longer than the sampling\ntime.\n\nfigure 5.10(a) presents a subsequence of learning trials, containing 1 out of each\n10 trials. all of these trials are among those starting with the pendulum pointing\ndown. these trajectories include the effects of exploration. figure 5.10(b) shows\na swing-up of the pendulum with the final policy and without exploration. the con-\ntroller successfully learns how to swing up and stabilize the pendulum, giving a good\nperformance roughly 120 s into learning. this is similar to the learning rate observed\nin the simulation experiments. figure 5.10(c) shows the final policy obtained, indicat-\ning also some of the state samples collected during learning; compare with the near-\noptimal policy of figure 5.1(b) on page 181. for small velocities (around the zero\ncoordinate on the vertical axis), the policy found online resembles the near-optimal\npolicy, but it is incorrect for large velocities. this is because, using the procedure de-\nscribed above to re-initialize the state in the beginning of each trial, the samples are\nconcentrated in the low-velocity areas of the state space. the performance would im-\nprove if a (suboptimal) controller were available to re-initialize the state to arbitrary\nvalues.\n\n "}, {"Page_number": 201, "text": "192\n\nchapter 5. online and continuous-action lspi\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n20 40 60 80 100 120 140 160 180 200 220 240 260 280\n\n0\n\n20 40 60 80 100 120 140 160 180 200 220 240 260 280\n\n\u221220\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n20 40 60 80 100 120 140 160 180 200 220 240 260 280\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n20 40 60 80 100 120 140 160 180 200 220 240 260 280\n\n0\n\nt [s]\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n2\n\n2\n\n2\n\n2\n\n(a) a subsequence of learning trials. each trial is 2 s\nlong, and only 1 out of every 10 trials is shown. the\nstarting time of each trial is given on the horizontal axis,\nand trials are separated by vertical lines.\n\n(b) a swing-up using the final policy.\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(c) policy, with some of the state samples observed\nduring learning indicated as gray dots.\n\nfigure 5.10 real-time results of online lspi for the inverted pendulum.\n\n5.6.2 online lspi for the two-link manipulator\n\nthis section examines the performance of online lspi for a system with higher di-\nmensionality than that of the inverted pendulum: a two-link robotic manipulator op-\nerating in a horizontal plane.\n\ntwo-link manipulator problem\n\nsince the two-link manipulator problem was already described in section 4.5.2, it\nis only briefly recapitulated here. the two-link manipulator, has 4 state variables, 2\naction variables, and the following continuous-time dynamics:\n\nm(\u03b1) \u00a8\u03b1+ c(\u03b1, \u02d9\u03b1) \u02d9\u03b1 = \u03c4\n\n "}, {"Page_number": 202, "text": "5.6. experimental study\n\n193\n\nwhere \u03b1 = [\u03b11,\u03b12]t contains the angular positions of the two links, \u03c4 = [\u03c41,\u03c42]t con-\ntains the torques of the two motors, m(\u03b1) is the mass matrix, and c(\u03b1, \u02d9\u03b1) is the cori-\nolis and centrifugal forces matrix. for the values of these matrices, see section 4.5.2,\npage 153; see also the schematic representation of the manipulator in figure 4.8. the\nstate signal contains the angles and angular velocities: x = [\u03b11, \u02d9\u03b11,\u03b12, \u02d9\u03b12]t, and the\ncontrol signal is u = \u03c4. the angles \u03b11,\u03b12 \u201cwrap around\u201d in the interval [\u2212\u03c0,\u03c0) rad,\nand the angular velocities \u02d9\u03b11, \u02d9\u03b12 are restricted to the interval [\u22122\u03c0, 2\u03c0] rad/s us-\ning saturation. the torques are constrained as follows: \u03c41 \u2208 [\u22121.5, 1.5] nm, \u03c42 \u2208\n[\u22121, 1] nm. the discrete time step is set to ts = 0.05 s, and the discrete-time dy-\nnamics f are obtained by numerically integrating (4.49) between consecutive time\nsteps.\n\nthe goal is to stabilize the system around \u03b1 = \u02d9\u03b1 = 0, and is expressed by the\n\nquadratic reward function:\n\n\u03c1(x, u) = \u2212xtqrewx, with qrew = diag[1, 0.05, 1, 0.05]\n\nthe discount factor is set to \u03b3 = 0.98.\n\napproximator, parameter settings, and performance criterion\n\nlike for the inverted pendulum, the q-function approximator combines state-\ndependent rbfs with discretized actions. an equidistant grid of 5\u00d7 5\u00d7 5\u00d7 5 iden-\ntically shaped axis-aligned rbfs is defined over the four-dimensional state space.\nthe discretized actions are [\u03c41,\u03c42]t \u2208 {\u22121.5, 0, 1.5}\u00d7{\u22121, 0, 1}. this leads to a to-\ntal number of 54 \u00b7 9 = 5625 state-action bfs. the learning experiment has a duration\n\nof 7200 s, and is divided into trials that have a length of 10 s (which is sufficient to\nstabilize the system) and start from uniformly distributed random initial states. the\npolicy is improved once every k\u03b8 = 50 transitions, and \u03b2\u03b3 is set to 0.001. the initial\nexploration rate is \u03b50 = 1 and decays with \u03b5d = 0.999041, leading to an exploration\nprobability of 0.001 at the end of the experiment.\n\nthe performance of the policies computed online is evaluated using the average\n\nreturn (score) over a set of initial states containing a regular grid of link angles:\n\nx0 = {\u2212\u03c0,\u22122\u03c0/3,\u2212\u03c0/3, ...,\u03c0}\u00d7{0}\u00d7{\u2212\u03c0,\u22122\u03c0/3,\u2212\u03c0/3, ...,\u03c0}\u00d7{0}\n\nthe returns are estimated with a precision of \u03b5mc = 0.1.\n\nresults of online lspi\n\nfigure 5.11 shows the performance of online lspi across 10 independent runs.\nthe algorithm first reaches a near-final performance after 1200 s, during which\n24000 samples are collected; a similar number of samples was required for the two-\ndimensional inverted pendulum. so, the learning rate of online lspi scales up well\nto the higher-dimensional manipulator problem.\n\nfigure 5.12 presents a policy found by online lspi, together with a represen-\ntative trajectory that is controlled by this policy; compare this solution, e.g., with\nthe fuzzy q-iteration solution shown in figure 4.9 on page 155, in chapter 4. the\n\n "}, {"Page_number": 203, "text": "194\n\nchapter 5. online and continuous-action lspi\n\ne\nr\no\nc\ns\n\n\u2212100\n\n\u2212150\n\n\u2212200\n\n\u2212250\n\n\u2212300\n\n\u2212350\n\n\u2212400\n\n\u2212450\n\n \n0\n\n \n\nmean\n95% confidence bounds\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\nt [s]\n\nfigure 5.11 performance of online lspi for the robotic manipulator.\n\nlarge-scale structure of the policy in figure 5.12(a) roughly resembles that of the\nfuzzy q-iteration policy, but the details are different and likely suboptimal. in fig-\nure 5.12(a), the system is stabilized after 2 s, but the trajectory exhibits chattering of\nthe control actions and, as a result, oscillation of the states. in comparison, the fuzzy\nq-iteration trajectories of figure 4.9 do not exhibit as much chattering.\n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n1\n\n \n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n\n1.5\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n\u22121.5\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n \n,\n\n\u03b1\n\n2\n\n1\n\n2\n\n0\n\n\u22122\n\n0\n\n5\n\n0\n\n\u22125\n\n0\n\n1\n\n0\n\n\u22121\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03b1\n\n \n,\n\n\u2019\n\n\u03b1\n\n2\n\n1\n\n]\n\nm\nn\n\n[\n \n\n\u03c4\n \n,\n\n\u03c4\n\n2\n\n1\n\n]\n\n\u2212\n\n[\n \nr\n\n0\n\n0\n\n\u221210\n\n\u221220\n\n0\n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n\u03c4\n(\u03b1\n,0,\u03b1\n,0) [nm]\n2\n1\n2\n\n\u22122\n\n0\n\u03b1\n [rad]\n1\n\n2\n\n \n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n1\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2\n\n2\n\nt [s]\n\n3\n\n3\n\n3\n\n3\n\n4\n\n4\n\n4\n\n4\n\n5\n\n5\n\n5\n\n5\n\n(a) a slice through the policy at \u02d9\u03b11 = \u02d9\u03b12 = 0 and\nparallel to the plane (\u03b11,\u03b12).\n\n(b) a controlled\n\n[\u2212\u03c0, 0,\u2212\u03c0, 0]t\n\nthick gray line \u2013 link 2).\n\ntrajectory\n\nfrom x0 =\n(thin black line \u2013 link 1,\n\nfigure 5.12 solution found by online lspi for the two-link manipulator.\n\n "}, {"Page_number": 204, "text": "5.6. experimental study\n\n195\n\nthe poorer performance of online lspi is caused mainly by the limitations of\nthe chosen approximator. while increasing the number of rbfs or discrete actions\nwould help, it may also lead to excessive computational costs.7 recall that online\nlspi has a time complexity of o(n3) = o(n3m3) per (policy improvement) step,\nwhere n = 54 is the number of state-dependent bfs and m = 9 the number of dis-\ncrete actions. the actual execution time of online lspi was around 20 hours per\nrun, longer than the 1200 s interval of simulated time, so the experiment cannot\nbe reproduced in real time. this illustrates the difficulty of using generic, uniform-\nresolution approximators in high-dimensional problems. additionally, having more\nrbfs means that more parameters must be determined, which in turn requires more\ndata. this can be problematic when data is costly.\n\na good way to avoid these difficulties is to determine automatically a small num-\nber of bfs well suited to the problem at hand. while we do not study this possibility\nhere, we have reviewed in section 3.6 some approaches for automatically finding\nbfs to be used in least-squares algorithms for policy evaluation. most of approaches\nthese work in a batch, offline setting (menache et al., 2005; mahadevan and mag-\ngioni, 2007; xu et al., 2007; kolter and ng, 2009), and would need to be modified\nto work online. approaches that work on a sample-by-sample basis are more readily\nadaptable to online lspi (engel et al., 2005; jung and polani, 2007a). another pos-\nsibility to reduce the computational demands is to employ incremental updates of the\nparameters, rather than solving linear systems of equations (geramifard et al., 2006,\n2007), but this approach does not reduce the data demands of the algorithm.\n\nfor completeness, we also attempted to apply offline lspi and online pi with\nlspe-q to the manipulator problem. offline lspi failed to converge even when\nprovided with 105 samples, while online pi with lspe-q exceeded the memory re-\nsources of our machine8 and therefore could not be run at all. recall from section 5.3\nthat, in practice, online lspi only needs to store a single n\u00d7 n estimate of \u03b3\u2212 \u03b3\u03bb,\nwhereas online pi with lspe-q must estimate \u03b3 and \u03bb separately.\n\n5.6.3 online lspi with prior knowledge for the dc motor\n\nin this section, we investigate the effects of using prior knowledge about the policy\nin online lspi. we consider prior knowledge about the monotonicity of the policy,\nas discussed in section 5.4, and perform an experimental study on the dc motor\nproblem, which was introduced in section 3.4.5 and used again in section 4.5.1.\n\ndc motor problem\n\nthe dc motor is described by the discrete-time model:\n\n7by comparison, fuzzy q-iteration can use a more accurate approximator without incurring excessive\ncomputational costs, because it only needs to store and update vectors of parameters, whereas lspi needs\nto store matrices and solve linear systems of equations.\n\n8our machine was equipped with 3 gb of ram and was configured to use 2 gb of swap space.\n\n "}, {"Page_number": 205, "text": "196\n\nchapter 5. online and continuous-action lspi\n\nf (x, u) = ax + bu\n\na =(cid:20)1\n\n0\n\n0.0049\n\n0.9540(cid:21) , b =(cid:20)0.0021\n0.8505(cid:21)\n\nwhere x1 = \u03b1 \u2208 [\u2212\u03c0,\u03c0] rad is the shaft angle, x2 = \u02d9\u03b1 \u2208 [\u221216\u03c0, 16\u03c0] rad/s is the\nangular velocity, and u\u2208 [\u221210, 10] v is the control input (voltage). the state variables\nare restricted to their domains using saturation. the goal is to stabilize the system\naround x = 0, and is described by the quadratic reward function:\n\n\u03c1(x, u) = \u2212xtqrewx\u2212 rrewu2\nqrew =(cid:20)5\n\n0.01(cid:21) , rrew = 0.01\n\n0\n\n0\n\nwith discount factor \u03b3 = 0.95.\n\nbecause the dynamics are linear and the reward function is quadratic, the optimal\npolicy would be a linear state feedback if the constraints on the state and action\nvariables were disregarded (bertsekas, 2007, section 3.2). the optimal feedback gain\ncan be computed using an extension of linear quadratic control to the discounted\ncase, as explained in footnote 16 of section 3.7.3. the resulting feedback gain for\n\nthe dc motor is [\u221212.92,\u22120.68]t. by additionally restricting the control input to the\nadmissible range [\u221210, 10] using saturation, the following policy is obtained:\n\nh(x) = sat(cid:8)[\u221212.92,\u22120.68]t \u00b7 x,\u221210, 10(cid:9)\n\n(5.18)\n\nwhich is monotonically decreasing along both axes of the state space. this mono-\ntonicity property will be used in the sequel to accelerate the learning rate of online\nlspi. note that the actual values of the linear state feedback gains are not required\nto derive this prior knowledge, but only their sign must be known. the policy (5.18)\nis shown in figure 5.13(a).\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n50\n\n0\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) constrained linear state feedback.\n\n(b) near-optimal policy found by fuzzy q-\niteration.\n\nfigure 5.13 near-optimal policies for the dc motor.\n\nfor comparison, figure 5.13(b) presents a near-optimal policy, computed by\nfuzzy q-iteration with an accurate approximator (this policy is repeated from fig-\nure 3.5(b)). over a large region of the state space, this policy is linear, and there-\nfore monotonic. the only nonlinear, nonmonotonic regions appear in the top-left and\n\n "}, {"Page_number": 206, "text": "5.6. experimental study\n\n197\n\nbottom-right corners of the figure, and are probably due to the constraints on the state\nvariables. so, the class of monotonic policies to which online lspi will be restricted\ndoes indeed contain near-optimal solutions.\n\napproximator, parameter settings, and performance criterion\n\nto apply online lspi with monotonic policies to the dc motor, the q-function is\napproximated using state-dependent rbfs and discretized actions, as in the inverted\npendulum and manipulator examples above. the rbfs are axis-aligned, their centers\nare arranged on an equidistant 9\u00d7 9 grid in the state space, and their width bd along\n2/2, where b\u2032d is the distance between adjacent rbfs\neach dimension d is equal to b\u2032d\nalong that dimension. these rbfs lead to a smooth interpolation over the state space.\nsince the domains of the state variables are [\u2212\u03c0,\u03c0] for the angle and [\u221216\u03c0, 16\u03c0]\nfor the angular velocity, we obtain b\u20321 = 2\u03c0\n9\u22121 \u2248 12.57, leading to\nb1 \u2248 0.31 and b2 \u2248 78.96. the discretized action space is ud = {\u221210, 0, 10}, leading\nto a total number of 92 \u00b7 3 = 243 state-action bfs.\nas explained in section 5.4, we employ a linear policy parametrization (5.4) and\nenforce the monotonicity constraints in the policy improvements (5.10). the pol-\nicy rbfs are identical to the q-function rbfs, so the policy has 81 parameters. an\nadded benefit of this parametrization is that it produces continuous actions. neverthe-\nless, as explained in section 5.4.1, these actions must be discretized during learning,\nbecause the q-function approximator can only employ discrete actions. to perform\nthe policy improvements (5.10), 1000 uniformly distributed, random state samples\nare generated. since these samples do not include information about the dynamics or\nthe rewards, a model is not required to generate them.\n\n9\u22121 \u2248 0.79 and b\u20322 = 32\u03c0\n\nthe learning experiment has a length of 600 s, and is divided into 1.5 s long trials\nwith uniformly distributed random initial states. the policy improvement interval is\nk\u03b8 = 100, and the exploration schedule starts from \u03b50 = 1 and decays with a rate\nof \u03b5d = 0.9886, leading to \u03b5 = 0.1 at t = 200 s. policies are evaluated by estimating\nwith precision \u03b5mc = 0.1 their average return (score) over the grid of initial states:\n\nx0 = {\u2212\u03c0,\u2212\u03c0/2, 0,\u03c0/2,\u03c0}\u00d7{\u221210\u03c0,\u22125\u03c0,\u22122\u03c0,\u2212\u03c0, 0,\u03c0, 2\u03c0, 5\u03c0, 10\u03c0}\n\nresults of online lspi with prior knowledge, and comparison to online lspi\nwithout prior knowledge\n\nfigure 5.14 shows the learning performance of online lspi with prior knowledge\nabout the monotonicity of the policy, in comparison to the performance of the original\nonline lspi algorithm, which does not use prior knowledge. mean values across\n40 independent runs are reported, together with 95% confidence intervals on these\nmeans. using prior knowledge leads to much faster and more reliable learning: the\nscore reliably converges in around 50 s of simulation time. in contrast, online lspi\nwithout prior knowledge requires more than 300 s of simulation time to reach a near-\noptimal performance, and has a larger variation in performance across the 40 runs,\nwhich can be seen in the wide 95% confidence intervals.\n\nthe mean execution time of online lspi with prior knowledge is 1034.2 s, with\n\n "}, {"Page_number": 207, "text": "198\n\nchapter 5. online and continuous-action lspi\n\n\u2212200\n\n\u2212300\n\n\u2212400\n\n\u2212500\n\n\u2212600\n\n\u2212700\n\n\u2212800\n\ne\nr\no\nc\ns\n\n \n\nprior knowledge, mean\n95% confidence bounds\nno prior knowledge, mean\n95% confidence bounds\n\n\u2212900\n \n0\n\n100\n\n200\n\n300\nt [s]\n\n400\n\n500\n\n600\n\nfigure 5.14\ncomparison between online lspi with prior knowledge and the original online lspi algo-\nrithm, in the dc motor problem.\n\na 95% confidence interval of [1019.6, 1048.7] s. for the original online lspi algo-\nrithm, the mean execution time is 87.6 s with a confidence interval of [84.0, 91.3] s.\nthe execution time is larger for online lspi with prior knowledge, because the con-\nstrained policy improvements (5.10) are more computationally demanding than the\noriginal policy improvements (5.2). note that the execution time of online lspi with\nprior knowledge is larger than the duration of the simulation (600 s), so the algorithm\ncannot be applied in real time.\n\nfigure 5.15 compares a representative solution obtained using prior knowledge\nwith one obtained by the original online lspi algorithm. the policy obtained without\nusing prior knowledge (figure 5.15(b)) violates monotonicity in several areas. in the\ncontrolled trajectory of figure 5.15(e), the control performance of the monotonic\npolicy is better, mainly because it outputs continuous actions. unfortunately, this\nadvantage cannot be exploited during learning, when the actions must be discretized\nto make them suitable for the q-function approximator. (recall also that the same\naction discretization is employed in both the experiment without prior knowledge\nand in the experiment employing prior knowledge.)\n\n5.6.4 lspi with continuous-action approximation for the inverted\n\npendulum\n\nin this fourth and final example, we return to the inverted pendulum problem, and\nuse it to evaluate the continuous-action approximators introduced in section 5.5. to\nthis end, offline lspi with continuous-action approximation is applied to the inverted\npendulum, and the results are compared to those obtained by offline lspi with dis-\ncrete actions.\n\napproximator, parameter settings, and performance criterion\n\nboth the continuous-action and the discrete-action representations of the q-function\nemploy a set of state-dependent rbfs. these rbfs are identical to those used in the\nonline lspi experiments of section 5.6.1. namely, they are axis-aligned, identically\n\n "}, {"Page_number": 208, "text": "5.6. experimental study\n\n199\n\n50\n\n0\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n10\n\n50\n\n0\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n5\n\n0\n\n\u22125\n\n\u221210\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n\u221250\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n(a) policy found using prior knowledge.\n\n(b) policy found without prior knowledge.\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212500\n\n\u22121000\n\n50\n\n)\n0\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n0\n\n\u2212500\n\n\u22121000\n\n50\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n0\n\n\u03b1\u2019 [rad/s]\n\n\u221250\n\n2\n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n(c) slice through the q-function found using prior\nknowledge, for u = 0.\n\n(d) slice through the q-function found without\nprior knowledge, for u = 0.\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n40\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n10\n\n]\n\nv\n\n[\n \n\nu\n\n0\n\n\u221210\n0\n\n0\n\n\u221250\n\n]\n\n\u2212\n\n[\n \nr\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\nt [s]\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.4\nt [s]\n\n(e) trajectory controlled by the policy found us-\ning prior knowledge.\n\n(f) trajectory controlled by the policy found\nwithout prior knowledge.\n\nfigure 5.15\nrepresentative solutions found using prior knowledge (left) and without prior knowledge\n(right) for the dc motor.\n\n "}, {"Page_number": 209, "text": "200\n\nchapter 5. online and continuous-action lspi\n\nshaped, and distributed on an equidistant 11\u00d7 11 grid. this state-space approxima-\ntor is held fixed throughout the experiments, while the action-space approximator is\nchanged as described next.\n\nthe continuous-action approximator combines the rbfs with chebyshev poly-\nnomials of the first kind, as in (5.12) and (5.13). the degree mp of the polynomi-\nals takes values in the set {2, 3, 4}. the discrete-action approximator combines the\nrbfs with discrete actions, as in (5.15). equidistant discrete actions are used, and\ntheir number m takes two values: 3 and 5. only odd numbers are used to ensure that\nthe zero action belongs to the discrete set. we consider polynomial approximators\nof degree mp side by side with discrete-action approximators having m = mp + 1\nactions, since they have the same number of parameters (section 5.5).\n\nthe samples for policy evaluation are drawn from a uniform distribution over\nthe continuous state-action space x \u00d7 u for the polynomial approximators, and over\nthe state-discrete action space x \u00d7 ud for the discrete-action approximators. for a\nfair comparison, the number of samples provided per q-function parameter is held\nconstant, by making the total number of samples ns proportional to the number of\nparameters n. because the state-space approximator is held fixed, ns is in fact pro-\nportional to mp + 1 in the continuous case, and to m in the discrete case. we choose\nns = 10000 for the approximators with m = mp + 1 = 3, leading to ns = 13334 for\nmp = 3 and to ns = 16667 for m = mp + 1 = 5.\n\nthe offline lspi algorithm is considered to have converged when the euclidean\nnorm of the difference between two consecutive parameter vectors does not exceed\n\u03b5lspi = 0.01, or when limit cycles are detected in the sequence of parameters. the\npolicy resulting from every convergent experiment is evaluated by estimating its av-\nerage return over the grid (5.16) of initial states.\n\nresults of lspi with continuous actions, and comparison with the discrete ac-\ntion results\n\nfigure 5.16 shows the performance and execution time of lspi with continuous-\naction approximation in comparison with discrete-action approximation. these\ngraphs report mean values and confidence intervals over 20 independent runs,9 and\nshow experiments with the same number of q-function parameters at the same hor-\nizontal coordinate. in figure 5.16(a), the performance differences between polyno-\nmial and discrete-action approximation are inconclusive for low-degree polynomials\n(mp = 2). when the degree increases, the performance of the polynomial approxima-\ntor becomes worse, probably due to overfitting. polynomial approximation leads to\na larger computational cost, which also grows with the degree of the polynomial, as\nshown in figure 5.16(b). among other reasons, this is because the policy improve-\nments (5.14) with polynomial approximation are more computationally demanding\nthan the discrete-action policy improvements.\n\nfigure 5.17 compares a representative continuous-action solution with a repre-\nsentative discrete-action solution. a continuous-action solution found using second-\n\n9note that 2 out of the 20 runs of the experiment with third-degree polynomial approximation were\n\nnot convergent, and were ignored when computing the means and confidence intervals.\n\n "}, {"Page_number": 210, "text": "5.7. summary and discussion\n\n201\n\ne\nr\no\nc\ns\n\n\u22121400\n\n\u22121450\n\n\u22121500\n\n\u22121550\n\n\u22121600\n\n\u22121650\n\n\u22121700\n \n\n3\n\ncontinuous actions, mean score\n95% confidence bounds\ndiscrete actions, mean score\n95% confidence bounds\n\n \n\n4\n10\n\n]\ns\n[\n \n\ncontinuous actions, mean execution time\n95% confidence bounds\ndiscrete actions, mean execution time\n95% confidence bounds\n\ne\nm\n\ni\nt\n \n\nn\no\n\n3\n10\n\ni\nt\n\nu\nc\ne\nx\ne\n\n2\n10\n\n \n3\n\n5\n\n4\nm; m\n+1\np\n\n4\nm; m\n+1\np\n\n(a) performance.\n\n(b) execution time.\n\n \n\n5\n\nfigure 5.16\ncomparison between continuous-action and discrete-action approximation in the inverted pen-\ndulum problem.\n\ndegree polynomial approximation was chosen, because it suffers less from overfitting\n(see figure 5.16(a)). a discrete-action solution with m = 3 was selected for compar-\nison, because it has the same number of parameters. the two policies have a similar\nstructure, and the q-functions are also similar. continuous actions are in fact useful\nto eliminate chattering in figure 5.17(e), even though this advantage is not apparent\nin the numerical scores shown in figure 5.16(a). this discrepancy can be explained\nby examining the nature of the swing-up trajectories. their first part can be well ap-\nproximated by a \u201cbang-off-bang\u201d control law (kirk, 2004, section 5.5), which can\nbe realized using only three discrete actions (maximum action in either direction, or\nzero action). any chattering in the final part of the trajectory, although undesirable,\nwill have little influence on the total return, due to the exponential discounting of the\nrewards. because swing-ups are necessary from many initial states, in this problem\nit is difficult to improve upon the returns obtained by the discrete actions.10 if it is\nessential to avoid chattering, the problem should be reformulated so that the reward\nfunction penalizes chattering more strongly.\n\n5.7 summary and discussion\n\nthis chapter has considered several extensions of lspi, an originally offline algo-\nrithm that represents q-functions using a linear parametrization, and finds the pa-\nrameters by lstd-q policy evaluation. more specifically, an online variant of lspi\n\n10note that the return obtained along the continuous-action trajectory of figure 5.17(e) is slightly worse\nthan for figure 5.17(f). this is because of the spurious nonmaximal actions in the interval [0.5, 0.7] s. on\nthe other hand, even though the continuous-action trajectory exhibits a steady-state error, the negative\ncontribution of this error to the return is less important than the negative contribution of the chattering in\nthe discrete-action trajectory.\n\n "}, {"Page_number": 211, "text": "202\n\nchapter 5. online and continuous-action lspi\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\n0\n\n)\n0\n\n\u22122000\n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n]\ns\n/\n\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n40\n\n20\n\n0\n\n\u221220\n\n\u221240\n\n \n\nh(\u03b1,\u03b1\u2019) [v]\n\n \n\n\u22122\n\n0\n\n\u03b1 [rad]\n\n2\n\n3\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n(a) continuous-action policy.\n\n(b) discrete-action policy.\n\n0\n\n)\n0\n\n\u22122000\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u22124000\n\n\u22126000\n\n40\n\n20\n\n0\n\n\u221220\n\n\u03b1\u2019 [rad/s]\n\n\u221240\n\n\u22122\n\n2\n\n0\n\n\u03b1 [rad]\n\n,\n\u2019\n\n,\n\n\u03b1\n\u03b1\n(\nq\n\n\u22124000\n\n\u22126000\n\n40\n\n20\n\n0\n\n\u221220\n\n\u03b1\u2019 [rad/s]\n\n\u221240\n\n\u22122\n\n2\n\n0\n\n\u03b1 [rad]\n\n(c) slice through the continuous-action q-function,\nfor u = 0.\n\n(d) slice through the discrete-action q-function, for\nu = 0.\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n2\n\n0\n\n\u22122\n\n0\n\n20\n\n0\n\n]\n\nd\na\nr\n[\n \n\n\u03b1\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n\u221220\n\n0\n\n2\n\n0\n\n]\n\nv\n\n[\n \n\nu\n\n\u22122\n\n0\n\n0\n\n]\n\n\u2212\n\n[\n \nr\n\n\u221250\n\n0\n\n2\n\n2\n\n2\n\n2\n\n0.5\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n1\n\nt [s]\n\n1.5\n\n1.5\n\n1.5\n\n1.5\n\n2\n\n2\n\n2\n\n2\n\n(e) swing-up using continuous actions.\n\n(f) swing-up using discrete actions.\n\nfigure 5.17\nrepresentative solutions found with continuous actions (mp = 2; left) and with discrete actions\n(m = 3; right) for the inverted pendulum.\n\n "}, {"Page_number": 212, "text": "5.7. summary and discussion\n\n203\n\nhas been introduced, together with an approach to integrate prior knowledge into this\nvariant, and with a continuous-action approximator for lspi.\n\nonline lspi provided fast learning in simulation and real-time experiments with\nan inverted pendulum. in the same problem, online lspi performed on par with its\noffline counterpart, and was more stable than online pi with lspe-q, probably be-\ncause lstd-q is more resilient than lspe-q to the frequent policy improvements\nnecessary in the online setting. online lspi also learned to stabilize a two-link ma-\nnipulator, but the solutions found were suboptimal, due to the limitations of the cho-\nsen approximator, which only employed equidistant bfs.\n\nin fact, such equidistant approximators were used in all the examples of this chap-\nter, in order to focus on fairly evaluating the extensions of lspi that were introduced,\nin isolation from the difficulties of designing the bfs. nevertheless, in general the\nnumber of equidistant bfs required to achieve a good accuracy may be prohibitive,\nand it is better to employ a smaller number of well-chosen bfs. this would also help\nto reduce the computational demands of the algorithms; for instance, some simula-\ntion experiments with online lspi took longer to execute than the interval of time\nthey simulated, and so they cannot be replicated in real-time. while we have not\nconsidered the bf design problem in this chapter, in section 3.6 we have reviewed\nsome approaches to find good bfs automatically in least-squares methods for policy\nevaluation. these approaches could be adapted to work with online lspi.\n\nthe performance guarantees of offline pi rely on bounded policy evaluation er-\nrors. unfortunately, these guarantees cannot be applied to the online case, because\nonline lspi improves the policy optimistically, before an accurate policy evaluation\ncan be completed. a different approach is required to theoretically analyze the per-\nformance of online, optimistic lspi. such an approach may also be useful to analyze\nonline pi with lspe-q.\n\nthe method presented to integrate prior knowledge into online lspi considers\nproblems in which the policy is known to be monotonic in the state variables. for\nan example involving the control of a dc motor, the use of this type of prior knowl-\nedge led to much faster (in terms of simulated time) and more reliable learning. the\nmonotonicity requirements can be made less restrictive, e.g., by only requiring that\nthe policy is monotonic in the neighborhood of an equilibrium. more general con-\nstraints on the policy can also be considered, but they may be more difficult to enforce\nin the policy improvement step.\n\nthe continuous-action q-function approximator developed for lspi combines\nstate-dependent basis functions with orthogonal polynomial approximation in the\naction space. this approach was evaluated in the inverted pendulum problem, where\na second-degree polynomial approximator helped eliminate chattering of the control\naction, although it did not obtain a better numerical performance (return) than the\ndiscrete-action solution. high-degree polynomials can lead to overfitting, so it would\nbe useful to develop continuous-action q-function approximators that are more re-\nsilient to this detrimental effect. however, care must be taken to ensure that greedy\nactions can be efficiently computed using these approximators.\n\nat this point in the book, we have considered in detail a method for approximate\nvalue iteration (chapter 4), and one for approximate policy iteration (this chapter). in\n\n "}, {"Page_number": 213, "text": "204\n\nchapter 5. online and continuous-action lspi\n\nthe next and final chapter, we will discuss in depth an algorithm from the third class\nof dp/rl methods, approximate policy search.\n\nbibliographical notes\n\nthe use of least-squares methods online was proposed, e.g., by lagoudakis and parr\n(2003a) and by bertsekas (2007), but at the time of this writing, not much is known\nabout how they behave in practice. online, optimistic pi with lspe-q was used by\njung and polani (2007a). li et al. (2009) evaluated lspi with online sample collec-\ntion, focusing on the issue of exploration. their method does not perform optimistic\npolicy improvements, but instead fully executes lspi between consecutive sample-\ncollection episodes. to the best of our knowledge, methods to employ prior knowl-\nedge about the policy in lspi have not yet been studied at the time of this writing.\nconcerning continuous-action results, pazis and lagoudakis (2009) proposed an ap-\nproach to use continuous actions in lspi that relies on iteratively refining discrete\nactions, rather than using polynomial approximation.\n\n "}, {"Page_number": 214, "text": "6\n\napproximate policy search with cross-entropy\noptimization of basis functions\n\nthis chapter describes an algorithm for approximate policy search in continuous-\nstate, discrete-action problems. the algorithm looks for the best policy that can be\nrepresented using a given number of basis functions associated with discrete actions.\nthe locations and shapes of the basis functions, together with the action assignments,\nare optimized using the cross-entropy method, so that the empirical return from a\nrepresentative set of initial states is maximized. the resulting cross-entropy policy\nsearch algorithm is evaluated in problems with two to six state variables.\n\n6.1 introduction\n\nthe previous two chapters have considered value iteration and policy iteration tech-\nniques for continuous-space problems. while these techniques have many benefits,\nthey also have limitations that can make them unsuitable for certain problems. a\ncentral difficulty is that representing value functions accurately becomes very de-\nmanding as the dimensionality of the problem increases. this is especially true for\nuniform-resolution representations of the value function, as seen in chapters 4 and\n5, since the complexity of such a representation grows exponentially with the num-\nber of dimensions. even methods that construct adaptive-resolution approximators\nare often applied only to relatively low-dimensional problems (munos and moore,\n2002; ernst et al., 2005; mahadevan and maggioni, 2007).\n\nin this final chapter of the book, we take a different approach, by designing a\npolicy search algorithm that does not require a value function and thereby avoids\nthe difficulty discussed above. instead, this algorithm parameterizes the policy and\nsearches for optimal parameters that lead to maximal returns (see also section 3.7).\nwe focus on the case where prior knowledge about the policy is not available, which\nmeans that a flexible policy parametrization must be employed. since this flexible\nparametrization may lead to a nondifferentiable optimization criterion with many\nlocal optima, a gradient-free global optimization technique is required. we thus build\non the framework of gradient-free policy search discussed in section 3.7.2. together\nwith chapters 4 and 5, this chapter completes the trio of examples that, at the end of\n\n205\n\n "}, {"Page_number": 215, "text": "206\n\nchapter 6. cross-entropy policy search\n\nchapter 3, we set out to develop for approximate value iteration, approximate policy\niteration, and approximate policy search.\n\nto obtain a flexible policy parametrization, we exploit ideas from the optimiza-\ntion of basis functions (bfs) for value function approximation (section 3.6.2). in\nparticular, we represent the policies using n state-dependent, optimized bfs that\nare associated with discrete actions in a many-to-one fashion. a discrete (or dis-\ncretized) action space is therefore required. the type of bfs and their number n\nare specified in advance and determine the complexity and the representation power\nof the parametrization, whereas the locations and shapes of the bfs, together with\nthe action assignments, are optimized by the policy search procedure. the optimiza-\ntion criterion is a weighted sum of the returns from a finite set of representative\ninitial states, in which each return is computed with monte carlo simulations. the\nrepresentative states and the weight function can be used to focus the algorithm on\nimportant parts of the state space.\n\nthis approach is expected to be efficient in high-dimensional state spaces, be-\ncause its computational demands are not inherently related to the number of state\nvariables. instead, they depend on how many representative initial states are chosen\n(since simulations must be run from every such state), and on how difficult it is to\nfind a policy that obtains near-optimal returns from these initial states (since this dic-\ntates the complexity of the optimization problem). note that such a policy can be\nsignificantly easier to find than a globally optimal one, because it only needs to take\ngood actions in the state space subregions reached by near-optimal trajectories from\nthe representative states. in contrast, a globally optimal policy must take good actions\nover the entire state space, which can be significantly larger than these subregions.\n\nwe select the cross-entropy (ce) method to optimize the parameters of the pol-\nicy (rubinstein and kroese, 2004). the resulting algorithm for ce policy search with\nadaptive bfs is evaluated in three problems, gradually increasing in dimensionality:\nthe optimal control of a double integrator, the balancing of a bicycle, and the control\nof the treatment for infection by the human immunodefficiency virus (hiv). the two-\ndimensional double-integrator is used to study ce policy search when it looks for a\npolicy that performs well over the entire state space. in this setting, ce policy search\nis also compared to value iteration and policy iteration with uniform-resolution ap-\nproximation, and to a policy search variant that employs a different optimization\nalgorithm called direct (jones, 2009). in the four-dimensional bicycle balancing\nproblem, we study the effects of the set of representative states and of stochastic\ntransitions. finally, ce policy search is applied to a realistic, six-dimensional hiv\ninfection control problem.\n\nwe next provide a recapitulation of ce optimization in section 6.2. section 6.3\nthen describes the policy parametrization and the ce algorithm to optimize the pa-\nrameters, and section 6.4 reports the results of the numerical experiments outlined\nabove. a summary and a discussion are given in section 6.5.\n\n "}, {"Page_number": 216, "text": "6.2. cross-entropy optimization\n\n207\n\n6.2 cross-entropy optimization\n\nin this section, the ce method for optimization (rubinstein and kroese, 2004) is\nbriefly introduced, specializing the discussion to the context of this chapter. this in-\ntroduction is mainly intended for the reader who skipped chapter 4, where the ce\nmethod was also described; other readers can safely start reading from only equa-\ntion (6.5) onwards, where the discussion is novel. also note that a more detailed\ndescription of the ce method can be found in appendix b.\n\nconsider the following optimization problem:\n\ns(a)\n\nmax\na\u2208a\n\n(6.1)\n\nwhere s : a \u2192 r is the score function (optimization criterion) to maximize, and the\nvariable a takes values in the domain a . denote the maximum of s by s\u2217. the ce\nmethod maintains a density1 with support a . at each iteration, a number of samples\nare drawn from this density and the score values of these samples are computed.\na smaller number of samples that have the best scores are kept, and the remaining\nsamples are discarded. the density is then updated using the selected samples, such\nthat the probability of drawing better samples is increased at the next iteration. the\nalgorithm stops when the score of the worst selected sample no longer improves\nsignificantly.\n\nformally, a family of probability densities {p(\u00b7; v)} must be chosen. this family\nhas support a and is parameterized by v. at each iteration \u03c4 \u2265 1 of the ce algo-\nrithm, a number nce of samples are drawn from the density p(\u00b7; v\u03c4\u22121), their scores\nare computed, and the (1 \u2212 \u03c1ce) quantile2 \u03bb\u03c4 of the sample scores is determined,\nwith \u03c1ce \u2208 (0, 1). then, a so-called associated stochastic problem is defined, which\ninvolves estimating the probability that the score of a sample drawn from p(\u00b7; v\u03c4\u22121)\nis at least \u03bb\u03c4:\n\npa\u223cp(\u00b7;v\u03c4\u22121)(s(a) \u2265 \u03bb\u03c4) = ea\u223cp(\u00b7;v\u03c4\u22121){i(s(a) \u2265 \u03bb\u03c4)}\n\n(6.2)\n\nwhere i is the indicator function, equal to 1 whenever its argument is true, and 0\notherwise.\n\nthe probability (6.2) can be estimated by importance sampling. for the associ-\nated stochastic problem, an importance sampling density is one that increases the\nprobability of the interesting event s(a) \u2265 \u03bb\u03c4. an optimal importance sampling\ndensity in the family {p(\u00b7; v)}, in the sense of the smallest cross-entropy (smallest\nkullback-leibler divergence), is given by a parameter that is a solution of:\n\narg max\n\nv\n\nea\u223cp(\u00b7;v\u03c4\u22121){i(s(a) \u2265 \u03bb\u03c4) ln p(a; v)}\n\n(6.3)\n\n1for simplicity, we will abuse the terminology by using the term \u201cdensity\u201d to refer to probability\ndensity functions (which describe probabilities of continuous random variables), as well as to probability\nmass functions (which describe probabilities of discrete random variables).\n\n2if the score values of the samples are ordered increasingly and indexed such that s1 \u2264 \u00b7\u00b7\u00b7 \u2264 snce , then\n\nthe (1\u2212 \u03c1ce) quantile is: \u03bb\u03c4 = s\u2308(1\u2212\u03c1ce)nce\u2309.\n\n "}, {"Page_number": 217, "text": "208\n\nchapter 6. cross-entropy policy search\n\nbv\u03c4 = v\u2021\n\ncounterpart:\n\nan approximate solution bv\u03c4 of (6.3) is computed with the so-called stochastic\n\n\u03c4, where v\u2021\n\n\u03c4 \u2208 arg max\n\nv\n\ni(s(ais) \u2265 \u03bb\u03c4) ln p(ais ; v)\n\n(6.4)\n\n1\nnce\n\nnce\n\u2211\nis=1\n\nonly the samples that satisfy s(ais) \u2265 \u03bb\u03c4 contribute to this formula, since the contri-\nbutions of the other samples are made to be zero by the product with the indicator\nfunction. in this sense, the updated density parameter only depends on these best\nsamples, and the other samples are discarded.\n\nce optimization proceeds with the next iteration using the new density parameter\n\nv\u03c4 =bv\u03c4 (note that the probability (6.2) is never actually computed). the updated den-\n\nsity aims at generating good samples with a higher probability than the old density,\nthus bringing \u03bb\u03c4+1 closer to the optimum s\u2217. the goal is to eventually converge to a\ndensity that, with very high probability, generates samples close to optimal value(s)\nof a. the algorithm can be stopped when the (1\u2212 \u03c1ce)-quantile of the sample per-\nformance improves for dce > 1 consecutive iterations, but these improvements do\nnot exceed a small positive constant \u03b5ce; alternatively, the algorithm stops when a\nmaximum number of iterations \u03c4max is reached. the best score among the samples\ngenerated in all the iterations is taken as the approximate solution of the optimiza-\ntion problem (6.1), and the corresponding sample as an approximate location of an\noptimum.\n\ncan also be updated incrementally:\n\ninstead of setting the new density parameter equal to the solutionbv\u03c4 of (6.4), it\n\nv\u03c4 = \u03b1cebv\u03c4 + (1\u2212 \u03b1ce)v\u03c4\u22121\n\n(6.5)\nwhere \u03b1ce \u2208 (0, 1]. this so-called smoothing procedure is useful to prevent ce opti-\nmization from becoming stuck in local optima (rubinstein and kroese, 2004).\nunder certain assumptions on a and p(\u00b7; v), the stochastic counterpart (6.4) can\nbe solved analytically. one particularly important case when this happens is when\np(\u00b7; v) belongs to the natural exponential family (morris, 1982). for instance, when\n{p(\u00b7; v)} is the family of gaussians parameterized by the mean \u03b7 and the standard\ndeviation \u03c3 (so that v = [\u03b7,\u03c3]t), the solution v\u03c4 of (6.4) consists of the mean and the\nstandard deviation of the best samples, i.e., of the samples ais for which s(ais) \u2265 \u03bb\u03c4.\nce optimization has been shown to perform well in many optimization prob-\nlems, often better than other randomized algorithms (rubinstein and kroese, 2004),\nand has found applications in many areas, among which are biomedicine (math-\nenya et al., 2007), power systems (ernst et al., 2007), vehicle routing (chepuri and\nde mello, 2005), vector quantization (boubezoul et al., 2008), and clustering (rubin-\nstein and kroese, 2004). while the convergence of ce optimization has not yet been\nproven in general, the algorithm is usually convergent in practice (rubinstein and\nkroese, 2004). for combinatorial (discrete-variable) optimization, the ce method\nprovably converges with probability 1 to a unit mass density, which always generates\nsamples equal to a single point. furthermore, the probability that this convergence\npoint is in fact an optimal solution can be made arbitrarily close to 1 by using a\nsufficiently small smoothing parameter \u03b1ce (costa et al., 2007).\n\n "}, {"Page_number": 218, "text": "6.3. cross-entropy policy search\n\n209\n\n6.3 cross-entropy policy search\n\nin this section, a general approach to policy optimization using the ce method is\ndescribed, followed by a version of the general algorithm that employs radial basis\nfunctions (rbfs) to parameterize the policy.\n\n6.3.1 general approach\n\nconsider a stochastic or deterministic markov decision process (mdp). in the se-\nquel, we employ the notation for stochastic mdps, but all the results can easily be\nspecialized to the deterministic case. denote by d the number of state variables of\nthe mdp (i.e., the dimension of x ). we assume that the action space of the mdp\nis discrete and contains m distinct actions, ud = {u1, . . . , um}. the set ud can result\nfrom the discretization of an originally larger (e.g., continuous) action space u .\nthe policy parametrization is introduced next, followed by the score function and\n\nby the ce procedure to optimize the parameters.\n\npolicy parametrization\n\nthe policy is represented using n basis functions (bfs) defined over the state space\nand parameterized by a vector \u03be \u2208 \u03be:\n\n\u03d5i(\u00b7;\u03be) : x \u2192 r,\n\ni = 1, . . . , n\n\nwhere the dot stands for the state argument x. the parameter vector \u03be typically gives\nthe locations and shapes of the bfs. the bfs are associated to discrete actions by a\nmany-to-one mapping, which can be represented as a vector \u03d1 \u2208 {1, . . . , m}n\nthat\nassociates each bf \u03d5i to a discrete action index \u03d1i, or equivalently to a discrete action\nu\u03d1i . a schematic representation of this parametrization is given in figure 6.1.\n\nj2\n\njn\n\nj1\n\nc\n1\n\nb\n1\n\nx\n\nu\n\nd\n\nu\n\n1\n\nu\n\n2\n\n\u03d1\n\nu\n\nm\n\nfigure 6.1\na schematic representation of the policy parametrization. the vector \u03d1 associates the bfs to\ndiscrete actions. in this example, the bfs are parameterized by their centers ci and widths bi,\nso that \u03be = [ct\nn ]t. reproduced with permission from (bus\u00b8oniu et al., 2009),\nc(cid:13) 2009 ieee.\n\n1 , . . . , ct\n\nn , bt\n\n1 , bt\n\n "}, {"Page_number": 219, "text": "210\n\nchapter 6. cross-entropy policy search\n\nthe complete policy parameter vector is thus [\u03bet,\u03d1t]t, ranging in the set \u03be\u00d7\n\n. for any x, the policy chooses the action associated to a bf that takes\n\n{1, . . . , m}n\n\nthe largest value at x:\n\nh(x;\u03be,\u03d1) = u\u03d1\n\ni\u2021 , where i\u2021 \u2208 arg max\n\ni\n\n\u03d5i(x;\u03be)\n\n(6.6)\n\nscore function\n\nthe goal of ce policy search is to find optimal parameters that maximize the\nweighted average of the returns obtained from a finite set x0 of representative ini-\ntial states. the return from every representative state is estimated using monte carlo\nsimulations. this goal was already discussed in our review of gradient-free policy\nsearch given in section 3.7.2, and this explanation is recapitulated here, specialized\nto ce policy search.\n\nthe score function (optimization criterion) can be written (3.63):\n\ns(\u03be,\u03d1) = \u2211\nx0\u2208x0\n\nw(x0)brh(\u00b7;\u03be,\u03d1)(x0)\n\n(6.7)\n\nwhere x0 is the set of representative states, weighted by w : x0 \u2192 (0, 1].3 the monte\ncarlo estimate of the return for each state x0 \u2208 x0 is (3.64):\n\n1\n\nnmc\n\nnmc\n\u2211\ni0=1\n\nk\n\n\u2211\nk=0\n\n\u03b3k \u02dc\u03c1(xi0,k, h(xi0,k;\u03be,\u03d1), xi0,k+1)\n\nbrh(\u00b7;\u03be,\u03d1)(x0) =\n\nwhere xi0,0 = x0, xi0,k+1 \u223c \u02dcf (xi0,k, h(xi0,k;\u03be,\u03d1),\u00b7), and nmc is the number of monte\ncarlo simulations to carry out. so, each simulation i0 makes use of a system trajec-\ntory that is k steps long and generated using the policy h(\u00b7;\u03be,\u03d1). the system tra-\njectories are generated independently, so the score computation is unbiased. given a\ndesired precision \u03b5mc > 0, the length k can be chosen using (3.65) to guarantee that\ntruncating the trajectory introduces an error of at most \u03b5mc in the estimate of the sum\nalong the original, infinitely long trajectory.\n\nthe set x0 of representative initial states, together with the weight function w,\ndetermines the performance of the resulting policy. some problems only require the\noptimal control of the system from a restricted set of initial states; x0 should then be\nequal to this set, or included in it when the set is too large. also, initial states that are\ndeemed more important can be assigned larger weights. when all the initial states\nare equally important, the elements of x0 should be uniformly spread over the state\nspace and identical weights equal to 1\nshould be assigned to every element of x0\n|x0|\n(recall that |\u00b7| denotes set cardinality). we study the influence of x0 in section 6.4.2\nfor a bicycle balancing problem.\n\n3more generally, a density \u02dcw over the initial states can be considered, and the score function is then\n\nex0\u223c \u02dcw(\u00b7)nrh(\u00b7;\u03be,\u03d1)(x0)o, i.e., the expected value of the return when x0 \u223c \u02dcw(\u00b7). such a score function can\n\nbe evaluated by monte carlo methods. in this chapter, we only use finite sets x0 associated with weighting\nfunctions w, as in (6.7).\n\n "}, {"Page_number": 220, "text": "6.3. cross-entropy policy search\n\n211\n\na general algorithm for cross-entropy policy search\n\na global, gradient-free, mixed-integer optimization problem must be solved to find\noptimal parameters \u03be\u2217, \u03d1\u2217 that maximize the score function (6.7). several techniques\nare available to solve this type of problem; in this chapter, we select the ce method\nas an illustrative example of such a technique. in section 6.4.1, we compare ce\noptimization with the direct optimization algorithm (jones, 2009) in the context\nof policy search.\n\nin order to define the associated stochastic problem (6.2) for ce optimization, it is\nnecessary to choose a family of densities with support \u03be\u00d7{1, . . . , m}n\n. in general, \u03be\nmay not be a discrete set, so it is convenient to use separate densities for the two parts\n\u03be and \u03d1 of the parameter vector. denote the density for \u03be by p\u03be(\u00b7; v\u03be), parameterized\nby v\u03be and with support \u03be, and the density for \u03d1 by p\u03d1(\u00b7; v\u03d1), parameterized by v\u03d1\nand with support {1, . . . , m}n\n. let nv\u03be denote the number of elements in the vector\nv\u03be, and nv\u03d1 the number of elements in v\u03d1. note that densities from which it is easy\nto sample (see press et al., 1986, chapter 7) are usually chosen; e.g., we will later\nuse gaussian densities for continuous variables and bernoulli densities for binary\nvariables.\n\nalgorithm 6.1\ncross-entropy policy search. reproduced with permission from (bus\u00b8oniu et al., 2009), c(cid:13)\n2009 ieee.\ninput: dynamics \u02dcf , reward function \u02dc\u03c1, discount factor \u03b3,\n\nrepresentative states x0, weight function w,\n\ndensity families(cid:8)p\u03be(\u00b7; v\u03be)(cid:9) ,{p\u03d1(\u00b7; v\u03d1)}, density parameter numbers nv\u03be , nv\u03d1 ,\n\nother parameters n , \u03c1ce, cce, \u03b1ce, dce, \u03b5ce, \u03b5mc, nmc, \u03c4max\n\n1: initialize density parameters v\u03be,0, v\u03d1,0\n2: nce \u2190 cce(nv\u03be + nv\u03d1)\n3: \u03c4 \u2190 0\n4: repeat\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\n14:\n\n\u03c4 \u2190 \u03c4+ 1\ngenerate samples \u03be1, . . . ,\u03bence from p\u03be(\u00b7; v\u03be,\u03c4\u22121)\ngenerate samples \u03d11, . . . ,\u03d1nce from p\u03d1(\u00b7; v\u03d1,\u03c4\u22121)\ncompute s(\u03beis,\u03d1is) with (6.7), is = 1, . . . , nce\nreorder and reindex s.t. s1 \u2264 \u00b7\u00b7\u00b7 \u2264 snce\n\u03bb\u03c4 \u2190 s\u2308(1\u2212\u03c1ce)nce\u2309\n\u03be,\u03c4 \u2208 arg maxv\u03be \u2211nce\n\u03d1,\u03c4 \u2208 arg maxv\u03d1 \u2211nce\n\nbv\u03be,\u03c4 \u2190 v\u2021\nbv\u03d1,\u03c4 \u2190 v\u2021\nv\u03be,\u03c4 \u2190 \u03b1cebv\u03be,\u03c4 + (1\u2212 \u03b1ce)v\u03be,\u03c4\u22121\nv\u03d1,\u03c4 \u2190 \u03b1cebv\u03d1,\u03c4 + (1\u2212 \u03b1ce)v\u03d1,\u03c4\u22121\n\n\u03be,\u03c4, where v\u2021\n\u03d1,\u03c4, where v\u2021\n\nis=\u2308(1\u2212\u03c1ce)nce\u2309\nis=\u2308(1\u2212\u03c1ce)nce\u2309\n\noutput: b\u03be\u2217,b\u03d1\u2217, the best sample; andbs\u2217 = s(b\u03be\u2217,b\u03d1\u2217)\n\nln p\u03be(\u03beis ; v\u03be)\nln p\u03d1(\u03d1is ; v\u03d1)\n\n15: until (\u03c4 > dce and |\u03bb\u03c4\u2212\u03c4\u2032 \u2212 \u03bb\u03c4\u2212\u03c4\u2032\u22121| \u2264 \u03b5ce, for \u03c4\u2032 = 0, . . . , dce \u2212 1) or \u03c4 = \u03c4max\n\n "}, {"Page_number": 221, "text": "212\n\nchapter 6. cross-entropy policy search\n\nthe ce method for policy search is summarized in algorithm 6.1. for easy refer-\nence, table 6.1 collects the meaning of the parameters and variables playing a role in\nthis algorithm. the stochastic counterparts at lines 11 and 12 of algorithm 6.1 were\nsimplified, using the fact that the samples are already sorted in the ascending order\nof their scores. the algorithm terminates after the variation of \u03bb is at most \u03b5ce for\ndce consecutive iterations, or when a maximum number \u03c4max of iterations has been\nreached. when \u03b5ce = 0, the algorithm is stopped only if \u03bb does not change at all\nfor dce consecutive iterations. the integer dce > 1 ensures that the decrease of the\nperformance variation below \u03b5ce is not accidental (e.g., due to random effects).\n\ntable 6.1\nparameters and variables of cross-entropy policy search. reproduced with permis-\nsion from (bus\u00b8oniu et al., 2009), c(cid:13) 2009 ieee.\n\nsymbol meaning\n\nn ; m\n\u03be; \u03d1\nv\u03be; v\u03d1\nnce\n\u03c1ce\n\u03bb\ncce\n\n\u03b1ce\nnmc\n\u03b5mc\n\u03b5ce\ndce\n\n\u03c4; \u03c4max\n\nnumber of bfs; number of discrete actions\nbf parameters; assignment of discrete actions to bfs\nparameters of the density for \u03be; and for \u03d1\nnumber of samples used at every ce iteration\nproportion of samples used in the ce updates\n(1\u2212 \u03c1ce) quantile of the sample performance\nhow many times the number of samples nce is larger than the\nnumber of density parameters\nsmoothing parameter\nnumber of monte carlo simulations for each state\nprecision in estimating the returns\nconvergence threshold\nhow many iterations the variation of \u03bb should be at most \u03b5ce\nto stop the algorithm\niteration index; maximum number of iterations\n\noften it is convenient to use densities with unbounded support (e.g., gaus-\nsians) when the bf parameters are continuous. however, the set \u03be must typically\nbe bounded, e.g., when \u03be contains centers of rbfs, which must remain inside a\nbounded state space. whenever this situation arises, samples can be generated from\nthe density with a larger (unbounded) support, and those samples that do not belong\nto \u03be can be rejected. the procedure continues until nce valid samples are generated,\nand the rest of the algorithm remains unchanged. the situation is entirely similar for\nthe discrete action assignments \u03d1, when it is convenient to use a family of densi-\nties p\u03d1(\u00b7; v\u03d1) with a support larger than {1, . . . , m}n\n. the theoretical basis of ce\noptimization remains valid when sample rejection is employed, since an equivalent\nalgorithm that uses all the samples can always be given by making the following two\nmodifications:\n\n "}, {"Page_number": 222, "text": "6.3. cross-entropy policy search\n\n213\n\n\u2022 the score function is extended to assign very large negative scores (larger in\nmagnitude than for any valid sample) to samples falling outside the domain.\n\n\u2022 at each iteration, the parameters nce and \u03c1ce are adapted so that a constant\nnumber of valid samples is generated, and a constant number of best samples\nis used for the parameter updates.\n\nthe most important parameters in ce policy search are, like in the general ce\noptimization, the number of samples, nce, and the proportion of best samples used\nto update the density, \u03c1ce. the parameter cce is taken greater than or equal to 2, so\nthat the number of samples is a multiple of the number of density parameters. the\nparameter \u03c1ce can be taken around 0.01 for large numbers of samples, or it can be\nlarger, around (ln nce)/nce, if the number of samples is smaller (nce < 100) (ru-\nbinstein and kroese, 2004). the number n of bfs determines the representation\npower of the policy approximator, and a good value for n depends on the problem\nat hand. in section 6.4, we study the effect of varying n in two example problems.\nfor deterministic mdps, it suffices to simulate a single trajectory for every initial\nstate in x0, so nmc = 1, whereas in the stochastic case, several trajectories should be\nsimulated, i.e., nmc > 1, with a good value of nmc depending on the problem. the\nparameter \u03b5mc > 0 should be chosen smaller than the difference between the return\nalong good trajectories and the return along undesirable trajectories, so that the op-\ntimization algorithm can effectively distinguish between these types of trajectories.\nthis choice can be difficult to make and may require some trial and error. as a de-\nfault initial value, \u03b5mc can be taken to be several orders of magnitude smaller than\nthe bound k\u03c1k\u221e\n1\u2212\u03b3 on the absolute value of the returns. since it does not make sense to\nimpose a convergence threshold smaller than the precision of the score function, \u03b5ce\nshould be chosen larger than or equal to \u03b5mc, and a good default value is \u03b5ce = \u03b5mc.\n\n6.3.2 cross-entropy policy search with radial basis functions\n\nin this section, we describe a version of ce policy search that uses state-dependent,\naxis-aligned gaussian rbfs to represent the policy. gaussian rbfs are chosen be-\ncause they are commonly used to represent approximate mdp solutions, see, e.g.,\nchapter 5 of this book and (tsitsiklis and van roy, 1996; ormoneit and sen, 2002;\nlagoudakis and parr, 2003a; menache et al., 2005). many other types of bfs could\nbe used instead, including, e.g., splines and polynomials.\n\nwe assume that the state space is a d-dimensional hyperbox centered in the ori-\n\ngin: x =(cid:8)x \u2208 rd(cid:12)(cid:12) |x| \u2264 xmax(cid:9), where xmax \u2208 (0, \u221e)d. in this formula, as well as in\n\nthe sequel, mathematical operations and conditions on vectors, such as the absolute\nvalue and relational operators, are applied element-wise. the hyperbox assumption\nis made here for simplicity and can be relaxed. for example, a simple relaxation is\nto allow hyperbox state spaces that are not centered in the origin, as will be done for\nthe hiv treatment control problem of section 6.4.3.\n\n "}, {"Page_number": 223, "text": "214\n\nchapter 6. cross-entropy policy search\n\nradial basis functions and their probability density\n\nthe gaussian rbfs are defined by:4\n\n\u03d5i(x;\u03be) = exp\"\u2212\n\nd\n\n\u2211\nd=1\n\n(xd \u2212 ci,d)2\n\nb2\ni,d\n\n#\n\n(6.8)\n\n1 , . . . , ct\n\n1 , . . . , bt\n\nn ]t and the vector of widths by b = [bt\n\nwhere d is the number of state variables, ci = [ci,1, . . . , ci,d]t is the d-dimensional\ncenter of the i-th rbf, and bi = [bi,1, . . . , bi,d]t is its width. denote the vector of cen-\nters by c = [ct\nn ]t. so, ci,d and\nbi,d are scalars, ci and bi are d-dimensional vectors that collect the scalars for all d\ndimensions, and c and b are dn -dimensional vectors that collect the d-dimensional\nvectors for all n rbfs. the rbf parameter vector is \u03be = [ct, bt]t and takes values\nin \u03be = x n \u00d7 (0, \u221e)dn , since the centers of the rbfs must lie within the bounded\nstate space, c \u2208 x n , and their widths must be strictly positive, b \u2208 (0, \u221e)dn .\n\nto define the associated stochastic problem (6.2) for optimizing the rbf param-\neters, independent gaussian densities are selected for each element of the parameter\nvector \u03be. note that this concatenation of densities can converge to a degenerate dis-\ntribution that always generates samples equal to a single value, such as a precise\noptimum location. the density for each center ci,d is parameterized by its mean \u03b7c\ni,d\nand its standard deviation \u03c3c\ni,d, while the density for a width bi,d is likewise param-\neterized by \u03b7b\ni,d. similarly to the centers and widths themselves, we denote\nthe dn -dimensional vectors of means and standard deviations by, respectively, \u03b7c,\n\u03c3c for the centers, and by \u03b7b, \u03c3b for the widths. the parameter of the density for the\nrbf parameters gathers all these vectors together:\n\ni,d and \u03c3b\n\nv\u03be = [(\u03b7c)t, (\u03c3c)t, (\u03b7b)t, (\u03c3b)t]t \u2208 r4dn\n\nnote that the support of the density for the rbf parameters is r2dn , which is larger\nthan the domain of the parameters \u03be = x n \u00d7 (0, \u221e)dn , and therefore samples that\ndo not belong to \u03be must be rejected and generated again.\n\nthe means and the standard deviations are initialized for all i as follows:\n\n\u03b7c\ni = 0, \u03c3c\n\ni = xmax, \u03b7b\n\ni =\n\nxmax\n\n2(n + 1)\n\n, \u03c3b\n\ni = \u03b7b\ni\n\nwhere \u201c0\u201d denotes a vector of d zeros. the initial density parameters for the rbf\ncenters ensure a good coverage of the state space, while the parameters for the rbf\nwidths are initialized heuristically to yield a similar overlap between rbfs for dif-\nferent values of n . the gaussian density belongs to the natural exponential family,\n\nso the solutionbv\u03be,\u03c4 of the stochastic counterpart at line 11 of algorithm 6.1 can be\n\ncomputed explicitly, as the element-wise mean and standard deviation of the best\nsamples (see also section 6.2). for instance, assuming without loss of generality that\n\n4note that the rbf width parameters in this definition are different from those used in the rbf formula\n(3.6) of chapter 3. this new variant makes it easier to formalize the optimization algorithm, but is of\ncourse entirely equivalent to the original description of axis-aligned rbfs.\n\n "}, {"Page_number": 224, "text": "6.3. cross-entropy policy search\n\n215\n\nthe samples are ordered in the ascending order of their scores, the density parameters\nfor the rbf centers are updated as follows:\n\n1\n\nnce \u2212 i\u03c4 + 1\n\nnce\n\u2211\nis=i\u03c4\n\ncis ,\n\n\u03c4 =\n\nb\u03b7c\n\n\u03c4 =vuut\nb\u03c3c\n\n1\n\nnce \u2212 i\u03c4 + 1\n\nnce\n\u2211\nis=i\u03c4\n\n\u03c4)2\n\n(cis \u2212b\u03b7c\n\nwhere i\u03c4 = \u2308(1\u2212 \u03c1ce)nce\u2309 denotes the index of the first of the best samples. recall\nalso that \u03b7c\n\u03c4, and cis are all dn -dimensional vectors, and that mathematical\noperations are performed element-wise.\n\n\u03c4, \u03c3c\n\ndiscrete action assignments and their probability density\n\nthe vector \u03d1 containing the assignments of discrete actions to bfs is represented\nin binary code, using n bin = \u2308log2 m\u2309 bits for each element \u03d1i. thus, the complete\nbinary representation of \u03d1 has n n bin bits. a binary representation is convenient\nbecause it allows us to work with bernoulli distributions, as described next.\n\nto define the associated stochastic problem (6.2) for optimizing \u03d1, every bit is\ndrawn from a bernoulli distribution parameterized by its mean \u03b7bin \u2208 [0, 1] (\u03b7bin\ngives the probability of selecting 1, while the probability of selecting 0 is 1\u2212 \u03b7bin).\nbecause every bit has its own bernoulli parameter, the total number of bernoulli pa-\nrameters v\u03d1 is n n bin. similarly to the gaussian densities above, this combination\nof independent bernoulli distributions can converge to a degenerate distribution con-\ncentrated on a single value, such as an optimum. note that if m is not a power of 2,\nbit combinations corresponding to invalid indices are rejected and generated again.\nfor instance, if m = 3, n bin = 2 is obtained, the binary value 00 points to the first\ndiscrete action u1 (since the binary representation is zero-based), 01 points to u2, 10\nto u3, and 11 is invalid and will be rejected.\n\nthe mean \u03b7bin for every bit is initialized to 0.5, which means that the bits 0 and\n1 are initially equiprobable. since the bernoulli distribution belongs to the natural\n\nexponential family, the solutionbv\u03d1,\u03c4 of the stochastic counterpart at line 12 of algo-\n\nrithm 6.1 can be computed explicitly, as the element-wise mean of the best samples\nin their binary representation.\n\ncomputational complexity\n\nwe now briefly examine the complexity of this version of ce policy search. the\nnumber of density parameters is nv\u03be = 4dn for the rbf centers and widths, and\nnv\u03d1 = n n bin for the action assignments. therefore, the total number of samples\nused is nce = cce(4dn + n n bin). most of the computational load is generated by\nthe simulations required to estimate the score of each sample. therefore, neglecting\nthe other computations, the complexity of one ce iteration is:\n\ntstep[ccen (4d + n bin)\u00b7|x0|\u00b7 nmck]\n\n(6.9)\n\nwhere k is the maximum length of each trajectory, and tstep is the time needed to\ncompute the policy for a given state and to simulate the controlled system for one\n\n "}, {"Page_number": 225, "text": "216\n\nchapter 6. cross-entropy policy search\n\ntime step. of course, if some trajectories terminate in fewer than k steps, the cost is\nreduced.\n\nthe complexity (6.9) is linear in the number |x0| of representative states, which\nsuggests one way to control the complexity of ce policy search: by limiting the\nnumber of initial states to the minimum necessary. the complexity is linear also\nin the number of state variables d. however, this does not necessarily mean that\nthe computational cost of ce policy search grows (only) linearly with the problem\ndimension, since the cost is influenced by the problem also in other ways, such as\nthrough the number n of rbfs required to represent a good policy.\n\nin the upcoming examples, we will use the name \u201cce policy search\u201d to refer to\n\nthis rbf-based version of the general procedure given in algorithm 6.1.\n\n6.4 experimental study\n\nin the sequel, to assess the performance of ce policy search, extensive numerical\nexperiments are carried out in three problems with a gradually increasing dimen-\nsionality: the optimal control of a double integrator (two dimensions), the balancing\nof a bicycle that rides at a constant speed (four dimensions), and the control of the\ntreatment of an hiv infection (six dimensions).\n\n6.4.1 discrete-time double integrator\n\nin this section, a double integrator optimal control problem is used to evaluate the\neffectiveness of ce policy search when looking for a policy that performs well over\nthe entire state space. in this setting, ce policy search is compared with fuzzy q-\niteration, with least-squares policy iteration, and with a policy search variant that\nemploys a different optimization algorithm called direct. the double integrator\nproblem is stated such that (near-)optimal trajectories from any state terminate in a\nsmall number of steps, which allows extensive simulation experiments to be run and\nan optimal solution to be found without excessive computational costs.\n\ndouble integrator problem\nthe double integrator is deterministic, has a continuous state space x = [\u22121, 1] \u00d7\n[\u22120.5, 0.5], a discrete action space ud = {\u22120.1, 0.1}, and the dynamics:\n\nxk+1 = f (xk, uk) = sat(cid:8)[x1,k + x2,k, x2,k + uk]t,\u2212xmax, xmax(cid:9)\n\nwhere xmax = [1, 0.5]t and the saturation is employed to bound the state variables to\ntheir domain x . every state for which |x1| = 1 is terminal, regardless of the value\nof x2. (recall that applying any action in a terminal state brings the process back\nto the same state, with a zero reward.) the goal is to drive the position x1 to either\nboundary of the interval [\u22121, 1], i.e., to a terminal state, so that when x1 reaches the\n\n(6.10)\n\n "}, {"Page_number": 226, "text": "6.4. experimental study\n\n217\n\nboundary, the speed x2 is as small as possible in magnitude. this goal is expressed\nby the reward function:\n\n2,k+1x2\n\n1,k+1\n\n(6.11)\n\nrk+1 = \u03c1(xk, uk) = \u2212(1\u2212(cid:12)(cid:12)x1,k+1(cid:12)(cid:12))2 \u2212 x2\n\nthe product \u2212x2\n1, i.e., to a terminal state. the discount factor \u03b3 is set to 0.95.\n\n1,k+1 penalizes large values of x2, but only when x1 is close to\n\n2,k+1x2\n\nfigure 6.2 presents an optimal solution for this problem. in particular, fig-\nure 6.2(a) shows an accurate representation of an optimal policy, consisting of the\noptimal actions for a regular grid of 101\u00d7 101 points covering the state space. these\noptimal actions were obtained using the following brute-force procedure. all the\npossible sequences of actions of a sufficient length were generated, and the system\nwas controlled with all these sequences starting from every state on the grid. for ev-\nery such state, a sequence that produced the best discounted return is by definition\noptimal, and the first action in this sequence is an optimal action. note that this brute-\nforce procedure could only be employed because the problem has terminal states, and\nall the optimal trajectories terminate in a small number of steps. for example, fig-\nure 6.2(b) shows an optimal trajectory from the initial state x0 = [0, 0]t, found by\napplying an optimal sequence of actions. a terminal state is reached after 8 steps,\nwith a zero final velocity.\n\n1\n\n1\n\nx\n\n0\n\n\u22121\n\n0\n\n0.5\n\n2\n\nx\n\n0\n\n\u22120.5\n\n0.1\n\n0\n\nu\n\n0\n\n\u22120.1\n\n0\n\n0\n\nr\n\n\u22120.5\n\n\u22121\n\n0\n\n0.1\n\n\u22120.1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n1\n\n2\n\n2\n\n3\n\n3\n\n4\n\n4\nk\n\n5\n\n5\n\n6\n\n6\n\n7\n\n7\n\n8\n\n8\n\nh*(x\n,x\n)\n2\n1\n\n0.5\n\n \n\n2\n\nx\n\n0\n\n\u22120.5\n\n \n\u22121\n\n\u22120.5\n\n0.5\n\n1\n\n0\nx\n\n1\n\n(a) optimal policy.\n\n(b) optimal trajectory from x0 = [0, 0]t.\n\nfigure 6.2\nan optimal solution for the double integrator, found by brute-force search.\n\nresults of ce policy search\n\nto apply ce policy search, we select representative states that are distributed across\nthe entire state space and equally weighted; the algorithm is thus required to achieve\na good performance over the entire state space. the set of representative states is:\n\nx0 = {\u22121,\u22120.9, . . . , 1}\u00d7{\u22120.5,\u22120.3,\u22120.1, 0, 0.1, 0.3, 0.5}\n\n "}, {"Page_number": 227, "text": "218\n\nchapter 6. cross-entropy policy search\n\nand the weight function is w(x0) = 1/|x0| for any x0. this set contains 21\u00d7 7 = 147\nstates, fewer than the grid of figure 6.2(a). ce policy search is run while gradually\nincreasing the number n of bfs from 4 to 18. the parameters for the algorithm are\nset (with little or no tuning) as follows: cce = 10, \u03c1ce = 0.01, \u03b1ce = 0.7, \u03b5ce = \u03b5mc =\n0.001, dce = 5, and \u03c4max = 100. because the system is deterministic, it is sufficient to\nsimulate only one trajectory from every initial state, i.e., nmc = 1. for every value of\nn , 20 independent runs were performed, in which the algorithm always converged\nbefore reaching the maximum number of iterations.\n\nfigure 6.3(a) presents the performance of the policies obtained by ce policy\nsearch (mean values across the 20 runs, together with 95% confidence intervals on\nthis mean). for comparison, this figure also shows the exact optimal score for x0,\ncomputed by looking for optimal action sequences with the brute-force procedure\nexplained earlier. ce policy search reliably obtains near-optimal performance for\nn \u2265 10, and sometimes finds good solutions for n as low as 7. figure 6.3(b)\npresents the mean execution time of the algorithm, which is roughly affine in n , as\nexpected from (6.9).5 the 95% confidence intervals are too small to be visible at the\nscale of the figure, so they are omitted.\n\n\u22120.7\n\n\u22120.72\n\n\u22120.74\n\n\u22120.76\n\n\u22120.78\n\n\u22120.8\n\n\u22120.82\n\n\u22120.84\n\n\u22120.86\n\ne\nr\no\nc\ns\n\n \n\n4\n10\n\n \n\n]\ns\n[\n \ne\nm\n\ni\nt\n \nn\no\ni\nt\nu\nc\ne\nx\ne\n\n2\n10\n\n0\n10\n\n\u22122\n\n10\n\n \n4\n\nmean score\n95% confidence bounds\noptimal score\n\n \n4\n\n5\n\n6\n\n7\n\n8\n\n9 10 11 12 13 14 15 16 17 18\nnumber of rbfs\n\n5\n\n6\n\n7\n\n8\n\n9 10 11 12 13 14 15 16 17 18\nnumber of rbfs\n\n(a) performance.\n\n(b) mean execution time.\n\nfigure 6.3 results of ce policy search for the double integrator.\n\nfigure 6.4 shows a representative solution found by ce policy search, for n =\n10 rbfs (compare with figure 6.2). the policy found resembles the optimal policy,\nbut the edges where the actions change are more curved due to their dependence\non the rbfs. the trajectory starting in x0 = [0, 0]t and controlled by this policy is\noptimal. more specifically, the state and action trajectories are the negatives of those\nin figure 6.2(b), and remain optimal despite this \u201creflection\u201d about the horizontal\naxis, because the dynamics and the reward function of the double integrator are also\nsymmetric with respect to the origin.\n\ncomparison with value iteration and policy iteration\n\nin this section, ce policy search is compared with representative algorithms for ap-\nproximate value iteration and policy iteration. from the approximate value iteration\n\n5all the computation times reported in this chapter were recorded while running the algorithms in\n\nmatlab r(cid:13) 7 on a pc with an intel core 2 duo e6550 2.33 ghz cpu and with 3 gb ram.\n\n "}, {"Page_number": 228, "text": "6.4. experimental study\n\n219\n\n1\n\n1\n\nx\n\n0\n\n\u22121\n\n0\n\n0.5\n\n2\n\nx\n\n0\n\n\u22120.5\n\n0.1\n\n0\n\nu\n\n0\n\n\u22120.1\n\n0\n\n0\n\nr\n\n\u22120.5\n\n\u22121\n\n0\n\n0.1\n\n\u22120.1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n1\n\n2\n\n2\n\n3\n\n3\n\n4\n\n4\nk\n\n5\n\n5\n\n6\n\n6\n\n7\n\n7\n\n8\n\n8\n\n(b) controlled trajectory from x0 = [0, 0]t.\n\n0.5\n\n \n\nh(x\n,x\n)\n2\n1\n\n2\n\nx\n\n0\n\n\u22120.5\n\n \n\u22121\n\n\u22120.5\n\n0.5\n\n1\n\n0\nx\n\n1\n\n(a) policy.\n\nfigure 6.4\na representative solution found by ce policy search for the double integrator.\n\nclass, fuzzy q-iteration is selected, which was discussed at length in chapter 4, and\nfrom the policy iteration class, least-squares policy iteration (lspi) is chosen, which\nwas introduced in section 3.5.5 and discussed in detail in chapter 5.\n\nrecall that fuzzy q-iteration relies on a linearly parameterized q-function\napproximator with n state-dependent, normalized membership functions (mfs)\n\u03c61, . . . ,\u03c6n : x \u2192 r, and with a discrete set of actions ud = {u1, . . . , um}. approx-\nimate q-values are computed with:\n\nn\n\n\u2211\ni=1\n\n\u03c6i(x)\u03b8[i, j]\n\n(6.12)\n\nbq(x, u j) =\n\nwhere \u03b8 \u2208 rnm is a vector of parameters, and [i, j] = i + ( j\u2212 1)n denotes the scalar\n\nindex corresponding to i and j. fuzzy q-iteration computes an approximately op-\ntimal q-function of the form (6.12), and then outputs a greedy policy in this q-\nfunction. the q-function and policy obtained have a bounded suboptimality, as de-\nscribed in theorem 4.5 of chapter 4. for the double integrator, the action space is\nalready discrete (ud = u = {\u22120.1, 0.1} and m = 2), so action discretization is not\nnecessary. triangular mfs are defined (see example 4.1), distributed on an equidis-\ntant grid with n\u2032 points along each dimension of the state space; this leads to a total\nof n = n\u20322 state-dependent mfs, corresponding to a total of 2n\u20322 parameters. such a\nregular placement of mfs provides a uniform resolution over the state space, which is\nthe best option given that prior knowledge about the optimal q-function is not avail-\nable. in these experiments, a fuzzy q-iteration run is considered convergent when\nthe (infinity-norm) difference between consecutive parameter vectors decreases be-\nlow \u03b5qi = 10\u22125.\n\na similar q-function approximator is chosen for lspi, combining state-\n\n "}, {"Page_number": 229, "text": "220\n\nchapter 6. cross-entropy policy search\n\ndependent normalized gaussian rbfs with the 2 discrete actions. the rbfs are\naxis-aligned, identically shaped, and their centers are placed on an equidistant grid\nwith n\u2032 points along each dimension of x . the widths bi,d of the rbfs along each\ndimension d were taken identical to the grid spacing along that dimension (using the\nrbf formula (6.8) from this chapter). this leads to a total of n\u20322 rbfs and 2n\u20322 pa-\nrameters. at every iteration, lspi approximates the q-function of the current policy\nusing a batch of transition samples, and then computes an improved, greedy policy in\nthis q-function. then, the q-function of the improved policy is estimated, and so on.\nthe sequence of policies produced in this way eventually converges to a subsequence\nalong which all of the policies have a bounded suboptimality; however, it may not\nconverge to a fixed policy. lspi is considered convergent when the (two-norm) dif-\nference of consecutive parameter vectors decreases below \u03b5lspi = 10\u22123, or when a\nlimit cycle is detected in the sequence of parameters.\n\nthe number n\u2032 of mfs (in fuzzy q-iteration) or of bfs (in lspi) for each state\nvariable is gradually increased from 4 to 18. fuzzy q-iteration is a deterministic\nalgorithm, so running it only once for every n\u2032 is sufficient. lspi requires a set of\nrandom samples, so each lspi experiment is run 20 times with independent sets of\nsamples. for n\u2032 = 4, 1000 samples are used, and for larger n\u2032 the number of samples\nis increased proportionally with the number 2n\u20322 of parameters, which means that\n1000 n\u20322/42 samples are used for each n\u2032. some side experiments confirmed that\nthis number of samples is sufficient, and that increasing it does not lead to a better\nperformance. figure 6.5(b) shows the score of the policies computed by fuzzy q-\niteration and lspi, measured by the average return across the set x0 of representative\nstates, as in ce policy search (compare with figure 6.3(a)). the execution time of\nthe algorithms is given in figure 6.5(b) (compare with figure 6.3(b)). some lspi\n\nruns for n\u2032 \u2264 8 did not converge within 100 iterations, and are therefore not taken\n\ninto account in figure 6.5.\n\n \n\n4\n10\n\n \n\ne\nr\no\nc\ns\n\n\u22120.7\n\n\u22120.72\n\n\u22120.74\n\n\u22120.76\n\n\u22120.78\n\n\u22120.8\n\n\u22120.82\n\n\u22120.84\n\n\u22120.86\n\n \n4\n\nfuzzy qi score\nlspi mean score\n95% confidence bounds\noptimal score\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n18\n\nnumber of mfs/bfs on each axis\n\n]\ns\n[\n \ne\nm\n\ni\nt\n \nn\no\ni\nt\nu\nc\ne\nx\ne\n\n2\n10\n\n0\n10\n\n\u22122\n\n10\n\n \n4\n\n(a) performance (values below \u22120.875 are not\nshown).\n\nfuzzy qi execution time\nlspi mean execution time\n95% confidence bounds\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n18\n\nnumber of mfs/bfs on each axis\n\n(b) execution time.\n\nfigure 6.5 results of fuzzy q-iteration and lspi for the double integrator.\n\nwhereas ce policy search reliably obtained near-optimal performance starting\nfrom n = 10 bfs in total, fuzzy q-iteration and lspi obtain good performance\nstarting from around n\u2032 = 10 bfs for each state variable; the total number of mfs\nor bfs is n\u20322, significantly larger. furthermore, ce policy search provides a steady\n\n "}, {"Page_number": 230, "text": "6.4. experimental study\n\n221\n\nperformance for larger values of n , whereas fuzzy q-iteration and lspi often lead\nto decreases in performance as the number of mfs or bfs increases. these differ-\nence are mainly due to the fact that the mfs of fuzzy q-iteration and the bfs of\nlspi are equidistant and identically shaped, whereas the ce algorithm optimizes\nparameters encoding the locations and shapes of the bfs. on the other hand, the\ncomputational cost of the value function based algorithms is smaller than the cost of\nce policy search (for fuzzy q-iteration, by several orders of magnitude). this indi-\ncates that when the performance over the entire state space must be optimized, value\nor policy iteration may be computationally preferable to ce policy search, at least\nin low-dimensional problems such as the double integrator. in such problems, ce\npolicy search can be beneficial when the performance must be optimized from only a\nsmaller number of initial states, or when a policy approximator of a fixed complexity\nis desired and the computational costs to optimize it are not a concern.\n\nfigure 6.6 shows representative solutions found by fuzzy q-iteration and lspi\nfor n\u2032 = 10. while the policies resemble the policy found by the ce algorithm (fig-\n\n0.5\n\n \n\n0.5\n\n \n\nh(x\n,x\n)\n2\n1\n\nh(x\n,x\n)\n2\n1\n\n2\n\nx\n\n0\n\n0.1\n\n\u22120.1\n\n2\n\nx\n\n0\n\n0.1\n\n\u22120.1\n\n\u22120.5\n\n \n\u22121\n\n\u22120.5\n\n0.5\n\n1\n\n0\nx\n\n1\n\n\u22120.5\n\n \n\u22121\n\n\u22120.5\n\n0.5\n\n1\n\n0\nx\n1\n\n(a) fuzzy q-iteration, policy.\n\n(b) lspi, policy.\n\n1\n\n1\n\nx\n\n0\n\n\u22121\n\n0\n\n0.5\n\n2\n\nx\n\n0\n\n\u22120.5\n\n0.1\n\n0\n\nu\n\n0\n\n\u22120.1\n\n0\n\n0\n\nr\n\n\u22120.5\n\n\u22121\n\n0\n\n1\n\n1\n\nx\n\n0\n\n\u22121\n\n0\n\n0.5\n\n2\n\nx\n\n0\n\n\u22120.5\n\n0.1\n\n0\n\nu\n\n0\n\n\u22120.1\n\n0\n\n0\n\nr\n\n\u22120.5\n\n\u22121\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n1\n\n2\n\n2\n\n3\n\n3\n\n4\n\n4\nk\n\n5\n\n5\n\n6\n\n6\n\n7\n\n7\n\n8\n\n8\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n1\n\n1\n\n2\n\n2\n\n3\n\n3\n\n4\n\n4\nk\n\n5\n\n5\n\n6\n\n6\n\n7\n\n7\n\n8\n\n8\n\n(c) fuzzy q-iteration, controlled trajectory from\n[0, 0]t.\n\n(d) lspi, controlled trajectory from [0, 0]t.\n\nfigure 6.6\nrepresentative solutions found by fuzzy q-iteration (left) and lspi (right) for the double\nintegrator.\n\n "}, {"Page_number": 231, "text": "222\n\nchapter 6. cross-entropy policy search\n\nure 6.4(a)) and the optimal policy (figure 6.2(a)), the trajectories from [0, 0]t are\nsuboptimal. for instance, the trajectory obtained by fuzzy q-iteration (figure 6.6(c))\nreaches the terminal state with a nonzero final velocity after 7 steps. the subopti-\nmality of this trajectory is reflected in the value of its return, which is \u22122.45. in\ncontrast, the optimal trajectories of figures 6.2(b) and 6.4(b) reached a zero final\nvelocity, accumulating a better return of \u22122.43. the lspi trajectory of figure 6.6(d)\nterminates after 6 steps, with an even larger final velocity, leading to a suboptimal\nreturn of \u22122.48. the reason for which the fuzzy q-iteration solution is better than\nthe lspi solution is not certain; possibly, the triangular mf approximator used by\nfuzzy q-iteration is more appropriate for this problem than the rbf approximator of\nlspi.\n\ncomparison of ce and direct optimization\n\nin our policy search approach (section 6.3), a global, mixed-integer, gradient-free\noptimization problem must be solved. one algorithm that can address this difficult\noptimization problem is direct (jones, 2009), and therefore, in this section, this al-\ngorithm is compared with ce optimization in the context of policy search.6 direct\nworks in hyperbox parameter spaces, by recursively splitting promising hyperboxes\nin three and sampling the center of each resulting hyperbox. the hyperbox selec-\ntion procedure leads both to a global exploration of the parameter space, and to a\nlocal search in the most promising regions discovered so far. the algorithm is espe-\ncially suitable for problems in which evaluating the score function is computationally\ncostly (jones, 2009), as is the case in policy search.\n\nnote that the original parameter space of the rbf policy representation is not a\n\nfinite hyperbox, because each rbf width can be arbitrarily large, bi \u2208 (0, \u221e)d. how-\n\never, in practice it is not useful to employ rbfs that are wider than the entire state\nspace, so for the purpose of applying direct, we restrict the width of the rbfs to\nbe at most the width of the state space, i.e., bi \u2264 2\u00b7 xmax, thereby obtaining a hyper-\nbox parameter space. another difference with ce policy search is that, for direct,\nthere is no reason to use a binary representation for the action assignments; instead,\nthey are represented directly as integer variables ranging in 1, . . . , m. therefore, the\nnumber of parameters to optimize is n + 2dn = 5n , consisting of 2dn rbf\nparameters and n integer action assignments.\n\nwe used direct to optimize the parameters of the policy (6.6) while gradually\nincreasing n from 4 to 18, as for ce optimization above. direct stops when the\nscore function (6.7) has been evaluated a given number of times; this stopping param-\neter was set to 2000\u00b7 5n for every n , i.e., 2000 times the number of parameters\nto optimize. since direct is a deterministic algorithm, each experiment was run\nonly once. the performance of the policies computed by direct, together with the\nexecution time of the algorithm, are shown in figure 6.7. for an easy comparison,\nthe ce policy search results from figure 6.3 are repeated.\n\ndirect performs worse than ce optimization for most values of n , while re-\nquiring more computations for all values of n . increasing the allowed number of\n\n6we use the direct implementation from the tomlab r(cid:13) 7 optimization toolbox for matlab.\n\n "}, {"Page_number": 232, "text": "\u22120.7\n\n\u22120.72\n\n\u22120.74\n\n\u22120.76\n\n\u22120.78\n\n\u22120.8\n\n\u22120.82\n\n\u22120.84\n\n\u22120.86\n\ne\nr\no\nc\ns\n\n6.4. experimental study\n\n \n\n4\n10\n\n223\n\n \n\nce mean score\nce 95% confidence bounds\ndirect score\noptimal score\n\n9 10 11 12 13 14 15 16 17 18\nnumber of rbfs\n\n \n4\n\n5\n\n6\n\n7\n\n8\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n2\n10\n\n0\n10\n\n\u22122\n\n10\n\n \n4\n\nce mean execution time\ndirect execution time\n\n5\n\n6\n\n7\n\n8\n\n9 10 11 12 13 14 15 16 17 18\nnumber of rbfs\n\n(a) performance.\n\n(b) execution time.\n\nfigure 6.7\nresults of direct for the double integrator, compared with the results of ce policy search.\n\nscore evaluations may improve the performance of direct, but would also make it\nmore computationally expensive, and therefore less competitive with ce optimiza-\ntion. the poor results of direct may be due to its reliance on splitting the parameter\nspace into hyperboxes: this approach can perform poorly when the parameter space\nis high-dimensional, as is the case for our policy parametrization.\n\n6.4.2 bicycle balancing\n\nin this section, ce policy search is applied to a more involved problem than the\ndouble integrator, namely the balancing of a bicycle riding at a constant speed on a\nhorizontal surface. the steering column of the bicycle is vertical, which means that\nthe bicycle is not self-stabilizing, but must be actively stabilized to prevent it from\nfalling (a regular bicycle is self-stabilizing under certain conditions, see \u02daastr\u00a8om et al.,\n2005). this is a variant of a bicycle balancing and riding problem that is widely used\nas a benchmark for reinforcement learning algorithms (randl\u00f8v and alstr\u00f8m, 1998;\nlagoudakis and parr, 2003a; ernst et al., 2005). we use the bicycle balancing prob-\nlem to study how ce policy search is affected by changes in the set of representative\nstates and by noise in the transition dynamics.\n\nbicycle problem\n\nfigure 6.8 provides a schematic representation of the bicycle, which includes the\nstate and control variables. the state variables are the roll angle \u03c9 [rad] of the bi-\ncycle measured from the vertical axis, the angle \u03b1 [rad] of the handlebar, equal to\n0 when the handlebar is in its neutral position, and the respective angular veloci-\nties \u02d9\u03c9, \u02d9\u03b1 [rad/s]. the control variables are the displacement \u03b4 \u2208 [\u22120.02, 0.02] m of\nthe bicycle-rider common center of mass perpendicular to the plane of the bicycle,\nand the torque \u03c4 \u2208 [\u22122, 2] nm applied to the handlebar. the state vector is therefore\n[\u03c9, \u02d9\u03c9,\u03b1, \u02d9\u03b1]t, and the action vector is u = [\u03b4,\u03c4]t. the displacement \u03b4 can be affected\nby additive noise z drawn from a uniform density over the interval [\u22120.02, 0.02] m.\n\n "}, {"Page_number": 233, "text": "224\n\nchapter 6. cross-entropy policy search\n\n\u03c9\n\ncenter\nof mass\n\n\u03b4 z+\n\n\u03c9\n\n\u03b1\n\n\u03b1\n\n\u03c4\n\nfigure 6.8\na schematic representation of the bicycle, as seen from behind (left) and from the top (right).\n\nthe continuous-time dynamics of the bicycle are (ernst et al., 2005):\n\n\u00a8\u03c9 =\n\n\u00a8\u03b1 =\n\n1\n\njbc(cid:20) sin(\u03b2)(mc + mr)gh\u2212 cos(\u03b2)h jdcv\njdl(cid:18)\u03c4\u2212\n\nsign(\u03b1)v2(cid:16) mdr\n\n\u02d9\u03c9(cid:19)\n\njdvv\n\n1\n\nr\n\nr\n\nl\n\n(|sin(\u03b1)| +|tan(\u03b1)|) +\n\n\u02d9\u03b1+\n\n(mc + mr)h\n\nrcm\n\n(cid:17)i(cid:21)\n\n(6.13)\n\n(6.14)\n\nwhere:\n\njbc =\n\njdv =\n\n13\n\n3\n3\nmdr2\n\n2\n\nmch2 + mr(h + dcm)2\n\njdc = mdr2\n\n\u03b2 = \u03c9+ arctan\n\n\u03b4 + z\n\nh\n\n1\nrcm\n\njdl =\n\nmdr2\n\n1\n\n2\n\n=\uf8f1\uf8f2\uf8f3\nh(l \u2212 c)2 + l2\n\n0\n\nsin2(\u03b1)i\u22121/2\n\nif \u03b1 6= 0\notherwise\n\nnote that the noise enters the model in (6.13), via the term \u03b2. to obtain the\ndiscrete-time transition function, as in (ernst et al., 2005), the dynamics (6.13)\u2013\n(6.14) are numerically integrated using the euler method with a sampling time of\nts = 0.01 s (see, e.g., ascher and petzold, 1998, chapter 3). when the magnitude\nof the roll angle is larger than 12\u03c0\n180 , the bicycle is considered to have fallen, and a\nterminal, failure state is reached. additionally, using saturation, the steering angle \u03b1\nis restricted to [\u221280\u03c0\n180 ] to reflect the physical constraints on the handlebar, and the\nangular velocities \u02d9\u03c9, \u02d9\u03b1 are restricted to [\u22122\u03c0, 2\u03c0].\n\ntable 6.2 shows the meanings and values of the parameters in the bicycle model.\n\n180 , 80\u03c0\n\nin the balancing task, the bicycle must be prevented from falling, i.e., the roll\n180 ]. the following reward\n\nangle must be kept within the allowed interval [\u221212\u03c0\nfunction is chosen to express this goal:\n\n180 , 12\u03c0\n\nrk+1 =(0\n\nif \u03c9k+1 \u2208 [\u221212\u03c0\n\n180 , 12\u03c0\n180 ]\n\n\u22121 otherwise\n\n(6.15)\n\n "}, {"Page_number": 234, "text": "6.4. experimental study\n\n225\n\ntable 6.2 parameters of the bicycle.\n\nsymbol value units meaning\n\nmc\nmd\nmr\ng\nv\nh\n\nl\n\nr\ndcm\nc\n\nkg\n15\nkg\n1.7\nkg\n60\nm/s2\n9.81\n10/3.6 m/s\n0.94\n\nm\n\n1.11\n\n0.34\n0.3\n0.66\n\nm\n\nm\nm\nm\n\nmass of the bicycle\nmass of a tire\nmass of the rider\ngravitational acceleration\nvelocity of the bicycle\nheight from the ground of the common center of mass\n(com) of the bicycle and the rider\ndistance between the front and back tires at the points\nwhere they touch the ground\nwheel radius\nvertical distance between bicycle com and rider com\nhorizontal distance between the point where the front\nwheel touches the ground and the common com\n\nthus, the reward is generally 0, except in the event of reaching a failure state, which\nis signaled by a (negative) reward of \u22121. the discount factor is \u03b3 = 0.98.\nto apply ce policy search, the rider displacement action is discretized into\n{\u22120.02, 0, 0.02}, and the torque on the handlebar into {\u22122, 0, 2}, leading to a dis-\ncrete action space with 9 elements, which is sufficient to balance the bicycle (as will\nbe seen in the upcoming experiments).\n\nrepresentative states\n\nwe consider the behavior of the bicycle starting from different initial rolls and roll ve-\nlocities, and the initial steering angle \u03b10 and velocity \u02d9\u03b10 are always taken to be equal\nto zero. two different sets of representative states are employed, in order to study the\ninfluence of the representative states on the performance of ce policy search.\n\nthe first set of representative states contains a few evenly-spaced values of the\n\nroll, while the remaining state variables are zero:\n\nx0,1 =(cid:8)\u221210\u03c0\n\n180 , \u22125\u03c0\n\n180 , . . . , 10\u03c0\n\n180(cid:9)\u00d7{0}\u00d7{0}\u00d7{0}\n\nthe roll values considered cover the entire acceptable roll domain [\u221212\u03c0\n180 ] except\nvalues too close to the boundaries, from which failure is difficult to avoid. the second\nset is the cross-product of a finer roll grid and a few values of the roll velocity:\n\n180 , 12\u03c0\n\nx0,2 =(cid:8)\u221210\u03c0\n\n180(cid:9)\u00d7(cid:8)\u221230\u03c0\n\n180 , \u22128\u03c0\n\n180 , . . . , 10\u03c0\n\n180 , \u221215\u03c0\n\n180 , . . . , 30\u03c0\n\n180(cid:9)\u00d7{0}\u00d7{0}\n\nfor both sets, the representative states are uniformly weighted, i.e., w(x0) = 1/|x0|\nfor any x0 \u2208 x0.\nbecause a good policy can always prevent the bicycle from falling for any state\nin x0,1, the optimal score (6.7) for this set is 0. this is no longer true for x0,2: when\n\u03c9 and \u02d9\u03c9 have the same sign and are too large in magnitude, the bicycle cannot be\n\n "}, {"Page_number": 235, "text": "226\n\nchapter 6. cross-entropy policy search\n\nprevented from falling by any control policy. so, the optimal score for x0,2 is strictly\nnegative. to prevent the inclusion of too many such states (from which falling is un-\navoidable) in x0,2, the initial roll velocities are not taken to be too large in magnitude.\n\nbalancing a deterministic bicycle\n\nfor the first set of experiments with the bicycle, the noise is eliminated from the sim-\nulations by taking zk = 0 at each step k. the ce policy search parameters are the same\nas for the double-integrator, i.e., cce = 10, \u03c1ce = 0.01, \u03b1ce = 0.7, \u03b5ce = \u03b5mc = 0.001,\ndce = 5, and \u03c4max = 100. because the system is deterministic, a single trajectory is\nsimulated from every state in x0, i.e., nmc = 1. ce policy search is run while grad-\nually increasing the number n of rbfs from 3 to 8. for each of the two sets of\nrepresentative states and every value of n , 10 independent runs were performed,\nduring which the algorithm always converged before reaching the maximum number\nof iterations.\n\nfigure 6.9 presents the performance and execution time of ce policy search\n(mean values across the 10 runs and 95% confidence intervals). for x0,1, in fig-\nure 6.9(a), all the experiments with n \u2265 4 reached the optimal score of 0. for x0,2,\nin figure 6.9(b), the performance is around \u22120.21 and does not improve as n grows,\nwhich suggests that it is already near optimal. if so, then ce policy search obtains\ngood results with as few as 3 rbfs, which is remarkable. the execution times, shown\nin figure 6.9(c), are of course larger for x0,2 than for x0,1, since x0,2 contains more\ninitial states than x0,1.7\n\nfigure 6.10 illustrates the quality of two representative policies found by ce pol-\nicy search with n = 7: one for x0,1 and the other for x0,2. the figure shows how\nthese policies generalize to unseen initial states, i.e., how they perform if applied\nwhen starting from initial states that do not belong to x0. these new initial states\nconsist of a grid of values in the (\u03c9, \u02d9\u03c9) plane; \u03b10 and \u02d9\u03b10 are always 0. the policy is\nconsidered successful from a given initial state if it balances the bicycle for at least\n50 s. this duration is chosen to verify whether the bicycle is balanced robustly for a\nlong time; it is roughly 10 times longer than the length of the trajectory used to eval-\nuate the score during the optimization procedure, which was 5.36 s (corresponding\nto k = 536, which is the number of steps necessary to achieve the imposed accuracy\nof \u03b5mc = 0.001 in estimating the return). the policy obtained with the smaller set\nx0,1 achieves a reasonable generalization, since it balances the bicycle from a set of\ninitial states larger than x0,1. using the larger set x0,2 is more beneficial, because it\nincreases the set of states from which the bicycle is balanced. recall also that the\n\n7the execution times for the bicycle are similar to or larger than those for the double integrator, even\nthough the number of representative states is smaller. this is due to the different nature of the two prob-\nlems. for the double integrator, the goal requires terminating the task, so better policies lead to earlier\ntermination and to shorter trajectories, which in turn requires less computationally intensive monte carlo\nsimulations as the optimization goes on and the policy improves. in contrast, trajectory termination repre-\nsents a failure for the bicycle, so better policies lead to longer trajectories, which require more computation\ntime to simulate. overall, this results in a larger computational cost per initial state over the entire opti-\nmization process.\n\n "}, {"Page_number": 236, "text": "6.4. experimental study\n\n227\n\nbicycle cannot be balanced at all when \u03c9 and \u02d9\u03c9 are too large in magnitude and have\nthe same sign, i.e., in the bottom-left and top-right corners of the (\u03c9, \u02d9\u03c9) plane.\n\n\u22126\n\nx 10\n\n2\n\n0\n\ne\nr\no\nc\ns\n\n\u22122\n\n\u22124\n\n\u22126\n\n \n3\n\n \n\n mean score\n\nx\n\nx\n\n0,1\n\n0,1\n\n 95% confidence bounds\n\n4\n\n5\n\n6\n\n7\n\n8\n\nnumber of rbfs\n\n\u22120.2\n\n\u22120.205\n\ne\nr\no\nc\ns\n\n\u22120.21\n\n\u22120.215\n\n\u22120.22\n\n \n3\n\n \n\n mean score\n\nx\n\nx\n\n0,2\n\n0,2\n\n 95% confidence bounds\n\n4\n\n5\n\n6\n\nnumber of rbfs\n\n7\n\n8\n\n(a) performance for x0,1.\n\n(b) performance for x0,2.\n\n \n\n4\n10\n\n3\n10\n\n2\n10\n\n]\ns\n[\n \n\ne\nm\n\ni\nt\n \n\nn\no\n\ni\nt\n\nu\nc\ne\nx\ne\n\n1\n10\n\n \n3\n\nx\n\nx\n\nx\n\nx\n\n0,1\n\n0,1\n\n0,2\n\n0,2\n\n mean execution time\n\n 95% confidence bounds\n\n mean execution time\n\n 95% confidence bounds\n\n4\n\n5\n\n6\n\nnumber of rbfs\n\n7\n\n8\n\n(c) execution time.\n\nfigure 6.9 results of ce policy search for the deterministic bicycle.\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03c9\n\n0\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03c9\n\n0\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n\u22120.2\n\n\u22120.1\n\n0\n\u03c9\n [rad]\n0\n\n0.1\n\n0.2\n\n\u22120.2\n\n\u22120.1\n\n0\n\u03c9\n [rad]\n0\n\n0.1\n\n0.2\n\n(a) generalization for x0,1.\n\n(b) generalization for x0,2.\n\nfigure 6.10\ngeneralization of typical policies found by ce policy search for the deterministic bicycle.\nwhite markers indicate the bicycle fell from that initial state, whereas gray markers indicate it\nwas successfully balanced for 50 s. black crosses mark the representative initial states.\n\n "}, {"Page_number": 237, "text": "228\n\nchapter 6. cross-entropy policy search\n\ncomparison with fuzzy q-iteration for the deterministic bicycle\n\nfor comparison purposes, fuzzy q-iteration was run with an equidistant grid of tri-\nangular mfs and with the same discrete actions as those employed by ce policy\nsearch. the number n\u2032 of mfs on each axis was gradually increased, and the first\nvalue of n\u2032 that gave good performance was 12; the score for this value of n\u2032 is 0\nfor x0,1, and \u22120.2093 for x0,2. this leads to a total of 124 = 20376 equidistant mfs,\nvastly more than the number of optimized bfs required by ce policy search. the\nexecution time of fuzzy q-iteration for n\u2032 = 12 was 1354 s, similar to the execution\ntime of ce policy search with x0,1 (see figure 6.9(c)). in contrast, for the double\nintegrator problem, the execution time of fuzzy q-iteration was much smaller than\nthat of ce policy search. this discrepancy arises because the complexity of fuzzy\nq-iteration grows faster than the complexity of ce policy search, when moving from\nthe two-dimensional double integrator to the four-dimensional bicycle problem. in\ngeneral, the complexity of fuzzy q-iteration with triangular mfs grows exponen-\ntially with the number of problem dimensions, whereas the complexity (6.9) of ce\npolicy search is linear in the number of dimensions (while crucially depending also\non other quantities, such as the number of representative states). therefore, as the di-\nmension of the problem increases, ce policy search may become preferable to value\niteration techniques from a computational point of view.\n\nbalancing a stochastic bicycle\n\nthe second set of experiments includes the effects of noise. recall that the noise z\nis added to the displacement \u03b4 of the bicycle-rider center of mass, and enters the\nmodel in the dynamics of \u02d9\u03c9 (6.13), via the term \u03b2. the noise zk is drawn at each step\nk from a uniform density over [\u22120.02, 0.02] m. to apply ce policy search, n = 7\nrbfs are employed, and nmc = 10 trajectories are simulated from every initial state\nto compute the score (this value for nmc is not selected to be too large in order to\nprevent excessive computational costs). the rest of the parameters remain the same\nas in the deterministic case. for each of the two sets of representative states, 10\nindependent runs were performed.\n\nthe performance of the resulting policies, together with the execution time of\nthe algorithm, are reported in table 6.3. for easy comparison, the results in the de-\nterministic case with n = 7 are also repeated in this table. all the scores for x0,1\nare optimal, and the scores for x0,2 are similar to those obtained in the deterministic\ncase, which illustrates that, in this problem, adding noise does not greatly diminish\nthe potential to accumulate good returns. the execution times are one order of mag-\nnitude larger than for the deterministic bicycle, which is expected because nmc = 10,\nrather than 1 as in the deterministic case.\n\nfigure 6.11 illustrates how the performance of representative policies generalizes\nto states not belonging to x0. whereas in the deterministic case (figure 6.10) there\nwas little difference between the generalization performance with x0,1 and x0,2, in\nthe stochastic case this is no longer true. instead, using the smaller set x0,1 of initial\nstates leads to a policy that balances the bicycle for a much smaller portion of the\n(\u03c9, \u02d9\u03c9) plane than using x0,2. this is because the noise makes the system visit a\n\n "}, {"Page_number": 238, "text": "6.4. experimental study\n\n229\n\ntable 6.3\nresults of ce policy search for the stochastic bicycle, compared with the deterministic case\n(mean; 95% confidence interval).\n\nexperiment\n\nscore\n\nexecution time [s]\n\n0; [0, 0]\nstochastic, x0,1\n0; [0, 0]\ndeterministic, x0,1\n\u22120.2093; [\u22120.2098,\u22120.2089]\nstochastic, x0,2\ndeterministic, x0,2 \u22120.2102; [\u22120.2115,\u22120.2089]\n\n22999; [21716, 24282]\n2400; [2248, 2552]\n185205; [170663, 199748]\n17154; [16119, 18190]\n\nlarger portion of the state space than in the deterministic case, and some of these new\nstates may not have been seen along trajectories starting only in x0,1.\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03c9\n\n0\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n]\ns\n/\n\nd\na\nr\n[\n \n\n\u2019\n\n\u03c9\n\n0\n\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n\u22120.2\n\n\u22120.1\n\n0\n\u03c9\n [rad]\n0\n\n0.1\n\n0.2\n\n\u22120.2\n\n\u22120.1\n\n0\n\u03c9\n [rad]\n0\n\n0.1\n\n0.2\n\n(a) generalization for x0,1.\n\n(b) generalization for x0,2.\n\nfigure 6.11\ngeneralization of typical policies found by ce policy search for the stochastic bicycle. white\nmarkers indicate the bicycle was never balanced starting from that initial state; the size of the\ngray markers is proportional to the number of times the bicycle was properly balanced out of\n10 experiments. reproduced with permission from (bus\u00b8oniu et al., 2009), c(cid:13) 2009 ieee.\n\nfigure 6.12 shows how the stochastic bicycle is balanced by a policy found with\nx0,2. the bicycle is successfully prevented from falling, but it is not brought into a\nvertical position (\u03c9 = 0), because the reward function (6.15) makes no difference\nbetween zero and nonzero roll angles; it simply indicates that the bicycle should not\nfall. the control actions exhibit chattering, which is generally necessary when only\ndiscrete actions are available to stabilize an unstable system like the bicycle.\n\n6.4.3 structured treatment interruptions for hiv infection control\n\nin this section, ce policy search is used in a highly challenging simulation problem,\ninvolving the optimal control of treatment for an hiv infection. prevalent hiv treat-\nment strategies involve two types of drugs, called reverse transcriptase inhibitors and\nprotease inhibitors; we will refer to them simply as d1 and d2 in the sequel. the\nnegative side effects of these drugs in the long term motivate the investigation of op-\n\n "}, {"Page_number": 239, "text": "230\n\nchapter 6. cross-entropy policy search\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03c9\n\n \n,\n]\nd\na\nr\n[\n \n\n\u03c9\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n\u22120.2\n\n \n0\n\n \n\n\u03c9\n\u03c9\u2019\n\n2\n\n4\n\n6\n\n8\n\n10\n\nt [s]\n\n]\ns\n/\nd\na\nr\n[\n \n\u2019\n\n\u03b1\n\n \n,\n]\nd\na\nr\n[\n \n\n\u03b1\n\n]\n\nm\n\n[\n \n\u03b4\n\n0.02\n\n0.01\n\n0\n\n\u22120.01\n\n\u22120.02\n\n]\n\nm\nn\n\n[\n \n\u03c4\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nt [s]\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n\u22123\n\n \n0\n\n2\n\n1\n\n0\n\n\u22121\n\n\u22122\n\n0\n\n \n\n\u03b1\n\u03b1\u2019\n\n2\n\n4\n\n6\n\n8\n\n10\n\nt [s]\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nt [s]\n\nfigure 6.12\na controlled trajectory of the bicycle, starting from \u03c90 = \u22127\u03c0\n180 , \u02d9\u03c90 = \u22125\u03c0\ntime axis of the action trajectories is truncated at 1 s to maintain readability.\n\n180 , \u03b10 = \u02d9\u03b10 = 0. the\n\ntimal strategies for their use. these strategies might also boost the patient\u2019s immune\ncontrol of the disease (wodarz and nowak, 1999). one such strategy involves struc-\ntured treatment interruptions (sti), where the patient is cycled on and off d1 and d2\ntherapy (see, e.g., adams et al., 2004).\n\nhiv infection dynamics and the sti problem\n\nthe following six-dimensional, nonlinear model of the hiv infection continuous-\ntime dynamics is considered (adams et al., 2004):\n\n\u02d9t1 = \u03bb1 \u2212 d1t1 \u2212 (1\u2212 \u03b51)k1v t1\n\u02d9t2 = \u03bb2 \u2212 d2t2 \u2212 (1\u2212 f \u03b51)k2v t2\n\u02d9t t\n1 = (1\u2212 \u03b51)k1v t1 \u2212 \u03b4t t\n2 = (1\u2212 f \u03b51)k2v t2 \u2212 \u03b4t t\n\u02d9t t\n\u02d9v = (1\u2212 \u03b52)nt \u03b4(t t\n1 + t t\n1 + t t\n2)\n\u02d9e = \u03bbe +\n2) + kb\n\nbe (t t\n(t t\n1 + t t\n\n1\n\n1 \u2212 m1et t\n2 \u2212 m2et t\n2)\u2212 cv \u2212 [(1\u2212 \u03b51)\u03c11k1t1 + (1\u2212 f \u03b51)\u03c12k2t2]v\ne +\n\n2\n\nde (t t\n(t t\n1 + t t\n\n1 + t t\n2)\n2) + kd\n\ne \u2212 \u03b4e e\n\nthis model describes two populations of target cells, called type 1 and type 2. the\nstate vector is x = [t1, t2, t t\n\n2,v, e]t, where:\n\n1, t t\n\n\u2022 t1 \u2265 0 and t2 \u2265 0 are the counts of healthy type 1 and type 2 target cells\n\n[cells/ml].\n\n "}, {"Page_number": 240, "text": "6.4. experimental study\n\n231\n\n\u2022 t t\n\n1 \u2265 0 and t t\n\n[cells/ml].\n\n2 \u2265 0 are the counts of infected type 1 and type 2 target cells\n\n\u2022 v \u2265 0 is the number of free virus copies [copies/ml].\n\u2022 e \u2265 0 is the number of immune response cells [cells/ml].\n\nthe positivity of the state variables is ensured during simulations by using saturation.\nthe variables \u03b51 \u2208 [0, 0.7] and \u03b52 \u2208 [0, 0.3] denote the effectiveness of the two drugs\nd1 and d2.\nthe values and meanings of the parameters in this model are given in table 6.4.\nfor a more detailed description of the model and the rationale behind the parameter\nvalues, we refer the reader to adams et al. (2004).\n\ntable 6.4 parameters of the hiv infection model.\n\nsymbol value\n\nunits meaning\n\n\u03bb1; \u03bb2\n\nd1; d2\n\nk1; k2\n\u03b4\n\n10, 000; 31.98 cells\nml\u00b7day\n1\n0.01; 0.01\nday\n8\u00b7 10\u22127; 10\u22124\n0.7\n0.34\n\nday\n\u2013\n\nml\n\n1\n\nf\nm1, m2 10\u22125; 10\u22125\n\nnt\n\nc\n\n\u03c11; \u03c12\n\u03bbe\n\nbe\n\nkb\n\nde\n\u03b4e\n\nkd\n\n100\n\n13\n\n1; 1\n\n1\n\n0.3\n\n100\n\n0.25\n\n0.1\n\n500\n\nproduction rates of target cell types 1 and 2\n\ndeath rates of target cell types 1 and 2\n\ncopies\u00b7day infection rates of populations 1 and 2\n\ninfected cell death date\n\ntreatment effectiveness reduction in population 2\n\nml\n\ncells\u00b7day\n\nimmune-induced clearance rates of populations 1\nand 2\n\nvirions\n\ncell\n1\n\nday\nvirions\n\ncell\ncells\nml\u00b7day\n1\nday\ncells\nml\n1\n\nday\n\n1\n\nday\ncells\nml\n\nvirions produced per infected cell\n\nvirus natural death rate\n\nmean number of virions infecting cell types 1 and 2\n\nimmune effector production rate\n\nmaximum birth rate for immune effectors\n\nsaturation constant for immune effector birth\n\nmaximum death rate for immune effectors\n\nnatural death rate for immune effectors\n\nsaturation constant for immune effector death\n\nwe do not model the relationship between the quantity of administered drugs\nand their effectiveness, but we assume instead that \u03b51 and \u03b52 can be directly con-\ntrolled, which leads to the two-dimensional control vector u = [\u03b51,\u03b52]t. in sti,\ndrugs are administered either fully (they are \u201con\u201d) or not at all (they are \u201coff\u201d).\na fully administered d1 drug corresponds to \u03b51 = 0.7, while a fully administered\nd2 drug corresponds to \u03b52 = 0.3. a set of 4 possible discrete actions emerges:\nud = {0, 0.7} \u00d7 {0, 0.3}. because it is not clinically feasible to change the treat-\nment daily, the state is measured and the drugs are switched on or off once every 5\n\n "}, {"Page_number": 241, "text": "232\n\nchapter 6. cross-entropy policy search\n\ndays (adams et al., 2004). the system is therefore controlled in discrete time with a\nsampling time of 5 days. the discrete-time transitions are obtained by numerically\nintegrating the continuous-time dynamics between consecutive time steps.\n\nthe hiv dynamics have three uncontrolled equilibria. the uninfected equilib-\nrium xn = [1000000, 3198, 0, 0, 0, 10]t is unstable: as soon as v becomes nonzero due\nto the introduction of virus copies, the patient becomes infected and the state drifts\naway from xn. the unhealthy equilibrium xu = [163573, 5, 11945, 46, 63919, 24]t\nis stable and represents a patient with a very low immune response,\nfor\nwhom the infection has reached dangerous levels. the healthy equilibrium xh =\n[967839, 621, 76, 6, 415, 353108]t is stable and represents a patient whose immune\nsystem controls the infection without the need for drugs.\n\nwe consider the problem of using sti from the unhealthy initial state xu such that\nthe immune response of the patient is maximized and that the number of virus copies\nis minimized, while also penalizing the quantity of drugs administered, to account\nfor their side effects. this is represented using the reward function (adams et al.,\n2004):\n\n\u03c1(x, u) = \u2212qv \u2212 r1\u03b52\n\n1 \u2212 r2\u03b52\n\n2 + se\n\n(6.16)\n\nwhere q = 0.1, r1 = r2 = 20000, s = 1000. the term se rewards the amount of im-\nmune response, \u2212qv penalizes the amount of virus copies, while \u2212r1\u03b52\n1 and \u2212r2\u03b52\n\n2\n\npenalize drug use.\n\nresults of ce policy search\n\nin order to apply ce policy search, a discount factor of \u03b3 = 0.99 is used. to compute\nthe score, the number of simulation steps is set to k = tf/ts = 800/5 = 160, where\ntf = 800 days is a sufficiently long time horizon for a good policy to control the infec-\ntion (see adams et al., 2004; ernst et al., 2006b). this is different from the examples\nabove, in which a precision \u03b5mc in estimating the return was first imposed, and then\nthe trajectory length was computed accordingly. to limit the effects of the large vari-\nation of the state variables, which span several orders of magnitude, a transformed\nstate vector is used, computed as the base 10 logarithm of the original state vector.\nthe policy is represented using n = 8 rbfs, and because we are only interested in\napplying sti from the unhealthy initial state xu, only this state is used to compute the\nscore: x0 = {xu}. the other parameters remain unchanged from the earlier examples\n(sections 6.4.1 and 6.4.2): cce = 10, \u03c1ce = 0.01, \u03b1ce = 0.7, \u03b5ce = 0.001, dce = 5,\nand \u03c4max = 100.\n\nfigure 6.13 shows a trajectory of the hiv system controlled from xu with a typ-\nical policy obtained by ce policy search. the execution time to obtain this policy\nwas 137864 s. for comparison, trajectories obtained with no treatment and with fully\nadministered treatment are also shown. the ce solution switches the d2 drug off\nafter approximately 300 days, but the d1 drug is left on in steady state, which means\nthat the healthy equilibrium xh is not reached. nevertheless, the infection is handled\nbetter than without sti, and the immune response e in steady state is strong.\n\nnote that because of the high dimensionality of the hiv problem, using a value-\n\nfunction technique with equidistant bf approximation is out of the question.\n\n "}, {"Page_number": 242, "text": "6.5. summary and discussion\n\n233\n\n5.9\n\n10\n\n]\nl\n\nm\n/\ns\n\n]\nl\n\nm\n/\ns\n\nl\nl\n\ne\nc\n[\n \n\nt\n\n1\n\nl\nl\n\ne\nc\n[\n \n\nt\n\n2\n\n0\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n5.1\n\n10\n\n5\n10\n\n0\n10\n\n\u22125\n\n10\n\n0\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n]\nl\n\nm\n/\ns\n\nl\nl\n\ne\nc\n[\n \n\nt\n\n2t\n\n]\n\n\u2212\n\n[\n \n\n\u03b5\n\n1\n\n1\n\n0.5\n\n0\n\n0\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n5\n10\n\n0\n10\n\n\u22125\n\n10\n\n0\n\n10\n\n10\n\n0\n10\n\n]\nl\n\ni\n\nm\n/\ns\ne\np\no\nc\n[\n \n\nv\n\n\u221210\n\n10\n\n0\n\n]\n\n\u2212\n\n[\n \n\n\u03b5\n\n2\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0\n\n]\nl\n\nm\n/\ns\n\nl\nl\n\ne\nc\n[\n \n\nt\n\n1t\n\n10\n\n10\n\n0\n10\n\n\u221210\n\n10\n\n0\n\n5\n10\n\n]\nl\n\nm\n/\ns\n\nl\nl\n\ne\nc\n[\n \n\ne\n\n0\n10\n\n0\n\n10\n\n10\n\n]\n\n\u2212\n\n[\n \nr\n\n5\n10\n\n0\n10\n\n0\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\n200\n\n400\n\nt [days]\n\n600\n\n800\n\nfigure 6.13\ntrajectories from xu for hiv infection control. black, continuous: policy computed with ce\npolicy search. gray: no treatment. black, dashed: fully administered treatment. the states and\nrewards are shown on a logarithmic scale, and negative values of the reward are ignored.\n\n6.5 summary and discussion\n\nin this final chapter of the book, we have considered a cross-entropy policy search al-\ngorithm for continuous-state, discrete-action problems. this algorithm uses a flexible\npolicy parametrization, inspired by the work on bf optimization for value function\napproximation. the optimization is carried out with the ce method and evaluates the\npolicies by their empirical return from a representative set of initial states. a detailed\nnumerical study of ce policy search has been performed, the more important results\nof which are the following. the algorithm gives a good performance using only a\nsmall number of bfs to represent the policy. when it must optimize the performance\nover the entire state space, ce policy search is more computationally demanding\nthan value and policy iteration, at least in problems with only a few (e.g., two) di-\nmensions. nevertheless, given a concise selection of representative states, ce policy\nsearch can become computationally preferable to value and policy iteration as the\ndimensionality increases (e.g., six dimensions or more).\n\nalthough ce policy search has been convergent in the experiments of this chap-\nter, it is an open problem whether it provably converges in general. since ce policy\nsearch involves the optimization of both continuous and discrete variables, the ce\nconvergence results of rubinstein and kroese (2004); costa et al. (2007), which\nonly concern the discrete case, cannot be applied directly. the convergence results\nfor the related model-reference adaptive search (chang et al., 2007) cover more gen-\n\n "}, {"Page_number": 243, "text": "234\n\nchapter 6. cross-entropy policy search\n\neral cases, including stochastic optimization criteria evaluated by monte carlo inte-\ngration (as in ce policy search), but they require the restrictive assumption that the\noptimal policy parameter is unique.\n\nin this chapter only discrete-action problems have been considered, but a natural\nextension of the discrete-action policy parametrization can be developed to handle\ncontinuous actions, by using the bf values to interpolate between the actions as-\nsigned to the bfs. it would also be useful to compare ce and direct optimization\nwith other techniques able to solve the global, mixed-integer, gradient-free problem\narising in policy search; such techniques include among others genetic algorithms,\nsimulated annealing, and tabu search.\n\nbibliographical notes\n\nthis chapter extends the authors\u2019 earlier work on policy search (bus\u00b8oniu et al., 2009).\ncompared to this earlier work, the policy parametrization has been simplified, the al-\ngorithm has been enhanced with a smoothing procedure, and the experimental study\nhas been extended.\n\nour policy parametrization is inspired by techniques to optimize bfs for value\nfunction approximation (e.g., singh et al., 1995; menache et al., 2005; whiteson and\nstone, 2006; bertsekas and yu, 2009).\n\nusing the ce method to optimize the policy was first proposed by mannor et al.\n(2003). chang et al. (2007, chapter 4) optimized the policy with the model-reference\nadaptive search, which is closely related to ce optimization. both mannor et al.\n(2003) and chang et al. (2007, chapter 4) focused on solving finite, small mdps,\nalthough they also proposed solving large mdps using parameterized policies.\n\n "}, {"Page_number": 244, "text": "appendix a\n\nextremely randomized trees\n\nthis appendix briefly introduces a nonparametric approximator employing an en-\nsemble of extremely randomized regression trees (extra-trees for short). this ap-\nproximator was proposed by geurts et al. (2006) and was combined with the fitted-q\niteration algorithm by ernst et al. (2005). our presentation largely follows from ma-\nterial in these two research papers. in this book, fitted q-iteration with extra-trees\napproximation was first employed in section 3.4.5, as an example of approximate\nvalue iteration, and then in section 4.5.2, as a baseline algorithm with which fuzzy\nq-iteration was compared. other successful applications of extra-trees approxima-\ntion in reinforcement learning can be found in (ernst et al., 2005, 2006a,b; jodogne\net al., 2006).\n\na.1 structure of the approximator\n\nthe extra-trees approximator consists of an ensemble of regression trees. each tree\nin the ensemble is built from a set of training samples provided in advance, using a\nprocedure that will be detailed in section a.2. each tree partitions the input space\ninto a number of disjoint regions, and determines a constant prediction in each region\nby averaging the output values of the samples that belong to this region.\nmore formally, assume that a set of ns training samples is provided:\n\ns = {(xis, yis) | is = 1, . . . , ns}\n\nwhere xis \u2208 rd is the isth input sample and yis \u2208 r is the corresponding output sam-\n\nple. a regression problem must be solved, which requires an approximationby(x) of\n\nthe underlying relationship between the input x and the output y to be inferred from\nthe samples. this relationship may be deterministic or stochastic, and in the latter\ncase, we aim to approximate the expected value of y given x.\n\nconsider the regression tree with index itr in the ensemble, and define a function\npitr(x) that associates each input x with the region to which it belongs in the partition\n\naverage output of the samples from the region pitr (x). we write this as:\n\ngiven by the tree. then, the prediction (approximate output)byitr(x) of the tree is the\n\nns\n\u2211\nis=1\n\n\u03ba(x, xis)yis\n\n(a.1)\n\n235\n\n "}, {"Page_number": 245, "text": "236\n\nappendix a. extremely randomized trees\n\nwith \u03ba(x, xis) given by:\n\n\u03ba(x, xis) =\n\ni(xis \u2208 pitr (x))\nis\u2032=1\n\ni(xis\u2032 \u2208 pitr (x))\n\n\u2211ns\n\n(a.2)\n\nhere, the indicator function i is equal to 1 if its argument is true, and 0 otherwise.\n\nthe complete, ensemble approximator consists of ntr trees, and averages the pre-\n\ndictions of these trees to obtain a final, aggregate prediction:\n\nby(x) =\n\n1\nntr\n\nntr\n\u2211\n\nitr=1byitr(x)\n\nthis final prediction can also be described by an equation of the form (a.1), in which\nthe function \u03ba(x, xis) is now given by:\n\n\u03ba(x, xis) =\n\n1\nntr\n\nntr\n\u2211\nitr=1\n\ni(xis \u2208 pitr (x))\nis\u2032=1\n\ni(xis\u2032 \u2208 pitr (x))\n\n\u2211ns\n\n(a.3)\n\nthe number of trees ntr is an important parameter of the algorithm. usually, the\nmore trees, the better the algorithm behaves. however, empirical studies suggest that,\noften, choosing a number of trees larger than 50 does not significantly improve the\naccuracy of the approximator (geurts et al., 2006).\n\nthe expression (a.1) was chosen to highlight the relationship between extra-trees\nand kernel-based approximators. the latter are described by an equation similar to\n(a.1), see section 3.3.2. a single tree can be interpreted as a kernel-based approxi-\nmator with kernel (a.2), whereas for the ensemble of extra-trees the kernel is given\nby (a.3).\n\na.2 building and using a tree\n\nalgorithm a.1 presents the recursive procedure to build one of the trees in the ensem-\nble. initially, there exists a single root node, which contains the entire set of samples.\nat every step of the algorithm, each leaf node that contains at least nmin\nsamples is\nsplit, where nmin\ntr \u2265 2 is an integer parameter. using a method that will be described\nshortly, a cut-direction (input dimension) d is selected, together with a scalar cut-\npoint \u00afxd (an input value along the selected dimension). the cut-direction and the\ncut-point constitute a so-called test. then, the set of samples s associated with the\ncurrent node is split into two disjoint sets sleft and sright, respectively containing\nthe samples to the \u201cleft\u201d and \u201cright\u201d of the cut-point \u00afxd:\n\ntr\n\nsleft =(cid:8)(xis, yis) \u2208 s (cid:12)(cid:12) xis,d < \u00afxd(cid:9)\nsright =(cid:8)(xis, yis) \u2208 s (cid:12)(cid:12) xis,d \u2265 \u00afxd(cid:9)\n\n(a.4)\n\n "}, {"Page_number": 246, "text": "a.2 building and using a tree\n\n237\n\nalgorithm a.1 construction of an extremely randomized tree.\ninput: set of samples s , parameters ntr, ktr, nmin\noutput: t = buildtree(s )\n\ntr\n\n1: procedure buildtree(s )\n2:\n\nthen\n\ntr\n\nif |s | < nmin\nelse\n\nreturn a leaf node t labeled by the value 1\n|s |\n\n\u2211(x,y)\u2208s y\n\n(d, \u00afxd) \u2190selecttest(s )\nsplit s into sleft and sright according to (d, \u00afxd); see (a.4)\ntleft \u2190buildtree(sleft), tright \u2190buildtree(sright)\nt \u2190 a node with test (d, \u00afxd), left subtree tleft, and right subtree tright\nreturn t\n\nend if\n\n10:\n11: end procedure\n\n12: procedure selecttest(s )\n13:\n\nselect ktr cut-directions {d1, . . . , dktr} uniformly random in {1, . . . , d}\nfor k = 1, . . . , ktr do\n\nxdk,min \u2190 min(x,y)\u2208s xdk , xdk,max \u2190 max(x,y)\u2208s xdk\nselect a cut-point \u00afxdk uniformly random in (xdk,min, xdk,max]\n\nend for\nreturn a test (dk\u2032, \u00afxdk\u2032\n\n18:\n19: end procedure\n\n) such that k\u2032 \u2208 arg maxk s(dk, \u00afxdk , s ); see (a.5)\n\n3:\n\n4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n14:\n\n15:\n\n16:\n\n17:\n\na left child node and a right child node are created for the current node, respectively\ncontaining these two sets. the selected test is also stored in the node. the procedure\ncontinues recursively, until each leaf node contains fewer than nmin\nsamples. each\nsuch leaf node is labeled with the average output of the samples associated with it.\nto determine the test at a node, the algorithm generates at random ktr \u2265 1 cut-\ndirections and, for each cut-direction, a random cut-point. a score is computed for\neach of these ktr tests, and a test that maximizes the score is chosen. the score used\nis the relative variance reduction, which for a test (d, \u00afxd) is defined as follows:\n\ntr\n\ns(d, \u00afxd, s ) =\n\nvar(s )\u2212 |sleft|ns\n\nvar(sleft)\u2212 |sright|ns\n\nvar(s )\n\nvar(sright)\n\n(a.5)\n\nwhere s is the set of samples contained by the node considered, var(\u00b7) is the variance\nof the output y across the argument set, and |\u00b7| denotes set cardinality. note that, if\nktr = 1, the cut-direction and the cut-point are chosen completely at random.\ngeurts et al. (2006) suggest choosing ktr to be equal to dimensionality d of the\ninput space. as a default value for nmin\ntr = 2, yielding\nfully developed trees, when the underlying input-output relationship is deterministic;\nand nmin\ntr = 5 for stochastic problems. it should be stressed that optimizing these pa-\nrameters, together with the number of trees ntr, for the problem at hand may improve\n\n, it is suggested to choose nmin\n\ntr\n\n "}, {"Page_number": 247, "text": "238\n\nappendix a. extremely randomized trees\n\nthe approximation accuracy. this optimization could for example be carried out by\nusing a cross-validation technique (see, e.g., duda et al., 2000).\n\nalgorithm a.2 presents a practical procedure to obtain a prediction (approximate\noutput) from a built tree. to compute this prediction, the algorithm starts from the\nroot node, and applies the test associated with this node. depending on the result of\nthe test, the algorithm continues along the left subtree or along the right subtree, and\nso on, until reaching a leaf node. then, the algorithm returns the label of this leaf\nnode, which is equal to the average output of the associated samples.\n\nalgorithm a.2 prediction using an extremely randomized tree.\ninput: tree t , input point x\n\n3:\n\n1: while t is not a leaf node do\n2:\n\n(d, \u00afxd) \u2190 test associated with root of t\nif xd \u2264 \u00afxd then t \u2190 tleft, the left subtree of t\nelse t \u2190 tright, the right subtree of t\nend if\n5:\n6: end while\n\n4:\n\noutput: the label of t\n\nto clarify the link with the formulas in section a.1, the input space partition\nthat corresponds to the tree (or, equivalently, the function p) must be defined. this\npartition contains a number of regions identical to the number of leaf nodes in the\ntree, and each region consists of all the points in rd for which algorithm a.2 reaches\nthe same leaf node. equivalently, for any x, p(x) gives the set of points for which\nalgorithm a.2 would obtain the same leaf node that it reaches when applied to x.\n\n "}, {"Page_number": 248, "text": "appendix b\n\nthe cross-entropy method\n\nthis appendix provides an introduction to the cross-entropy (ce) method. first, the\nce algorithm for rare-event simulation is given, followed by the ce algorithm for\noptimization. the presentation is based on sections 2.3, 2.4, and 4.2 of the textbook\nby rubinstein and kroese (2004).\n\nb.1 rare-event simulation using the cross-entropy method\n\nwe consider the problem of estimating the probability of a rare event using sam-\npling. because the event is rare, its probability is small, and straightforward monte\ncarlo sampling is impractical because it would require too many samples. instead,\nan importance sampling density1 must be chosen that increases the probability of the\ninteresting event. the ce method for rare-event simulation looks for the best im-\nportance sampling density from a given, parameterized class of densities, using an\niterative approach. at the first iteration, the algorithm draws a set of samples from\nan initial density. using these samples, an easier problem than the original one is\ndefined, in which the probability of the rare event is artificially increased in order to\nmake a good importance sampling density easier to find. this density is then used\nto obtain better samples in the next iteration, which allow the definition of a more\ndifficult problem, therefore giving a sampling density closer to the optimal one, and\nso on. when the problems considered at every iteration become at least as difficult\nas the original problem, the current density can be used for importance sampling in\nthe original problem.\n\nwe will next formally describe the ce method for rare-event simulation. let a be\na random vector taking values in the space a . let {p(\u00b7; v)} be a family of probability\ndensities on a , parameterized by the vector v \u2208 rnv m and let a nominal parameter\n\u00afv \u2208 rnv be given. given a score function s : a \u2192 r, the goal is to estimate the\nprobability that s(a) \u2265 \u03bb, where the level \u03bb \u2208 r is also given and a is drawn from\nthe density p(\u00b7; \u00afv) with the nominal parameter \u00afv. this probability can be written as:\n(b.1)\n\n\u03bd = pa\u223cp(\u00b7; \u00afv)(s(a) \u2265 \u03bb) = ea\u223cp(\u00b7; \u00afv){i(s(a) \u2265 \u03bb)}\n\n1for simplicity, we will abuse the terminology by using the term \u201cdensity\u201d to refer to probability\ndensity functions (which describe probabilities of continuous random variables), as well as to probability\nmass functions (which describe probabilities of discrete random variables).\n\n239\n\n "}, {"Page_number": 249, "text": "240\n\nappendix b. the cross-entropy method\n\nwhere i(s(a) \u2265 \u03bb) is the indicator function, equal to 1 whenever s(a) \u2265 \u03bb and 0\notherwise. when the probability (b.1) is very small (10\u22126 or less), the event {s(a) \u2265\n\u03bb} is called a rare event.\na straightforward way to estimate \u03bd is to use monte carlo simulations. a set of\nrandom samples a1, . . . , ance are drawn from p(\u00b7; \u00afv), and the estimated value of \u03bd is\ncomputed as:\n\n1\nnce\n\nnce\n\u2211\nis=1\n\ni(s(ais) \u2265 \u03bb)\n\n(b.2)\n\nb\u03bd =\n\nhowever, this procedure is computationally inefficient when {s(a) \u2265 \u03bb} is a rare\nevent, since a very large number of samples nce must be used for an accurate esti-\nmation of \u03bd. a better way to estimate \u03bd is to draw the samples from an importance\nsampling density q(\u00b7) on a , instead of p(\u00b7; \u00afv). the density q(\u00b7) is chosen to increase\nthe probability of the interesting event {s(a) \u2265 \u03bb}, thereby requiring fewer samples\nfor an accurate estimation of \u03bd. the parameter \u03bd can then be estimated using the\nimportance sampling estimator:\n\n1\nnce\n\nnce\n\u2211\nis=1\n\ni(s(ais) \u2265 \u03bb)\n\np(ais ; \u00afv)\nq(ais)\n\nb\u03bd =\n\nfrom (b.3), it follows that the importance sampling density:\n\nq\u2217(a) =\n\ni(s(a) \u2265 \u03bb)p(a; \u00afv)\n\n\u03bd\n\n(b.3)\n\n(b.4)\n\nmakes the argument of the summation equal to \u03bd, when substituted into (b.3). there-\nfore, a single sample a for which i(s(a) \u2265 \u03bb) is nonzero suffices for finding \u03bd, and\nthe density q\u2217(\u00b7) is optimal in this sense. it is important to note that the entire proce-\ndure is driven by the value of the nominal parameter \u00afv. hence, among others, q, q\u2217,\nand \u03bd all depend on \u00afv.\n\nthe obvious difficulty is that \u03bd is unknown. moreover, q\u2217 can in general have\na complicated shape, which makes it difficult to find. it is often more convenient to\nchoose an importance sampling density from the family of densities {p(\u00b7; v)}. the\nbest importance sampling density in this family can be found by minimizing over the\nparameter v a measure of the distance between p(\u00b7; v) and q\u2217(\u00b7). in the ce method,\n\nthis measure is the cross-entropy, also known as kullback-leibler divergence, which\nis defined as follows:\n\nd(q\u2217(\u00b7), p(\u00b7; v)) = ea\u223cq\u2217(\u00b7)(cid:26)ln\n\nq(a)\n\np(a; v)(cid:27)\n\n=z q\u2217(a) ln q\u2217(a)da\u2212z q\u2217(a) ln p(a; v)da\n\n(b.5)\n\nthe first term in this distance does not depend on v, and by using (b.4) in the\n\nsecond term, we obtain a parameter that minimizes the cross-entropy as:\n\nv\u2217 = v\u2021, where v\u2021 \u2208 arg max\ni.e., v\u2021 \u2208 arg max\n\nv\n\nv\n\nz i(s(a) \u2265 \u03bb)p(a; \u00afv)\nea\u223cp(\u00b7; \u00afv){i(s(a) \u2265 \u03bb) ln p(a; v)}\n\nln p(a; v)da,\n\n\u03bd\n\n(b.6)\n\n "}, {"Page_number": 250, "text": "b.1 rare-event simulation using the cross-entropy method\n\n241\n\nunfortunately, the expectation ea\u223cp(\u00b7; \u00afv){i(s(a) \u2265 \u03bb) ln p(a; v)} is difficult to\ncompute by direct monte carlo sampling, because the indicators i(s(a) \u2265 \u03bb) will\nstill be 0 for a most of the samples. therefore, this expectation must also be com-\nputed with importance sampling. for an importance sampling density given by the\nparameter z, the maximization problem (b.6) is rewritten as:\n\nv\u2217 = v\u2021, where v\u2021 \u2208 arg max\n\nv\n\nea\u223cp(\u00b7;z){i(s(a) \u2265 \u03bb)w (a; \u00afv, z) ln p(a; v)}\n\n(b.7)\n\ndrawing a set of random samples a1, . . . , ance from the importance density p(\u00b7; z)\nand solving:\n\nin which w (a; \u00afv, z) = p(a; \u00afv)/p(a; z). an approximate solution bv\u2217 is computed by\nbv\u2217 = v\u2021, where v\u2021 \u2208 arg max\n\ni(s(ais) \u2265 \u03bb)w (a; \u00afv, z) ln p(ais ; v)\n\nthe problem (b.8) is called the stochastic counterpart of (b.7).\n\nnce\n\u2211\nis=1\n\n1\nnce\n\n(b.8)\n\nv\n\nunder certain assumptions on a and p(\u00b7; v), the stochastic counterpart can be\nsolved explicitly. one particularly important case when this happens is when p(\u00b7; v)\nbelongs to the natural exponential family (morris, 1982). for instance, when {p(\u00b7; v)}\nis the family of gaussians parameterized by mean \u03b7 and standard deviation \u03c3 (so,\nv = [\u03b7,\u03c3]t), the solution of the stochastic counterpart is the mean and the standard\ndeviation of the best samples:\n\n\u2211nce\nis=1 i(s(ais) \u2265 \u03bb)ais\n\u2211nce\nis=1 i(s(ais) \u2265 \u03bb)\n\nb\u03b7 =\nb\u03c3 =vuut \u2211nce\n\nis=1 i(s(ais) \u2265 \u03bb)(ais \u2212b\u03b7)2\n\n\u2211nce\nis=1 i(s(ais) \u2265 \u03bb)\n\n(b.9)\n\n(b.10)\n\nchoosing directly a good importance sampling parameter z is difficult. if z is\npoorly chosen, most of the indicators i(s(ais) \u2265 \u03bb) in (b.8) will be 0, and the esti-\nalleviate this difficulty, the ce algorithm uses an iterative approach. each iteration \u03c4\ncan be viewed as the application of the above methodology using a modified level \u03bb\u03c4\nand the importance density parameter z = v\u03c4\u22121, where:\n\nmated parameter bv\u2217 will be a poor approximation of the optimal parameter v\u2217. to\n\n\u2022 the level \u03bb\u03c4 is chosen at each iteration such that the probability of event\n{s(a) \u2265 \u03bb\u03c4} under density p(\u00b7; v\u03c4\u22121) is approximately \u03c1ce \u2208 (0, 1), with \u03c1ce\nchosen not too small (e.g., \u03c1ce = 0.05).\n\n\u2022 the parameter v\u03c4\u22121, \u03c4 \u2265 2, is the solution of the stochastic counterpart at the\n\nprevious iteration; v0 is initialized at \u00afv.\n\nthe value \u03bb\u03c4 is computed as the (1 \u2212 \u03c1ce) quantile of the score values of the\nrandom sample a1, . . . , ance , which are drawn from p(\u00b7; v\u03c4\u22121). if these score values\n\n "}, {"Page_number": 251, "text": "242\n\nappendix b. the cross-entropy method\n\nare ordered increasingly and indexed such that s1 \u2264 \u00b7\u00b7\u00b7 \u2264 snce , then the (1 \u2212 \u03c1ce)\nquantile is:\n(b.11)\nwhere \u2308\u00b7\u2309 rounds the argument to the next greater or equal integer number (ceiling).\nwhen the inequality \u03bb\u03c4\u2217 \u2265 \u03bb is satisfied for some \u03c4\u2217 \u2265 1, the rare-event proba-\nbility is estimated using the density p(\u00b7; v\u03c4\u2217 ) for importance sampling (n1 \u2208 n\u2217):\n\n\u03bb\u03c4 = s\u2308(1\u2212\u03c1ce)nce\u2309\n\n1\nn1\n\nn1\n\u2211\nis=1\n\nb\u03bd =\n\ni(s(ais) \u2265 \u03bb)w (ais ; \u00afv, v\u03c4\u2217 )\n\n(b.12)\n\nb.2 cross-entropy optimization\n\nconsider the following optimization problem:\n\ns(a)\n\nmax\na\u2208a\n\n(b.13)\n\nwhere s : a \u2192 r is the score function (optimization criterion) to maximize, and\nthe variable a takes values in the domain a . denote the maximum by s\u2217. the ce\nmethod for optimization maintains a density with support a . at each iteration, a\nnumber of samples are drawn from this density and the score values of these samples\nare computed. a (smaller) number of samples that have the best scores are kept, and\nthe remaining samples are discarded. the density is then updated using the selected\nsamples, such that during the next iteration the probability of drawing better samples\nis increased. the algorithm stops when the score of the worst selected sample no\nlonger improves significantly.\n\nformally, a family of densities {p(\u00b7; v)} with support a and parameterized by v\nmust be chosen. an associatedstochasticproblem to (b.13) is the problem of finding\nthe probability:\n\n\u03bd(\u03bb) = pa\u223cp(\u00b7;v\u2032)(s(a) \u2265 \u03bb) = ea\u223cp(\u00b7;v\u2032){i(s(a) \u2265 \u03bb)}\n\n(b.14)\n\nwhere the random vector a has the density p(\u00b7; v\u2032) for some parameter vector v\u2032.\nconsider now the problem of estimating \u03bd(\u03bb) for a \u03bb that is close to s\u2217. typically,\n{s(a) \u2265 \u03bb} is a rare event. the ce procedure can therefore be exploited to solve\n(b.14).\ncontrary to the ce method for rare-event simulation, in optimization there is\nno known nominal \u03bb; its place is taken by s\u2217, which is unknown. the ce method\nfor optimization circumvents this difficulty by redefining the associated stochastic\nproblem at every iteration \u03c4, using the density with parameter v\u03c4\u22121, so that \u03bb\u03c4 is\nexpected to converge to s\u2217 as \u03c4 increases. consequently, the stochastic counterpart at\niteration \u03c4 of ce optimization:\n\n\u03c4, where v\u2021\n\n\u03c4 \u2208 arg max\n\nv\n\nbv\u03c4 = v\u2021\n\n1\nnce\n\nnce\n\u2211\nis=1\n\ni(s(ais) \u2265 \u03bb\u03c4) ln p(ais ; v)\n\n(b.15)\n\n "}, {"Page_number": 252, "text": "b.2 cross-entropy optimization\n\n243\n\nis different from the one used in rare-event simulation (b.8), and corresponds to max-\nimizing over v the expectation ea\u223cp(\u00b7;v\u03c4\u22121){i(s(ais) \u2265 \u03bb\u03c4) ln p(ais ; v)}. this deter-\nmines the (approximately) optimal parameter associated with pa\u223cp(\u00b7;v\u03c4\u22121)(s(a) \u2265 \u03bb\u03c4),\nrather than with pa\u223cp(\u00b7; \u00afv)(s(a) \u2265 \u03bb\u03c4) as in rare-event simulation. so, the term w from\n(b.7) and (b.8) does not play a role here. the parameter \u00afv, which in rare-event sim-\nulation was the fixed nominal parameter under which the rare-event probability has\nto be estimated, no longer plays a role either. instead, in ce optimization an initial\nvalue v0 of the density parameter is required, which only serves to define the asso-\nciated stochastic problem at the first iteration, and which can be chosen in a fairly\narbitrary way.\n\ncan also be updated incrementally:\n\ninstead of setting the new density parameter equal to the solutionbv\u03c4 of (b.15), it\n\n(b.16)\n\nv\u03c4 = \u03b1cebv\u03c4 + (1\u2212 \u03b1ce)v\u03c4\u22121\n\nwhere \u03b1ce \u2208 (0, 1]. this so-called \u201csmoothing procedure\u201d is useful to prevent ce\noptimization from becoming stuck in local optima (rubinstein and kroese, 2004).\nthe most important parameters in the ce method for optimization are the number\nnce of samples and the quantile \u03c1ce of best samples used to update the density. the\nnumber of samples should be at least a multiple of the number of parameters nv, so\nnce = ccenv with cce \u2208 n, cce \u2265 2. the parameter \u03c1ce can be chosen around 0.01 for\nlarge numbers of samples, or it can be larger, around (ln nce)/nce, if there are only a\nfew samples (nce < 100) (rubinstein and kroese, 2004). the smoothing parameter\n\u03b1ce is often chosen around 0.7.\n\nthe ce method for optimization is summarized in algorithm b.1. note that at\nline 8, the stochastic counterpart (b.15) was simplified by using the fact that the\nsamples are already sorted in the ascending order of their scores. when \u03b5ce = 0, the\n\nalgorithm b.1 cross-entropy optimization.\n\ninput: family {p(\u00b7; v)}, score function s,\nparameters \u03c1ce, nce, dce, \u03b5ce, \u03b1ce, \u03c4max\n1: \u03c4 \u2190 1\n2: initialize density parameters v0\n3: repeat\n4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\ngenerate sample a1, . . . , ance from p(\u00b7; v\u03c4\u22121)\ncompute scores s(ais), is = 1, . . . , nce\nreorder and reindex s.t. s1 \u2264 \u00b7\u00b7\u00b7 \u2264 snce\n\u03bb\u03c4 \u2190 s\u2308(1\u2212\u03c1ce)nce\u2309\n\u03c4, where v\u2021\n\n\u03c4 \u2208 arg maxv \u2211nce\n\nis=\u2308(1\u2212\u03c1ce)nce\u2309\n\nbv\u03c4 \u2190 v\u2021\nv\u03c4 \u2190 \u03b1cebv\u03c4 + (1\u2212 \u03b1ce)v\u03c4\u22121\n\n\u03c4 \u2190 \u03c4+ 1\n\nln p(ais ; v)\n\n11: until (\u03c4 > dce and |\u03bb\u03c4\u2212\u03c4\u2032 \u2212 \u03bb\u03c4\u2212\u03c4\u2032\u22121| \u2264 \u03b5ce, for \u03c4\u2032 = 0, . . . , dce \u2212 1) or \u03c4 = \u03c4max\n\noutput: ba\u2217, the best sample encountered at any iteration \u03c4; andbs\u2217 = s(ba\u2217)\n\n "}, {"Page_number": 253, "text": "244\n\nappendix b. the cross-entropy method\n\nalgorithm terminates when \u03bb remains constant for dce consecutive iterations. when\n\u03b5ce > 0, the algorithm terminates when \u03bb improves for dce consecutive iterations, but\nthese improvements do not exceed \u03b5ce. the integer dce > 1 accounts for the random\nnature of the algorithm, by ensuring that the latest performance improvements did\nnot decrease below \u03b5ce accidentally (due to random effects), but that instead the\ndecrease remains steady for dce iterations. a maximum number of iterations \u03c4max is\nalso chosen, to ensure that the algorithm terminates in finite time.\n\nce optimization has been shown to lead to good performance, often outperform-\ning other randomized algorithms (rubinstein and kroese, 2004), and has found many\napplications in recent years, e.g., in biomedicine (mathenya et al., 2007), power sys-\ntems (ernst et al., 2007), vehicle routing (chepuri and de mello, 2005), vector quanti-\nzation (boubezoul et al., 2008), and clustering (rubinstein and kroese, 2004). while\nthe convergence of ce optimization has not yet been proven in general, the algorithm\nis usually convergent in practice (rubinstein and kroese, 2004). for combinatorial\n(discrete-variable) optimization, the ce method provably converges with probability\n1 to a unit mass density, which always generates samples equal to a single point.\nfurthermore, the probability that this convergence point is in fact an optimal solution\ncan be made arbitrarily close to 1 by using a sufficiently small smoothing parameter\n\u03b1ce (costa et al., 2007).\n\n "}, {"Page_number": 254, "text": "symbols and abbreviations\n\nlist of symbols and notations\n\nthe most important mathematical symbols and notations used in this book are listed\nbelow, organized by topic.\n\ngeneral notations\n\n|\u00b7|\nk\u00b7kp\n\u230a\u00b7\u230b\n\u2308\u00b7\u2309\ng(\u00b7;\u03b8)\nlg\n\nabsolute value (for numeric arguments); cardinality (for sets)\np-norm of the argument\nthe largest integer smaller than or equal to the argument (floor)\nthe smallest integer larger than or equal to the argument (ceiling)\ngeneric function g has argument \u201c\u00b7\u201d and is parameterized by \u03b8\nlipschitz constant of generic function g\n\nprobability theory\n\np\na \u223c p(\u00b7)\np (\u00b7)\ne{\u00b7}\n\u03b7; \u03c3\n\u03b7bin\n\nprobability density\nrandom sample a is drawn from the density p\nprobability of the random variable given as argument\nexpectation of the random variable given as argument\nmean of a gaussian density; standard deviation of a gaussian density\nparameter (mean, success probability) of a bernoulli density\n\nclassical dynamic programming and reinforcement learning\n\nx; x\nu; u\nr\nf ; \u02dcf\n\u03c1; \u02dc\u03c1\n\nh; \u02dch\nr\n\u03b3\nk; k\nts\nq; v\n\nstate; state space\ncontrol action; action space\nreward\ndeterministic transition function; stochastic transition function\nreward function for deterministic transitions; reward function for\nstochastic transitions\ndeterministic control policy; stochastic control policy\nreturn\ndiscount factor\ndiscrete time index; discrete time horizon\nsampling time\nq-function; v-function\n\n245\n\n "}, {"Page_number": 255, "text": "symbols and abbreviations\n\n246\n\nqh; v h\nq\u2217; v \u2217\nh\u2217\nq\n\nq-function of policy h; v-function of policy h\noptimal q-function; optimal v-function\noptimal policy\nset of all q-functions\nq-iteration mapping\npolicy evaluation mapping for policy h\nprimary iteration index; number of primary iterations\nsecondary iteration index\nlearning rate (step size)\nexploration probability\n\nt\nt h\n\u2113; l\n\u03c4\n\u03b1\n\u03b5\n\u03b5qi,\u03b5pe, etc. convergence threshold, always subscripted by algorithm type (in these\n\nexamples, \u03b5qi for q-iteration and \u03b5pe for policy evaluation)\n\napproximate dynamic programming and reinforcement learning\n\nbq;bv\nbh\n\nd; d\nf\np\nud\n\u03b8; \u03c6\n\n\u03d1; \u03d5\n\u00af\u03c6\n\u03ba\nn\n\nn\n\nn\nm\nns\nns\nl\ni\n\nj\n[i, j]\n\nls\nis\n\napproximate q-function; approximate v-function\napproximate policy\nindex of state dimension (variable); number of state space dimensions\napproximation mapping\nprojection mapping\nset of discrete actions\nvalue function parameter vector; basis functions for value function\napproximation\npolicy parameter vector; basis functions for policy approximation\nstate-dependent basis functions for q-function approximation\nkernel function\nnumber of parameters and of state-action basis functions for q-function\napproximation\nnumber of parameters and of state-dependent basis functions for policy\napproximation\nnumber of state-dependent basis functions for q-function approximation\nnumber of discrete actions\nnumber of state-action samples for q-function approximation\nnumber of state samples for policy approximation\nindex of q-function parameter and of state-action basis function\nindex of state-dependent basis function, of policy parameter, or of dis-\ncrete state\nindex of discrete action\nscalar index corresponding to the two-dimensional index (i, j); usually,\n[i, j] = i + ( j\u2212 1)n\nindex of state-action sample\nindex of state sample, as well as generic sample index\n\n "}, {"Page_number": 256, "text": "symbols and abbreviations\n\n247\n\n\u03be; \u03be\nc\nb; b\n\u03c2\n\u03b3\n\u03bb\nz\nw\ns\nx0\nnmc\n\u03b5mc\n\nparameter vector of basis functions; set of such parameter vectors\ncenter of basis function, given as a vector\nwidth of basis function, given as a vector; and given as a matrix\napproximation error\nmatrix on the left-hand side of the projected bellman equation\nmatrix on the right-hand side of the projected bellman equation\nvector on the right-hand side of the projected bellman equation\nweight function of approximation errors, of representative states, etc.\nscore function\nset of representative initial states\nnumber of monte carlo simulations for each representative state\nadmissible error in the estimation of the return along a trajectory\n\nfuzzy q-iteration\n\n\u03c7; \u00b5\n\u03c6\nxi\ns\n\u03b4x; \u03b4u\n\nfuzzy set; membership function\nin this context, normalized membership function (degree of fulfillment)\ncore of the ith fuzzy set\nasynchronous fuzzy q-iteration mapping\nstate resolution step; action resolution step\n\nonline and continuous-action least-squares policy iteration\n\nk\u03b8\n\u03b5d\nttrial\n\u03b4mon\n\u03c8\nmp\n\nnumber of transitions between two consecutive policy improvements\ndecay rate of exploration probability\ntrial length\nmonotonicity direction\npolynomial\ndegree of polynomial approximator\n\ncross-entropy policy search and cross-entropy optimization\n\nv\ni{\u00b7}\nnce\n\u03c1ce\n\u03bb\ncce\n\n\u03b1ce\n\u03b5ce\ndce\n\nparameter vector of a parameterized probability density\nindicator function, equal to 1 when the argument is true, and 0 otherwise\nnumber of samples used at every iteration\nproportion of samples used in the cross-entropy updates\nprobability level or (1\u2212 \u03c1ce) quantile of the sample performance\nhow many times the number of samples nce is larger than the number of\ndensity parameters\nsmoothing parameter\nconvergence threshold of the cross-entropy algorithm\nhow many iterations the variation of \u03bb should be at most \u03b5ce to stop the\nalgorithm\n\n "}, {"Page_number": 257, "text": "248\n\nsymbols and abbreviations\n\nexperimental studies\n\nt\n\u03b1\n\u03c4\nqrew\nrrew\n\ncontinuous time variable\nin this context, angle\nin this context, motor torque\nweight matrix for the states, used in the reward function\nweight matrix or scalar for the actions, used in the reward function\n\nfurthermore, the following conventions are adopted throughout the book:\n\n\u2022 all the vectors used are column vectors. the transpose of a vector is denoted\n\nby the superscript t, such that, e.g., the transpose of \u03b8 is \u03b8t.\n\n\u2022 boldface notation is used for vector or matrix representations of functions and\nmappings, e.g., qqq is a vector representation of a q-function q. however, ordi-\nnary vectors and matrices are displayed in a normal font, e.g., \u03b8, \u03b3.\n\n\u2022 calligraphic notation is used to differentiate variables related to policy approx-\nimation, from variables related to value function approximation. for instance,\nthe policy parameter is \u03d1, whereas the value function parameter is \u03b8.\n\nlist of abbreviations\n\nthe following list collects, in alphabetical order, the abbreviations used in this book.\n\nbf\nce\ndc\ndp\nhiv\nlspe (lspe-q)\nlspi\nlstd (lstd-q)\nmdp\nmf\npi\nrbf\nrl\nsti\ntd (td-q)\n\nbasis function\ncross-entropy\ndirect current\ndynamic programming\nhuman immunodeficiency virus\nleast-squares policy evaluation (for q-functions)\nleast-squares policy iteration\nleast-squares temporal difference (for q-functions)\nmarkov decision process\nmembership function\npolicy iteration\nradial basis function\nreinforcement learning\nstructured treatment interruptions\ntemporal difference (for q-functions)\n\n "}, {"Page_number": 258, "text": "bibliography\n\n\u02daastr\u00a8om, k. j., klein, r. e., and lennartsson, a. (2005). bicycle dynamics and con-\n\ntrol. ieee control systemsmagazine, 24(4):26\u201347.\n\nabonyi, j., babu\u02c7ska, r., and szeifert, f. (2001). fuzzy modeling with multivariate\nmembership functions: gray-box identification and control design. ieee trans-\nactions on systems,man, and cybernetics\u2014part b: cybernetics, 31(5):755\u2013767.\n\nadams, b., banks, h., kwon, h.-d., and tran, h. (2004). dynamic multidrug thera-\npies for hiv: optimal and sti control approaches. mathematicalbiosciencesand\nengineering, 1(2):223\u2013241.\n\nantos, a., munos, r., and szepesv\u00b4ari, cs. (2008a). fitted q-iteration in continuous\naction-space mdps. in platt, j. c., koller, d., singer, y., and roweis, s. t., editors,\nadvances in neural information processing systems 20, pages 9\u201316. mit press.\n\nantos, a., szepesv\u00b4ari, cs., and munos, r. (2008b). learning near-optimal policies\nwith bellman-residual minimization based fitted policy iteration and a single sam-\nple path. machine learning, 71(1):89\u2013129.\n\nascher, u. and petzold, l. (1998). computer methods for ordinary differential\nequations and differential-algebraic equations. society for industrial and applied\nmathematics (siam).\n\naudibert, j.-y., munos, r., and szepesv\u00b4ari, cs. (2007). tuning bandit algorithms in\nstochastic environments. in proceedings 18th international conference on algo-\nrithmic learning theory (alt-07), pages 150\u2013165, sendai, japan.\n\nauer, p., cesa-bianchi, n., and fischer, p. (2002). finite time analysis of multiarmed\n\nbandit problems. machine learning, 47(2\u20133):235\u2013256.\n\nauer, p., jaksch, t., and ortner, r. (2009). near-optimal regret bounds for reinforce-\nment learning. in koller, d., schuurmans, d., bengio, y., and bottou, l., editors,\nadvancesinneuralinformationprocessingsystems21, pages 89\u201396. mit press.\n\nbaird, l. (1995). residual algorithms: reinforcement learning with function ap-\nproximation. in proceedings 12th international conference on machine learning\n(icml-95), pages 30\u201337, tahoe city, us.\n\nbalakrishnan, s., ding, j., and lewis, f. (2008). issues on stability of adp feed-\nback controllers for dynamical systems. ieeetransactionsonsystems,man,and\ncybernetics\u2014part b: cybernetics, 4(38):913\u2013917.\n\n249\n\n "}, {"Page_number": 259, "text": "250\n\nbibliography\n\nbarash, d. (1999). a genetic search in policy space for solving markov decision pro-\ncesses. in aaai spring symposium on search techniques for problem solving\nunder uncertainty and incomplete information, palo alto, us.\n\nbarto, a. and mahadevan, s. (2003). recent advances in hierarchical reinforcement\nlearning. discrete event dynamic systems: theory and applications, 13(4):341\u2013\n379.\n\nbarto, a. g., sutton, r. s., and anderson, c. w. (1983). neuronlike adaptive el-\nements that can solve difficult learning control problems. ieee transactions on\nsystems,man, and cybernetics, 13(5):833\u2013846.\n\nberenji, h. r. and khedkar, p. (1992). learning and tuning fuzzy logic controllers\n\nthrough reinforcements. ieee transactions on neural networks, 3(5):724\u2013740.\n\nberenji, h. r. and vengerov, d. (2003). a convergent actor-critic-based frl al-\ngorithm with application to power management of wireless transmitters. ieee\ntransactions on fuzzy systems, 11(4):478\u2013485.\n\nbertsekas, d. p. (2005a). dynamic programming and optimal control, volume 1.\n\nathena scientific, 3rd edition.\n\nbertsekas, d. p. (2005b). dynamic programming and suboptimal control: a survey\nfrom adp to mpc. europeanjournalofcontrol, 11(4\u20135):310\u2013334. special issue\nfor the cdc-ecc-05 in seville, spain.\n\nbertsekas, d. p. (2007). dynamic programming and optimal control, volume 2.\n\nathena scientific, 3rd edition.\n\nbertsekas, d. p., borkar, v., and nedi\u00b4c, a. (2004). improved temporal difference\nmethods with linear function approximation. in si, j., barto, a., and powell, w.,\neditors, learning and approximate dynamic programming. ieee press.\n\nbertsekas, d. p. and casta\u02dcnon, d. a. (1989). adaptive aggregation methods for\ninfinite horizon dynamic programming. ieeetransactionsonautomaticcontrol,\n34(6):589\u2013598.\n\nbertsekas, d. p. and ioffe, s. (1996). temporal differences-based policy itera-\ntion and applications in neuro-dynamic programming. technical report lids-\np-2349, massachusetts institute of technology, cambridge, us. available at\nhttp://web.mit.edu/dimitrib/www/tempdif.pdf.\n\nbertsekas, d. p. and shreve, s. e. (1978). stochastic optimal control: the discrete\n\ntime case. academic press.\n\nbertsekas, d. p. and tsitsiklis, j. n. (1996). neuro-dynamic programming. athena\n\nscientific.\n\n "}, {"Page_number": 260, "text": "bibliography\n\n251\n\nbertsekas, d. p. and yu, h. (2009). basis function adaptation methods for cost ap-\nproximation in mdp.\nin proceedings 2009 ieee symposium on approximate\ndynamic programming and reinforcement learning (adprl-09), pages 74\u201381,\nnashville, us.\n\nbethke, b., how, j., and ozdaglar, a. (2008). approximate dynamic programming\nusing support vector regression. in proceedings 47th ieee conference on deci-\nsion and control (cdc-08), pages 3811\u20133816, cancun, mexico.\n\nbhatnagar, s., sutton, r., ghavamzadeh, m., and lee, m. (2009). natural actor-critic\n\nalgorithms. automatica, 45(11):2471\u20132482.\n\nbirge, j. r. and louveaux, f. (1997). introduction to stochastic programming.\n\nspringer.\n\nborkar, v. (2005). an actor-critic algorithm for constrained markov decision pro-\n\ncesses. systems & control letters, 54(3):207\u2013213.\n\nboubezoul, a., paris, s., and ouladsine, m. (2008). application of the cross entropy\n\nmethod to the glvq algorithm. pattern recognition, 41(10):3173\u20133178.\n\nboyan, j. (2002). technical update: least-squares temporal difference learning. ma-\n\nchine learning, 49:233\u2013246.\n\nbradtke, s. j. and barto, a. g. (1996). linear least-squares algorithms for temporal\n\ndifference learning. machine learning, 22(1\u20133):33\u201357.\n\nbreiman, l. (2001). random forests. machine learning, 45(1):5\u201332.\n\nbreiman, l., friedman, j., stone, c. j., and olshen, r. (1984). classification and\n\nregression trees. wadsworth international.\n\nbrown, m. and harris, c. (1994). neurofuzzyadaptivemodelingandcontrol. pren-\n\ntice hall.\n\nbubeck, s., munos, r., stoltz, g., and szepesv\u00b4ari, c. (2009). online optimization\nin x-armed bandits. in koller, d., schuurmans, d., bengio, y., and bottou, l.,\neditors, advances in neural information processing systems 21, pages 201\u2013208.\nmit press.\n\nbus\u00b8oniu, l., babu\u02c7ska, r., and de schutter, b. (2008a). a comprehensive survey\nof multi-agent reinforcement learning. ieee transactions on systems, man, and\ncybernetics. part c: applications and reviews, 38(2):156\u2013172.\n\nbus\u00b8oniu, l., ernst, d., de schutter, b., and babu\u02c7ska, r. (2007). fuzzy approxi-\nmation for convergent model-based reinforcement learning. in proceedings 2007\nieee international conference on fuzzy systems (fuzz-ieee-07), pages 968\u2013\n973, london, uk.\n\n "}, {"Page_number": 261, "text": "252\n\nbibliography\n\nbus\u00b8oniu, l., ernst, d., de schutter, b., and babu\u02c7ska, r. (2008b). consistency\nof fuzzy model-based reinforcement learning. in proceedings 2008 ieee inter-\nnational conference on fuzzy systems (fuzz-ieee-08), pages 518\u2013524, hong\nkong.\n\nbus\u00b8oniu, l., ernst, d., de schutter, b., and babu\u02c7ska, r. (2008c). continuous-state\nreinforcement learning with fuzzy approximation. in tuyls, k., now\u00b4e, a., gues-\nsoum, z., and kudenko, d., editors, adaptive agents and multi-agent systems\niii, volume 4865 of lecture notes in computer science, pages 27\u201343. springer.\n\nbus\u00b8oniu, l., ernst, d., de schutter, b., and babu\u02c7ska, r. (2008d). fuzzy partition\noptimization for approximate fuzzy q-iteration. in proceedings 17th ifacworld\ncongress (ifac-08), pages 5629\u20135634, seoul, korea.\n\nbus\u00b8oniu, l., ernst, d., de schutter, b., and babu\u02c7ska, r. (2009). policy search with\ncross-entropy optimization of basis functions. in proceedings 2009 ieee interna-\ntionalsymposiumonadaptivedynamicprogrammingandreinforcementlearn-\ning (adprl-09), pages 153\u2013160, nashville, us.\n\ncamacho, e. f. and bordons, c. (2004). modelpredictivecontrol. springer-verlag.\n\ncao, x.-r. (2007). stochastic learning and optimization: a sensitivity-based ap-\n\nproach. springer.\n\nchang, h. s., fu, m. c., hu, j., and marcus, s. i. (2007). simulation-based algo-\n\nrithms formarkov decision processes. springer.\n\nchepuri, k. and de mello, t. h. (2005). solving the vehicle routing problem with\nstochastic demands using the cross-entropy method. annals of operations re-\nsearch, 134(1):153\u2013181.\n\nchin, h. h. and jafari, a. a. (1998). genetic algorithm methods for solving the\nbest stationary policy of finite markov decision processes. in proceedings 30th\nsoutheastern symposium on system theory, pages 538\u2013543, morgantown, us.\n\nchow, c.-s. and tsitsiklis, j. n. (1991). an optimal one-way multigrid algorithm\nfor discrete-time stochastic control. ieee transactions on automatic control,\n36(8):898\u2013914.\n\ncosta, a., jones, o. d., and kroese, d. (2007). convergence properties of the\ncross-entropy method for discrete optimization. operations research letters,\n35(5):573\u2013580.\n\ncristianini, n. and shawe-taylor, j. (2000). an introduction to support vector ma-\n\nchines and other kernel-based learningmethods. cambridge university press.\n\ndavies, s. (1997). multidimensional triangulation and interpolation for reinforce-\nment learning. in mozer, m. c., jordan, m. i., and petsche, t., editors, advances\nin neural information processing systems 9, pages 1005\u20131011. mit press.\n\n "}, {"Page_number": 262, "text": "bibliography\n\n253\n\ndefourny, b., ernst, d., and wehenkel, l. (2008). lazy planning under uncertainties\nby optimizing decisions on an ensemble of incomplete disturbance trees. in girgin,\ns., loth, m., munos, r., preux, p., and ryabko, d., editors, recent advances in\nreinforcement learning, volume 5323 of lecture notes in computer science,\npages 1\u201314. springer.\n\ndefourny, b., ernst, d., and wehenkel, l. (2009). planning under uncertainty, en-\nsembles of disturbance trees and kernelized discrete action spaces. in proceedings\n2009ieeeinternationalsymposiumonadaptivedynamicprogrammingandre-\ninforcement learning (adprl-09), pages 145\u2013152, nashville, us.\n\ndeisenroth, m. p., rasmussen, c. e., and peters, j. (2009). gaussian process dynamic\n\nprogramming. neurocomputing, 72(7\u20139):1508\u20131524.\n\ndietterich, t. g. (2000). hierarchical reinforcement learning with the maxq value\nfunction decomposition. journal of artificial intelligence research, 13:227\u2013303.\n\ndimitrakakis, c. and lagoudakis, m. (2008). rollout sampling approximate policy\n\niteration. machine learning, 72(3):157\u2013171.\n\ndorigo, m. and colombetti, m. (1994). robot shaping: developing autonomous\n\nagents through learning. artificial intelligence, 71(2):321\u2013370.\n\ndoya, k. (2000). reinforcement learning in continuous time and space. neural\n\ncomputation, 12(1):219\u2013245.\n\nduda, r. o., hart, p. e., and stork, d. g. (2000). pattern classification. wiley, 2nd\n\nedition.\n\ndupacov\u00b4a, j., consigli, g., and wallace, s. w. (2000). scenarios for multistage\n\nstochastic programs. annals of operations research, 100(1\u20134):25\u201353.\n\nedelman, a. and murakami, h. (1995). polynomial roots from companion matrix\n\neigenvalues. mathematics of computation, 64:763\u2013776.\n\nengel, y., mannor, s., and meir, r. (2003). bayes meets bellman: the gaussian pro-\ncess approach to temporal difference learning. in proceedings 20th international\nconference onmachine learning (icml-03), pages 154\u2013161, washington, us.\n\nengel, y., mannor, s., and meir, r. (2005). reinforcement learning with gaussian\nprocesses. in proceedings 22nd international conference on machine learning\n(icml-05), pages 201\u2013208, bonn, germany.\n\nernst, d. (2005). selecting concise sets of samples for a reinforcement learning\nin proceedings 3rd international conference on computational intelli-\n\nagent.\ngence, robotics and autonomous systems (ciras-05), singapore.\n\nernst, d., geurts, p., and wehenkel, l. (2005). tree-based batch mode reinforcement\n\nlearning. journal ofmachine learning research, 6:503\u2013556.\n\n "}, {"Page_number": 263, "text": "254\n\nbibliography\n\nernst, d., glavic, m., capitanescu, f., and wehenkel, l. (2009). reinforcement\nlearning versus model predictive control: a comparison on a power system prob-\nlem. ieeetransactionsonsystems,man,andcybernetics\u2014partb:cybernetics,\n39(2):517\u2013529.\n\nernst, d., glavic, m., geurts, p., and wehenkel, l. (2006a). approximate value iter-\nation in the reinforcement learning context. application to electrical power system\ncontrol. internationaljournalofemergingelectricpowersystems, 3(1). 37 pages.\n\nernst, d., glavic, m., stan, g.-b., mannor, s., and wehenkel, l. (2007). the cross-\nentropy method for power system combinatorial optimization problems. in pro-\nceedings of power tech 2007, pages 1290\u20131295, lausanne, switzerland.\n\nernst, d., stan, g.-b., gonc\u00b8alves, j., and wehenkel, l. (2006b). clinical data based\noptimal sti strategies for hiv: a reinforcement learning approach. in proceed-\nings 45th ieee conference on decision & control, pages 667\u2013672, san diego,\nus.\n\nfantuzzi, c. and rovatti, r. (1996). on the approximation capabilities of the homo-\ngeneous takagi-sugeno model. in proceedings5thieeeinternationalconference\non fuzzy systems (fuzz-ieee\u201996), pages 1067\u20131072, new orleans, us.\n\nfarahmand, a. m., ghavamzadeh, m., szepesv\u00b4ari, cs., and mannor, s. (2009a). reg-\nularized fitted q-iteration for planning in continuous-space markovian decision\nproblems. in proceedings 2009 american control conference (acc-09), pages\n725\u2013730, st. louis, us.\n\nfarahmand, a. m., ghavamzadeh, m., szepesv\u00b4ari, cs., and mannor, s. (2009b). reg-\nularized policy iteration. in koller, d., schuurmans, d., bengio, y., and bottou, l.,\neditors, advances in neural information processing systems 21, pages 441\u2013448.\nmit press.\n\nfeldbaum, a. (1961). dual control theory, parts i and ii. automation and remote\n\ncontrol, 21(9):874\u2013880.\n\nfranklin, g. f., powell, j. d., and workman, m. l. (1998). digital control of dy-\n\nnamic systems. prentice hall, 3rd edition.\n\ngeramifard, a., bowling, m., zinkevich, m., and sutton, r. s. (2007). ilstd: eligi-\nbility traces & convergence analysis. in sch\u00a8olkopf, b., platt, j., and hofmann, t.,\neditors, advances in neural information processing systems 19, pages 440\u2013448.\nmit press.\n\ngeramifard, a., bowling, m. h., and sutton, r. s. (2006). incremental least-squares\ntemporal difference learning. in proceedings 21st national conference on artifi-\ncial intelligence and 18th innovative applications of artificial intelligence con-\nference (aaai-06), pages 356\u2013361, boston, us.\n\ngeurts, p., ernst, d., and wehenkel, l. (2006). extremely randomized trees. machine\n\nlearning, 36(1):3\u201342.\n\n "}, {"Page_number": 264, "text": "bibliography\n\n255\n\nghavamzadeh, m. and mahadevan, s. (2007). hierarchical average reward reinforce-\n\nment learning. journal ofmachine learning research, 8:2629\u20132669.\n\nglorennec, p. y. (2000). reinforcement learning: an overview. in proceedings eu-\nropean symposium on intelligent techniques (esit-00), pages 17\u201335, aachen,\ngermany.\n\nglover, f. and laguna, m. (1997). tabu search. kluwer.\n\ngoldberg, d. e. (1989). genetic algorithms in search, optimization and machine\n\nlearning. addison-wesley.\n\ngomez, f. j., schmidhuber, j., and miikkulainen, r. (2006). efficient non-linear\ncontrol through neuroevolution.\nin proceedings 17th european conference on\nmachine learning (ecml-06), volume 4212 of lecture notes in computer sci-\nence, pages 654\u2013662, berlin, germany.\n\ngonzalez, r. l. and rofman, e. (1985). on deterministic control problems: an\napproximation procedure for the optimal cost i. the stationary problem. siam\njournal on control and optimization, 23(2):242\u2013266.\n\ngordon, g. (1995). stable function approximation in dynamic programming. in pro-\nceedings 12th international conference on machine learning (icml-95), pages\n261\u2013268, tahoe city, us.\n\ngordon, g. j. (2001). reinforcement learning with function approximation con-\nin leen, t. k., dietterich, t. g., and tresp, v., editors,\nverges to a region.\nadvances in neural information processing systems 13, pages 1040\u20131046. mit\npress.\n\ngr\u00a8une, l. (2004).\n\nerror estimation and adaptive discretization for the dis-\ncrete stochastic hamilton-jacobi-bellman equation. numerische mathematik,\n99(1):85\u2013112.\n\nhassoun, m. (1995). fundamentals of artificial neural networks. mit press.\n\nhengst, b. (2002). discovering hierarchy in reinforcement learning with hexq.\nin proceedings 19th international conference on machine learning (icml-02),\npages 243\u2013250, sydney, australia.\n\nhoriuchi, t., fujino, a., katai, o., and sawaragi, t. (1996). fuzzy interpolation-\nbased q-learning with continuous states and actions. in proceedings 5th ieee in-\nternationalconferenceonfuzzysystems(fuzz-ieee-96), pages 594\u2013600, new\norleans, us.\n\nhren, j.-f. and munos, r. (2008). optimistic planning of deterministic systems.\nin girgin, s., loth, m., munos, r., preux, p., and ryabko, d., editors, recent\nadvancesinreinforcementlearning, volume 5323 of lecturenotesincomputer\nscience, pages 151\u2013164. springer.\n\n "}, {"Page_number": 265, "text": "256\n\nbibliography\n\nistratescu, v. i. (2002). fixed point theory: an introduction. springer.\n\njaakkola, t., jordan, m. i., and singh, s. p. (1994). on the convergence of stochas-\ntic iterative dynamic programming algorithms. neural computation, 6(6):1185\u2013\n1201.\n\njodogne, s., briquet, c., and piater, j. h. (2006). approximate policy iteration for\nclosed-loop learning of visual tasks. in proceedings 17th european conference\non machine learning (ecml-06), volume 4212 of lecture notes in computer\nscience, pages 210\u2013221, berlin, germany.\n\njones, d. r. (2009). direct global optimization algorithm. in floudas, c. a. and\npardalos, p. m., editors, encyclopedia of optimization, pages 725\u2013735. springer.\n\njouffe, l. (1998). fuzzy inference system learning by reinforcement methods. ieee\ntransactions on systems, man, and cybernetics\u2014part c: applications and re-\nviews, 28(3):338\u2013355.\n\njung, t. and polani, d. (2007a). kernelizing lspe(\u03bb). in proceedings 2007 ieee\nsymposiumonapproximatedynamicprogrammingandreinforcementlearning\n(adprl-07), pages 338\u2013345, honolulu, us.\n\njung, t. and polani, d. (2007b). learning robocup-keepaway with kernels. in gaus-\nsianprocessesinpractice, volume 1 of jmlrworkshopandconferenceproceed-\nings, pages 33\u201357.\n\njung, t. and stone, p. (2009). feature selection for value function approximation\nusing bayesian model selection. in machine learning and knowledge discovery\nin databases, european conference (ecml-pkdd-09), volume 5781 of lecture\nnotes in computer science, pages 660\u2013675, bled, slovenia.\n\njung, t. and uthmann, t. (2004). experiments in value function approximation with\nsparse support vector regression. in proceedings 15th european conference on\nmachine learning (ecml-04), volume 3201 of lecture notes in artificial intel-\nligence, pages 180\u2013191, pisa, italy.\n\nkaelbling, l. p. (1993). learning in embedded systems. mit press.\n\nkaelbling, l. p., littman, m. l., and cassandra, a. r. (1998). planning and acting in\npartially observable stochastic domains. artificial intelligence, 101(1\u20132):99\u2013134.\n\nkaelbling, l. p., littman, m. l., and moore, a. w. (1996). reinforcement learning:\n\na survey. journal of artificial intelligence research, 4:237\u2013285.\n\nkakade, s. (2001). a natural policy gradient. in dietterich, t. g., becker, s., and\nghahramani, z., editors, advances in neural information processing systems 14,\npages 1531\u20131538. mit press.\n\nkalyanakrishnan, s. and stone, p. (2007). batch reinforcement learning in a complex\ndomain. in proceedings6thinternationalconferenceonautonomousagentsand\nmulti-agent systems, pages 650\u2013657, honolulu, us.\n\n "}, {"Page_number": 266, "text": "bibliography\n\n257\n\nkeller, p. w., mannor, s., and precup, d. (2006). automatic basis function construc-\ntion for approximate dynamic programming and reinforcement learning. in pro-\nceedings 23rd international conference on machine learning (icml-06), pages\n449\u2013456, pittsburgh, us.\n\nkhalil, h. k. (2002). nonlinear systems. prentice hall, 3rd edition.\n\nkirk, d. e. (2004). optimal control theory: an introduction. dover publications.\n\nklir, g. j. and yuan, b. (1995). fuzzy sets and fuzzy logic: theory and applica-\n\ntions. prentice hall.\n\nknuth, d. e. (1976). big omicron and big omega and big theta. sigact news,\n\n8(2):18\u201324.\n\nkolter, j. z. and ng, a. (2009). regularization and feature selection in least-squares\nin proceedings 26th international conference on\n\ntemporal difference learning.\nmachine learning (icml-09), pages 521\u2013528, montreal, canada.\n\nkonda, v. (2002). actor-critic algorithms. phd thesis, massachusetts institute of\n\ntechnology, cambridge, us.\n\nkonda, v. r. and tsitsiklis, j. n. (2000). actor-critic algorithms. in solla, s. a.,\nleen, t. k., and m\u00a8uller, k.-r., editors, advances in neural information process-\ning systems 12, pages 1008\u20131014. mit press.\n\nkonda, v. r. and tsitsiklis, j. n. (2003). on actor-critic algorithms. siam journal\n\non control and optimization, 42(4):1143\u20131166.\n\nkruse, r., gebhardt, j. e., and klowon, f. (1994). foundations of fuzzy systems.\n\nwiley.\n\nlagoudakis, m., parr, r., and littman, m. (2002). least-squares methods in rein-\nforcement learning for control. in methods and applications of artificial intel-\nligence, volume 2308 of lecture notes in artificial intelligence, pages 249\u2013260.\nspringer.\n\nlagoudakis, m. g. and parr, r. (2003a). least-squares policy iteration. journal of\n\nmachine learning research, 4:1107\u20131149.\n\nlagoudakis, m. g. and parr, r. (2003b). reinforcement learning as classification:\nleveraging modern classifiers. in proceedings 20th international conference on\nmachine learning (icml-03), pages 424\u2013431. washington, us.\n\nlevine, w. s., editor (1996). the control handbook. crc press.\n\nlewis, r. m. and torczon, v. (2000). pattern search algorithms for linearly con-\n\nstrained minimization. siam journal on optimization, 10(3):917\u2013941.\n\n "}, {"Page_number": 267, "text": "258\n\nbibliography\n\nli, l., littman, m. l., and mansley, c. r. (2009). online exploration in least-squares\npolicy iteration. in proceedings8thinternationaljointconferenceonautonomous\nagents and multiagent systems (aamas-09), volume 2, pages 733\u2013739, bu-\ndapest, hungary.\n\nlin, c.-k. (2003). a reinforcement learning adaptive fuzzy controller for robots.\n\nfuzzy sets and systems, 137(3):339\u2013352.\n\nlin, l.-j. (1992). self-improving reactive agents based on reinforcement learning,\nplanning and teaching. machine learning, 8(3\u20134):293\u2013321. special issue on\nreinforcement learning.\n\nliu, d., javaherian, h., kovalenko, o., and huang, t. (2008). adaptive critic learn-\ning techniques for engine torque and air-fuel ratio control. ieee transactions on\nsystems,man, and cybernetics\u2014part b: cybernetics, 38(4):988\u2013993.\n\nlovejoy, w. s. (1991). computationally feasible bounds for partially observed\n\nmarkov decision processes. operations research, 39(1):162\u2013175.\n\nmaciejowski, j. m. (2002). predictive control with constraints. prentice hall.\n\nmadani, o. (2002). on policy iteration as a newton\u2019s method and polynomial policy\niteration algorithms. in proceedings 18th national conference on artificial intel-\nligence and 14th conference on innovative applications of artificial intelligence\naaai/iaai-02, pages 273\u2013278, edmonton, canada.\n\nmahadevan, s. (2005). samuel meets amarel: automating value function approxi-\nmation using global state space analysis. in proceedings20thnationalconference\nonartificialintelligenceandthe17thinnovative applicationsofartificialintelli-\ngence conference (aaai-05), pages 1000\u20131005, pittsburgh, us.\n\nmahadevan, s. and maggioni, m. (2007). proto-value functions: a laplacian frame-\nwork for learning representation and control in markov decision processes. journal\nofmachine learning research, 8:2169\u20132231.\n\nmamdani, e. (1977). application of fuzzy logic to approximate reasoning using\n\nlinguistic systems. ieee transactions on computers, 26:1182\u20131191.\n\nmannor, s., rubinstein, r. y., and gat, y. (2003). the cross-entropy method for fast\npolicy search. in proceedings20thinternationalconferenceonmachinelearning\n(icml-03), pages 512\u2013519, washington, us.\n\nmarbach, p. and tsitsiklis, j. n. (2003). approximate gradient methods in policy-\nspace optimization of markov reward processes. discrete event dynamic sys-\ntems: theory and applications, 13(1\u20132):111\u2013148.\n\nmatari\u00b4c, m. j. (1997). reinforcement learning in the multi-robot domain. au-\n\ntonomous robots, 4(1):73\u201383.\n\n "}, {"Page_number": 268, "text": "bibliography\n\n259\n\nmathenya, m. e., resnic, f. s., arora, n., and ohno-machado, l. (2007). ef-\nfects of svm parameter optimization on discrimination and calibration for post-\nprocedural pci mortality. journal of biomedical informatics, 40(6):688\u2013697.\n\nmelo, f. s., meyn, s. p., and ribeiro, m. i. (2008). an analysis of reinforcement\nlearning with function approximation. in proceedings 25th international confer-\nence onmachine learning (icml-08), pages 664\u2013671, helsinki, finland.\n\nmenache, i., mannor, s., and shimkin, n. (2005). basis function adaptation in\ntemporal difference reinforcement learning. annals of operations research,\n134(1):215\u2013238.\n\nmill\u00b4an, j. d. r., posenato, d., and dedieu, e. (2002). continuous-action q-learning.\n\nmachine learning, 49(2\u20133):247\u2013265.\n\nmoore, a. w. and atkeson, c. r. (1995). the parti-game algorithm for variable res-\nolution reinforcement learning in multidimensional state-spaces. machine learn-\ning, 21(3):199\u2013233.\n\nmorris, c. (1982). natural exponential families with quadratic variance functions.\n\nannals of statistics, 10(1):65\u201380.\n\nmunos, r. (1997). finite-element methods with local triangulation refinement for\ncontinuous reinforcement learning problems. in proceedings 9th european con-\nferenceonmachinelearning (ecml-97), volume 1224 of lecture notesinarti-\nficial intelligence, pages 170\u2013182, prague, czech republic.\n\nmunos, r. (2006). policy gradient in continuous time. journal ofmachine learning\n\nresearch, 7:771\u2013791.\n\nmunos, r. and moore, a. (2002). variable-resolution discretization in optimal con-\n\ntrol. machine learning, 49(2\u20133):291\u2013323.\n\nmunos, r. and szepesv\u00b4ari, cs. (2008). finite time bounds for fitted value iteration.\n\njournal ofmachine learning research, 9:815\u2013857.\n\nmurphy, s. (2005). a generalization error for q-learning. journalofmachinelearn-\n\ning research, 6:1073\u20131097.\n\nnakamura, y., moria, t., satoc, m., and ishiia, s. (2007). reinforcement learning for\na biped robot based on a cpg-actor-critic method. neural networks, 20(6):723\u2013\n735.\n\nnedi\u00b4c, a. and bertsekas, d. p. (2003). least-squares policy evaluation algorithms\nwith linear function approximation. discreteeventdynamicsystems:theoryand\napplications, 13(1\u20132):79\u2013110.\n\nng, a. y., harada, d., and russell, s. (1999). policy invariance under reward trans-\nformations: theory and application to reward shaping. in proceedings 16th in-\nternational conference on machine learning (icml-99), pages 278\u2013287, bled,\nslovenia.\n\n "}, {"Page_number": 269, "text": "260\n\nbibliography\n\nng, a. y. and jordan, m. i. (2000). pegasus: a policy search method for large\nmdps and pomdps. in proceedings 16th conference in uncertainty in artificial\nintelligence (uai-00), pages 406\u2013415, palo alto, us.\n\nnocedal, j. and wright, s. j. (2006). numerical optimization. springer-verlag, 2nd\n\nedition.\n\normoneit, d. and sen, s. (2002). kernel-based reinforcement learning. machine\n\nlearning, 49(2\u20133):161\u2013178.\n\npanait, l. and luke, s. (2005). cooperative multi-agent learning: the state of the\n\nart. autonomous agents andmulti-agent systems, 11(3):387\u2013434.\n\nparr, r., li, l., taylor, g., painter-wakefield, c., and littman, m. (2008). an analy-\nsis of linear models, linear value-function approximation, and feature selection for\nreinforcement learning. in proceedings 25th annual international conference on\nmachine learning (icml-08), pages 752\u2013759, helsinki, finland.\n\npazis, j. and lagoudakis, m. (2009). binary action search for learning continuous-\naction control policies. in proceedings of the 26th annual international confer-\nence onmachine learning (icml-09), pages 793\u2013800, montreal, canada.\n\np\u00b4erez-uribe, a. (2001). using a time-delay actor-critic neural architecture with\ndopamine-like reinforcement signal for learning in autonomous robots.\nin\nwermter, s., austin, j., and willshaw, d. j., editors, emergent neural compu-\ntational architectures based on neuroscience, volume 2036 of lecture notes in\ncomputer science, pages 522\u2013533. springer.\n\nperkins, t. and barto, a. (2002). lyapunov design for safe reinforcement learning.\n\njournal ofmachine learning research, 3:803\u2013832.\n\npeters, j. and schaal, s. (2008). natural actor-critic. neurocomputing, 71(7\u2013\n\n9):1180\u20131190.\n\npineau, j., gordon, g. j., and thrun, s. (2006). anytime point-based approximations\nfor large pomdps. journal of artificial intelligence research (jair), 27:335\u2013\n380.\n\nporta, j. m., vlassis, n., spaan, m. t., and poupart, p. (2006). point-based value it-\neration for continuous pomdps. journalofmachinelearningresearch, 7:2329\u2013\n2367.\n\npowell, w. b. (2007). approximate dynamic programming: solving the curses of\n\ndimensionality. wiley.\n\npress, w. h., flannery, b. p., teukolsky, s. a., and vetterling, w. t. (1986). numer-\n\nical recipes: the art of scientific computing. cambridge university press.\n\nprokhorov, d. and wunsch, d.c., i. (1997). adaptive critic designs. ieee transac-\n\ntions on neural networks, 8(5):997\u20131007.\n\n "}, {"Page_number": 270, "text": "bibliography\n\n261\n\nputerman, m. l. (1994). markovdecisionprocesses\u2014discretestochasticdynamic\n\nprogramming. wiley.\n\nrandl\u00f8v, j. and alstr\u00f8m, p. (1998). learning to drive a bicycle using reinforcement\nlearning and shaping. in proceedings 15th international conference on machine\nlearning (icml-98), pages 463\u2013471, madison, us.\n\nrasmussen, c. e. and kuss, m. (2004). gaussian processes in reinforcement learn-\nin thrun, s., saul, l. k., and sch\u00a8olkopf, b., editors, advances in neural\n\ning.\ninformation processing systems 16. mit press.\n\nrasmussen, c. e. and williams, c. k. i. (2006). gaussian processes for machine\n\nlearning. mit press.\n\nratitch, b. and precup, d. (2004). sparse distributed memories for on-line value-\nbased reinforcement learning. in proceedings 15th european conference on ma-\nchine learning (ecml-04), volume 3201 of lecture notes in computer science,\npages 347\u2013358, pisa, italy.\n\nreynolds, s. i. (2000). adaptive resolution model-free reinforcement learning: deci-\nsion boundary partitioning. in proceedings seventeenth international conference\nonmachine learning (icml-00), pages 783\u2013790, stanford university, us.\n\nriedmiller, m. (2005). neural fitted q-iteration \u2013 first experiences with a data effi-\ncient neural reinforcement learning method. in proceedings 16th european con-\nferenceonmachinelearning(ecml-05), volume 3720 of lecturenotesincom-\nputer science, pages 317\u2013328, porto, portugal.\n\nriedmiller, m., peters, j., and schaal, s. (2007). evaluation of policy gradient\nin proceedings 2007 ieee\nmethods and variants on the cart-pole benchmark.\nsymposium on approximate dynamic programming and reinforcement learn-\ning (adprl-07), pages 254\u2013261, honolulu, us.\n\nrubinstein, r. y. and kroese, d. p. (2004). the cross entropy method: a unified\napproach to combinatorial optimization, monte-carlo simulation, and machine\nlearning. springer.\n\nrummery, g. a. and niranjan, m. (1994). on-line q-learning using connectionist\nsystems. technical report cued/f-infeng/tr166, engineering department,\ncambridge university, uk. available at http://mi.eng.cam.ac.uk/reports/svr-\nftp/rummery tr166.ps.z.\n\nrussell, s. and norvig, p. (2003). artificialintelligence:amodernapproach. pren-\n\ntice hall, 2nd edition.\n\nrussell, s. j. and zimdars, a. (2003). q-decomposition for reinforcement learn-\ning agents. in proceedings 20th international conference of machine learning\n(icml-03), pages 656\u2013663, washington, us.\n\n "}, {"Page_number": 271, "text": "262\n\nbibliography\n\nsantamaria, j. c., sutton, r. s., and ram, a. (1998). experiments with reinforcement\nlearning in problems with continuous state and action spaces. adaptive behavior,\n6(2):163\u2013218.\n\nsantos, m. s. and vigo-aguiar, j. (1998). analysis of a numerical dynamic pro-\ngramming algorithm applied to economic models. econometrica, 66(2):409\u2013426.\n\nschervish, m. j. (1995). theory of statistics. springer.\n\nschmidhuber, j. (2000). sequential decision making based on direct search. in sun,\nr. and giles, c. l., editors, sequence learning, volume 1828 of lecture notes in\ncomputer science, pages 213\u2013240. springer.\n\nsch\u00a8olkopf, b., burges, c., and smola, a. (1999). advances in kernel methods:\n\nsupport vector learning. mit press.\n\nshawe-taylor, j. and cristianini, n. (2004). kernel methods for pattern analysis.\n\ncambridge university press.\n\nsherstov, a. and stone, p. (2005). function approximation via tile coding: au-\ntomating parameter choice. in proceedings 6th international symposium on ab-\nstraction,reformulationandapproximation(sara-05), volume 3607 of lecture\nnotes in computer science, pages 194\u2013205, airth castle, uk.\n\nshoham, y., powers, r., and grenager, t. (2007).\n\nif multi-agent learning is the\n\nanswer, what is the question? artificial intelligence, 171(7):365\u2013377.\n\nsingh, s., jaakkola, t., littman, m. l., and szepesv\u00b4ari, cs. (2000). convergence re-\nsults for single-step on-policy reinforcement-learning algorithms. machinelearn-\ning, 38(3):287\u2013308.\n\nsingh, s. and sutton, r. (1996). reinforcement learning with replacing eligibility\n\ntraces. machine learning, 22(1\u20133):123\u2013158.\n\nsingh, s. p., jaakkola, t., and jordan, m. i. (1995). reinforcement learning with soft\nstate aggregation. in tesauro, g., touretzky, d. s., and leen, t. k., editors, ad-\nvances in neural information processing systems 7, pages 361\u2013368. mit press.\n\nsingh, s. p., james, m. r., and rudary, m. r. (2004). predictive state representations:\na new theory for modeling dynamical systems. in proceedings 20th conference\nin uncertainty in artificial intelligence (uai-04), pages 512\u2013518, banff, canada.\n\nsmola, a. j. and sch\u00a8olkopf, b. (2004). a tutorial on support vector regression.\n\nstatistics and computing, 14(3):199\u2013222.\n\nsutton, r., maei, h., precup, d., bhatnagar, s., silver, d., szepesvari, cs., and\nwiewiora, e. (2009a). fast gradient-descent methods for temporal-difference\nin proceedings 26th interna-\nlearning with linear function approximation.\ntional conference on machine learning (icml-09), pages 993\u20131000, montreal,\ncanada.\n\n "}, {"Page_number": 272, "text": "bibliography\n\n263\n\nsutton, r. s. (1988). learning to predict by the method of temporal differences.\n\nmachine learning, 3:9\u201344.\n\nsutton, r. s. (1990).\n\nintegrated architectures for learning, planning, and reacting\nbased on approximating dynamic programming. in proceedings 7th international\nconference onmachine learning (icml-90), pages 216\u2013224, austin, us.\n\nsutton, r. s. (1996). generalization in reinforcement learning: successful exam-\nin touretzky, d. s., mozer, m. c., and has-\nples using sparse coarse coding.\nselmo, m. e., editors, advances in neural information processing systems 8,\npages 1038\u20131044. mit press.\n\nsutton, r. s. and barto, a. g. (1998). reinforcement learning: an introduction.\n\nmit press.\n\nsutton, r. s., barto, a. g., and williams, r. j. (1992). reinforcement learning is\n\nadaptive optimal control. ieee control systemsmagazine, 12(2):19\u201322.\n\nsutton, r. s., mcallester, d. a., singh, s. p., and mansour, y. (2000). policy gra-\ndient methods for reinforcement learning with function approximation. in solla,\ns. a., leen, t. k., and m\u00a8uller, k.-r., editors, advances in neural information\nprocessing systems 12, pages 1057\u20131063. mit press.\n\nsutton, r. s., szepesv\u00b4ari, cs., and maei, h. r. (2009b). a convergent o(n) temporal-\ndifference algorithm for off-policy learning with linear function approximation.\nin koller, d., schuurmans, d., bengio, y., and bottou, l., editors, advances in\nneural information processing systems 21, pages 1609\u20131616. mit press.\n\nszepesv\u00b4ari, cs. and munos, r. (2005). finite time bounds for sampling based fit-\nin proceedings 22nd international conference on machine\n\nted value iteration.\nlearning (icml-05), pages 880\u2013887, bonn, germany.\n\nszepesv\u00b4ari, cs. and smart, w. d. (2004). interpolation-based q-learning. in pro-\nceedings 21st international conference on machine learning (icml-04), pages\n791\u2013798, bannf, canada.\n\ntakagi, t. and sugeno, m. (1985). fuzzy identification of systems and its applica-\ntions to modeling and control. ieee transactions on systems, man, and cyber-\nnetics, 15(1):116\u2013132.\n\ntaylor, g. and parr, r. (2009). kernelized value function approximation for rein-\nin proceedings 26th international conference on machine\n\nforcement learning.\nlearning (icml-09), pages 1017\u20131024, montreal, canada.\n\nthrun, s. (1992). the role of exploration in learning control.\n\nin white, d. and\nsofge, d., editors, handbook for intelligent control: neural, fuzzy and adaptive\napproaches. van nostrand reinhold.\n\ntorczon, v. (1997). on the convergence of pattern search algorithms. siamjournal\n\non optimization, 7(1):1\u201325.\n\n "}, {"Page_number": 273, "text": "264\n\nbibliography\n\ntouzet, c. f. (1997). neural reinforcement learning for behaviour synthesis.\n\nrobotics and autonomous systems, 22(3\u20134):251\u2013281.\n\ntsitsiklis, j. n. (1994). asynchronous stochastic approximation and q-learning.\n\nmachine learning, 16(1):185\u2013202.\n\ntsitsiklis, j. n. (2002). on the convergence of optimistic policy iteration. journal of\n\nmachine learning research, 3:59\u201372.\n\ntsitsiklis, j. n. and van roy, b. (1996). feature-based methods for large scale\n\ndynamic programming. machine learning, 22(1\u20133):59\u201394.\n\ntsitsiklis, j. n. and van roy, b. (1997). an analysis of temporal difference learn-\nieee transactions on automatic control,\n\ning with function approximation.\n42(5):674\u2013690.\n\ntuyls, k., maes, s., and manderick, b. (2002). q-learning in simulated robotic soc-\ncer \u2013 large state spaces and incomplete information. in proceedings 2002 inter-\nnational conference on machine learning and applications (icmla-02), pages\n226\u2013232, las vegas, us.\n\nuther, w. t. b. and veloso, m. m. (1998). tree based discretization for continuous\nstate space reinforcement learning. in proceedings 15th national conference on\nartificial intelligence and 10th innovative applications of artificial intelligence\nconference (aaai-98/iaai-98), pages 769\u2013774, madison, us.\n\nvrabie, d., pastravanu, o., abu-khalaf, m., and lewis, f. (2009). adaptive optimal\ncontrol for continuous-time linear systems based on policy iteration. automatica,\n45(2):477\u2013484.\n\nwaldock, a. and carse, b. (2008). fuzzy q-learning with an adaptive representa-\ntion. in proceedings 2008 ieee world congress on computational intelligence\n(wcci-08), pages 720\u2013725, hong kong.\n\nwatkins, c. j. c. h. (1989). learning from delayed rewards. phd thesis, king\u2019s\n\ncollege, oxford, uk.\n\nwatkins, c. j. c. h. and dayan, p. (1992). q-learning. machine learning, 8:279\u2013\n\n292.\n\nwhiteson, s. and stone, p. (2006). evolutionary function approximation for rein-\n\nforcement learning. journal ofmachine learning research, 7:877\u2013917.\n\nwiering, m. (2004). convergence and divergence in standard and averaging rein-\nforcement learning. in proceedings15theuropeanconferenceonmachinelearn-\ning (ecml-04), volume 3201 of lecture notes in artificial intelligence, pages\n477\u2013488, pisa, italy.\n\nwilliams, r. j. and baird, l. c. (1994). tight performance bounds on greedy poli-\ncies based on imperfect value functions. in proceedings 8th yale workshop on\nadaptive and learning systems, pages 108\u2013113, new haven, us.\n\n "}, {"Page_number": 274, "text": "bibliography\n\n265\n\nwodarz, d. and nowak, m. a. (1999). specific therapy regimes could lead to long-\nterm immunological control of hiv. proceedings of the national academy of\nsciences of the united states of america, 96(25):14464\u201314469.\n\nxu, x., hu, d., and lu, x. (2007). kernel-based least-squares policy iteration for\nreinforcement learning. ieee transactions on neural networks, 18(4):973\u2013992.\n\nxu, x., xie, t., hu, d., and lu, x. (2005). kernel least-squares temporal difference\n\nlearning. international journal of information technology, 11(9):54\u201363.\n\nyen, j. and langari, r. (1999). fuzzy logic: intelligence, control, and information.\n\nprentice hall.\n\nyu, h. and bertsekas, d. p. (2006).\n\nconvergence results for some tem-\nporal difference methods based on least-squares.\ntechnical report lids\n2697, massachusetts institute of technology, cambridge, us. available at\nhttp://www.mit.edu/people/dimitrib/lspe lids final.pdf.\n\nyu, h. and bertsekas, d. p. (2009). convergence results for some temporal differ-\nence methods based on least squares. ieee transactions on automatic control,\n54(7):1515\u20131531.\n\n "}, {"Page_number": 275, "text": "list of algorithms\n\n.\n\n. . . .\n\n. . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\nmdps. . .\n\nstate spaces.\n\n.\n. . . .\n\n. . . .\n. . . .\n\n. . . .\n. . . .\n\n25\n25\n30\n31\n32\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n. . . . .\n. . . . .\n\n33\n38\n57\n60\n61\n63\n72\n\n. . . . .\n. .\n. . . . .\n. . . . .\n. . . .\n\n2.1 q-iteration for deterministic mdps.\n. .\n2.2 q-iteration for stochastic mdps with countable state spaces.\n. .\n2.3 q-learning with \u03b5-greedy exploration.\n. .\n2.4 policy iteration with q-functions. . . .\n. .\n2.5 policy evaluation for q-functions in deterministic mdps.\n. .\n2.6 policy evaluation for q-functions in stochastic mdps with countable\n. .\n. . . . .\n. . . . .\n2.7 sarsa with \u03b5-greedy exploration. . .\n. .\n. . . . .\n. .\n3.1 least-squares approximate q-iteration for deterministic mdps. .\n. .\n3.2 least-squares fitted q-iteration with parametric approximation. .\n3.3 q-learning with a linear parametrization and \u03b5-greedy exploration.\n.\n. .\n3.4 fitted q-iteration with nonparametric approximation.\n. .\n3.5 approximate policy iteration with q-functions.\n3.6 approximate policy evaluation for q-functions in deterministic\n74\n. .\n75\n. .\n80\n. .\n80\n. .\n83\n. .\n87\n. .\n. .\n88\n. . 106\n. . 124\n. . 125\n. . 146\n. . 169\n. . 172\n. . 178\n. . 189\n. . 211\n. . 237\n. . 238\n. . 243\n\n. . . . .\n. . . .\n. . . . .\n. . . .\n3.7 fitted policy evaluation for q-functions.\n. . . . .\n. .\n3.8 least-squares temporal difference for q-functions.\n3.9 least-squares policy evaluation for q-functions. . . . .\n. . . . .\n3.10 temporal difference for q-functions, with \u03b5-greedy exploration.\n3.11 least-squares policy iteration.\n. . . . .\n3.12 sarsa with a linear parametrization and \u03b5-greedy exploration.\n. . . .\n3.13 actor-critic with gaussian exploration. . . . . .\n. . . . .\n. . . . .\n. . . .\n. . . . .\n4.1 synchronous fuzzy q-iteration. . . . .\n. . . . .\n4.2 asynchronous fuzzy q-iteration.\n. . . .\n. . .\n. . . . .\n4.3 fuzzy q-iteration with cross-entropy mf optimization.\n. . . . .\n5.1 offline least-squares policy iteration. .\n. . . .\n. . . . .\n5.2 online least-squares policy iteration with \u03b5-greedy exploration. .\n5.3 online least-squares policy iteration with monotonic policies. . .\n5.4 online policy iteration with lspe-q and \u03b5-greedy exploration. .\n6.1 cross-entropy policy search.\n. . . . .\n. . . . .\n. . . . .\na.1 construction of an extremely randomized tree. .\n. . . . .\na.2 prediction using an extremely randomized tree.\nb.1 cross-entropy optimization.\n. . . . .\n. . . . .\n\n. . . .\n. . . .\n. . . .\n. . . .\n\n. . . . .\n. . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . . .\n\n. . . .\n\n. . . .\n\n267\n\n "}]}